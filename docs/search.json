[
  {
    "objectID": "homework_solutions.html",
    "href": "homework_solutions.html",
    "title": "Homework Solutions",
    "section": "",
    "text": "Note\n\n\n\n\nAccessing solutions will require signing into Sakai.\nSolutions will not be accessible until after their due date and after you turn in the assignment.\n\n\n\nLINK to homework solutions page on Sakai."
  },
  {
    "objectID": "weeks/week_01.html",
    "href": "weeks/week_01.html",
    "title": "Week 1",
    "section": "",
    "text": "Syllabus on Sakai\nWeek 1 info on Sakai\n\nIncludes links to background survey and BSAT 511/611 Slack group"
  },
  {
    "objectID": "weeks/week_01.html#general-info",
    "href": "weeks/week_01.html#general-info",
    "title": "Week 1",
    "section": "",
    "text": "Syllabus on Sakai\nWeek 1 info on Sakai\n\nIncludes links to background survey and BSAT 511/611 Slack group"
  },
  {
    "objectID": "weeks/week_01.html#day01",
    "href": "weeks/week_01.html#day01",
    "title": "Week 1",
    "section": "Day01",
    "text": "Day01\nIntro to R & RStudio\n\nHandout with directions on installing R & RStudio\n\n\nSlides\n\nhtml\npdf\nwebpage\nqmd"
  },
  {
    "objectID": "weeks/week_01.html#homework---coming-soon",
    "href": "weeks/week_01.html#homework---coming-soon",
    "title": "Week 1",
    "section": "Homework - coming soon!",
    "text": "Homework - coming soon!\n\nHW 1 due on Sat, 10/12\n\nCovers Days 1-3"
  },
  {
    "objectID": "weeks/week_01.html#recordings",
    "href": "weeks/week_01.html#recordings",
    "title": "Week 1",
    "section": "Recordings",
    "text": "Recordings"
  },
  {
    "objectID": "weeks/week_02.html",
    "href": "weeks/week_02.html",
    "title": "Week 2",
    "section": "",
    "text": "Data Collection Principles & Intro to Data (Fall 2023: Day 2)\n\nBook sections 1.1-1.3\n\n\n\n\n\n\nSummarizing Numerical Data (Fall 2023: Day 2)\n\nBook section 1.4\n\nProbability (Fall 2023: Day 4)\n\nSections 2.1 - 2.2.4"
  },
  {
    "objectID": "weeks/week_02.html#overview-of-week-2",
    "href": "weeks/week_02.html#overview-of-week-2",
    "title": "Week 2",
    "section": "",
    "text": "Data Collection Principles & Intro to Data (Fall 2023: Day 2)\n\nBook sections 1.1-1.3\n\n\n\n\n\n\nSummarizing Numerical Data (Fall 2023: Day 2)\n\nBook section 1.4\n\nProbability (Fall 2023: Day 4)\n\nSections 2.1 - 2.2.4"
  },
  {
    "objectID": "weeks/week_02.html#slides-recordings",
    "href": "weeks/week_02.html#slides-recordings",
    "title": "Week 2",
    "section": "Slides & Recordings",
    "text": "Slides & Recordings\n\nPre-recorded lessons are on Echo Cloud (aka echo 360 or echo video).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nTopic\nSlides (F23): html\nSlides: pdf\nSlides: web-page\nSlides with notes\nRecording Link (F23)\nDuration\nCode: qmd\nCode: html\n\n\n\n\n2\nData Collection Principles (Book section 1.3)\nDay 2: slides 1-17\n\n\n\nDay 2 Part 1\n33 min\n\n\n\n\n\nIntro to Data (Book section 1.2)\nDay 2: slides 18-32\n\n\n\nDay 2 Part 2\n42 min\n\n\n\n\n3\nSummarizing Numerical Data (Book section 1.4)\nDay 2: slides 33-46\n\n\n\nDay 2 Part 3\n37 min\n\n\n\n\n\nProbability (Book sections 2.1.1-2.1.6)\nDay 4: slides 1-4\n\n\n\nDay 4 Part 1\n23 min\n\n\n\n\n\nProbability (Book sections 2.1.7-2.2.4)\nDay 4: slides 5-9\n\n\n\nDay 4 Part 2\n24 min"
  },
  {
    "objectID": "weeks/week_02.html#class-discussion",
    "href": "weeks/week_02.html#class-discussion",
    "title": "Week 2",
    "section": "Class discussion",
    "text": "Class discussion\n\nDay02\nDuring class you will be working in groups discussing the following:\n\nFrom HW 1\n\nBook problems: 1.8, 1.12, 1.20\nR2: BRFSS - parts a)-b)\n\n\n\n\nDay03\n\nSummarizing Numerical Data (Fall 2023: Day 2)\n\nSlide 46: Robust estimates\nFrom HW 1\n\nBook problem: 1.31\nR2: BRFSS - parts c)-e)\n\n\nProbability (Fall 2023: Day 4)\n\nExample 2.3 on pg 4 of slides\nExample 2.6 on pgs 8-9 of slides\nFrom HW 1\n\nBook problems: 2.6,2.14"
  },
  {
    "objectID": "weeks/week_02.html#homework",
    "href": "weeks/week_02.html#homework",
    "title": "Week 2",
    "section": "Homework",
    "text": "Homework\n\nHW 1 due on Sat, 10/12\n\nCovers Days 1-3"
  },
  {
    "objectID": "weeks/week_03.html",
    "href": "weeks/week_03.html",
    "title": "Week 3",
    "section": "",
    "text": "Bayes’ Rule (Fall 2023: Day 5)\n\nSection 2.2.5\n\nR Packages (Fall 2023: Day 2)\n\n\n\n\nFall 2023: Day 3\n\nData visualization, exploratory data analysis (EDA), and summarizing categorical data\n\nSections 1.5-1.7 & supplementary material\nBelow I posted additional “Day 3 part 3” slides with extra data wrangling code. I refer to some of the slides in the homework assignment. You are welcome to go through all of the slides to learn some data wrangling techniques. We will be covering topics from these slides as needed throughout the quarter.\nBonus:\n\nExamples on dealing with overlapping labels in figures (Slides 65, 66, & 68 from BERD R intro workshop)\nCheck out Ted Laderas’s Better Plots presentation\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs I mentioned in the recording, the various options in more complex figures I usually look up how to implement when I need them. I show them so that you are aware of some of the functionality and options one has in creating figures using the ggplot2 package. At the end of the slides, we cover how to create tables summarizing data, especially categorical data. This part has some very useful code and data wrangling in it."
  },
  {
    "objectID": "weeks/week_03.html#overview-of-week-3",
    "href": "weeks/week_03.html#overview-of-week-3",
    "title": "Week 3",
    "section": "",
    "text": "Bayes’ Rule (Fall 2023: Day 5)\n\nSection 2.2.5\n\nR Packages (Fall 2023: Day 2)\n\n\n\n\nFall 2023: Day 3\n\nData visualization, exploratory data analysis (EDA), and summarizing categorical data\n\nSections 1.5-1.7 & supplementary material\nBelow I posted additional “Day 3 part 3” slides with extra data wrangling code. I refer to some of the slides in the homework assignment. You are welcome to go through all of the slides to learn some data wrangling techniques. We will be covering topics from these slides as needed throughout the quarter.\nBonus:\n\nExamples on dealing with overlapping labels in figures (Slides 65, 66, & 68 from BERD R intro workshop)\nCheck out Ted Laderas’s Better Plots presentation\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs I mentioned in the recording, the various options in more complex figures I usually look up how to implement when I need them. I show them so that you are aware of some of the functionality and options one has in creating figures using the ggplot2 package. At the end of the slides, we cover how to create tables summarizing data, especially categorical data. This part has some very useful code and data wrangling in it."
  },
  {
    "objectID": "weeks/week_03.html#slides-recordings",
    "href": "weeks/week_03.html#slides-recordings",
    "title": "Week 3",
    "section": "Slides & Recordings",
    "text": "Slides & Recordings\n\nPre-recorded lessons are on Echo Cloud (aka echo 360 or echo video).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nTopic\nSlides (F23): html\nSlides: pdf\nSlides: web-page\nSlides with notes\nRecording Link (F23)\nDuration\nCode: qmd\nCode: html\n\n\n\n\n4\nBayes’ Rule (Book section 2.2.5)\nDay 5: slides 1-2\n\n\n\nDay 5 Part 1\n22 min\n\n\n\n\n\nBayes’ Rule (Book section 2.2.5)\nDay 5: slides 3-6\n\n\n\nDay 5 Part 2\n22 min\n\n\n\n\n\nR packages\nDay 2: slides 47-54\n\n\n\nDay 2 Part 4\n30 min\n\n\n\n\n5\nExploratory Data Analysis\nDay 3: slides 1-59\n\n\n\nDay 3\n1 hr 18 min\n\n\n\n\n\nExtra EDA slides\nDay 3 Part 2 slides\n\n\n\n\n\n\n\n\n\n\nDay 3 Part 3: Extra data wrangling code"
  },
  {
    "objectID": "weeks/week_03.html#class-discussion",
    "href": "weeks/week_03.html#class-discussion",
    "title": "Week 3",
    "section": "Class discussion",
    "text": "Class discussion\nDuring class you will be working in groups discussing the following:\n\n\nDay04\n\nR Packages (Fall 2023: Day 2)\n\nR packages: any difficulty installing packages?\n\nBayes’ Rule (Fall 2023: Day 5)\n\nExample 2.10 on pg 6 of slides\n\nFrom HW 2\n\nBook exercises\n\n2.24\n\nNon-book exercise\n\n\n\n\nDay05\n\nCan distribution shape and sample size be determined from a boxplot? (see slide 30)\nIs there evidence of ethnicity (Hispanic vs. White non-Hispanic) discrimination in DDS expenditures? Why or why not?\n\nSee Slides table above for Day 3 Part 2 slide deck (and code) that I presented in class\n\nFrom HW 2\n\nR1 & R2: NHANES - parts 1 & 2"
  },
  {
    "objectID": "weeks/week_03.html#homework",
    "href": "weeks/week_03.html#homework",
    "title": "Week 3",
    "section": "Homework",
    "text": "Homework\n\nHW 2 due on Sat, 10/19\n\nSee extra Day 3 Part 3 slides (both pdf and code) in table above with slides & recording links."
  },
  {
    "objectID": "weeks/week_04.html",
    "href": "weeks/week_04.html",
    "title": "Week 4",
    "section": "",
    "text": "Random variables: Section 3.1\nBinomial distribution: Section 3.2\n\n\n\n\n\nNormal distribution: Section 3.3\nPoisson distribution: Section 3.4"
  },
  {
    "objectID": "weeks/week_04.html#overview-of-week-4",
    "href": "weeks/week_04.html#overview-of-week-4",
    "title": "Week 4",
    "section": "",
    "text": "Random variables: Section 3.1\nBinomial distribution: Section 3.2\n\n\n\n\n\nNormal distribution: Section 3.3\nPoisson distribution: Section 3.4"
  },
  {
    "objectID": "weeks/week_04.html#slides-recordings",
    "href": "weeks/week_04.html#slides-recordings",
    "title": "Week 4",
    "section": "Slides & Recordings",
    "text": "Slides & Recordings\n\nPre-recorded lessons are on Echo Cloud (aka echo 360 or echo video).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nTopic\nSlides: html\nSlides: pdf\nSlides: web-page\nSlides with notes\nRecording Link\nDuration\nCode: qmd\nCode: html\n\n\n\n\n\nCalculating probabilities in R\n\n\n\n\n\n\n\n\n\n\n6\nRandom Variables (3.1.1-3.1.3)\nDay 6: slides 1-6\n\n\n\nDay 6 Part 1\n24 min\n\n\n\n\n\nLinear combinations of RV’s (3.1.4)\nDay 6: slides 7-10\nsame\n\nsame\nDay 6 Part 2\n14 min\n\n\n\n\n\nBinomial dist (3.2)\nDay 6: slides 11-15\nsame\n\nsame\nDay 6 Part 3\n27 min\n\n\n\n\n7\nNormal dist and prob’s for standard normal (3.3.1, 3.3.3)\nDay 7: slides 1-5\n\n\n\nDay 7 Part 1\n27 min\n\n\n\n\n\nNormal prob’s using z-scores (3.3.2, 3.3.4)\nDay 7: slides 6-7\nsame\n\nsame\nDay 7 Part 2\n15 min\n\n\n\n\n\nNormal approx of binomial dist (3.3.6)\nDay 7: slides 8-9\nsame\n\nsame\nDay 7 Part 3\n14 min\n\n\n\n\n\nPoisson dist (3.4)\nDay 7: slides 10-12\nsame\n\nsame\nDay 7 Part 4\n15 min"
  },
  {
    "objectID": "weeks/week_04.html#class-discussion",
    "href": "weeks/week_04.html#class-discussion",
    "title": "Week 4",
    "section": "Class discussion",
    "text": "Class discussion\nDuring class you will be working in groups discussing the following:\n\nDay06\n\nExample 3.11 on pg 6 of slides\nExample 3.17 on pg 10 of slides\n\nHint: If you are unsure how to define the random variable T, take a look at how we defined the random variable M in example 3.16 for the amount of money one gets from 3 rolls of a die.\n\nExample 3.22 parts (1) and (2) on pg 14 of slides\nFrom HW 3\n\nBook exercises\n\n3.4, 3.8\n\nNon-book exercise\n\nNB 1\n\n\nIf you want some R practice, try the following:\n\nExample 3.3 on pg 2 of slides\n\nWrite R code to calculate the expected value by defining vectors in R with the relevant values and then doing the appropriate calculations with the vectors.\nHint: start with x &lt;- 1:6.\n\nExample 3.10 on pg 5 of slides\n\nWrite R code to calculate the variance and standard deviation by defining vectors in R with the relevant values and then doing the appropriate calculations with the vectors.\n\n\n\n\n\nDay07\n\nExample 3.3 parts (1) and (3) on pg 7 of slides\nExample 3.8 parts (2) and (3) on pg 11 of slides\nFrom HW 3\n\nBook exercises\n\n3.22, 3.32, 3.40"
  },
  {
    "objectID": "weeks/week_04.html#homework---coming-soon",
    "href": "weeks/week_04.html#homework---coming-soon",
    "title": "Week 4",
    "section": "Homework - coming soon!",
    "text": "Homework - coming soon!\n\nHW 3 due on Sat, 10/26\nMake sure to check out the calculating probabilities in R code file: qmd, html"
  },
  {
    "objectID": "instructors.html",
    "href": "instructors.html",
    "title": "Instructors",
    "section": "",
    "text": "Email: niederha@ohsu.edu\nOffice: VPT 660E\n\nPronouns: she/her/hers\nBest method to contact: Email or Slack.\n\n\n\nWednesdays 2:50-3:30 pm\nVPT 660E\nFridays 3:30-4:30 pm\nVirtual (link on Sakai)"
  },
  {
    "objectID": "instructors.html#instructor-meike-niederhausen-phd",
    "href": "instructors.html#instructor-meike-niederhausen-phd",
    "title": "Instructors",
    "section": "",
    "text": "Email: niederha@ohsu.edu\nOffice: VPT 660E\n\nPronouns: she/her/hers\nBest method to contact: Email or Slack.\n\n\n\nWednesdays 2:50-3:30 pm\nVPT 660E\nFridays 3:30-4:30 pm\nVirtual (link on Sakai)"
  },
  {
    "objectID": "instructors.html#ta-katie-hand",
    "href": "instructors.html#ta-katie-hand",
    "title": "Instructors",
    "section": "TA: Katie Hand",
    "text": "TA: Katie Hand\n\nEmail: handka@ohsu.edu\n\n\nOffice Hours: TBD"
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "Assignments",
    "section": "",
    "text": "Note\n\n\n\n\nBelow are links to homework assignments.\nThe dates in the Date column are due dates.\nSee the Grading rubric tab to the left for a description of how assignments will be graded.\n\n\n\n\nCheck out this spreadsheet for textbook typos.\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n10/12/24\n\n\nHW 1: BSTA 511/611 F24\n\n\n\n\n10/19/24\n\n\nHW 2: BSTA 511/611 F24\n\n\n\n\n10/26/24\n\n\nHW 3: BSTA 511/611 F24\n\n\n\n\n11/9/24\n\n\nHW 4: BSTA 511/611 F24\n\n\n\n\n11/16/24\n\n\nHW 5: BSTA 511/611 F24\n\n\n\n\n11/23/24\n\n\nHW 6: BSTA 511/611 F24\n\n\n\n\n12/7/24\n\n\nHW 7: BSTA 511/611 F24\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resources/Projects_in_R.html",
    "href": "resources/Projects_in_R.html",
    "title": "Projects in RStudio",
    "section": "",
    "text": "I highly, highly, HIGHLY recommend using R Projects to organize your analyses and make it easier to load data files and also save output.\nWhen you create an R Project on your computer, the Project is associated with the folder (directory) you created it in.\n\nThis folder becomes the “root” of your working directory, and RStudio’s point of reference from where to load files from and to.\n\nI create separate Projects for every analysis project and every class I teach.\nYou can run multiple sessions of RStudio by opening different Projects, and the environments (or working directory) of each are working independently of each other.\n\n\n\n\n\n\n\nNote\n\n\n\n\nAlthough we are using Quarto files,\n\nI will show how to set up and use a “regular” R Project\ninstead of “Quarto Project”\n\nQuarto Projects include extra features and thus complexity. Once you are used to how regular R Projects work, you can try out a Quarto Project.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nMany (if not most) of the resources below refer to RMarkdown instead of Quarto files to create reproducible reports.\nThis is not a problem. Projects work the same way with Quarto files.\n\n\n\n\n1 Learn more about Projects and why you should always be using them\nBelow are various resources to learn more about RStudio Projects.\n\nRStudio Projects and Working Directories: A Beginner’s Guide\n\nFrom R-bloggers website\nTOC\n\nWhat is a RStudio Project, and why?\nEasy file path referencing with RStudio Projects\nStructuring your working directory\nFurther reading\n\n\nPosit’s (RStudio’s) directions on creating Projects\n\nhttps://support.rstudio.com/hc/en-us/articles/200526207-Using-RStudio-Projects\nThis page provides concise directions, but not much motivation on why you should be using Projects.\n\nR for Data Science\n\nExcellent book R for Data Science (2nd edition) by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund\nSee Chapter 6 Section 2: Projects (https://r4ds.hadley.nz/workflow-scripts#projects) for\n\nWhat is the source of truth?\nWhere does your analysis live?\nRStudio Projects\nRelative and absolute paths\nSummary\n\n\nJenny Bryant’s STAT 545 class notes\n\nCheck out Chapter 2 R basics and workflows https://stat545.com/r-basics.html\nSection 2.2: Workspace and working directory\nSection 2.3: RStudio Projects\n\n\n\n\n2 YouTube videos on how to create an R Project\nThis video demonstrates how to create an R Project, and how to open RStudio using the Project file.\n\nThe Basics of Projects in RStudio by Sam Burer\nhttps://www.youtube.com/watch?v=hKoSJGWnFFA\nIntroduction to RStudio Projects by Michael Seaman https://www.youtube.com/watch?v=wqOme7xsZvs"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BSTA 511/611 Fall 2024",
    "section": "",
    "text": "Welcome to BSTA 511/611!\n\n\nEstimation and Hypothesis Testing\nApplied Biostatistics I\n\nFall 2024\n\n\n\n\n\n\n\nInstructor\n Meike Niederhausen, PhD\n VPT 660E\n niederha@ohsu.edu\n\n\nTA\n Katie Hand\n handka@ohsu.edu\n\n\n\nCourse details\n Mon, Wed\n 1:00 - 2:50 PM\n VPT 620M\n 9/30 - 12/11\n\n\nOffice Hours\n Mondays 3-4 pm (Katie)\n VPT Student Success Cntr\n Wednesdays 2:50-3:30 pm (Meike)\n VPT 660E\n Thursdays 3-4 pm (Katie)\n Webex (link on Sakai)\n Fridays 3:30-4:30 pm (Meike)\n  Webex (link on Sakai)\n\n\n\n\n\n\n\nContacting me\nE-mail or Slack is the best way to get in contact with me. I will try to respond to all course-related e-mails within 24 hours Monday-Friday.\n\n\n View the source on GitHub"
  },
  {
    "objectID": "slides/Day01_bsta511.html#what-is-r",
    "href": "slides/Day01_bsta511.html#what-is-r",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "What is R?",
    "text": "What is R?\n\n\n\nA programming language\nFocus on statistical modeling and data analysis\n\nimport data, manipulate data, run statistics, make plots\n\nUseful for data science\nGreat visualizations\nAlso useful for most anything else you’d want to tell a computer to do\nInterfaces with other languages i.e. python, C++, bash\n\n\n\n\n\nFor the history and details: Wikipedia\n\nan interpreted language (run it through a command line)\nprocedural programming with functions\nWhy “R”?? Scheme inspired S (invented at Bell Labs in 1976) which inspired R since 1st letters of original authors (free and open source! in 2000)"
  },
  {
    "objectID": "slides/Day01_bsta511.html#what-is-rstudio",
    "href": "slides/Day01_bsta511.html#what-is-rstudio",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "What is RStudio?",
    "text": "What is RStudio?\n\n\nR is a programming language\n\nRStudio is an integrated development environment (IDE)\n= an interface to use R (with perks!)\n\n\n\nModern Dive"
  },
  {
    "objectID": "slides/Day01_bsta511.html#open-rstudio-on-your-computer-not-r",
    "href": "slides/Day01_bsta511.html#open-rstudio-on-your-computer-not-r",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Open RStudio on your computer (not R!)",
    "text": "Open RStudio on your computer (not R!)\n\nModern Dive"
  },
  {
    "objectID": "slides/Day01_bsta511.html#rstudio-anatomy",
    "href": "slides/Day01_bsta511.html#rstudio-anatomy",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "RStudio anatomy",
    "text": "RStudio anatomy\n\nEmma RandRead more about RStudio’s layout in Section 3.4 of “Getting Used to R, RStudio, and R Markdown” (Ismay and Kennedy 2016)"
  },
  {
    "objectID": "slides/Day01_bsta511.html#coding-in-the-console",
    "href": "slides/Day01_bsta511.html#coding-in-the-console",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Coding in the console",
    "text": "Coding in the console\n\n\nWhen you first open R, the console should be empty.\n\n\n\n\n\n\nTyping and executing code in the console \n\nType code in the console (blue text)\nPress return to execute the code\nOutput shown below in black"
  },
  {
    "objectID": "slides/Day01_bsta511.html#math-calculations-using-r",
    "href": "slides/Day01_bsta511.html#math-calculations-using-r",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Math calculations using R",
    "text": "Math calculations using R\n\nRules for order of operations are followed\nSpaces between numbers and characters are ignored\n\n\n\n\n10^2\n\n[1] 100\n\n3 ^ 7\n\n[1] 2187\n\n6/9\n\n[1] 0.6666667\n\n9-43\n\n[1] -34\n\n\n\n\n4^3-2* 7+9 /2\n\n[1] 54.5\n\n\nThe equation above is computed as \\[4^3 − (2 \\cdot 7) + \\frac{9}{2}\\]"
  },
  {
    "objectID": "slides/Day01_bsta511.html#variables-saved-r-objects",
    "href": "slides/Day01_bsta511.html#variables-saved-r-objects",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Variables (saved R objects)",
    "text": "Variables (saved R objects)\nVariables are used to store data, figures, model output, etc.\n\n\n\nCan assign a variable using either = or &lt;-\n\nUsing &lt;- is preferable\n\n\nAssign just one value:\n\nx = 5\nx\n\n[1] 5\n\nx &lt;- 5\nx\n\n[1] 5\n\n\n\n\n\nAssign a vector of values\n\nConsecutive integers using :\n\n\na &lt;- 3:10\na\n\n[1]  3  4  5  6  7  8  9 10\n\n\n\nConcatenate a string of numbers\n\n\nb &lt;- c(5, 12, 2, 100, 8)\nb\n\n[1]   5  12   2 100   8"
  },
  {
    "objectID": "slides/Day01_bsta511.html#doing-math-with-variables",
    "href": "slides/Day01_bsta511.html#doing-math-with-variables",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Doing math with variables",
    "text": "Doing math with variables\n\n\nMath using variables with just one value\n\nx &lt;- 5\nx\n\n[1] 5\n\nx + 3\n\n[1] 8\n\ny &lt;- x^2\ny\n\n[1] 25\n\n\n\nMath on vectors of values:\nelement-wise computation\n\na &lt;- 3:6\na\n\n[1] 3 4 5 6\n\na+2; a*3\n\n[1] 5 6 7 8\n\n\n[1]  9 12 15 18\n\na*a\n\n[1]  9 16 25 36"
  },
  {
    "objectID": "slides/Day01_bsta511.html#variables-can-include-text-characters",
    "href": "slides/Day01_bsta511.html#variables-can-include-text-characters",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Variables can include text (characters)",
    "text": "Variables can include text (characters)\n\nhi &lt;- \"hello\"\nhi\n\n[1] \"hello\"\n\ngreetings &lt;- c(\"Guten Tag\", \"Hola\", hi)\ngreetings\n\n[1] \"Guten Tag\" \"Hola\"      \"hello\""
  },
  {
    "objectID": "slides/Day01_bsta511.html#using-functions",
    "href": "slides/Day01_bsta511.html#using-functions",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Using functions",
    "text": "Using functions\n\nmean() is an example of a function\nfunctions have “arguments” that can be specified within the ()\n?mean in console will show help file for mean()\n\n\n\nFunction arguments specified by name:\n\nmean(x = 1:4)\n\n[1] 2.5\n\n\n\nseq(from = 1, to = 12, by = 3)\n\n[1]  1  4  7 10\n\nseq(by = 3, to = 12, from = 1)\n\n[1]  1  4  7 10\n\n\n\nFunction arguments not specified, but listed in order:\n\nmean(1:4)\n\n[1] 2.5\n\n\n\nseq(1, 12, 3)\n\n[1]  1  4  7 10"
  },
  {
    "objectID": "slides/Day01_bsta511.html#common-console-errors-12",
    "href": "slides/Day01_bsta511.html#common-console-errors-12",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Common console errors (1/2)",
    "text": "Common console errors (1/2)\nIncomplete commands\n\n\n\nWhen the console is waiting for a new command, the prompt line begins with &gt;\n\nIf the console prompt is +, then a previous command is incomplete\nYou can finish typing the command in the console window\n\n\n\nExample:\n\n&gt; 3 + (2*6\n+ )\n\n[1] 15"
  },
  {
    "objectID": "slides/Day01_bsta511.html#common-console-errors-22",
    "href": "slides/Day01_bsta511.html#common-console-errors-22",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Common console errors (2/2)",
    "text": "Common console errors (2/2)\nObject is not found\n\nThis happens when text is entered for a non-existent variable (object)\n\nExample:\n\nhello\n\nError in eval(expr, envir, enclos): object 'hello' not found\n\n\n\nCan be due to missing quotes\n\n\ninstall.packages(dplyr) \n\nError in install.packages(dplyr): object 'dplyr' not found\n\n# correct code is: install.packages(\"dplyr\")"
  },
  {
    "objectID": "slides/Day01_bsta511.html#example-creating-an-html-file",
    "href": "slides/Day01_bsta511.html#example-creating-an-html-file",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Example: creating an html file",
    "text": "Example: creating an html file\n\n\n.qmd file \n\nhtml output"
  },
  {
    "objectID": "slides/Day01_bsta511.html#quarto-.qmd-file-code-text",
    "href": "slides/Day01_bsta511.html#quarto-.qmd-file-code-text",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Quarto = .qmd file = Code + text",
    "text": "Quarto = .qmd file = Code + text\nknitr is a package that converts .qmd files containing code + markdown syntax to a plain text .md markdown file, and then to other formats (html, pdf, Word, etc)\n\nArtwork from “Hello, Quarto” keynote by Julia Lowndes and Mine Çetinkaya-Rundel, presented at RStudio Conference 2022. Illustrated by Allison Horst."
  },
  {
    "objectID": "slides/Day01_bsta511.html#create-a-quarto-file-.qmd",
    "href": "slides/Day01_bsta511.html#create-a-quarto-file-.qmd",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "1. Create a Quarto file (.qmd)",
    "text": "1. Create a Quarto file (.qmd)\nTwo options:\n\nclick on File \\(\\rightarrow\\) New File \\(\\rightarrow\\) Quarto Document…\\(\\rightarrow\\) OK,\nor in upper left corner of RStudio click on  \\(\\rightarrow\\) \n\n\n\nPop-up window selections:\n\nEnter a title and your name\nSelect HTML output format (default)\nEngine: select Knitr\nEditor: Select Use visual markdown editor\nClick Create"
  },
  {
    "objectID": "slides/Day01_bsta511.html#create-a-quarto-file-.qmd-1",
    "href": "slides/Day01_bsta511.html#create-a-quarto-file-.qmd-1",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "2. Create a Quarto file (.qmd)",
    "text": "2. Create a Quarto file (.qmd)\n\nAfter clicking on Create, you should then see the following in your editor window:"
  },
  {
    "objectID": "slides/Day01_bsta511.html#save-the-quarto-file-.qmd",
    "href": "slides/Day01_bsta511.html#save-the-quarto-file-.qmd",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "3. Save the Quarto file (.qmd)",
    "text": "3. Save the Quarto file (.qmd)\n\nSave the file by\n\nselecting File -&gt; Save,\nor clicking on  (towards the left above the scripting window),\nor keyboard shortcut\n\nPC: Ctrl + s\nMac: Command + s\n\n\nYou will need to specify\n\na filename to save the file as\n\nALWAYS use .qmd as the filename extension for Quarto files\n\nthe folder to save the file in"
  },
  {
    "objectID": "slides/Day01_bsta511.html#create-html-file",
    "href": "slides/Day01_bsta511.html#create-html-file",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "4. Create html file",
    "text": "4. Create html file\nWe create the html file by rendering the .qmd file.\nTwo options:\n\nclick on the Render icon  at the top of the editor window,\nor use keyboard shortcuts\n\nMac: Command+Shift+K\nPC: Ctrl+Shift+K\n\n\n\nA new window will open with the html output.\nYou will now see both .qmd and .html files in the folder where you saved the .qmd file.\n\n\n\n\n\n\n\nNote\n\n\n\nThe template .qmd file that RStudio creates will render to an html file by default.\nThe output format can be changed to create a Word doc, pdf, slides, etc."
  },
  {
    "objectID": "slides/Day01_bsta511.html#qmd-file-vs.-its-html-output",
    "href": "slides/Day01_bsta511.html#qmd-file-vs.-its-html-output",
    "title": "Day 1: Intro to R & Rstudio",
    "section": ".qmd file vs. its html output",
    "text": ".qmd file vs. its html output\n\n\n.qmd file \n\nhtml output"
  },
  {
    "objectID": "slides/Day01_bsta511.html#formatting-text",
    "href": "slides/Day01_bsta511.html#formatting-text",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Formatting text",
    "text": "Formatting text\n\nbold, italics, superscripts & subscripts, strikethrough, verbatim, etc.\n\n\nText is formatted through a markup language called Markdown (Wikipedia)\n\nOther markup languages include html (webapges) and LaTeX (math)\nAll text formatting is specified via code\n“Markdown is a plain text format that is designed to be easy to write, and, even more importantly, easy to read” 1\n\nNewer versions of RStudio include a Visual editor as well that makes formatting text similar to using a word processor.\n\n\n\n\n\n\nFrom Quarto’s Markdown Basics webpage, https://quarto.org/docs/authoring/markdown-basics.html"
  },
  {
    "objectID": "slides/Day01_bsta511.html#formatting-text-visual-editor",
    "href": "slides/Day01_bsta511.html#formatting-text-visual-editor",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Formatting text: Visual editor",
    "text": "Formatting text: Visual editor\n\nUsing the Visual editor is similar to using a wordprocessor, such as Word\nKeyboard shortcuts usually work as well (shown for Mac below)"
  },
  {
    "objectID": "slides/Day01_bsta511.html#practice",
    "href": "slides/Day01_bsta511.html#practice",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Practice",
    "text": "Practice\n\nPart 1\n\nUsing the visual editor, practice formatting text in your qmd file, such as making text bold, italicized, and in code format.\nAdd 1st, 2nd, and 3rd level headers\nAdd a list with a\n\nsub-list (bullet and/or numbered)\n\nAdd a table\nAdd whatever else you are interested in!\n\nPart 2\n\nSwitch back to the Source editor and examine the markdown code that was used for the formatting.\n\n\nQuestions:\n\nWhat went smoothly?\nWhat hurdles did you encounter?"
  },
  {
    "objectID": "slides/Day01_bsta511.html#formatting-text-markdown",
    "href": "slides/Day01_bsta511.html#formatting-text-markdown",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Formatting text: Markdown",
    "text": "Formatting text: Markdown\n\n\n\n\n\n\n\nMarkdown:\nOutput:\n\n\n\n\n*This text is in italics*, but _so is this text_.\nThis text is in italics, but so is this text.\n\n\n**Bold** also has __2 options__\nBold also has 2 options\n\n\n~~Should this be deleted?~~\nShould this be deleted?\n\n\nNeed^super^ or~sub~ scripts?\nNeedsuper orsub scripts?\n\n\n`Code is often formatted as verbatim`\nCode is often formatted as verbatim\n\n\n&gt;This is a block quote.\n\nThis is a block quote."
  },
  {
    "objectID": "slides/Day01_bsta511.html#headers",
    "href": "slides/Day01_bsta511.html#headers",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Headers",
    "text": "Headers\n\nOrganize your documents using headers to create sections and subsections\nUse # at the beginning of the line to create headers\n\n\n\nText in editor:\n\n\n\n\n\n\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nMake sure there is no space before the #, and there IS a space after the # in order for the header to work properly."
  },
  {
    "objectID": "slides/Day01_bsta511.html#rstudio-tip",
    "href": "slides/Day01_bsta511.html#rstudio-tip",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "RStudio tip",
    "text": "RStudio tip\nYou can easily navigate through your .qmd file if you use headers to outline your text"
  },
  {
    "objectID": "slides/Day01_bsta511.html#code-chunks",
    "href": "slides/Day01_bsta511.html#code-chunks",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Code chunks",
    "text": "Code chunks\n\n\n.qmd file \n\nhtml output"
  },
  {
    "objectID": "slides/Day01_bsta511.html#create-a-code-chunk",
    "href": "slides/Day01_bsta511.html#create-a-code-chunk",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Create a code chunk",
    "text": "Create a code chunk\n3 options to create a code chunk\n\nClick on  at top right of the editor window, or\nKeyboard shortcut\n\n\n\n\nMac\nCommand + Option + I\n\n\nPC\nCtrl + Alt + I\n\n\n\n\nVisual editor: Select Insert -&gt; Executable Cell -&gt; R"
  },
  {
    "objectID": "slides/Day01_bsta511.html#what-does-a-code-chunk-look-like",
    "href": "slides/Day01_bsta511.html#what-does-a-code-chunk-look-like",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "What does a code chunk look like?",
    "text": "What does a code chunk look like?\nAn empty code chunk looks like this:\nVisual editor\n\nSource editor\n\n\n\n\n\n\n\nImportant\n\n\nNote that a code chunks start with ```{r} and ends with ```. Make sure there is no space before ```."
  },
  {
    "objectID": "slides/Day01_bsta511.html#enter-and-run-code-1n",
    "href": "slides/Day01_bsta511.html#enter-and-run-code-1n",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Enter and run code (1/n)",
    "text": "Enter and run code (1/n)\n\nType R code inside code chunks\nSelect code you want to run, by\n\nplacing the cursor in the line of code you want to run,\nor highlighting the code you want to run\n\n\n\n\n\nRun selected code by\n\nclicking on the  button in the top right corner of the scripting window and choosing Run Selected Line(s),\nor typing one of the following key combinations:\n\n\n\n\n\nMac\nctrl + return\n\n\nPC\ncommand + return\n\n\n\n\nWhere does the output appear?"
  },
  {
    "objectID": "slides/Day01_bsta511.html#enter-and-run-code-2n",
    "href": "slides/Day01_bsta511.html#enter-and-run-code-2n",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Enter and run code (2/n)",
    "text": "Enter and run code (2/n)\n\n\nRun all code in a chunk by\n\nby clicking the play button in the top right corner of the chunk\n\nThe code output appears below the code chunk\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe output should also appear in the Console.\nSettings can be changed so that the output appears only in the Console and not below the code chunk:\n\nSelect  (to right of Render button) and then Chunk Output in Console."
  },
  {
    "objectID": "slides/Day01_bsta511.html#useful-keyboard-shortcuts",
    "href": "slides/Day01_bsta511.html#useful-keyboard-shortcuts",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Useful keyboard shortcuts",
    "text": "Useful keyboard shortcuts\nFull list of keyboard shortcuts\n \n\n\n\n\n\n\n\n\naction\nmac\nwindows/linux\n\n\n\n\nRun code in qmd (or script)\ncmd + enter\nctrl + enter\n\n\n&lt;-\noption + -\nalt + -\n\n\ninterrupt currently running command\nesc\nesc\n\n\nin console, retrieve previously run code\nup/down\nup/down\n\n\nkeyboard shortcut help\noption + shift + k\nalt + shift + k\n\n\n\n\n\nPractice\nTry typing code below in your qmd (with shortcut) and evaluating it:\n\ny &lt;- 5\ny"
  },
  {
    "objectID": "slides/Day01_bsta511.html#yaml-metadata",
    "href": "slides/Day01_bsta511.html#yaml-metadata",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "YAML metadata",
    "text": "YAML metadata\nMany output options can be set in the YAML metadata, which is the first set of code in the file starting and ending with ---.\n\nIt sets the configuration specifications for the output file\nYAML is an acronym for\n\nyet another markup language, or\nYAML ain’t markup language"
  },
  {
    "objectID": "slides/Day01_bsta511.html#simple-yaml-example",
    "href": "slides/Day01_bsta511.html#simple-yaml-example",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Simple YAML example",
    "text": "Simple YAML example\n\n\nThe default YAML includes a title and author that appear at the top of the output file. In the example below, I also added in a date option\n\n\n\n\nYAML:\n\n---\ntitle: \"My first Quarto file\"\nauthor: \"Meike\"\ndate: \"9/25/2023\"\nformat: html\neditor: visual\n---\n\n\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe YAML must start and end with 3 dashes ---.\nThe first set of --- must be on the very first line."
  },
  {
    "objectID": "slides/Day01_bsta511.html#change-the-output-file-type",
    "href": "slides/Day01_bsta511.html#change-the-output-file-type",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Change the output file type",
    "text": "Change the output file type\n\n\n\nThe YAML specifies the format of the output file:\n\nhtml, Word, pdf, slides, website, book, etc.\n\nThis is done by changing the format: option\n\n\n\n\n\nIllustration by Alison Hill and Allison Horst, for RStudio.\n\n\n\n\n\n\n\n---\ntitle: \"My first Quarto file\"\nauthor: \"Meike\"\ndate: \"9/25/2023\"\nformat: html\neditor: visual\n---\n\n\n\n\n\n\n\n\n\nOutput format\nYAML\n\n\n\n\nhtml\nformat: html\n\n\nWord\nformat: docx\n\n\npdf1\nformat: pdf\n\n\nhtml slides\nformat: revealjs\n\n\nPPT slides\nformat: pptx\n\n\n\n\n\nrequires LaTeX installation"
  },
  {
    "objectID": "slides/Day01_bsta511.html#section",
    "href": "slides/Day01_bsta511.html#section",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "",
    "text": "Artwork by @allison_horst"
  },
  {
    "objectID": "slides/Day01_bsta511.html#you-will-get-frustrated-while-learning-r",
    "href": "slides/Day01_bsta511.html#you-will-get-frustrated-while-learning-r",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "You WILL get frustrated while learning R!",
    "text": "You WILL get frustrated while learning R!\nFrom Garrett Grolemund’s Prologue of his book Hands-On Programming with R1:\n\nAs you learn to program, you are going to get frustrated. You are learning a new language, and it will take time to become fluent. But frustration is not just natural, it’s actually a positive sign that you should watch for. Frustration is your brain’s way of being lazy; it’s trying to get you to quit and go do something easy or fun. If you want to get physically fitter, you need to push your body even though it complains. If you want to get better at programming, you’ll need to push your brain. Recognize when you get frustrated and see it as a good thing: you’re now stretching yourself. Push yourself a little further every day, and you’ll soon be a confident programmer.\n\nGrolemund, Garrett. 2014. Hands-on Programming with R. O’Reilly. https://rstudio-education.github.io/hopr/"
  },
  {
    "objectID": "slides/Day01_bsta511.html#resources",
    "href": "slides/Day01_bsta511.html#resources",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Resources",
    "text": "Resources\n\nOfficial Quarto guide: https://quarto.org/docs/guide/\n\nMarkdown basics: https://quarto.org/docs/authoring/markdown-basics.html\n\nText formatting, headings, linnks, images, lists, tables, equations, diagrams, page breaks, keyboard shortcuts, and more!\n\nCode blocks: https://quarto.org/docs/computations/r.html#code-blocks\n\nChunk options: https://quarto.org/docs/computations/r.html#chunk-options\n\n\nMine Çetinkaya-Rundel’s Quarto tip a day: https://mine-cetinkaya-rundel.github.io/quarto-tip-a-day/\n\nHadley Wickham’s R for Data Science: https://r4ds.hadley.nz/ _ See Chapter 29 for Quarto"
  },
  {
    "objectID": "slides/Day03_bsta511_part2.html#case-study-discrimination-in-developmental-disability-support-1.7.1",
    "href": "slides/Day03_bsta511_part2.html#case-study-discrimination-in-developmental-disability-support-1.7.1",
    "title": "Day 3: Data visualization - Part 2",
    "section": "Case study: discrimination in developmental disability support (1.7.1)",
    "text": "Case study: discrimination in developmental disability support (1.7.1)\n\nPrevious research\n\nResearchers examined DDS expenditures for developmentally disabled residents by ethnicity\nFound that the mean annual expenditures on Hispanics was less than that on White non-Hispanics.\n\nResult: an allegation of ethnic discrimination was brought against the California DDS.\nQuestion: Are the data sufficient evidence of ethnic discrimination?"
  },
  {
    "objectID": "slides/Day03_bsta511_part2.html#load-dds.discr-dataset-from-oibiostat-package",
    "href": "slides/Day03_bsta511_part2.html#load-dds.discr-dataset-from-oibiostat-package",
    "title": "Day 3: Data visualization - Part 2",
    "section": "Load dds.discr dataset from oibiostat package",
    "text": "Load dds.discr dataset from oibiostat package\n\n\nThe textbook’s datasets are in the R package oibiostat\nMake sure the oibiostat package is installed before running the code below.\nLoad the oibiostat package and the dataset dds.discr\n\nthe code below needs to be run every time you restart R or render a Qmd file\n\nlibrary(oibiostat)\ndata(\"dds.discr\")\n\n\nAfter loading the dataset dds.discr using data(\"dds.discr\"), you will see dds.discr in the Data list of the Environment window."
  },
  {
    "objectID": "slides/Day03_bsta511_part2.html#glimpse",
    "href": "slides/Day03_bsta511_part2.html#glimpse",
    "title": "Day 3: Data visualization - Part 2",
    "section": "glimpse()",
    "text": "glimpse()\n\nNew: glimpse()\n\nUse glimpse() from the tidyverse package (technically it’s from the dplyr package) to get information about variable types.\nglimpse() tends to have nicer output for tibbles than str()\n\n\n\nlibrary(tidyverse)\nglimpse(dds.discr)  # from tidyverse package (dplyr)\n\nRows: 1,000\nColumns: 6\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ gender       &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ ethnicity    &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…"
  },
  {
    "objectID": "slides/Day03_bsta511_part2.html#recall-previous-data-viz",
    "href": "slides/Day03_bsta511_part2.html#recall-previous-data-viz",
    "title": "Day 3: Data visualization - Part 2",
    "section": "Recall previous data viz",
    "text": "Recall previous data viz"
  },
  {
    "objectID": "slides/Day03_bsta511_part2.html#visualize-in-more-detail",
    "href": "slides/Day03_bsta511_part2.html#visualize-in-more-detail",
    "title": "Day 3: Data visualization - Part 2",
    "section": "Visualize in more detail:",
    "text": "Visualize in more detail:\nethnicity, age, and expenditures (code on next slide)"
  },
  {
    "objectID": "slides/Day03_bsta511_part2.html#code-for-visualize-in-more-detail-ethnicity-age-and-expenditures",
    "href": "slides/Day03_bsta511_part2.html#code-for-visualize-in-more-detail-ethnicity-age-and-expenditures",
    "title": "Day 3: Data visualization - Part 2",
    "section": "Code for Visualize in more detail: ethnicity, age, and expenditures",
    "text": "Code for Visualize in more detail: ethnicity, age, and expenditures\n\nPlot on previous slide\n\n\ndds.discr_Hips_WhnH &lt;- dds.discr %&gt;%  \n  filter(ethnicity == \"White not Hispanic\" | ethnicity == \"Hispanic\" ) %&gt;% \n  droplevels()   # remove empty factor levels\n\nggplot(data = dds.discr_Hips_WhnH,   \n       aes(x = expenditures,\n           y = age.cohort)) + \n  geom_boxplot(color=\"darkgrey\") + \n  facet_grid(rows = \"ethnicity\") +   \n  labs(x = \"Annual Expenditures ($)\",\n       y = \"Race and ethnicity\") +\n  geom_jitter(     \n    aes(color = ethnicity),      \n    alpha = 0.3,      \n    show.legend = FALSE,      \n    position = position_jitter(     \n      height = 0.4))"
  },
  {
    "objectID": "slides/Day03_bsta511_part2.html#mean-annual-dds-expenditures-by-raceethnicity-default-long-format",
    "href": "slides/Day03_bsta511_part2.html#mean-annual-dds-expenditures-by-raceethnicity-default-long-format",
    "title": "Day 3: Data visualization - Part 2",
    "section": "Mean annual DDS expenditures by race/ethnicity: default long format",
    "text": "Mean annual DDS expenditures by race/ethnicity: default long format\n\n\n\nmean_expend &lt;- \n  dds.discr_Hips_WhnH %&gt;% \n  group_by(\n    ethnicity, age.cohort)%&gt;% \n  summarize(\n    ave = mean(expenditures))\n\n\n\nmean_expend\n\n# A tibble: 12 × 3\n# Groups:   ethnicity [2]\n   ethnicity          age.cohort    ave\n   &lt;fct&gt;              &lt;fct&gt;       &lt;dbl&gt;\n 1 Hispanic           0-5         1393.\n 2 Hispanic           6-12        2312.\n 3 Hispanic           13-17       3955.\n 4 Hispanic           18-21       9960.\n 5 Hispanic           22-50      40924.\n 6 Hispanic           51+        55585 \n 7 White not Hispanic 0-5         1367.\n 8 White not Hispanic 6-12        2052.\n 9 White not Hispanic 13-17       3904.\n10 White not Hispanic 18-21      10133.\n11 White not Hispanic 22-50      40188.\n12 White not Hispanic 51+        52670."
  },
  {
    "objectID": "slides/Day03_bsta511_part2.html#mean-annual-dds-expenditures-by-raceethnicity-wide-format",
    "href": "slides/Day03_bsta511_part2.html#mean-annual-dds-expenditures-by-raceethnicity-wide-format",
    "title": "Day 3: Data visualization - Part 2",
    "section": "Mean annual DDS expenditures by race/ethnicity: wide format",
    "text": "Mean annual DDS expenditures by race/ethnicity: wide format\n\n\n\nmean_expend_wide &lt;- \n  mean_expend %&gt;% \n  pivot_wider(\n    names_from = ethnicity,\n    values_from = ave)\n\n\n\nmean_expend_wide\n\n# A tibble: 6 × 3\n  age.cohort Hispanic `White not Hispanic`\n  &lt;fct&gt;         &lt;dbl&gt;                &lt;dbl&gt;\n1 0-5           1393.                1367.\n2 6-12          2312.                2052.\n3 13-17         3955.                3904.\n4 18-21         9960.               10133.\n5 22-50        40924.               40188.\n6 51+          55585                52670."
  },
  {
    "objectID": "slides/Day03_bsta511_part2.html#differences-in-mean-annual-dds-expenditures-by-age-cohort-and-raceethnicity",
    "href": "slides/Day03_bsta511_part2.html#differences-in-mean-annual-dds-expenditures-by-age-cohort-and-raceethnicity",
    "title": "Day 3: Data visualization - Part 2",
    "section": "Differences in mean annual DDS expenditures by age cohort and race/ethnicity",
    "text": "Differences in mean annual DDS expenditures by age cohort and race/ethnicity\n\nmean_expend_wide &lt;- mean_expend_wide %&gt;% \n  mutate(diff_mean = `White not Hispanic` - Hispanic)\n\nmean_expend_wide\n\n# A tibble: 6 × 4\n  age.cohort Hispanic `White not Hispanic` diff_mean\n  &lt;fct&gt;         &lt;dbl&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n1 0-5           1393.                1367.     -26.3\n2 6-12          2312.                2052.    -260. \n3 13-17         3955.                3904.     -50.9\n4 18-21         9960.               10133.     173. \n5 22-50        40924.               40188.    -736. \n6 51+          55585                52670.   -2915. \n\n\n\nQuestion: Are the data sufficient evidence of ethnic discrimination in DDS expenditures when comparing Hispanics with White non-Hispanics?"
  },
  {
    "objectID": "slides/Day03_bsta511_part2.html#simpsons-paradox",
    "href": "slides/Day03_bsta511_part2.html#simpsons-paradox",
    "title": "Day 3: Data visualization - Part 2",
    "section": "Simpson’s paradox",
    "text": "Simpson’s paradox\n\n\nThis case study is an example of confounding known as Simpson’s paradox\nSimpson’s paradox happens when an association observed in several groups disappears or reverses direction when the groups are combined.\nIn other words, an association between two variables \\(X\\) and \\(Y\\) may disappear or reverse direction once data are partitioned into subpopulations based on a third variable \\(Z\\) (i.e., a confounding variable)."
  },
  {
    "objectID": "slides/Day03_bsta511_part2.html#tools-for-wrangling-data",
    "href": "slides/Day03_bsta511_part2.html#tools-for-wrangling-data",
    "title": "Day 3: Data visualization - Part 2",
    "section": "Tools for wrangling data",
    "text": "Tools for wrangling data\n\n\ntidyverse functions\n\ntidyverse is a suite of packages that implement tidy methods for data importing, cleaning, wrangling, and visualizing\nload the tidyverse packages by running the code library(tidyverse)\n\nDon’t forget to first install tidyverse!\n\n\nFunctions to easily work with rows and columns, such as\n\nsubset rows/columns\nadd new rows/columns\njoin together different data sets\nmake data long or wide\n\nThere are often many steps to tidy data\n\nwe string together commands\nto be performed sequentially\nusing pipes %&gt;%"
  },
  {
    "objectID": "slides/Day03_bsta511_part2.html#summary-of-data-wrangling-so-far",
    "href": "slides/Day03_bsta511_part2.html#summary-of-data-wrangling-so-far",
    "title": "Day 3: Data visualization - Part 2",
    "section": "Summary of data wrangling so far",
    "text": "Summary of data wrangling so far\n\n\nThe pipe %&gt;% to string together commands in sequence\nmutate() to add a new variable to a dataset\nselect() to select columns (or deselect columns with -variable)\nfilter() to select specific rows\npivot_wider() to reshape a dataset from a long to a wide format\n\nSummarizing data\n\ntabyl() from janitor package to make frequency tables of categorical variables\nsummarize() to get summary statistics of variables\ngroup_by() to group data by categorical variables before finding summaries"
  },
  {
    "objectID": "slides/Day03_bsta511_part2.html#what-packages-are-included-in-the-tidyverse",
    "href": "slides/Day03_bsta511_part2.html#what-packages-are-included-in-the-tidyverse",
    "title": "Day 3: Data visualization - Part 2",
    "section": "What packages are included in the tidyverse?",
    "text": "What packages are included in the tidyverse?\n\n\nCore packages\nThese automatically load when loading the tidyverse package\n\n\n\nhttps://www.tidyverse.org/\n\n\n\nList of all packages:\n\ntidyverse_packages(include_self = TRUE)\n\n [1] \"broom\"         \"conflicted\"    \"cli\"           \"dbplyr\"       \n [5] \"dplyr\"         \"dtplyr\"        \"forcats\"       \"ggplot2\"      \n [9] \"googledrive\"   \"googlesheets4\" \"haven\"         \"hms\"          \n[13] \"httr\"          \"jsonlite\"      \"lubridate\"     \"magrittr\"     \n[17] \"modelr\"        \"pillar\"        \"purrr\"         \"ragg\"         \n[21] \"readr\"         \"readxl\"        \"reprex\"        \"rlang\"        \n[25] \"rstudioapi\"    \"rvest\"         \"stringr\"       \"tibble\"       \n[29] \"tidyr\"         \"xml2\"          \"tidyverse\"    \n\n\n\nPackages not a part of the core get installed with the tidyverse suite, but need to be loaded separately.\n\nSee https://www.tidyverse.org/packages/ for more info."
  },
  {
    "objectID": "slides/Day04_bsta511.html",
    "href": "slides/Day04_bsta511.html",
    "title": "Day 3 (F23 Day 4): Probability Part 1",
    "section": "",
    "text": "Download slides\n\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "slides/Day07_bsta511.html",
    "href": "slides/Day07_bsta511.html",
    "title": "Day 7: Normal and Poisson distributions",
    "section": "",
    "text": "Download slides\n\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "slides/Day02_bsta511.html#useful-keyboard-shortcuts",
    "href": "slides/Day02_bsta511.html#useful-keyboard-shortcuts",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Useful keyboard shortcuts",
    "text": "Useful keyboard shortcuts\nFull list of keyboard shortcuts\n \n\n\n\n\n\n\n\n\naction\nmac\nwindows/linux\n\n\n\n\nRun code in qmd (or script)\ncmd + enter\nctrl + enter\n\n\n&lt;-\noption + -\nalt + -\n\n\ninterrupt currently running command\nesc\nesc\n\n\nin console, retrieve previously run code\nup/down\nup/down\n\n\nkeyboard shortcut help\noption + shift + k\nalt + shift + k\n\n\n\n\n\nPractice\nTry typing code below in your qmd (with shortcut) and evaluating it:\n\ny &lt;- 5\ny"
  },
  {
    "objectID": "slides/Day02_bsta511.html#another-resource-for-an-introduction-to-r",
    "href": "slides/Day02_bsta511.html#another-resource-for-an-introduction-to-r",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Another resource for an introduction to R",
    "text": "Another resource for an introduction to R\n\nIf you would like another perspective on what we covered the first week, you might find Danielle Navarro’s online book Learning Statistics with R to be helpful.\nDownload free pdf: https://learningstatisticswithr.com/\nSee Sections 3.1-3.7.1 for some of the topics we covered on first day"
  },
  {
    "objectID": "slides/Day02_bsta511.html#population-vs.-sample",
    "href": "slides/Day02_bsta511.html#population-vs.-sample",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Population vs. sample",
    "text": "Population vs. sample\n\n\n(Target) Population\n\ngroup of interest being studied\ngroup from which the sample is selected\n\nstudies often have inclusion and/or exclusion criteria\n\n\n\n\nSample\n\ngroup on which data are collected\noften a small subset of the population"
  },
  {
    "objectID": "slides/Day02_bsta511.html#sampling-methods-14",
    "href": "slides/Day02_bsta511.html#sampling-methods-14",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Sampling methods (1/4)",
    "text": "Sampling methods (1/4)\nGoal is to get a representative sample of the population:\nthe characteristics of the sample are similar to the characteristics of the population\n\n\nSimple random sample (SRS)\n\neach individual of a population has the same chance of being sampled\nrandomly sampled\nconsidered best way to sample\n\n\n\n\n\n\n\n\n\nConvenience sample\n\neasily accessible individuals are more likely to be included in the sample than other individuals\na common “pitfall”"
  },
  {
    "objectID": "slides/Day02_bsta511.html#sampling-methods-24",
    "href": "slides/Day02_bsta511.html#sampling-methods-24",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Sampling methods (2/4)",
    "text": "Sampling methods (2/4)\nGood sampling plans don’t guarantee samples representative of the population\n\n\nNon-response bias\n\nnon-response rates can be high\nare all groups within a population being reached?\nunrepresentative sample\n=&gt; skewed results\n\n\n\n\n\n\n\n\n\n“Random” samples can be unrepresentative by random chance\n\nIn a SRS each case in the population has an equal chance of being included in the sample\nBut by random chance alone a random sample might contain a higher proportion of one group over another\nEx: a SRS might by chance include 70% men (unlikely, but theoretically possible)"
  },
  {
    "objectID": "slides/Day02_bsta511.html#sampling-methods-34",
    "href": "slides/Day02_bsta511.html#sampling-methods-34",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Sampling methods (3/4)",
    "text": "Sampling methods (3/4)\n\n\n\nSimple random sample (SRS)\n\neach individual of a population has the same chance of being sampled\nstatistical methods taught in this class assume a SRS!\n\nStratified sampling\n\ndivide population into groups (strata) before selecting cases within each stratum (often via SRS)\nusually cases within a strata are similar, but are different from other strata with respect to the outcome of interest, such as gender or age groups"
  },
  {
    "objectID": "slides/Day02_bsta511.html#sampling-methods-44",
    "href": "slides/Day02_bsta511.html#sampling-methods-44",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Sampling methods (4/4)",
    "text": "Sampling methods (4/4)\n\n\n\nCluster sample\n\nfirst divide population into groups (clusters)\nthen sample a fixed number of clusters, and include all observations from chosen clusters\nclusters are often hospitals, clinicians, schools, etc., where each cluster will have similar services/ policies/ etc.\ncases within clusters usually very diverse\n\nMultistage sample\n\nsimilar to a cluster sample, but select a random sample within each selected cluster instead of all individuals"
  },
  {
    "objectID": "slides/Day02_bsta511.html#experiments-12",
    "href": "slides/Day02_bsta511.html#experiments-12",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Experiments (1/2)",
    "text": "Experiments (1/2)\n\n\nResearchers assign individuals to different treatment or intervention groups\n\ncontrol group: often receive a placebo or usual care\ndifferent treatment groups are often called study arms\n\nRandomization\n\ngroup assignment is usually random to ensure similar (balanced) study arms for all variables (observed and unobserved)\nrandomization allows study arm differences in outcomes to be attributed to treatment rather than variability in patient characteristics\n\ntreatment is the only systematic difference between groups\nestablish causality\n\nblocking (stratification): group individuals into blocks (strata) before randomizing if there are certain characteristics that may influence the outcome other than treatment (i.e. gender, age group)"
  },
  {
    "objectID": "slides/Day02_bsta511.html#experiments-22",
    "href": "slides/Day02_bsta511.html#experiments-22",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Experiments (2/2)",
    "text": "Experiments (2/2)\n\n\nReplication\n\naccomplished by collecting a sufficiently large sample\nresults usually more reliable with a large sample size\n\noften less variability\nmore likely to be representative of population\n\n\nSome studies are not ethical to carry out as experiments"
  },
  {
    "objectID": "slides/Day02_bsta511.html#observational-studies",
    "href": "slides/Day02_bsta511.html#observational-studies",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Observational studies",
    "text": "Observational studies\n\n\ndata are observed and recorded without interference\noften done via surveys, electronic health records, or medical chart reviews\ncohorts\nassociations between variables can be established, but not causality\n\nIndividuals with different characteristics may also differ in other ways that influence response\n\nconfounding variables (lurking variable)\n\nvariables associated with both the explanatory and response variables\n\nprospective vs. retrospective studies"
  },
  {
    "objectID": "slides/Day02_bsta511.html#comparing-study-designs",
    "href": "slides/Day02_bsta511.html#comparing-study-designs",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Comparing study designs",
    "text": "Comparing study designs\n\nScience Media Centre"
  },
  {
    "objectID": "slides/Day02_bsta511.html#systematic-reviews-example",
    "href": "slides/Day02_bsta511.html#systematic-reviews-example",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Systematic Reviews example",
    "text": "Systematic Reviews example\n\n\n\nSTEM: Systematically Testing the Evidence on Marijuana\n\n\n\n\n\nSTEM is a collaborative project between the US Department of Veterans Affairs and the Center for Evidence-based Policy at Oregon Health & Science University.\nThe project is funded by the US Department of Veterans Affairs: Office of Rural Health."
  },
  {
    "objectID": "slides/Day02_bsta511.html#how-are-data-stored-how-do-we-use-them",
    "href": "slides/Day02_bsta511.html#how-are-data-stored-how-do-we-use-them",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "How are data stored, how do we use them?",
    "text": "How are data stored, how do we use them?\n\nOften, data are in an Excel sheet, or a plain text file (.csv, .txt)\n.csv files open in Excel automatically, but actually are plain text\nUsually, columns are variables/measures and rows are observations (i.e. a person’s measurements)\n\nData in R\n\nWe can import data from many file types, including .csv, .txt., and .xlsx\n\nWe will cover this on a later date\n\nOnce imported, R typically stores data as data frames, or tibbles if using the tidyverse package (more on this later).\n\nFor our purposes, these are essentially the same, and I will tend to use the terms interchangeably.\nThese are examples of what we call object types in R."
  },
  {
    "objectID": "slides/Day02_bsta511.html#data-frame-example",
    "href": "slides/Day02_bsta511.html#data-frame-example",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Data frame example",
    "text": "Data frame example\n\n\n\ndf &lt;- data.frame(\n  IDs=1:3, \n  gender=c(\"male\", \"female\", \"Male\"), \n  age=c(28, 35.5, 31),\n  trt = c(\"control\", \"1\", \"1\"),\n  Veteran = c(FALSE, TRUE, TRUE)\n  )\ndf\n\n  IDs gender  age     trt Veteran\n1   1   male 28.0 control   FALSE\n2   2 female 35.5       1    TRUE\n3   3   Male 31.0       1    TRUE\n\n\n\nVectors vs. data frames\n\na data frame is a collection (or array or table) of vectors\n\n\n\n\n\n\n\nDifferent columns can be of different data types (i.e. numeric vs. text)\nBoth numeric and text can be stored within a column (stored together as text).\nVectors and data frames are examples of objects in R.\n\nThere are other types of R objects to store data, such as matrices, lists."
  },
  {
    "objectID": "slides/Day02_bsta511.html#observations-variables",
    "href": "slides/Day02_bsta511.html#observations-variables",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Observations & variables",
    "text": "Observations & variables\n\n\n\ndf\n\n  IDs gender  age     trt Veteran\n1   1   male 28.0 control   FALSE\n2   2 female 35.5       1    TRUE\n3   3   Male 31.0       1    TRUE\n\n\n\n\n\nISLBS\n\n\n\n\n\nBook refers to a dataset as a data matrix\nRows are usually observations\nColumns are usually variables\nHow many observations are in this dataset?\nWhat are the variable types in this dataset?"
  },
  {
    "objectID": "slides/Day02_bsta511.html#variable-column-types",
    "href": "slides/Day02_bsta511.html#variable-column-types",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Variable (column) types",
    "text": "Variable (column) types\n\n\n\n\n\n\n\n\n\nR type\nvariable type\ndescription\n\n\n\n\ninteger\ndiscrete\ninteger-valued numbers\n\n\ndouble or numeric\ncontinuous\nnumbers that are decimals\n\n\nfactor\ncategorical\ncategorical variables stored with levels (groups)\n\n\ncharacter\ncategorical\ntext, “strings”\n\n\nlogical\ncategorical\nboolean (TRUE, FALSE)\n\n\n\n\n\nView the structure of our data frame to see what the variable types are:\n\n\n\nstr(df)\n\n'data.frame':   3 obs. of  5 variables:\n $ IDs    : int  1 2 3\n $ gender : chr  \"male\" \"female\" \"Male\"\n $ age    : num  28 35.5 31\n $ trt    : chr  \"control\" \"1\" \"1\"\n $ Veteran: logi  FALSE TRUE TRUE"
  },
  {
    "objectID": "slides/Day02_bsta511.html#fishers-or-andersons-iris-data-set",
    "href": "slides/Day02_bsta511.html#fishers-or-andersons-iris-data-set",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Fisher’s (or Anderson’s) Iris data set",
    "text": "Fisher’s (or Anderson’s) Iris data set\nData description:\n\nn = 150\n3 species of Iris flowers (Setosa, Virginica, and Versicolour)\n\n50 measurements of each type of Iris\n\nvariables:\n\nsepal length, sepal width, petal length, petal width, and species\n\n\nCan the iris species be determined by these variables?\n\n\n\nGareth Duffy"
  },
  {
    "objectID": "slides/Day02_bsta511.html#view-the-iris-dataset",
    "href": "slides/Day02_bsta511.html#view-the-iris-dataset",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "View the iris dataset",
    "text": "View the iris dataset\n\n\nThe iris dataset is already pre-loaded in base R and ready to use.\nType the following command in the console window\n\nWarning: this command cannot be rendered. It will give an error.\n\n\n\n\n\n\n\n\nView(iris)\n\n\nA new tab in the scripting window should appear with the iris dataset."
  },
  {
    "objectID": "slides/Day02_bsta511.html#data-structure",
    "href": "slides/Day02_bsta511.html#data-structure",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Data structure",
    "text": "Data structure\n\nWhat are the different variable types in this data set?\n\n\n\n\nstr(iris)   # structure of data\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ..."
  },
  {
    "objectID": "slides/Day02_bsta511.html#data-set-summary",
    "href": "slides/Day02_bsta511.html#data-set-summary",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Data set summary",
    "text": "Data set summary\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50"
  },
  {
    "objectID": "slides/Day02_bsta511.html#data-set-info",
    "href": "slides/Day02_bsta511.html#data-set-info",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Data set info",
    "text": "Data set info\n\ndim(iris)\n\n[1] 150   5\n\nnrow(iris)\n\n[1] 150\n\nncol(iris)\n\n[1] 5\n\nnames(iris)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\""
  },
  {
    "objectID": "slides/Day02_bsta511.html#view-the-beginning-or-end-of-a-dataset",
    "href": "slides/Day02_bsta511.html#view-the-beginning-or-end-of-a-dataset",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "View the beginning or end of a dataset",
    "text": "View the beginning or end of a dataset\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\ntail(iris)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n145          6.7         3.3          5.7         2.5 virginica\n146          6.7         3.0          5.2         2.3 virginica\n147          6.3         2.5          5.0         1.9 virginica\n148          6.5         3.0          5.2         2.0 virginica\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica"
  },
  {
    "objectID": "slides/Day02_bsta511.html#specify-how-many-rows-to-view-at-beginning-or-end-of-a-dataset",
    "href": "slides/Day02_bsta511.html#specify-how-many-rows-to-view-at-beginning-or-end-of-a-dataset",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Specify how many rows to view at beginning or end of a dataset",
    "text": "Specify how many rows to view at beginning or end of a dataset\n\nhead(iris, 3)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n\ntail(iris, 2)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica"
  },
  {
    "objectID": "slides/Day02_bsta511.html#the",
    "href": "slides/Day02_bsta511.html#the",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "The $",
    "text": "The $\n\nSuppose we want to single out the column of petal width values.\nOne way to do this is to use the $\n\nDatSetName$VariableName\n\n\n\niris$Petal.Width\n\n  [1] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 0.2 0.2 0.1 0.1 0.2 0.4 0.4 0.3\n [19] 0.3 0.3 0.2 0.4 0.2 0.5 0.2 0.2 0.4 0.2 0.2 0.2 0.2 0.4 0.1 0.2 0.2 0.2\n [37] 0.2 0.1 0.2 0.2 0.3 0.3 0.2 0.6 0.4 0.3 0.2 0.2 0.2 0.2 1.4 1.5 1.5 1.3\n [55] 1.5 1.3 1.6 1.0 1.3 1.4 1.0 1.5 1.0 1.4 1.3 1.4 1.5 1.0 1.5 1.1 1.8 1.3\n [73] 1.5 1.2 1.3 1.4 1.4 1.7 1.5 1.0 1.1 1.0 1.2 1.6 1.5 1.6 1.5 1.3 1.3 1.3\n [91] 1.2 1.4 1.2 1.0 1.3 1.2 1.3 1.3 1.1 1.3 2.5 1.9 2.1 1.8 2.2 2.1 1.7 1.8\n[109] 1.8 2.5 2.0 1.9 2.1 2.0 2.4 2.3 1.8 2.2 2.3 1.5 2.3 2.0 2.0 1.8 2.1 1.8\n[127] 1.8 1.8 2.1 1.6 1.9 2.0 2.2 1.5 1.4 2.3 2.4 1.8 1.8 2.1 2.4 2.3 1.9 2.3\n[145] 2.5 2.3 1.9 2.0 2.3 1.8"
  },
  {
    "objectID": "slides/Day02_bsta511.html#example-using-the",
    "href": "slides/Day02_bsta511.html#example-using-the",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Example using the $",
    "text": "Example using the $\nThe $ is helpful if you want to create a new dataset for just that one variable, or, more commonly, if you want to calculate summary statistics for that one variable.\n\n\n\nmean(iris$Petal.Width)\n\n[1] 1.199333\n\nsd(iris$Petal.Width)\n\n[1] 0.7622377\n\nmedian(iris$Petal.Width)\n\n[1] 1.3"
  },
  {
    "objectID": "slides/Day02_bsta511.html#inline-code",
    "href": "slides/Day02_bsta511.html#inline-code",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Inline code",
    "text": "Inline code\n\n\nWith markdown you can also report R code output inline with the text instead of using a chunk.\n\n\n\nText in editor:\n\n\n\n\n\n\nOutput:\nThe mean petal width for all 3 species combined is 1.2 (SD = 0.8) cm.\n\n\n\nReporting summary statistics this way in a report, makes the numbers computationally reproducible.\nFor example, if this were for an abstract and a year later you are wondering where the numbers came from, your R code will tell you exactly which dataset was used to calculate the values."
  },
  {
    "objectID": "slides/Day02_bsta511.html#table-1-example",
    "href": "slides/Day02_bsta511.html#table-1-example",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Table 1 example",
    "text": "Table 1 example\n\n\n\n\n\n\n\n\nAre We on the Same Page?: A Cross-Sectional Study of Patient-Clinician Goal Concordance in Rheumatoid Arthritis\nJ Barton et al.\nArthritis Care & Research.\n2021 Sep 27 https://pubmed.ncbi.nlm.nih.gov/34569172/"
  },
  {
    "objectID": "slides/Day02_bsta511.html#measures-of-center-mean",
    "href": "slides/Day02_bsta511.html#measures-of-center-mean",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of center: mean",
    "text": "Measures of center: mean\n\nSample mean: the average value of observations\n\\[\\overline{x} = \\frac{x_1+x_2+\\cdots+x_n}{n} = \\sum_{i=1}^{n}\\frac{x_i}{n}\\]\nwhere \\(x_1, x_2, \\ldots, x_n\\) represent the \\(n\\) observed values in a sample\nExample: What is the mean age in the toy dataset df defined earlier?\n\n\ndf\n\n  IDs gender  age     trt Veteran\n1   1   male 28.0 control   FALSE\n2   2 female 35.5       1    TRUE\n3   3   Male 31.0       1    TRUE\n\nmean(df$age)\n\n[1] 31.5"
  },
  {
    "objectID": "slides/Day02_bsta511.html#measures-of-center-median",
    "href": "slides/Day02_bsta511.html#measures-of-center-median",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of center: median",
    "text": "Measures of center: median\n\n\nThe median is the middle value of the observations in a sample.\nThe median is the 50th percentile, meaning\n\n50% of observations lie below and\n50% of observations lie above the median.\n\n\n\n\n\n\n\nIf the number of observations is\n\nodd: the median is the middle observed value\neven: the median is the average of the two middle observed values\n\n\n\n\n\ndf$age\n\n[1] 28.0 35.5 31.0\n\nmedian(df$age)\n\n[1] 31\n\nmedian(c(df$age, 67))\n\n[1] 33.25"
  },
  {
    "objectID": "slides/Day02_bsta511.html#measures-of-center-mean-vs.-median",
    "href": "slides/Day02_bsta511.html#measures-of-center-mean-vs.-median",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of center: mean vs. median",
    "text": "Measures of center: mean vs. median\n\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50"
  },
  {
    "objectID": "slides/Day02_bsta511.html#measures-of-center-mode",
    "href": "slides/Day02_bsta511.html#measures-of-center-mode",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of center: mode",
    "text": "Measures of center: mode\nmode: the most frequent value in a dataset"
  },
  {
    "objectID": "slides/Day02_bsta511.html#measures-of-spread-standard-deviation-sd-13",
    "href": "slides/Day02_bsta511.html#measures-of-spread-standard-deviation-sd-13",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of spread: standard deviation (SD) (1/3)",
    "text": "Measures of spread: standard deviation (SD) (1/3)\nstandard deviation is (approximately) the average distance between a typical observation and the mean\n\nAn observation’s deviation is the distance between its value \\(x\\) and the sample mean \\(\\overline{x}\\): deviation = \\(x - \\overline{x}\\)."
  },
  {
    "objectID": "slides/Day02_bsta511.html#measures-of-spread-sd-23",
    "href": "slides/Day02_bsta511.html#measures-of-spread-sd-23",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of spread: SD (2/3)",
    "text": "Measures of spread: SD (2/3)\n\nThe sample variance \\(s^2\\) is the sum of squared deviations divided by the number of observations minus 1. \\[s^2 = \\frac{(x_1 - \\overline{x})^2+(x_2 - \\overline{x})^2+\\cdots+(x_n - \\overline{x})^2}{n-1} = \\sum_{i=1}^{n}\\frac{(x_i - \\overline{x})^2}{n-1}\\] where \\(x_1, x_2, \\dots, x_n\\) represent the \\(n\\) observed values.\nThe standard deviation \\(s\\) is the square root of the variance. \\[s = \\sqrt{\\frac{({x_1 - \\overline{x})}^{2}+({x_2 - \\overline{x})}^{2}+\\cdots+({x_n - \\overline{x})}^{2}}{n-1}} = \\sqrt{\\sum_{i=1}^{n}\\frac{(x_i - \\overline{x})^2}{n-1}}\\]"
  },
  {
    "objectID": "slides/Day02_bsta511.html#measures-of-spread-sd-33",
    "href": "slides/Day02_bsta511.html#measures-of-spread-sd-33",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of spread: SD (3/3)",
    "text": "Measures of spread: SD (3/3)\n\n\n\nLet’s calculate the sample standard deviation for our toy example\n\n\ndf$age\n\n[1] 28.0 35.5 31.0\n\n\n\n\nmean(df$age)\n\n[1] 31.5\n\nsd(df$age)\n\n[1] 3.774917\n\n\n\n\n\\(s = \\sqrt{\\sum_{i=1}^{n}\\frac{(x_i - \\overline{x})^2}{n-1}} =\\)"
  },
  {
    "objectID": "slides/Day02_bsta511.html#empirical-rule-one-way-to-think-about-the-sd-12",
    "href": "slides/Day02_bsta511.html#empirical-rule-one-way-to-think-about-the-sd-12",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Empirical Rule: one way to think about the SD (1/2)",
    "text": "Empirical Rule: one way to think about the SD (1/2)\n\n\n\nFor symmetric bell-shaped data, about\n\n68% of the data are within 1 SD of the mean\n95% of the data are within 2 SD’s of the mean\n99.7% of the data are within 3 SD’s of the mean\n\nThese percentages are based off of percentages of a true normal distribution.\n\n\n\n\nhttps://statistics-made-easy.com/empirical-rule/"
  },
  {
    "objectID": "slides/Day02_bsta511.html#empirical-rule-one-way-to-think-about-the-sd-22",
    "href": "slides/Day02_bsta511.html#empirical-rule-one-way-to-think-about-the-sd-22",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Empirical Rule: one way to think about the SD (2/2)",
    "text": "Empirical Rule: one way to think about the SD (2/2)\n\n\n\nhist(iris$Sepal.Width)\n\n\n\n\n\n\nmean(iris$Sepal.Width)\n\n[1] 3.057333\n\nsd(iris$Sepal.Width)\n\n[1] 0.4358663"
  },
  {
    "objectID": "slides/Day02_bsta511.html#measures-of-spread-interquartile-range-iqr-12",
    "href": "slides/Day02_bsta511.html#measures-of-spread-interquartile-range-iqr-12",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of spread: interquartile range (IQR) (1/2)",
    "text": "Measures of spread: interquartile range (IQR) (1/2)\nThe \\(p^{th}\\) percentile is the observation such that \\(p\\%\\) of the remaining observations fall below this observation.\n\nThe first quartile \\(Q_1\\) is the \\(25^{th}\\) percentile.\nThe second quartile \\(Q_2\\), i.e., the median, is the \\(50^{th}\\) percentile.\nThe third quartile \\(Q_3\\) is the \\(75^{th}\\) percentile.\n\nThe interquartile range (IQR) is the distance between the third and first quartiles. \\[IQR = Q_3 - Q_1\\]\n\nIQR is the width of the middle half of the data"
  },
  {
    "objectID": "slides/Day02_bsta511.html#measures-of-spread-iqr-22",
    "href": "slides/Day02_bsta511.html#measures-of-spread-iqr-22",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of spread: IQR (2/2)",
    "text": "Measures of spread: IQR (2/2)\n5 number summary\n\nsummary(iris$Sepal.Width)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.000   2.800   3.000   3.057   3.300   4.400 \n\n\n\n\n\n\n\n\n\n\nWhat is the IQR of the sepal widths?\n\nquantile(iris$Sepal.Width, c(.25, .75))\n\n25% 75% \n2.8 3.3 \n\ndiff(quantile(iris$Sepal.Width, c(.25, .75)))\n\n75% \n0.5 \n\nIQR(iris$Sepal.Width)\n\n[1] 0.5"
  },
  {
    "objectID": "slides/Day02_bsta511.html#robust-estimates",
    "href": "slides/Day02_bsta511.html#robust-estimates",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Robust estimates",
    "text": "Robust estimates\nSummary statistics are called robust estimates if extreme observations have little effect on their values\n\n\n\nestimate\nrobust?\n\n\n\n\nmean\n\n\n\nmedian\n\n\n\nmode\n\n\n\nstandard deviaiton\n\n\n\nIQR\n\n\n\nrange"
  },
  {
    "objectID": "slides/Day02_bsta511.html#r-packages",
    "href": "slides/Day02_bsta511.html#r-packages",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "R Packages",
    "text": "R Packages\nA good analogy for R packages is that they\nare like apps you can download onto a mobile phone:\n\nModernDive Figure 1.4"
  },
  {
    "objectID": "slides/Day02_bsta511.html#installing-packages",
    "href": "slides/Day02_bsta511.html#installing-packages",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Installing packages",
    "text": "Installing packages\n\n\nPackages contain additional functions and data\n\nTwo options to install packages:\n\ninstall.packages() or\nThe “Packages” tab in Files/Plots/Packages/Help/Viewer window\n\n\n\ninstall.packages(\"dplyr\")   # only do this ONCE, use quotes\n\n\n\n\n\nOnly install packages once (unless you want to update them)\nInstalled from Comprehensive R Archive Network (CRAN) = package mothership"
  },
  {
    "objectID": "slides/Day02_bsta511.html#video-on-installing-packages",
    "href": "slides/Day02_bsta511.html#video-on-installing-packages",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Video on installing packages",
    "text": "Video on installing packages\n\nDanielle Navarro’s YouTube video on Installing and loading R packages: https://www.youtube.com/watch?v=kpHZVyDvEhQ"
  },
  {
    "objectID": "slides/Day02_bsta511.html#load-packages-with-library-command",
    "href": "slides/Day02_bsta511.html#load-packages-with-library-command",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Load packages with library() command",
    "text": "Load packages with library() command\n\n\nTip: at the top of your Rmd file, create a chunk that loads all of the R packages you want to use in that file.\nUse the library() command to load each required package.\nPackages need to be reloaded every time you open Rstudio.\n\n\n\nlibrary(dplyr)    # run this every time you open Rstudio\n\n\n\nYou can use a function without loading the package with PackageName::CommandName\n\n\n\ndplyr::arrange(iris, Petal.Width)   # what does arrange do?\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1            4.9         3.1          1.5         0.1     setosa\n2            4.8         3.0          1.4         0.1     setosa\n3            4.3         3.0          1.1         0.1     setosa\n4            5.2         4.1          1.5         0.1     setosa\n5            4.9         3.6          1.4         0.1     setosa\n6            5.1         3.5          1.4         0.2     setosa\n7            4.9         3.0          1.4         0.2     setosa\n8            4.7         3.2          1.3         0.2     setosa\n9            4.6         3.1          1.5         0.2     setosa\n10           5.0         3.6          1.4         0.2     setosa\n11           5.0         3.4          1.5         0.2     setosa\n12           4.4         2.9          1.4         0.2     setosa\n13           5.4         3.7          1.5         0.2     setosa\n14           4.8         3.4          1.6         0.2     setosa\n15           5.8         4.0          1.2         0.2     setosa\n16           5.4         3.4          1.7         0.2     setosa\n17           4.6         3.6          1.0         0.2     setosa\n18           4.8         3.4          1.9         0.2     setosa\n19           5.0         3.0          1.6         0.2     setosa\n20           5.2         3.5          1.5         0.2     setosa\n21           5.2         3.4          1.4         0.2     setosa\n22           4.7         3.2          1.6         0.2     setosa\n23           4.8         3.1          1.6         0.2     setosa\n24           5.5         4.2          1.4         0.2     setosa\n25           4.9         3.1          1.5         0.2     setosa\n26           5.0         3.2          1.2         0.2     setosa\n27           5.5         3.5          1.3         0.2     setosa\n28           4.4         3.0          1.3         0.2     setosa\n29           5.1         3.4          1.5         0.2     setosa\n30           4.4         3.2          1.3         0.2     setosa\n31           5.1         3.8          1.6         0.2     setosa\n32           4.6         3.2          1.4         0.2     setosa\n33           5.3         3.7          1.5         0.2     setosa\n34           5.0         3.3          1.4         0.2     setosa\n35           4.6         3.4          1.4         0.3     setosa\n36           5.1         3.5          1.4         0.3     setosa\n37           5.7         3.8          1.7         0.3     setosa\n38           5.1         3.8          1.5         0.3     setosa\n39           5.0         3.5          1.3         0.3     setosa\n40           4.5         2.3          1.3         0.3     setosa\n41           4.8         3.0          1.4         0.3     setosa\n42           5.4         3.9          1.7         0.4     setosa\n43           5.7         4.4          1.5         0.4     setosa\n44           5.4         3.9          1.3         0.4     setosa\n45           5.1         3.7          1.5         0.4     setosa\n46           5.0         3.4          1.6         0.4     setosa\n47           5.4         3.4          1.5         0.4     setosa\n48           5.1         3.8          1.9         0.4     setosa\n49           5.1         3.3          1.7         0.5     setosa\n50           5.0         3.5          1.6         0.6     setosa\n51           4.9         2.4          3.3         1.0 versicolor\n52           5.0         2.0          3.5         1.0 versicolor\n53           6.0         2.2          4.0         1.0 versicolor\n54           5.8         2.7          4.1         1.0 versicolor\n55           5.7         2.6          3.5         1.0 versicolor\n56           5.5         2.4          3.7         1.0 versicolor\n57           5.0         2.3          3.3         1.0 versicolor\n58           5.6         2.5          3.9         1.1 versicolor\n59           5.5         2.4          3.8         1.1 versicolor\n60           5.1         2.5          3.0         1.1 versicolor\n61           6.1         2.8          4.7         1.2 versicolor\n62           5.8         2.7          3.9         1.2 versicolor\n63           5.5         2.6          4.4         1.2 versicolor\n64           5.8         2.6          4.0         1.2 versicolor\n65           5.7         3.0          4.2         1.2 versicolor\n66           5.5         2.3          4.0         1.3 versicolor\n67           5.7         2.8          4.5         1.3 versicolor\n68           6.6         2.9          4.6         1.3 versicolor\n69           5.6         2.9          3.6         1.3 versicolor\n70           6.1         2.8          4.0         1.3 versicolor\n71           6.4         2.9          4.3         1.3 versicolor\n72           6.3         2.3          4.4         1.3 versicolor\n73           5.6         3.0          4.1         1.3 versicolor\n74           5.5         2.5          4.0         1.3 versicolor\n75           5.6         2.7          4.2         1.3 versicolor\n76           5.7         2.9          4.2         1.3 versicolor\n77           6.2         2.9          4.3         1.3 versicolor\n78           5.7         2.8          4.1         1.3 versicolor\n79           7.0         3.2          4.7         1.4 versicolor\n80           5.2         2.7          3.9         1.4 versicolor\n81           6.1         2.9          4.7         1.4 versicolor\n82           6.7         3.1          4.4         1.4 versicolor\n83           6.6         3.0          4.4         1.4 versicolor\n84           6.8         2.8          4.8         1.4 versicolor\n85           6.1         3.0          4.6         1.4 versicolor\n86           6.1         2.6          5.6         1.4  virginica\n87           6.4         3.2          4.5         1.5 versicolor\n88           6.9         3.1          4.9         1.5 versicolor\n89           6.5         2.8          4.6         1.5 versicolor\n90           5.9         3.0          4.2         1.5 versicolor\n91           5.6         3.0          4.5         1.5 versicolor\n92           6.2         2.2          4.5         1.5 versicolor\n93           6.3         2.5          4.9         1.5 versicolor\n94           6.0         2.9          4.5         1.5 versicolor\n95           5.4         3.0          4.5         1.5 versicolor\n96           6.7         3.1          4.7         1.5 versicolor\n97           6.0         2.2          5.0         1.5  virginica\n98           6.3         2.8          5.1         1.5  virginica\n99           6.3         3.3          4.7         1.6 versicolor\n100          6.0         2.7          5.1         1.6 versicolor\n101          6.0         3.4          4.5         1.6 versicolor\n102          7.2         3.0          5.8         1.6  virginica\n103          6.7         3.0          5.0         1.7 versicolor\n104          4.9         2.5          4.5         1.7  virginica\n105          5.9         3.2          4.8         1.8 versicolor\n106          6.3         2.9          5.6         1.8  virginica\n107          7.3         2.9          6.3         1.8  virginica\n108          6.7         2.5          5.8         1.8  virginica\n109          6.5         3.0          5.5         1.8  virginica\n110          6.3         2.7          4.9         1.8  virginica\n111          7.2         3.2          6.0         1.8  virginica\n112          6.2         2.8          4.8         1.8  virginica\n113          6.1         3.0          4.9         1.8  virginica\n114          6.4         3.1          5.5         1.8  virginica\n115          6.0         3.0          4.8         1.8  virginica\n116          5.9         3.0          5.1         1.8  virginica\n117          5.8         2.7          5.1         1.9  virginica\n118          6.4         2.7          5.3         1.9  virginica\n119          7.4         2.8          6.1         1.9  virginica\n120          5.8         2.7          5.1         1.9  virginica\n121          6.3         2.5          5.0         1.9  virginica\n122          6.5         3.2          5.1         2.0  virginica\n123          5.7         2.5          5.0         2.0  virginica\n124          5.6         2.8          4.9         2.0  virginica\n125          7.7         2.8          6.7         2.0  virginica\n126          7.9         3.8          6.4         2.0  virginica\n127          6.5         3.0          5.2         2.0  virginica\n128          7.1         3.0          5.9         2.1  virginica\n129          7.6         3.0          6.6         2.1  virginica\n130          6.8         3.0          5.5         2.1  virginica\n131          6.7         3.3          5.7         2.1  virginica\n132          6.4         2.8          5.6         2.1  virginica\n133          6.9         3.1          5.4         2.1  virginica\n134          6.5         3.0          5.8         2.2  virginica\n135          7.7         3.8          6.7         2.2  virginica\n136          6.4         2.8          5.6         2.2  virginica\n137          6.4         3.2          5.3         2.3  virginica\n138          7.7         2.6          6.9         2.3  virginica\n139          6.9         3.2          5.7         2.3  virginica\n140          7.7         3.0          6.1         2.3  virginica\n141          6.9         3.1          5.1         2.3  virginica\n142          6.8         3.2          5.9         2.3  virginica\n143          6.7         3.0          5.2         2.3  virginica\n144          6.2         3.4          5.4         2.3  virginica\n145          5.8         2.8          5.1         2.4  virginica\n146          6.3         3.4          5.6         2.4  virginica\n147          6.7         3.1          5.6         2.4  virginica\n148          6.3         3.3          6.0         2.5  virginica\n149          7.2         3.6          6.1         2.5  virginica\n150          6.7         3.3          5.7         2.5  virginica"
  },
  {
    "objectID": "slides/Day02_bsta511.html#install-the-packages-listed-below-before-day-3",
    "href": "slides/Day02_bsta511.html#install-the-packages-listed-below-before-day-3",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Install the packages listed below before Day 3",
    "text": "Install the packages listed below before Day 3\n\n\nknitr\n\nthis might actually already be installed\ncheck your packages list\n\ntidyverse\n\nthis is actually a bundle of packages\nWarning: it will take a while to install!!!\nsee more info at https://tidyverse.tidyverse.org/\n\nrstatix\n\nfor summary statistics of a dataset\n\njanitor\n\nfor cleaning and exploring data\n\nggridges\n\nfor creating ridgeline plots\n\ndevtools\n\nused to create R packages\nfor our purposes, needed to install some packages\n\noi_biostat_data\n\nthis package is on github\nsee the next slide for directions on how to install oi_biostat_data"
  },
  {
    "objectID": "slides/Day02_bsta511.html#directions-for-installing-package-oibiostat",
    "href": "slides/Day02_bsta511.html#directions-for-installing-package-oibiostat",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Directions for installing package oibiostat",
    "text": "Directions for installing package oibiostat\n\n\nThe textbook’s datasets are in the R package oibiostat\nExplanation of code below\n\nInstallation of oibiostat package requires first installing devtools package\nThe code devtools::install_github() tells R to use the command install_github() from the devtools package without loading the entire package and all of its commands (which library(devtools) would do).\n\n\n\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"OI-Biostat/oi_biostat_data\", force = TRUE)\n\n\n\nAfter running the code above, put # in front of the commands so that RStudio doesn’t evaluate them when rendering.\nNow load the oibiostat package\n\nthe code below needs to be run every time you restart R or knit an Rmd file\n\n\n\n\nlibrary(oibiostat)"
  },
  {
    "objectID": "slides/Day05_bsta511.html",
    "href": "slides/Day05_bsta511.html",
    "title": "Day 4 (F23 Day 5): Probability Part 2 - Bayes’ Rule",
    "section": "",
    "text": "Download slides\n\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "slides/Day03_bsta511.html#goals-for-today",
    "href": "slides/Day03_bsta511.html#goals-for-today",
    "title": "Day 3: Data visualization",
    "section": "Goals for today",
    "text": "Goals for today\n\n\n\nExploratory Data Analysis (EDA) (Sections 1.4, 1.5, 1.6, 1.7.1)\n\nData visualization with ggplot\n\nnumerical & categorical variables, and relationships between variables\n\nSummarizing numerical data\nFrequency (two-way) tables\n\nSome data wrangling techniques along the way\n\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "slides/Day03_bsta511.html#international-day-of-women-in-statistics-and-data-science",
    "href": "slides/Day03_bsta511.html#international-day-of-women-in-statistics-and-data-science",
    "title": "Day 3: Data visualization",
    "section": "International Day of Women in Statistics and Data Science",
    "text": "International Day of Women in Statistics and Data Science\nTuesday, October 8, 2024\n12 am - 11:59 pm UTC (5pm 10/7 to 4:59 pm 10/8 here)\n\nInternational Day of Women in Statistics and Data Science"
  },
  {
    "objectID": "slides/Day03_bsta511.html#mimis-tip-of-the-day-sending-messages-in-slack",
    "href": "slides/Day03_bsta511.html#mimis-tip-of-the-day-sending-messages-in-slack",
    "title": "Day 3: Data visualization",
    "section": "Mimi’s tip of the day: sending messages in Slack",
    "text": "Mimi’s tip of the day: sending messages in Slack\n\n\nAre you frustrated that Slack sends a message when you press Enter? You can change that!"
  },
  {
    "objectID": "slides/Day03_bsta511.html#recap-of-last-time",
    "href": "slides/Day03_bsta511.html#recap-of-last-time",
    "title": "Day 3: Data visualization",
    "section": "Recap of last time",
    "text": "Recap of last time\n\n(1.3) Data collection principles\n\nPopulation vs. sample\nSampling methods\nExperiments vs. Observational studies\n\n(1.2) Intro to Data\n\nData types\n\nNumerical: discrete (integer in R), continuous (double or numeric in R)\nCategorical: ordinal, nominal\n\ncharacter or factor in R\n\n\nHow are data stored in R? data frames, tibbles\nWorking with data in R: dim(), nrow(), ncol(), names(), str(), summary(), head(), tail(), $\n\n(1.4) Summarizing numerical data\n\nmean(), median(), sd(), quantile()"
  },
  {
    "objectID": "slides/Day03_bsta511.html#from-last-time-install-the-pacakges-listed-below",
    "href": "slides/Day03_bsta511.html#from-last-time-install-the-pacakges-listed-below",
    "title": "Day 3: Data visualization",
    "section": "From last time: Install the pacakges listed below",
    "text": "From last time: Install the pacakges listed below\n\n\nknitr\n\nthis might actually already be installed\ncheck your packages list\n\ntidyverse\n\nthis is actually a bundle of packages\nWarning: it will take a while to install!!!\nsee more info at https://tidyverse.tidyverse.org/\n\nrstatix\n\nfor summary statistics of a dataset\n\njanitor\n\nfor cleaning and exploring data\n\nggridges\n\nfor creating ridgeline plots\n\ndevtools\n\nused to create R packages\nfor our purposes, needed to install some packages\n\noi_biostat_data\n\nthis package is on github\nsee the next slide for directions on how to install oi_biostat_data"
  },
  {
    "objectID": "slides/Day03_bsta511.html#directions-for-installing-package-oibiostat",
    "href": "slides/Day03_bsta511.html#directions-for-installing-package-oibiostat",
    "title": "Day 3: Data visualization",
    "section": "Directions for installing package oibiostat",
    "text": "Directions for installing package oibiostat\n\n\nThe textbook’s datasets are in the R package oibiostat\nExplanation of code below\n\nInstallation of oibiostat package requires first installing devtools package\nThe code devtools::install_github() tells R to use the command install_github() from the devtools package without loading the entire package and all of its commands (which library(devtools) would do).\n\n\n\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"OI-Biostat/oi_biostat_data\", force = TRUE)\n\n\n\nAfter running the code above, put # in front of the commands so that RStudio doesn’t evaluate them when rendering.\nNow load the oibiostat package\n\nthe code below needs to be run every time you restart R or render a Qmd file\n\n\n\n\nlibrary(oibiostat)"
  },
  {
    "objectID": "slides/Day03_bsta511.html#load-packages-with-library-command",
    "href": "slides/Day03_bsta511.html#load-packages-with-library-command",
    "title": "Day 3: Data visualization",
    "section": "Load packages with library() command",
    "text": "Load packages with library() command\n\n\nTip: at the top of your Qmd file, create a chunk that loads all of the R packages you want to use in that file.\nUse the library() command to load each required package.\n\nPackages need to be reloaded every time you open Rstudio.\nlibrary() commands to load needed packages must be in the Qmd file\n\n\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(ggridges)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary) # NEW!!\n\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides/Day03_bsta511.html#case-study-description",
    "href": "slides/Day03_bsta511.html#case-study-description",
    "title": "Day 3: Data visualization",
    "section": "Case Study Description",
    "text": "Case Study Description\n\n\nIn the US, individuals with developmental disabilities typically receive services and support from state governments.\n\nCalifornia allocates funds to developmentally disabled residents through the Department of Developmental Services (DDS)\nRecipients of DDS funds are referred to as “consumers.”\n\nDataset dds.discr\n\nsample of 1,000 DDS consumers (out of a total of ~ 250,000)\ndata include age, gender, race/ethnicity, and annual DDS financial support per consumer\n\nPrevious research\n\nResearchers examined expenditures on consumers by ethnicity\nFound that the mean annual expenditure on Hispanics was less than that on White non-Hispanics.\n\nResult: an allegation of ethnic discrimination was brought against the California DDS.\nQuestion: Are the data sufficient evidence of ethnic discrimination?\nSee Section 1.7.1 in the textbook for more details"
  },
  {
    "objectID": "slides/Day03_bsta511.html#load-dds.discr-dataset-from-oibiostat-package",
    "href": "slides/Day03_bsta511.html#load-dds.discr-dataset-from-oibiostat-package",
    "title": "Day 3: Data visualization",
    "section": "Load dds.discr dataset from oibiostat package",
    "text": "Load dds.discr dataset from oibiostat package\n\n\nThe textbook’s datasets are in the R package oibiostat\nMake sure the oibiostat package is installed before running the code below.\nLoad the oibiostat package and the dataset dds.discr\n\nthe code below needs to be run every time you restart R or render a Qmd file\n\nlibrary(oibiostat)\ndata(\"dds.discr\")\n\n\nAfter loading the dataset dds.discr using data(\"dds.discr\"), you will see dds.discr in the Data list of the Environment window."
  },
  {
    "objectID": "slides/Day03_bsta511.html#getting-to-know-the-dataset",
    "href": "slides/Day03_bsta511.html#getting-to-know-the-dataset",
    "title": "Day 3: Data visualization",
    "section": "Getting to know the dataset",
    "text": "Getting to know the dataset\n\ndim(dds.discr)\n\n[1] 1000    6\n\nnames(dds.discr)\n\n[1] \"id\"           \"age.cohort\"   \"age\"          \"gender\"       \"expenditures\"\n[6] \"ethnicity\"   \n\nlength(unique(dds.discr$id)) # How many unique id's are there?\n\n[1] 1000"
  },
  {
    "objectID": "slides/Day03_bsta511.html#str-structure",
    "href": "slides/Day03_bsta511.html#str-structure",
    "title": "Day 3: Data visualization",
    "section": "str() structure",
    "text": "str() structure\n\n\nWe previously used the base R structure command str() to get information about variable types in a dataset.\nNote this dataset is a tibble instead of a data.frame\n\n\n\nstr(dds.discr)      # base R\n\ntibble [1,000 × 6] (S3: tbl_df/tbl/data.frame)\n $ id          : int [1:1000] 10210 10409 10486 10538 10568 10690 10711 10778 10820 10823 ...\n $ age.cohort  : Factor w/ 6 levels \"0-5\",\"6-12\",\"13-17\",..: 3 5 1 4 3 3 3 3 3 3 ...\n $ age         : int [1:1000] 17 37 3 19 13 15 13 17 14 13 ...\n $ gender      : Factor w/ 2 levels \"Female\",\"Male\": 1 2 2 1 2 1 1 2 1 2 ...\n $ expenditures: int [1:1000] 2113 41924 1454 6400 4412 4566 3915 3873 5021 2887 ...\n $ ethnicity   : Factor w/ 8 levels \"American Indian\",..: 8 8 4 4 8 4 8 3 8 4 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   ID = col_integer(),\n  ..   `Age Cohort` = col_character(),\n  ..   Age = col_integer(),\n  ..   Gender = col_character(),\n  ..   Expenditures = col_integer(),\n  ..   Ethnicity = col_character()\n  .. )"
  },
  {
    "objectID": "slides/Day03_bsta511.html#glimpse",
    "href": "slides/Day03_bsta511.html#glimpse",
    "title": "Day 3: Data visualization",
    "section": "glimpse()",
    "text": "glimpse()\n\nNew: glimpse()\n\nUse glimpse() from the tidyverse package (technically it’s from the dplyr package) to get information about variable types.\nglimpse() tends to have nicer output for tibbles than str()\n\n\n\nlibrary(tidyverse)\nglimpse(dds.discr)  # from tidyverse package (dplyr)\n\nRows: 1,000\nColumns: 6\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ gender       &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ ethnicity    &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…"
  },
  {
    "objectID": "slides/Day03_bsta511.html#summary",
    "href": "slides/Day03_bsta511.html#summary",
    "title": "Day 3: Data visualization",
    "section": "summary()",
    "text": "summary()\n\n\nWe previously used the base R structure command summary() to get summary information about variables\n\n\n\nsummary(dds.discr)      # base R\n\n       id        age.cohort       age          gender     expenditures  \n Min.   :10210   0-5  : 82   Min.   : 0.0   Female:503   Min.   :  222  \n 1st Qu.:31809   6-12 :175   1st Qu.:12.0   Male  :497   1st Qu.: 2899  \n Median :55384   13-17:212   Median :18.0                Median : 7026  \n Mean   :54663   18-21:199   Mean   :22.8                Mean   :18066  \n 3rd Qu.:76135   22-50:226   3rd Qu.:26.0                3rd Qu.:37713  \n Max.   :99898   51+  :106   Max.   :95.0                Max.   :75098  \n                                                                        \n              ethnicity  \n White not Hispanic:401  \n Hispanic          :376  \n Asian             :129  \n Black             : 59  \n Multi Race        : 26  \n American Indian   :  4  \n (Other)           :  5"
  },
  {
    "objectID": "slides/Day03_bsta511.html#tbl_summary-summary-table",
    "href": "slides/Day03_bsta511.html#tbl_summary-summary-table",
    "title": "Day 3: Data visualization",
    "section": "tbl_summary(): summary table",
    "text": "tbl_summary(): summary table\n\n\n\n\nNew: Use tbl_summary() from the gtsummary package to get summary information\n\n\n\n# library(gtsummary)\ntbl_summary(dds.discr)"
  },
  {
    "objectID": "slides/Day03_bsta511.html#what-data-variables-are-included-in-the-plot-below",
    "href": "slides/Day03_bsta511.html#what-data-variables-are-included-in-the-plot-below",
    "title": "Day 3: Data visualization",
    "section": "What data (variables) are included in the plot below?",
    "text": "What data (variables) are included in the plot below?"
  },
  {
    "objectID": "slides/Day03_bsta511.html#basics-of-a-ggplot",
    "href": "slides/Day03_bsta511.html#basics-of-a-ggplot",
    "title": "Day 3: Data visualization",
    "section": "Basics of a ggplot",
    "text": "Basics of a ggplot"
  },
  {
    "objectID": "slides/Day03_bsta511.html#grammar-of-ggplot2",
    "href": "slides/Day03_bsta511.html#grammar-of-ggplot2",
    "title": "Day 3: Data visualization",
    "section": "Grammar of ggplot2",
    "text": "Grammar of ggplot2\n\nKieran Healy"
  },
  {
    "objectID": "slides/Day03_bsta511.html#histograms",
    "href": "slides/Day03_bsta511.html#histograms",
    "title": "Day 3: Data visualization",
    "section": "Histograms",
    "text": "Histograms\nWhat is being measured on the vertical axes?\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram() \n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = expenditures)) +\n  geom_histogram()"
  },
  {
    "objectID": "slides/Day03_bsta511.html#histograms-showing-proportions",
    "href": "slides/Day03_bsta511.html#histograms-showing-proportions",
    "title": "Day 3: Data visualization",
    "section": "Histograms showing proportions",
    "text": "Histograms showing proportions\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram(\n    aes(y = stat(density)))  \n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram(\n    aes(y = stat(density))) +  \n  scale_y_continuous(labels =   \n      scales::percent_format())"
  },
  {
    "objectID": "slides/Day03_bsta511.html#distribution-shapes",
    "href": "slides/Day03_bsta511.html#distribution-shapes",
    "title": "Day 3: Data visualization",
    "section": "Distribution shapes",
    "text": "Distribution shapes"
  },
  {
    "objectID": "slides/Day03_bsta511.html#density-plots",
    "href": "slides/Day03_bsta511.html#density-plots",
    "title": "Day 3: Data visualization",
    "section": "Density plots",
    "text": "Density plots\nWhat is being measured on the vertical axes?\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_density() \n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram()"
  },
  {
    "objectID": "slides/Day03_bsta511.html#dot-plots",
    "href": "slides/Day03_bsta511.html#dot-plots",
    "title": "Day 3: Data visualization",
    "section": "Dot plots",
    "text": "Dot plots\n\n\nBetter for smaller samples\nWhat is being measured on the vertical axes?\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_dotplot(binwidth =1) \n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram(binwidth =1)"
  },
  {
    "objectID": "slides/Day03_bsta511.html#boxplots",
    "href": "slides/Day03_bsta511.html#boxplots",
    "title": "Day 3: Data visualization",
    "section": "Boxplots",
    "text": "Boxplots\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) + \n  geom_boxplot() \n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(y = age)) + \n  geom_boxplot()"
  },
  {
    "objectID": "slides/Day03_bsta511.html#boxplots-5-number-summary-visualization",
    "href": "slides/Day03_bsta511.html#boxplots-5-number-summary-visualization",
    "title": "Day 3: Data visualization",
    "section": "Boxplots: 5 number summary visualization",
    "text": "Boxplots: 5 number summary visualization\n\nNo outliers: \nWith outliers:"
  },
  {
    "objectID": "slides/Day03_bsta511.html#side-by-side-boxplots",
    "href": "slides/Day03_bsta511.html#side-by-side-boxplots",
    "title": "Day 3: Data visualization",
    "section": "Side-by-side boxplots",
    "text": "Side-by-side boxplots\n\n\n\nggplot(data = dds.discr, \n       aes(x = expenditures,\n           y = ethnicity)) + \n  geom_boxplot() + \n  labs(x = \"Annual Expenditures ($)\", \n       y = \"Race and ethnicity\")  \n\n\nCan you determine the following using boxplots?\n\ndistribution shape\nsample size"
  },
  {
    "objectID": "slides/Day03_bsta511.html#side-by-side-boxplots-with-data-points",
    "href": "slides/Day03_bsta511.html#side-by-side-boxplots-with-data-points",
    "title": "Day 3: Data visualization",
    "section": "Side-by-side boxplots with data points",
    "text": "Side-by-side boxplots with data points\n\n\n\nggplot(data = dds.discr, \n       aes(x = expenditures,\n           y = ethnicity)) + \n  geom_boxplot(color=\"darkgrey\") + \n  labs(x = \"Annual Expenditures ($)\",\n       y = \"Race and ethnicity\") +\n  geom_jitter(     \n    aes(color = ethnicity),      \n    alpha = 0.3,      \n    show.legend = FALSE,      \n    position = position_jitter(     \n      height = 0.4))      \n\n\nCan you determine the following using boxplots?\n\ndistribution shape\nsample size"
  },
  {
    "objectID": "slides/Day03_bsta511.html#density-plots-by-group",
    "href": "slides/Day03_bsta511.html#density-plots-by-group",
    "title": "Day 3: Data visualization",
    "section": "Density plots by group",
    "text": "Density plots by group\n\nggplot(data = dds.discr, \n       aes(x = expenditures,\n           color = ethnicity)) + \n  geom_density() + \n  labs(x = \"Annual Expenditures ($)\")"
  },
  {
    "objectID": "slides/Day03_bsta511.html#ridgeline-plot",
    "href": "slides/Day03_bsta511.html#ridgeline-plot",
    "title": "Day 3: Data visualization",
    "section": "Ridgeline plot",
    "text": "Ridgeline plot\n\n\n\n# library(ggridges)\nggplot(data = dds.discr,\n       aes(x = expenditures,\n           y = ethnicity,      \n           fill = ethnicity)      \n       ) + \n  geom_density_ridges(      \n    alpha = 0.3,      \n    show.legend = FALSE) +      \n  labs(x = \"Annual Expenditures ($)\",\n       y = \"Race and ethnicity\",\n       title =        \n\"Expenditures by race and \n       \\nethnicity\")"
  },
  {
    "objectID": "slides/Day03_bsta511.html#transforming-data-1.4.5",
    "href": "slides/Day03_bsta511.html#transforming-data-1.4.5",
    "title": "Day 3: Data visualization",
    "section": "Transforming data (1.4.5)",
    "text": "Transforming data (1.4.5)\n\n\nWe sometimes apply a transformation to highly skewed data to make it more symmetric\nLog transformations are often used for skewed right data\n\n\n\n\nx = expenditures\n\nggplot(data = dds.discr, \n       aes(x = expenditures)) +  \n  geom_density() \n\n\n\n\n\nx = log(expenditures)\n\nggplot(data = dds.discr, \n       aes(x = log(expenditures))) +  \n  geom_density()"
  },
  {
    "objectID": "slides/Day03_bsta511.html#scatterplots",
    "href": "slides/Day03_bsta511.html#scatterplots",
    "title": "Day 3: Data visualization",
    "section": "Scatterplots",
    "text": "Scatterplots\n\n\n\nggplot(data = dds.discr, \n       aes(x = age,\n           y = expenditures)) + \n  geom_point() +       \n  labs(x = \"Age\",\n       y = \"Annual Expenditures ($)\") \n\n\nResponse vs. explanatory variables (Section 1.2.3)\n\nA response variable measures the outcome of interest in a study\nA study will typically examine whether the values of a response variable differ as values of an explanatory variable change\n\n\n\n\n\n\n\n\n\n\n\nDescribe the association between the variables"
  },
  {
    "objectID": "slides/Day03_bsta511.html#describing-associations-between-2-numerical-variables",
    "href": "slides/Day03_bsta511.html#describing-associations-between-2-numerical-variables",
    "title": "Day 3: Data visualization",
    "section": "Describing associations between 2 numerical variables",
    "text": "Describing associations between 2 numerical variables\n\n\n\nTwo variables \\(x\\) and \\(y\\) are\n\npositively associated if \\(y\\) increases as \\(x\\) increases.\nnegatively associated if \\(y\\) decreases as \\(x\\) increases.\nIf there is no association between the variables, then we say they are uncorrelated or independent.\n\n\n\n\n\n\n\n\n\n\n\n\nThe term “association” is a very general term.\n\nCan be used for numerical or categorical variables\nNot specifically referring to linear associations"
  },
  {
    "objectID": "slides/Day03_bsta511.html#pearson-correlation-coefficient-r",
    "href": "slides/Day03_bsta511.html#pearson-correlation-coefficient-r",
    "title": "Day 3: Data visualization",
    "section": "(Pearson) Correlation coefficient \\(r\\)",
    "text": "(Pearson) Correlation coefficient \\(r\\)\n\n\n\\(r = -1\\) indicates a perfect negative linear relationship: As one variable increases, the value of the other variable tends to go down, following a straight line.\n\\(r = 0\\) indicates no linear relationship: The values of both variables go up/down independently of each other.\n\\(r = 1\\) indicates a perfect positive linear relationship: As the value of one variable goes up, the value of the other variable tends to go up as well in a linear fashion.\nThe closer \\(r\\) is to ±1, the stronger the linear association."
  },
  {
    "objectID": "slides/Day03_bsta511.html#pearson-correlation-coefficient-r-formula",
    "href": "slides/Day03_bsta511.html#pearson-correlation-coefficient-r-formula",
    "title": "Day 3: Data visualization",
    "section": "(Pearson) Correlation coefficient (r): formula",
    "text": "(Pearson) Correlation coefficient (r): formula\n\nThe (Peasron) correlation coefficient of variables \\(x\\) and \\(y\\) can be computed using the formula \\[r = \\frac{1}{n-1}\\sum_{i=1}^{n}\\Big(\\frac{x_i - \\bar{x}}{s_x}\\Big)\\Big(\\frac{y_i - \\bar{y}}{s_y}\\Big)\\] where\n\n\\((x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\) are the \\(n\\) paired values of the variables \\(x\\) and \\(y\\)\n\\(s_x\\) and \\(s_y\\) are the sample standard deviations of the variables \\(x\\) and \\(y\\), respectively\n\n\n\ncor(dds.discr$age, dds.discr$expenditures)\n\n[1] 0.8432422"
  },
  {
    "objectID": "slides/Day03_bsta511.html#guess-the-correlation-game",
    "href": "slides/Day03_bsta511.html#guess-the-correlation-game",
    "title": "Day 3: Data visualization",
    "section": "Guess the correlation game!",
    "text": "Guess the correlation game!\n\n\n\nRossman & Chance’s applet\n\n\n\n\n\nTracks performance of guess vs. actual, error vs. actual, and error vs. trial\nhttp://www.rossmanchance.com/applets/GuessCorrelation.html\n\nOr, for the Atari-like experience\n\n\n\nhttp://guessthecorrelation.com/"
  },
  {
    "objectID": "slides/Day03_bsta511.html#scatterplots-with-color-coded-dots",
    "href": "slides/Day03_bsta511.html#scatterplots-with-color-coded-dots",
    "title": "Day 3: Data visualization",
    "section": "Scatterplots with color-coded dots",
    "text": "Scatterplots with color-coded dots\nDescribe the association between the variables\n\nggplot(data = dds.discr, \n       aes(x = age, y = expenditures,\n           color = ethnicity)) +   \n  geom_point(alpha = .5) +       \n  labs(x = \"Age\", y = \"Annual Expenditures ($)\") +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "slides/Day03_bsta511.html#barplots",
    "href": "slides/Day03_bsta511.html#barplots",
    "title": "Day 3: Data visualization",
    "section": "Barplots",
    "text": "Barplots\n\n\n\nCounts (below) vs.\npercentages (right)\n\n\nggplot(data = dds.discr, \n       aes(x = ethnicity)) +\n  geom_bar() \n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = ethnicity)) +\n  geom_bar(aes(y = stat(prop),  \n               group = 1)) + \n  scale_y_continuous(labels =  \n      scales::percent_format())"
  },
  {
    "objectID": "slides/Day03_bsta511.html#barplots-with-2-variables-segmented-bar-plots",
    "href": "slides/Day03_bsta511.html#barplots-with-2-variables-segmented-bar-plots",
    "title": "Day 3: Data visualization",
    "section": "Barplots with 2 variables: segmented bar plots",
    "text": "Barplots with 2 variables: segmented bar plots\n\n\n\nggplot(data = dds.discr, \n       aes(x = ethnicity,\n           fill = age.cohort)) + \n  geom_bar() \n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = ethnicity,\n           fill = age.cohort)) + \n  geom_bar(position = \"fill\")"
  },
  {
    "objectID": "slides/Day03_bsta511.html#barplots-with-2-variables-side-by-side-bar-plots",
    "href": "slides/Day03_bsta511.html#barplots-with-2-variables-side-by-side-bar-plots",
    "title": "Day 3: Data visualization",
    "section": "Barplots with 2 variables: side-by-side bar plots",
    "text": "Barplots with 2 variables: side-by-side bar plots\n\nggplot(data = dds.discr, \n       aes(x = ethnicity,\n           fill = age.cohort)) + \n  geom_bar(position = \"dodge\")"
  },
  {
    "objectID": "slides/Day03_bsta511.html#frequency-tables-count",
    "href": "slides/Day03_bsta511.html#frequency-tables-count",
    "title": "Day 3: Data visualization",
    "section": "Frequency tables: count()",
    "text": "Frequency tables: count()\n\n\n\n\ncount is from the dplyr package\nthe output is a long tibble, and not a “nice” table\n\n\n\ndds.discr %&gt;% count(ethnicity)\n\n# A tibble: 8 × 2\n  ethnicity              n\n  &lt;fct&gt;              &lt;int&gt;\n1 American Indian        4\n2 Asian                129\n3 Black                 59\n4 Hispanic             376\n5 Multi Race            26\n6 Native Hawaiian        3\n7 Other                  2\n8 White not Hispanic   401\n\n\n\n\ndds.discr %&gt;% \n  count(ethnicity, age.cohort)\n\n# A tibble: 35 × 3\n   ethnicity       age.cohort     n\n   &lt;fct&gt;           &lt;fct&gt;      &lt;int&gt;\n 1 American Indian 13-17          1\n 2 American Indian 22-50          1\n 3 American Indian 51+            2\n 4 Asian           0-5            8\n 5 Asian           6-12          18\n 6 Asian           13-17         20\n 7 Asian           18-21         41\n 8 Asian           22-50         29\n 9 Asian           51+           13\n10 Black           0-5            3\n# ℹ 25 more rows"
  },
  {
    "objectID": "slides/Day03_bsta511.html#how-to-use-the-pipe",
    "href": "slides/Day03_bsta511.html#how-to-use-the-pipe",
    "title": "Day 3: Data visualization",
    "section": "How to use the pipe %>%",
    "text": "How to use the pipe %&gt;%\n\nThe pipe operator %&gt;% strings together commands to be performed sequentially\n\n\ndds.discr %&gt;% head(n=3)      # pronounce %&gt;% as \"then\"\n\n# A tibble: 3 × 6\n     id age.cohort   age gender expenditures ethnicity         \n  &lt;int&gt; &lt;fct&gt;      &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt;             \n1 10210 13-17         17 Female         2113 White not Hispanic\n2 10409 22-50         37 Male          41924 White not Hispanic\n3 10486 0-5            3 Male           1454 Hispanic          \n\n\n\n\nAlways first list the tibble that the commands are being applied to\nCan use multiple pipes to run multiple commands in sequence\n\nWhat does the following code do?\n\n\n\n\ndds.discr %&gt;% head(n=3) %&gt;% summary()"
  },
  {
    "objectID": "slides/Day03_bsta511.html#frequency-tables-janitor-packages-tabyl-function",
    "href": "slides/Day03_bsta511.html#frequency-tables-janitor-packages-tabyl-function",
    "title": "Day 3: Data visualization",
    "section": "Frequency tables: janitor package’s tabyl function",
    "text": "Frequency tables: janitor package’s tabyl function\n\n\n\n# default table\ndds.discr %&gt;% \n  tabyl(ethnicity)  \n\n          ethnicity   n percent\n    American Indian   4   0.004\n              Asian 129   0.129\n              Black  59   0.059\n           Hispanic 376   0.376\n         Multi Race  26   0.026\n    Native Hawaiian   3   0.003\n              Other   2   0.002\n White not Hispanic 401   0.401\n\n\n\nadorn_ your table!\n\ndds.discr %&gt;% \n  tabyl(ethnicity) %&gt;%\n  adorn_totals(\"row\") %&gt;% \n  adorn_pct_formatting(digits=2)  \n\n          ethnicity    n percent\n    American Indian    4   0.40%\n              Asian  129  12.90%\n              Black   59   5.90%\n           Hispanic  376  37.60%\n         Multi Race   26   2.60%\n    Native Hawaiian    3   0.30%\n              Other    2   0.20%\n White not Hispanic  401  40.10%\n              Total 1000 100.00%"
  },
  {
    "objectID": "slides/Day03_bsta511.html#relative-frequency-table",
    "href": "slides/Day03_bsta511.html#relative-frequency-table",
    "title": "Day 3: Data visualization",
    "section": "Relative frequency table",
    "text": "Relative frequency table\n\n\n\n\nA relative frequency table shows proportions (or percentages) instead of counts\nTo the right I removed (deselected) the counts column (n) to create a relative frequency table\n\n\n\n\ndds.discr %&gt;% \n  tabyl(ethnicity) %&gt;%\n  adorn_totals(\"row\") %&gt;% \n  adorn_pct_formatting(digits=2) %&gt;%   \n  select(-n) \n\n          ethnicity percent\n    American Indian   0.40%\n              Asian  12.90%\n              Black   5.90%\n           Hispanic  37.60%\n         Multi Race   2.60%\n    Native Hawaiian   0.30%\n              Other   0.20%\n White not Hispanic  40.10%\n              Total 100.00%"
  },
  {
    "objectID": "slides/Day03_bsta511.html#contingency-tables-two-way-tables",
    "href": "slides/Day03_bsta511.html#contingency-tables-two-way-tables",
    "title": "Day 3: Data visualization",
    "section": "Contingency tables (two-way tables)",
    "text": "Contingency tables (two-way tables)\n\n\n\n\nContingency tables summarize data for two categorical variables\n\nwith each value in the table representing the number of times\na particular combination of outcomes occurs\n\nRow & column totals\nare sometimes called marginal totals\n\n\n\n\ndds.discr %&gt;% \n  tabyl(ethnicity, gender) %&gt;%    \n  adorn_totals(c(\"row\", \"col\"))    \n\n          ethnicity Female Male Total\n    American Indian      3    1     4\n              Asian     61   68   129\n              Black     26   33    59\n           Hispanic    192  184   376\n         Multi Race     13   13    26\n    Native Hawaiian      2    1     3\n              Other      1    1     2\n White not Hispanic    205  196   401\n              Total    503  497  1000"
  },
  {
    "objectID": "slides/Day03_bsta511.html#contingency-tables-with-percentages",
    "href": "slides/Day03_bsta511.html#contingency-tables-with-percentages",
    "title": "Day 3: Data visualization",
    "section": "Contingency tables with percentages",
    "text": "Contingency tables with percentages\n\ndds.discr %&gt;% \n  tabyl(ethnicity, age.cohort) %&gt;%\n  adorn_totals(c(\"row\")) %&gt;%\n  adorn_percentages(\"row\") %&gt;%   \n  adorn_pct_formatting(digits=0) %&gt;%    \n  adorn_ns()    \n\n          ethnicity      0-5      6-12      13-17     18-21     22-50       51+\n    American Indian  0%  (0)  0%   (0)  25%   (1)  0%   (0) 25%   (1) 50%   (2)\n              Asian  6%  (8) 14%  (18)  16%  (20) 32%  (41) 22%  (29) 10%  (13)\n              Black  5%  (3) 19%  (11)  20%  (12) 15%   (9) 29%  (17) 12%   (7)\n           Hispanic 12% (44) 24%  (91)  27% (103) 21%  (78) 11%  (43)  5%  (17)\n         Multi Race 27%  (7) 35%   (9)  27%   (7)  8%   (2)  4%   (1)  0%   (0)\n    Native Hawaiian  0%  (0)  0%   (0)   0%   (0)  0%   (0) 67%   (2) 33%   (1)\n              Other  0%  (0)  0%   (0) 100%   (2)  0%   (0)  0%   (0)  0%   (0)\n White not Hispanic  5% (20) 11%  (46)  17%  (67) 17%  (69) 33% (133) 16%  (66)\n              Total  8% (82) 18% (175)  21% (212) 20% (199) 23% (226) 11% (106)"
  },
  {
    "objectID": "slides/Day03_bsta511.html#mean-annual-dds-expenditures-by-raceethnicity",
    "href": "slides/Day03_bsta511.html#mean-annual-dds-expenditures-by-raceethnicity",
    "title": "Day 3: Data visualization",
    "section": "Mean annual DDS expenditures by race/ethnicity",
    "text": "Mean annual DDS expenditures by race/ethnicity\n\n\n\nmean(dds.discr$expenditures)\n\n[1] 18065.79\n\ndds.discr %&gt;% \n  summarize(\n    ave = mean(expenditures),\n    SD = sd(expenditures),\n    med = median(expenditures))\n\n# A tibble: 1 × 3\n     ave     SD   med\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 18066. 19543.  7026\n\n\n\n\ndds.discr %&gt;% \n  group_by(ethnicity) %&gt;% \n  summarize(\n    ave = mean(expenditures),\n    SD = sd(expenditures),\n    med = median(expenditures))\n\n# A tibble: 8 × 4\n  ethnicity             ave     SD    med\n  &lt;fct&gt;               &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 American Indian    36438. 25694. 41818.\n2 Asian              18392. 19209.  9369 \n3 Black              20885. 20549.  8687 \n4 Hispanic           11066. 15630.  3952 \n5 Multi Race          4457.  7332.  2622 \n6 Native Hawaiian    42782.  6576. 40727 \n7 Other               3316.  1836.  3316.\n8 White not Hispanic 24698. 20604. 15718"
  },
  {
    "objectID": "slides/Day03_bsta511.html#get_summary_stats-from-rstatix-package",
    "href": "slides/Day03_bsta511.html#get_summary_stats-from-rstatix-package",
    "title": "Day 3: Data visualization",
    "section": "get_summary_stats() from rstatix package",
    "text": "get_summary_stats() from rstatix package\n\ndds.discr %&gt;% get_summary_stats()\n\n# A tibble: 3 × 13\n  variable         n   min   max median     q1     q3   iqr    mad   mean     sd\n  &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 id            1000 10210 99898 55384. 31809. 76135. 44326 3.27e4 5.47e4 2.56e4\n2 age           1000     0    95    18     12     26     14 1.04e1 2.28e1 1.85e1\n3 expenditures  1000   222 75098  7026   2899. 37713. 34814 7.76e3 1.81e4 1.95e4\n# ℹ 2 more variables: se &lt;dbl&gt;, ci &lt;dbl&gt;\n\ndds.discr %&gt;% \n  group_by(ethnicity) %&gt;%\n  get_summary_stats(expenditures, type = \"common\")\n\n# A tibble: 8 × 11\n  ethnicity variable     n   min   max median    iqr   mean     sd     se     ci\n  &lt;fct&gt;     &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 American… expendi…     4  3726 58392 41818. 34085. 36438. 25694. 12847. 40885.\n2 Asian     expendi…   129   374 75098  9369  30892  18392. 19209.  1691.  3346.\n3 Black     expendi…    59   240 60808  8687  37987  20885. 20549.  2675.  5355.\n4 Hispanic  expendi…   376   222 65581  3952   7961. 11066. 15630.   806.  1585.\n5 Multi Ra… expendi…    26   669 38619  2622   2060.  4457.  7332.  1438.  2962.\n6 Native H… expendi…     3 37479 50141 40727   6331  42782.  6576.  3797. 16337.\n7 Other     expendi…     2  2018  4615  3316.  1298.  3316.  1836.  1298. 16499.\n8 White no… expendi…   401   340 68890 15718  39157  24698. 20604.  1029.  2023."
  },
  {
    "objectID": "slides/Day03_bsta511.html#how-to-force-all-output-to-be-shown-12",
    "href": "slides/Day03_bsta511.html#how-to-force-all-output-to-be-shown-12",
    "title": "Day 3: Data visualization",
    "section": "How to force all output to be shown? (1/2)",
    "text": "How to force all output to be shown? (1/2)\nUse kable() from the knitr package.\n\ndds.discr %&gt;% get_summary_stats() %&gt;% kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nn\nmin\nmax\nmedian\nq1\nq3\niqr\nmad\nmean\nsd\nse\nci\n\n\n\n\nid\n1000\n10210\n99898\n55384.5\n31808.75\n76134.75\n44326\n32734.325\n54662.85\n25643.673\n810.924\n1591.310\n\n\nage\n1000\n0\n95\n18.0\n12.00\n26.00\n14\n10.378\n22.80\n18.462\n0.584\n1.146\n\n\nexpenditures\n1000\n222\n75098\n7026.0\n2898.75\n37712.75\n34814\n7760.670\n18065.79\n19542.831\n617.999\n1212.724"
  },
  {
    "objectID": "slides/Day03_bsta511.html#how-to-force-all-output-to-be-shown-knitr-22",
    "href": "slides/Day03_bsta511.html#how-to-force-all-output-to-be-shown-knitr-22",
    "title": "Day 3: Data visualization",
    "section": "How to force all output to be shown? knitr (2/2)",
    "text": "How to force all output to be shown? knitr (2/2)\nUse kable() from the knitr package.\n\ndds.discr %&gt;% \n  group_by(ethnicity) %&gt;%\n  get_summary_stats(expenditures, type = \"common\") %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nethnicity\nvariable\nn\nmin\nmax\nmedian\niqr\nmean\nsd\nse\nci\n\n\n\n\nAmerican Indian\nexpenditures\n4\n3726\n58392\n41817.5\n34085.25\n36438.250\n25693.912\n12846.956\n40884.748\n\n\nAsian\nexpenditures\n129\n374\n75098\n9369.0\n30892.00\n18392.372\n19209.225\n1691.278\n3346.482\n\n\nBlack\nexpenditures\n59\n240\n60808\n8687.0\n37987.00\n20884.593\n20549.274\n2675.288\n5355.170\n\n\nHispanic\nexpenditures\n376\n222\n65581\n3952.0\n7961.25\n11065.569\n15629.847\n806.048\n1584.940\n\n\nMulti Race\nexpenditures\n26\n669\n38619\n2622.0\n2059.75\n4456.731\n7332.135\n1437.950\n2961.514\n\n\nNative Hawaiian\nexpenditures\n3\n37479\n50141\n40727.0\n6331.00\n42782.333\n6576.462\n3796.922\n16336.838\n\n\nOther\nexpenditures\n2\n2018\n4615\n3316.5\n1298.50\n3316.500\n1836.356\n1298.500\n16499.007\n\n\nWhite not Hispanic\nexpenditures\n401\n340\n68890\n15718.0\n39157.00\n24697.549\n20604.376\n1028.933\n2022.793"
  },
  {
    "objectID": "slides/Day03_bsta511.html#case-study-discrimination-in-developmental-disability-support-1.7.1",
    "href": "slides/Day03_bsta511.html#case-study-discrimination-in-developmental-disability-support-1.7.1",
    "title": "Day 3: Data visualization",
    "section": "Case study: discrimination in developmental disability support (1.7.1)",
    "text": "Case study: discrimination in developmental disability support (1.7.1)\n\nPrevious research\n\nResearchers examined DDS expenditures for developmentally disabled residents by ethnicity\nFound that the mean annual expenditures on Hispanics was less than that on White non-Hispanics.\n\nResult: an allegation of ethnic discrimination was brought against the California DDS.\nQuestion: Are the data sufficient evidence of ethnic discrimination?"
  },
  {
    "objectID": "slides/Day06_bsta511.html",
    "href": "slides/Day06_bsta511.html",
    "title": "Day 6: Random variables and Binomial distribution",
    "section": "",
    "text": "Download slides\n\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "class_slides.html",
    "href": "class_slides.html",
    "title": "Class Slides",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\n10/2/24\n\n\nDay 1: Intro to R & Rstudio\n\n\nWeek 1\n\n\n\n\n10/7/24\n\n\nDay 2: Data collection & numerical summaries\n\n\nWeek 2\n\n\n\n\n10/9/24\n\n\nDay 3 (F23 Day 4): Probability Part 1\n\n\nWeek 2\n\n\n\n\n10/14/24\n\n\nDay 4 (F23 Day 5): Probability Part 2 - Bayes’ Rule\n\n\nWeek 3\n\n\n\n\n10/16/24\n\n\nDay 3: Data visualization - Part 2\n\n\nWeek 3\n\n\n\n\n10/16/24\n\n\nDay 3: Data visualization\n\n\nWeek 3\n\n\n\n\n10/21/24\n\n\nDay 6: Random variables and Binomial distribution\n\n\nWeek 4\n\n\n\n\n10/23/24\n\n\nDay 7: Normal and Poisson distributions\n\n\nWeek 4\n\n\n\n\n10/28/24\n\n\nDay 8: Variability in estimates\n\n\nWeek 5\n\n\n\n\n11/4/24\n\n\nDay 9: Confidence intervals (4.2)\n\n\nWeek 6\n\n\n\n\n11/6/24\n\n\nDay 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)\n\n\nWeek 6\n\n\n\n\n11/6/24\n\n\nDay 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)\n\n\nWeek 6\n\n\n\n\n11/11/24\n\n\nDay 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)\n\n\nWeek 7\n\n\n\n\n11/13/24\n\n\nDay 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)\n\n\nWeek 7\n\n\n\n\n11/18/24\n\n\nDay 13: Chi-squared tests (Sections 8.3-8.4)\n\n\nWeek 8\n\n\n\n\n11/20/24\n\n\nDay 14: Comparing Means with ANOVA (Section 5.5)\n\n\nWeek 8\n\n\n\n\n11/27/24\n\n\nDay 15: Simple Linear Regression (Sections 6.1-6.2)\n\n\nWeek 9\n\n\n\n\n12/2/24\n\n\nDay 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)\n\n\nWeek 10\n\n\n\n\n12/4/24\n\n\nDay 17: Nonparametric tests - Supplemental material\n\n\nWeek 10\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "weeks.html",
    "href": "weeks.html",
    "title": "Weekly Pages",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\n9/30/24\n\n\nWeek 1\n\n\nIntro to R & RStudio\n\n\n\n\n10/7/24\n\n\nWeek 2\n\n\nData collection & Probability\n\n\n\n\n10/14/24\n\n\nWeek 3\n\n\nBayes’ Rule & EDA\n\n\n\n\n10/21/24\n\n\nWeek 4\n\n\nBayes’ Rule & EDA\n\n\n\n\n10/28/24\n\n\nWeek 5\n\n\nVariability in estimates and Exam 1\n\n\n\n\n11/4/24\n\n\nWeek 6\n\n\nConfidence Intervals and Hypothesis Testing\n\n\n\n\n11/11/24\n\n\nWeek 7\n\n\nInference for difference in means, one proportion, and difference in proportions, and power\n\n\n\n\n11/18/24\n\n\nWeek 8\n\n\nChi-squared tests and ANOVA\n\n\n\n\n11/25/24\n\n\nWeek 9\n\n\nChi-squared tests and ANOVA\n\n\n\n\n12/2/24\n\n\nWeek 10\n\n\nSimple Linear Regression (cont’d) and Nonparametric Tests\n\n\n\n\n12/9/24\n\n\nWeek 11\n\n\nFinals week\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "homework/HW_2_F24_bsta511.html",
    "href": "homework/HW_2_F24_bsta511.html",
    "title": "HW 2: BSTA 511/611 F24",
    "section": "",
    "text": "Due 10/19/24\nDownload the .qmd file for this assignment from https://github.com/niederhausen/BSTA_511_F24/blob/main/homework/HW_2_F24_bsta511.qmd"
  },
  {
    "objectID": "homework/HW_2_F24_bsta511.html#graded-exercises",
    "href": "homework/HW_2_F24_bsta511.html#graded-exercises",
    "title": "HW 2: BSTA 511/611 F24",
    "section": "Graded exercises",
    "text": "Graded exercises\nThe exercises listed below will be graded for this assignment. You are strongly encouraged to complete the entire assignment. You will receive feedback on exercises you turn in that are not being graded.\n\nBook exercises\n\n2.24\n\nNon-book exercise\nR exercises\n\nR1: NHANES - part 1\nR2: NHANES - part 2"
  },
  {
    "objectID": "homework/HW_2_F24_bsta511.html#directions",
    "href": "homework/HW_2_F24_bsta511.html#directions",
    "title": "HW 2: BSTA 511/611 F24",
    "section": "Directions",
    "text": "Directions\n\n\n\n\n\n\nImportant\n\n\n\n\n* Starred exercises in the section Book exercises may be completed by hand (such as on paper or using a tablet) not using Quarto.\nIf you complete this part of the assignment not using Quarto, you will be uploading 3 files on Sakai for this HW: qmd & html files for your R work, and a pdf with your written work.\nIf you are completing the homework on paper, you can use a scanning app, such as Adobe Scan, to create a pdf of your assignment.\nIf you decided to type the solutions to these exercises in this Quarto file, I highly recommend using LaTeX to format the equations.\n\nFor instrctions on creating equations in the Visual editor, check out https://quarto.org/docs/get-started/authoring/rstudio.html#equations.\nYou can see examples of LaTeX formatting in the solutions to HW 1.\nAlso check out examples of LaTeX formatting for statistics created by recent biostats alumn Ariel Weingarten.\nIf you have difficulty rendering the LaTeX equations, I recommend installing and running the R package tinytex. See this website for instructions.\n\n\n\n\n\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file.\n\nUse the assignment .qmd file linked to above as a template for your own assignment.\n\nPlease always use the following naming convention for submitting your files:\n\nLastname_Firstname_HWx.qmd, such as Niederhausen_Meike_HW2.qmd\nLastname_Firstname_HWx.html, such as Niederhausen_Meike_HW2.html\n\nFor each question, make sure to show all of your work. This includes all code and resulting output in the html file to support your answers for exercises requiring work done in R (including any arithmetic calculations).\nFor each question, include a sentence summarizing the answer for that question in the context of the research question.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your Qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_2_F24_bsta511.html#income-and-education-in-us-counties",
    "href": "homework/HW_2_F24_bsta511.html#income-and-education-in-us-counties",
    "title": "HW 2: BSTA 511/611 F24",
    "section": "1.24 Income and education in US counties",
    "text": "1.24 Income and education in US counties"
  },
  {
    "objectID": "homework/HW_2_F24_bsta511.html#mix-and-match",
    "href": "homework/HW_2_F24_bsta511.html#mix-and-match",
    "title": "HW 2: BSTA 511/611 F24",
    "section": "1.28 Mix-and-match",
    "text": "1.28 Mix-and-match"
  },
  {
    "objectID": "homework/HW_2_F24_bsta511.html#associations",
    "href": "homework/HW_2_F24_bsta511.html#associations",
    "title": "HW 2: BSTA 511/611 F24",
    "section": "1.36 Associations",
    "text": "1.36 Associations"
  },
  {
    "objectID": "homework/HW_2_F24_bsta511.html#predisposition-for-thrombosis",
    "href": "homework/HW_2_F24_bsta511.html#predisposition-for-thrombosis",
    "title": "HW 2: BSTA 511/611 F24",
    "section": "* 2.18 Predisposition for thrombosis",
    "text": "* 2.18 Predisposition for thrombosis"
  },
  {
    "objectID": "homework/HW_2_F24_bsta511.html#breast-cancer-and-age",
    "href": "homework/HW_2_F24_bsta511.html#breast-cancer-and-age",
    "title": "HW 2: BSTA 511/611 F24",
    "section": "* 2.24 Breast cancer and age",
    "text": "* 2.24 Breast cancer and age"
  },
  {
    "objectID": "homework/HW_2_F24_bsta511.html#load-all-the-packages-you-need-below-here.",
    "href": "homework/HW_2_F24_bsta511.html#load-all-the-packages-you-need-below-here.",
    "title": "HW 2: BSTA 511/611 F24",
    "section": "!!! Load all the packages you need below here.",
    "text": "!!! Load all the packages you need below here."
  },
  {
    "objectID": "homework/HW_2_F24_bsta511.html#a-dimensions-and-columns",
    "href": "homework/HW_2_F24_bsta511.html#a-dimensions-and-columns",
    "title": "HW 2: BSTA 511/611 F24",
    "section": "a) Dimensions and columns",
    "text": "a) Dimensions and columns\nWhat are the dimensions and column names of the dataset?"
  },
  {
    "objectID": "homework/HW_2_F24_bsta511.html#b-unique-id-identifiers",
    "href": "homework/HW_2_F24_bsta511.html#b-unique-id-identifiers",
    "title": "HW 2: BSTA 511/611 F24",
    "section": "b) Unique ID identifiers",
    "text": "b) Unique ID identifiers\nHow many unique ID identifiers are in the dataset? Compare this to the number of rows in the dataset. What is the explanation for these two different numbers?"
  },
  {
    "objectID": "homework/HW_2_F24_bsta511.html#c-age-distributions",
    "href": "homework/HW_2_F24_bsta511.html#c-age-distributions",
    "title": "HW 2: BSTA 511/611 F24",
    "section": "c) Age distributions",
    "text": "c) Age distributions\nUsing numerical and graphical summaries, describe the distribution of ages among study participants."
  },
  {
    "objectID": "homework/HW_2_F24_bsta511.html#d-height-distributions",
    "href": "homework/HW_2_F24_bsta511.html#d-height-distributions",
    "title": "HW 2: BSTA 511/611 F24",
    "section": "d) Height distributions",
    "text": "d) Height distributions\nUsing numerical and graphical summaries, describe the distribution of heights among study participants."
  },
  {
    "objectID": "homework/HW_2_F24_bsta511.html#e-adult-height-age",
    "href": "homework/HW_2_F24_bsta511.html#e-adult-height-age",
    "title": "HW 2: BSTA 511/611 F24",
    "section": "e) Adult height age",
    "text": "e) Adult height age\nInvestigate at which age people generally reach their adult height. Is it possible to do the same for weight; why or why not?\nUse whatever EDA tools you think are appropriate to answer this question."
  },
  {
    "objectID": "homework/HW_2_F24_bsta511.html#f-poverty-variable",
    "href": "homework/HW_2_F24_bsta511.html#f-poverty-variable",
    "title": "HW 2: BSTA 511/611 F24",
    "section": "f) Poverty variable",
    "text": "f) Poverty variable\nCalculate the median and interquartile range of the distribution of the variable Poverty. Write a sentence explaining the median and IQR in the context of these data."
  },
  {
    "objectID": "homework/HW_2_F24_bsta511.html#a-heights-in-inches",
    "href": "homework/HW_2_F24_bsta511.html#a-heights-in-inches",
    "title": "HW 2: BSTA 511/611 F24",
    "section": "a) Heights in inches",
    "text": "a) Heights in inches\nUse the mutate() command explained on Slide 23 (pdf pg 22) of the Day 3 Part 3 notes to create a new column (variable) in the NHANES dataset for the heights in inches. Then find the mean and standard deviation of the heights in inches. Note that 1 centimeter is approximately 0.3937 inches.\n\n\n\n\n\n\nTips\n\n\n\n\nColumn names cannot start with a number or symbol\nNames are case sensitive\nIt’s easier to work with column names that don’t have spaces in them. I usually use _ or . instead of spaces."
  },
  {
    "objectID": "homework/HW_2_F24_bsta511.html#b-college-graduates",
    "href": "homework/HW_2_F24_bsta511.html#b-college-graduates",
    "title": "HW 2: BSTA 511/611 F24",
    "section": "b) College graduates",
    "text": "b) College graduates\nWhat proportion of Americans at least 25 years of age are college graduates? Hint: First create a new dataset that is restricted to Americans at least 25 years old."
  },
  {
    "objectID": "homework/HW_2_F24_bsta511.html#c-college-graduates-v2",
    "href": "homework/HW_2_F24_bsta511.html#c-college-graduates-v2",
    "title": "HW 2: BSTA 511/611 F24",
    "section": "c) College graduates v2",
    "text": "c) College graduates v2\nWhat proportion of Americans at least 25 years of age with a high school degree are college graduates?"
  },
  {
    "objectID": "homework/HW_2_F24_bsta511.html#d-two-way-table",
    "href": "homework/HW_2_F24_bsta511.html#d-two-way-table",
    "title": "HW 2: BSTA 511/611 F24",
    "section": "d) Two-way table",
    "text": "d) Two-way table\nConstruct a two-way table (contingency table), with PhysActive as the row variable and Diabetes as the column variable. Among participants who are not physically active, what proportion have diabetes? What proportion of physically active participants have diabetes?"
  },
  {
    "objectID": "homework/HW_2_F24_bsta511.html#e-relative-risk",
    "href": "homework/HW_2_F24_bsta511.html#e-relative-risk",
    "title": "HW 2: BSTA 511/611 F24",
    "section": "e) Relative Risk",
    "text": "e) Relative Risk\nIn this context, relative risk is the ratio of the proportion of participants who have diabetes among those who are not physically active to the proportion of participants with diabetes among those physically active. Relative risks greater than 1 indicate that people who are not physically active seem to be at a higher risk for diabetes than physically active people. Calculate the relative risk of diabetes for the participants. From these calculations, is it possible to conclude that being physically active reduces one’s chance of becoming diabetic?"
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html",
    "href": "homework/HW_1_F24_bsta511.html",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "",
    "text": "Due 10/12/24\nDownload the .qmd file for this assignment from https://github.com/niederhausen/BSTA_511_F24/blob/main/homework/HW_1_F24_bsta511.qmd"
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#graded-exercises",
    "href": "homework/HW_1_F24_bsta511.html#graded-exercises",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "Graded exercises",
    "text": "Graded exercises\nThe exercises listed below will be graded for this assignment. You are strongly encouraged to complete the entire assignment. You will receive feedback on exercises you turn in that are not being graded.\n\nBook exercises\n\n1.12, 1.20, 1.31, 2.6, 2.14\n\nR exercises\n\nR2: BRFSS"
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#directions",
    "href": "homework/HW_1_F24_bsta511.html#directions",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "Directions",
    "text": "Directions\n\n\n\n\n\n\nImportant\n\n\n\n\n* Starred exercises in the section Book exercises may be completed by hand (such as on paper or using a tablet) not using Quarto.\nIf you complete this part of the assignment not using Quarto, you will be uploading 3 files on Sakai for this HW: qmd & html files for your R work, and a pdf with your written work.\nIf you are completing the homework on paper, you can use a scanning app, such as Adobe Scan, to create a pdf of your assignment.\n\n\n\n\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file.\n\nUse the assignment .qmd file linked to above as a template for your own assignment.\n\nFor each question, make sure to show all of your work. This includes all code and resulting output in the html file to support your answers for exercises requiring work done in R (including any arithmetic calculations).\nFor each question, include a sentence summarizing the answer for that question in the context of the research question.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your Qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#a-upload-a-photo-using-sakai-submission",
    "href": "homework/HW_1_F24_bsta511.html#a-upload-a-photo-using-sakai-submission",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "a) Upload a photo using Sakai submission",
    "text": "a) Upload a photo using Sakai submission\nTo help me learn your names and faces, please upload a photo of yourself on Sakai. You will find the Upload Photo “assignment” in the Assignments section of Sakai. These photos will only be seen by me and the TA."
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#b-background-survey",
    "href": "homework/HW_1_F24_bsta511.html#b-background-survey",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "b) Background survey",
    "text": "b) Background survey\n\nPlease fill out the background survey at https://forms.gle/dG5RLcurrhJzXMrX9.\n\nNo work to be shown here."
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#c-slack-post",
    "href": "homework/HW_1_F24_bsta511.html#c-slack-post",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "c) Slack post",
    "text": "c) Slack post\n\nIntroduce yourself to the class by posting a message in the #random channel on the BSTA 511/611 Slack group.\n\nSlack invite link: https://join.slack.com/t/bsta511611f24/shared_invite/zt-2rla7k81t-_SMUapKP6pVCpJgYZAfRkg\nNo work to be shown here."
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#sinusitis-and-antibiotics-part-i.",
    "href": "homework/HW_1_F24_bsta511.html#sinusitis-and-antibiotics-part-i.",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "1.2 Sinusitis and antibiotics, Part I.",
    "text": "1.2 Sinusitis and antibiotics, Part I.\n\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file.\nWrite your answers in complete sentences as if communicating the results to a collaborator.\nIf you are having difficulty with exercise 1.2, take a look at exercise 1.1, whose answers are at the back of the book."
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#buteyko-method-study-components",
    "href": "homework/HW_1_F24_bsta511.html#buteyko-method-study-components",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "1.4 Buteyko method, study components",
    "text": "1.4 Buteyko method, study components"
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#smoking-habits-of-uk-residents",
    "href": "homework/HW_1_F24_bsta511.html#smoking-habits-of-uk-residents",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "1.8 Smoking habits of UK residents",
    "text": "1.8 Smoking habits of UK residents"
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#herbal-remedies",
    "href": "homework/HW_1_F24_bsta511.html#herbal-remedies",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "1.12 Herbal remedies",
    "text": "1.12 Herbal remedies"
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#city-council-survey",
    "href": "homework/HW_1_F24_bsta511.html#city-council-survey",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "1.20 City council survey",
    "text": "1.20 City council survey"
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#income-at-the-coffee-shop",
    "href": "homework/HW_1_F24_bsta511.html#income-at-the-coffee-shop",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "1.31 Income at the coffee shop",
    "text": "1.31 Income at the coffee shop"
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#midrange",
    "href": "homework/HW_1_F24_bsta511.html#midrange",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "1.32 Midrange",
    "text": "1.32 Midrange"
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#smoking-and-stenosis",
    "href": "homework/HW_1_F24_bsta511.html#smoking-and-stenosis",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "1.38 Smoking and stenosis",
    "text": "1.38 Smoking and stenosis\nSee Section 1.6.2 for more on how the relative risk is calculated."
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#poverty-and-language",
    "href": "homework/HW_1_F24_bsta511.html#poverty-and-language",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "* 2.6 Poverty and language",
    "text": "* 2.6 Poverty and language\nPart (b) asks you to create a Venn Diagram. If you are submitting this question in R, you do not need to turn this part in. If you want an R challenge though, you can use the VennDiagram or other package to create one. See https://www.geeksforgeeks.org/how-to-create-a-venn-diagram-in-r/ for some examples."
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#school-absences",
    "href": "homework/HW_1_F24_bsta511.html#school-absences",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "* 2.8 School absences",
    "text": "* 2.8 School absences\nPart (b) asks you to create a Venn Diagram. If you are submitting this question in R, you do not need to turn this part in. If you want an R challenge though, you can use the VennDiagram or other package to create one. See https://www.geeksforgeeks.org/how-to-create-a-venn-diagram-in-r/ for some examples."
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#health-coverage-frequencies",
    "href": "homework/HW_1_F24_bsta511.html#health-coverage-frequencies",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "* 2.10 Health coverage, frequencies",
    "text": "* 2.10 Health coverage, frequencies"
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#health-coverage-relative-frequencies",
    "href": "homework/HW_1_F24_bsta511.html#health-coverage-relative-frequencies",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "* 2.14 Health coverage, relative frequencies",
    "text": "* 2.14 Health coverage, relative frequencies"
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#load-all-the-packages-you-need-below-here.",
    "href": "homework/HW_1_F24_bsta511.html#load-all-the-packages-you-need-below-here.",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "!!! Load all the packages you need below here.",
    "text": "!!! Load all the packages you need below here."
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#r1-formatting-text-practice",
    "href": "homework/HW_1_F24_bsta511.html#r1-formatting-text-practice",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "R1: Formatting text practice",
    "text": "R1: Formatting text practice\nWrite a sentence (or a few) using all the different types of formatting text shown in slide 29 of the Day 1 slides. Your choice of text does not matter or even need to make sense. Although the TA will appreciate it if you make them laugh."
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#r2-brfss",
    "href": "homework/HW_1_F24_bsta511.html#r2-brfss",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "R2: BRFSS",
    "text": "R2: BRFSS\n\nThe Behavioral Risk Factor Surveillance System (BRFSS) is an annual telephone survey of 350,000 people in the United States. The BRFSS is designed to identify risk factors in the adult population and report emerging health trends. For example, respondents are asked about their diet, weekly exercise, possible tobacco use, and health care coverage.\n\n\nThe dataset cdc is a sample of 20,000 people from the survey conducted in 2000, and contains responses from a subset of the questions asked on the survey.\n\n\nLoad the cdc dataset from the web using the source() command below:\n\n\nsource(\"http://www.openintro.org/stat/data/cdc.R\")\n\n\nAnswer the questions below about the cdc dataset.\nPlease do not delete the statements of the questions so that they remained numbered in the correct order.\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the knitted html file.\nWrite your answers in complete sentences as if communicating the results to a collaborator."
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#a-how-many-rows-and-columns-are-in-the-dataset",
    "href": "homework/HW_1_F24_bsta511.html#a-how-many-rows-and-columns-are-in-the-dataset",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "a) How many rows and columns are in the dataset?",
    "text": "a) How many rows and columns are in the dataset?"
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#b-variable-types",
    "href": "homework/HW_1_F24_bsta511.html#b-variable-types",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "b) Variable types",
    "text": "b) Variable types\nFor each variable, what identify both its “statistical” variable type (numerical (discrete, continuous) or categorical (nominal, ordinal) and its R variable type.\nFill in your answers in the table I created below.\n\n\n\nvariable name\nR type\nvariable type\n\n\n\n\ngenhlth\nfill in\nfill in\n\n\nexerany\netc.\n\n\n\nhlthplan\n\n\n\n\nsmoke100\n\n\n\n\nheight\n\n\n\n\nweight\n\n\n\n\nwtdesire\n\n\n\n\nage\n\n\n\n\ngender"
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#c-average-weight-vs.-desired-weight",
    "href": "homework/HW_1_F24_bsta511.html#c-average-weight-vs.-desired-weight",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "c) Average weight vs. desired weight",
    "text": "c) Average weight vs. desired weight\nWhat is the difference between the average weight and the average desired weight?"
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#d-compare-variability",
    "href": "homework/HW_1_F24_bsta511.html#d-compare-variability",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "d) Compare variability",
    "text": "d) Compare variability\nWhich of the height, weight, and desired weight variables has the most variability? Which has the least variability?"
  },
  {
    "objectID": "homework/HW_1_F24_bsta511.html#e-mean-of-the-hlthplan",
    "href": "homework/HW_1_F24_bsta511.html#e-mean-of-the-hlthplan",
    "title": "HW 1: BSTA 511/611 F24",
    "section": "e) Mean of the hlthplan",
    "text": "e) Mean of the hlthplan\nCalculate the mean of the hlthplan variable. How do we interpret this mean? In other words, what does this mean measure?"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html",
    "href": "slides_md/Day02_bsta511_md.html",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "",
    "text": "(1.3) Data collection principles - Day 2 in F24\n\nPopulation vs. sample\nSampling methods\nExperiments vs. Observational studies\n\n(1.2) Intro to Data - Day 2 in F24\n\nData types\nHow are data stored in R?\nWorking with data in R\n\n(1.4) Summarizing numerical data - Day 3 in F24\n\nMean, median, mode, SD, IQR, range, 5 number summary\nEmpirical Rule\nrobust statistics\n\nR packages - Day 4 in F24 -&gt; install for Day 5!!!"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#useful-keyboard-shortcuts",
    "href": "slides_md/Day02_bsta511_md.html#useful-keyboard-shortcuts",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Useful keyboard shortcuts",
    "text": "Useful keyboard shortcuts\nFull list of keyboard shortcuts\n \n\n\n\n\n\n\n\n\naction\nmac\nwindows/linux\n\n\n\n\nRun code in qmd (or script)\ncmd + enter\nctrl + enter\n\n\n&lt;-\noption + -\nalt + -\n\n\ninterrupt currently running command\nesc\nesc\n\n\nin console, retrieve previously run code\nup/down\nup/down\n\n\nkeyboard shortcut help\noption + shift + k\nalt + shift + k\n\n\n\n\n\n\nPractice\nTry typing code below in your qmd (with shortcut) and evaluating it:\n\ny &lt;- 5\ny"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#another-resource-for-an-introduction-to-r",
    "href": "slides_md/Day02_bsta511_md.html#another-resource-for-an-introduction-to-r",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Another resource for an introduction to R",
    "text": "Another resource for an introduction to R\n\nIf you would like another perspective on what we covered the first week, you might find Danielle Navarro’s online book Learning Statistics with R to be helpful.\nDownload free pdf: https://learningstatisticswithr.com/\nSee Sections 3.1-3.7.1 for some of the topics we covered on first day"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#population-vs.-sample",
    "href": "slides_md/Day02_bsta511_md.html#population-vs.-sample",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Population vs. sample",
    "text": "Population vs. sample\n\n\n\n(Target) Population\n\ngroup of interest being studied\ngroup from which the sample is selected\n\nstudies often have inclusion and/or exclusion criteria\n\n\n\n\n\n\nSample\n\ngroup on which data are collected\noften a small subset of the population"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#sampling-methods-14",
    "href": "slides_md/Day02_bsta511_md.html#sampling-methods-14",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Sampling methods (1/4)",
    "text": "Sampling methods (1/4)\nGoal is to get a representative sample of the population:\nthe characteristics of the sample are similar to the characteristics of the population\n\n\nSimple random sample (SRS)\n\neach individual of a population has the same chance of being sampled\nrandomly sampled\nconsidered best way to sample\n\n\n\n\n\n\n\n\n\nConvenience sample\n\neasily accessible individuals are more likely to be included in the sample than other individuals\na common “pitfall”"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#sampling-methods-24",
    "href": "slides_md/Day02_bsta511_md.html#sampling-methods-24",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Sampling methods (2/4)",
    "text": "Sampling methods (2/4)\nGood sampling plans don’t guarantee samples representative of the population\n\n\nNon-response bias\n\nnon-response rates can be high\nare all groups within a population being reached?\nunrepresentative sample\n=&gt; skewed results\n\n\n\n\n\n\n\n\n\n“Random” samples can be unrepresentative by random chance\n\nIn a SRS each case in the population has an equal chance of being included in the sample\nBut by random chance alone a random sample might contain a higher proportion of one group over another\nEx: a SRS might by chance include 70% men (unlikely, but theoretically possible)"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#sampling-methods-34",
    "href": "slides_md/Day02_bsta511_md.html#sampling-methods-34",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Sampling methods (3/4)",
    "text": "Sampling methods (3/4)\n\n\n\nSimple random sample (SRS)\n\neach individual of a population has the same chance of being sampled\nstatistical methods taught in this class assume a SRS!\n\nStratified sampling\n\ndivide population into groups (strata) before selecting cases within each stratum (often via SRS)\nusually cases within a strata are similar, but are different from other strata with respect to the outcome of interest, such as gender or age groups"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#sampling-methods-44",
    "href": "slides_md/Day02_bsta511_md.html#sampling-methods-44",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Sampling methods (4/4)",
    "text": "Sampling methods (4/4)\n\n\n\nCluster sample\n\nfirst divide population into groups (clusters)\nthen sample a fixed number of clusters, and include all observations from chosen clusters\nclusters are often hospitals, clinicians, schools, etc., where each cluster will have similar services/ policies/ etc.\ncases within clusters usually very diverse\n\nMultistage sample\n\nsimilar to a cluster sample, but select a random sample within each selected cluster instead of all individuals"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#experiments-12",
    "href": "slides_md/Day02_bsta511_md.html#experiments-12",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Experiments (1/2)",
    "text": "Experiments (1/2)\n\n\nResearchers assign individuals to different treatment or intervention groups\n\ncontrol group: often receive a placebo or usual care\ndifferent treatment groups are often called study arms\n\nRandomization\n\ngroup assignment is usually random to ensure similar (balanced) study arms for all variables (observed and unobserved)\nrandomization allows study arm differences in outcomes to be attributed to treatment rather than variability in patient characteristics\n\ntreatment is the only systematic difference between groups\nestablish causality\n\nblocking (stratification): group individuals into blocks (strata) before randomizing if there are certain characteristics that may influence the outcome other than treatment (i.e. gender, age group)"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#experiments-22",
    "href": "slides_md/Day02_bsta511_md.html#experiments-22",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Experiments (2/2)",
    "text": "Experiments (2/2)\n\n\nReplication\n\naccomplished by collecting a sufficiently large sample\nresults usually more reliable with a large sample size\n\noften less variability\nmore likely to be representative of population\n\n\nSome studies are not ethical to carry out as experiments"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#observational-studies",
    "href": "slides_md/Day02_bsta511_md.html#observational-studies",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Observational studies",
    "text": "Observational studies\n\n\ndata are observed and recorded without interference\noften done via surveys, electronic health records, or medical chart reviews\ncohorts\nassociations between variables can be established, but not causality\n\nIndividuals with different characteristics may also differ in other ways that influence response\n\nconfounding variables (lurking variable)\n\nvariables associated with both the explanatory and response variables\n\nprospective vs. retrospective studies"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#comparing-study-designs",
    "href": "slides_md/Day02_bsta511_md.html#comparing-study-designs",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Comparing study designs",
    "text": "Comparing study designs\n\n\n\nScience Media Centre"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#systematic-reviews-example",
    "href": "slides_md/Day02_bsta511_md.html#systematic-reviews-example",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Systematic Reviews example",
    "text": "Systematic Reviews example\n\n\n\nSTEM: Systematically Testing the Evidence on Marijuana\n\n\n\n\n\nSTEM is a collaborative project between the US Department of Veterans Affairs and the Center for Evidence-based Policy at Oregon Health & Science University.\nThe project is funded by the US Department of Veterans Affairs: Office of Rural Health."
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#how-are-data-stored-how-do-we-use-them",
    "href": "slides_md/Day02_bsta511_md.html#how-are-data-stored-how-do-we-use-them",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "How are data stored, how do we use them?",
    "text": "How are data stored, how do we use them?\n\nOften, data are in an Excel sheet, or a plain text file (.csv, .txt)\n.csv files open in Excel automatically, but actually are plain text\nUsually, columns are variables/measures and rows are observations (i.e. a person’s measurements)\n\n\nData in R\n\nWe can import data from many file types, including .csv, .txt., and .xlsx\n\nWe will cover this on a later date\n\nOnce imported, R typically stores data as data frames, or tibbles if using the tidyverse package (more on this later).\n\nFor our purposes, these are essentially the same, and I will tend to use the terms interchangeably.\nThese are examples of what we call object types in R."
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#data-frame-example",
    "href": "slides_md/Day02_bsta511_md.html#data-frame-example",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Data frame example",
    "text": "Data frame example\n\n\n\ndf &lt;- data.frame(\n  IDs=1:3, \n  gender=c(\"male\", \"female\", \"Male\"), \n  age=c(28, 35.5, 31),\n  trt = c(\"control\", \"1\", \"1\"),\n  Veteran = c(FALSE, TRUE, TRUE)\n  )\ndf\n\n  IDs gender  age     trt Veteran\n1   1   male 28.0 control   FALSE\n2   2 female 35.5       1    TRUE\n3   3   Male 31.0       1    TRUE\n\n\n\nVectors vs. data frames\n\na data frame is a collection (or array or table) of vectors\n\n\n\n\n\n\n\nDifferent columns can be of different data types (i.e. numeric vs. text)\nBoth numeric and text can be stored within a column (stored together as text).\nVectors and data frames are examples of objects in R.\n\nThere are other types of R objects to store data, such as matrices, lists."
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#observations-variables",
    "href": "slides_md/Day02_bsta511_md.html#observations-variables",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Observations & variables",
    "text": "Observations & variables\n\n\n\ndf\n\n  IDs gender  age     trt Veteran\n1   1   male 28.0 control   FALSE\n2   2 female 35.5       1    TRUE\n3   3   Male 31.0       1    TRUE\n\n\n\n\n\nISLBS\n\n\n\n\n\nBook refers to a dataset as a data matrix\nRows are usually observations\nColumns are usually variables\nHow many observations are in this dataset?\nWhat are the variable types in this dataset?"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#variable-column-types",
    "href": "slides_md/Day02_bsta511_md.html#variable-column-types",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Variable (column) types",
    "text": "Variable (column) types\n\n\n\n\n\n\n\n\n\nR type\nvariable type\ndescription\n\n\n\n\ninteger\ndiscrete\ninteger-valued numbers\n\n\ndouble or numeric\ncontinuous\nnumbers that are decimals\n\n\nfactor\ncategorical\ncategorical variables stored with levels (groups)\n\n\ncharacter\ncategorical\ntext, “strings”\n\n\nlogical\ncategorical\nboolean (TRUE, FALSE)\n\n\n\n\n\nView the structure of our data frame to see what the variable types are:\n\n\n\nstr(df)\n\n'data.frame':   3 obs. of  5 variables:\n $ IDs    : int  1 2 3\n $ gender : chr  \"male\" \"female\" \"Male\"\n $ age    : num  28 35.5 31\n $ trt    : chr  \"control\" \"1\" \"1\"\n $ Veteran: logi  FALSE TRUE TRUE"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#fishers-or-andersons-iris-data-set",
    "href": "slides_md/Day02_bsta511_md.html#fishers-or-andersons-iris-data-set",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Fisher’s (or Anderson’s) Iris data set",
    "text": "Fisher’s (or Anderson’s) Iris data set\nData description:\n\nn = 150\n3 species of Iris flowers (Setosa, Virginica, and Versicolour)\n\n50 measurements of each type of Iris\n\nvariables:\n\nsepal length, sepal width, petal length, petal width, and species\n\n\nCan the iris species be determined by these variables?\n\n\n\nGareth Duffy"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#view-the-iris-dataset",
    "href": "slides_md/Day02_bsta511_md.html#view-the-iris-dataset",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "View the iris dataset",
    "text": "View the iris dataset\n\n\nThe iris dataset is already pre-loaded in base R and ready to use.\nType the following command in the console window\n\nWarning: this command cannot be rendered. It will give an error.\n\n\n\n\n\n\n\n\nView(iris)\n\n\nA new tab in the scripting window should appear with the iris dataset."
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#data-structure",
    "href": "slides_md/Day02_bsta511_md.html#data-structure",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Data structure",
    "text": "Data structure\n\nWhat are the different variable types in this data set?\n\n\n\n\nstr(iris)   # structure of data\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ..."
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#data-set-summary",
    "href": "slides_md/Day02_bsta511_md.html#data-set-summary",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Data set summary",
    "text": "Data set summary\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#data-set-info",
    "href": "slides_md/Day02_bsta511_md.html#data-set-info",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Data set info",
    "text": "Data set info\n\ndim(iris)\n\n[1] 150   5\n\nnrow(iris)\n\n[1] 150\n\nncol(iris)\n\n[1] 5\n\nnames(iris)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\""
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#view-the-beginning-or-end-of-a-dataset",
    "href": "slides_md/Day02_bsta511_md.html#view-the-beginning-or-end-of-a-dataset",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "View the beginning or end of a dataset",
    "text": "View the beginning or end of a dataset\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\ntail(iris)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n145          6.7         3.3          5.7         2.5 virginica\n146          6.7         3.0          5.2         2.3 virginica\n147          6.3         2.5          5.0         1.9 virginica\n148          6.5         3.0          5.2         2.0 virginica\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#specify-how-many-rows-to-view-at-beginning-or-end-of-a-dataset",
    "href": "slides_md/Day02_bsta511_md.html#specify-how-many-rows-to-view-at-beginning-or-end-of-a-dataset",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Specify how many rows to view at beginning or end of a dataset",
    "text": "Specify how many rows to view at beginning or end of a dataset\n\nhead(iris, 3)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n\ntail(iris, 2)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#the",
    "href": "slides_md/Day02_bsta511_md.html#the",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "The $",
    "text": "The $\n\nSuppose we want to single out the column of petal width values.\nOne way to do this is to use the $\n\nDatSetName$VariableName\n\n\n\niris$Petal.Width\n\n  [1] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 0.2 0.2 0.1 0.1 0.2 0.4 0.4 0.3\n [19] 0.3 0.3 0.2 0.4 0.2 0.5 0.2 0.2 0.4 0.2 0.2 0.2 0.2 0.4 0.1 0.2 0.2 0.2\n [37] 0.2 0.1 0.2 0.2 0.3 0.3 0.2 0.6 0.4 0.3 0.2 0.2 0.2 0.2 1.4 1.5 1.5 1.3\n [55] 1.5 1.3 1.6 1.0 1.3 1.4 1.0 1.5 1.0 1.4 1.3 1.4 1.5 1.0 1.5 1.1 1.8 1.3\n [73] 1.5 1.2 1.3 1.4 1.4 1.7 1.5 1.0 1.1 1.0 1.2 1.6 1.5 1.6 1.5 1.3 1.3 1.3\n [91] 1.2 1.4 1.2 1.0 1.3 1.2 1.3 1.3 1.1 1.3 2.5 1.9 2.1 1.8 2.2 2.1 1.7 1.8\n[109] 1.8 2.5 2.0 1.9 2.1 2.0 2.4 2.3 1.8 2.2 2.3 1.5 2.3 2.0 2.0 1.8 2.1 1.8\n[127] 1.8 1.8 2.1 1.6 1.9 2.0 2.2 1.5 1.4 2.3 2.4 1.8 1.8 2.1 2.4 2.3 1.9 2.3\n[145] 2.5 2.3 1.9 2.0 2.3 1.8"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#example-using-the",
    "href": "slides_md/Day02_bsta511_md.html#example-using-the",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Example using the $",
    "text": "Example using the $\nThe $ is helpful if you want to create a new dataset for just that one variable, or, more commonly, if you want to calculate summary statistics for that one variable.\n\n\n\nmean(iris$Petal.Width)\n\n[1] 1.199333\n\nsd(iris$Petal.Width)\n\n[1] 0.7622377\n\nmedian(iris$Petal.Width)\n\n[1] 1.3"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#inline-code",
    "href": "slides_md/Day02_bsta511_md.html#inline-code",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Inline code",
    "text": "Inline code\n\n\nWith markdown you can also report R code output inline with the text instead of using a chunk.\n\n\n\nText in editor:\n\n\n\n\n\n\nOutput:\nThe mean petal width for all 3 species combined is 1.2 (SD = 0.8) cm.\n\n\n\nReporting summary statistics this way in a report, makes the numbers computationally reproducible.\nFor example, if this were for an abstract and a year later you are wondering where the numbers came from, your R code will tell you exactly which dataset was used to calculate the values."
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#table-1-example",
    "href": "slides_md/Day02_bsta511_md.html#table-1-example",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Table 1 example",
    "text": "Table 1 example\n\n\n\n\n\n\n\n\nAre We on the Same Page?: A Cross-Sectional Study of Patient-Clinician Goal Concordance in Rheumatoid Arthritis\nJ Barton et al.\nArthritis Care & Research.\n2021 Sep 27 https://pubmed.ncbi.nlm.nih.gov/34569172/"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#measures-of-center-mean",
    "href": "slides_md/Day02_bsta511_md.html#measures-of-center-mean",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of center: mean",
    "text": "Measures of center: mean\n\nSample mean: the average value of observations\n\\[\\overline{x} = \\frac{x_1+x_2+\\cdots+x_n}{n} = \\sum_{i=1}^{n}\\frac{x_i}{n}\\]\nwhere \\(x_1, x_2, \\ldots, x_n\\) represent the \\(n\\) observed values in a sample\nExample: What is the mean age in the toy dataset df defined earlier?\n\n\ndf\n\n  IDs gender  age     trt Veteran\n1   1   male 28.0 control   FALSE\n2   2 female 35.5       1    TRUE\n3   3   Male 31.0       1    TRUE\n\nmean(df$age)\n\n[1] 31.5"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#measures-of-center-median",
    "href": "slides_md/Day02_bsta511_md.html#measures-of-center-median",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of center: median",
    "text": "Measures of center: median\n\n\nThe median is the middle value of the observations in a sample.\nThe median is the 50th percentile, meaning\n\n50% of observations lie below and\n50% of observations lie above the median.\n\n\n\n\n\n\n\nIf the number of observations is\n\nodd: the median is the middle observed value\neven: the median is the average of the two middle observed values\n\n\n\n\n\ndf$age\n\n[1] 28.0 35.5 31.0\n\nmedian(df$age)\n\n[1] 31\n\nmedian(c(df$age, 67))\n\n[1] 33.25"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#measures-of-center-mean-vs.-median",
    "href": "slides_md/Day02_bsta511_md.html#measures-of-center-mean-vs.-median",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of center: mean vs. median",
    "text": "Measures of center: mean vs. median\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#measures-of-center-mode",
    "href": "slides_md/Day02_bsta511_md.html#measures-of-center-mode",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of center: mode",
    "text": "Measures of center: mode\nmode: the most frequent value in a dataset\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#measures-of-spread-standard-deviation-sd-13",
    "href": "slides_md/Day02_bsta511_md.html#measures-of-spread-standard-deviation-sd-13",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of spread: standard deviation (SD) (1/3)",
    "text": "Measures of spread: standard deviation (SD) (1/3)\nstandard deviation is (approximately) the average distance between a typical observation and the mean\n\nAn observation’s deviation is the distance between its value \\(x\\) and the sample mean \\(\\overline{x}\\): deviation = \\(x - \\overline{x}\\).\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#measures-of-spread-sd-23",
    "href": "slides_md/Day02_bsta511_md.html#measures-of-spread-sd-23",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of spread: SD (2/3)",
    "text": "Measures of spread: SD (2/3)\n\nThe sample variance \\(s^2\\) is the sum of squared deviations divided by the number of observations minus 1. \\[s^2 = \\frac{(x_1 - \\overline{x})^2+(x_2 - \\overline{x})^2+\\cdots+(x_n - \\overline{x})^2}{n-1} = \\sum_{i=1}^{n}\\frac{(x_i - \\overline{x})^2}{n-1}\\] where \\(x_1, x_2, \\dots, x_n\\) represent the \\(n\\) observed values.\nThe standard deviation \\(s\\) is the square root of the variance. \\[s = \\sqrt{\\frac{({x_1 - \\overline{x})}^{2}+({x_2 - \\overline{x})}^{2}+\\cdots+({x_n - \\overline{x})}^{2}}{n-1}} = \\sqrt{\\sum_{i=1}^{n}\\frac{(x_i - \\overline{x})^2}{n-1}}\\]"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#measures-of-spread-sd-33",
    "href": "slides_md/Day02_bsta511_md.html#measures-of-spread-sd-33",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of spread: SD (3/3)",
    "text": "Measures of spread: SD (3/3)\n\n\n\nLet’s calculate the sample standard deviation for our toy example\n\n\ndf$age\n\n[1] 28.0 35.5 31.0\n\n\n\n\nmean(df$age)\n\n[1] 31.5\n\nsd(df$age)\n\n[1] 3.774917\n\n\n\n\n\\(s = \\sqrt{\\sum_{i=1}^{n}\\frac{(x_i - \\overline{x})^2}{n-1}} =\\)"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#empirical-rule-one-way-to-think-about-the-sd-12",
    "href": "slides_md/Day02_bsta511_md.html#empirical-rule-one-way-to-think-about-the-sd-12",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Empirical Rule: one way to think about the SD (1/2)",
    "text": "Empirical Rule: one way to think about the SD (1/2)\n\n\n\nFor symmetric bell-shaped data, about\n\n68% of the data are within 1 SD of the mean\n95% of the data are within 2 SD’s of the mean\n99.7% of the data are within 3 SD’s of the mean\n\nThese percentages are based off of percentages of a true normal distribution.\n\n\n\n\nhttps://statistics-made-easy.com/empirical-rule/"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#empirical-rule-one-way-to-think-about-the-sd-22",
    "href": "slides_md/Day02_bsta511_md.html#empirical-rule-one-way-to-think-about-the-sd-22",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Empirical Rule: one way to think about the SD (2/2)",
    "text": "Empirical Rule: one way to think about the SD (2/2)\n\n\n\nhist(iris$Sepal.Width)\n\n\n\n\n\n\nmean(iris$Sepal.Width)\n\n[1] 3.057333\n\nsd(iris$Sepal.Width)\n\n[1] 0.4358663"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#measures-of-spread-interquartile-range-iqr-12",
    "href": "slides_md/Day02_bsta511_md.html#measures-of-spread-interquartile-range-iqr-12",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of spread: interquartile range (IQR) (1/2)",
    "text": "Measures of spread: interquartile range (IQR) (1/2)\nThe \\(p^{th}\\) percentile is the observation such that \\(p\\%\\) of the remaining observations fall below this observation.\n\nThe first quartile \\(Q_1\\) is the \\(25^{th}\\) percentile.\nThe second quartile \\(Q_2\\), i.e., the median, is the \\(50^{th}\\) percentile.\nThe third quartile \\(Q_3\\) is the \\(75^{th}\\) percentile.\n\nThe interquartile range (IQR) is the distance between the third and first quartiles. \\[IQR = Q_3 - Q_1\\]\n\nIQR is the width of the middle half of the data"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#measures-of-spread-iqr-22",
    "href": "slides_md/Day02_bsta511_md.html#measures-of-spread-iqr-22",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of spread: IQR (2/2)",
    "text": "Measures of spread: IQR (2/2)\n5 number summary\n\nsummary(iris$Sepal.Width)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.000   2.800   3.000   3.057   3.300   4.400 \n\n\n\n\n\n\n\n\n\n\nWhat is the IQR of the sepal widths?\n\nquantile(iris$Sepal.Width, c(.25, .75))\n\n25% 75% \n2.8 3.3 \n\ndiff(quantile(iris$Sepal.Width, c(.25, .75)))\n\n75% \n0.5 \n\nIQR(iris$Sepal.Width)\n\n[1] 0.5"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#robust-estimates",
    "href": "slides_md/Day02_bsta511_md.html#robust-estimates",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Robust estimates",
    "text": "Robust estimates\nSummary statistics are called robust estimates if extreme observations have little effect on their values\n\n\n\nestimate\nrobust?\n\n\n\n\nmean\n\n\n\nmedian\n\n\n\nmode\n\n\n\nstandard deviation\n\n\n\nIQR\n\n\n\nrange"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#r-packages",
    "href": "slides_md/Day02_bsta511_md.html#r-packages",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "R Packages",
    "text": "R Packages\nA good analogy for R packages is that they\nare like apps you can download onto a mobile phone:\n\n\n\nModernDive Figure 1.4"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#installing-packages",
    "href": "slides_md/Day02_bsta511_md.html#installing-packages",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Installing packages",
    "text": "Installing packages\n\n\nPackages contain additional functions and data\n\nTwo options to install packages:\n\ninstall.packages() or\nThe “Packages” tab in Files/Plots/Packages/Help/Viewer window\n\n\n\ninstall.packages(\"dplyr\")   # only do this ONCE, use quotes\n\n\n\n\n\nOnly install packages once (unless you want to update them)\nInstalled from Comprehensive R Archive Network (CRAN) = package mothership"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#video-on-installing-packages",
    "href": "slides_md/Day02_bsta511_md.html#video-on-installing-packages",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Video on installing packages",
    "text": "Video on installing packages\n\nDanielle Navarro’s YouTube video on Installing and loading R packages: https://www.youtube.com/watch?v=kpHZVyDvEhQ"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#load-packages-with-library-command",
    "href": "slides_md/Day02_bsta511_md.html#load-packages-with-library-command",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Load packages with library() command",
    "text": "Load packages with library() command\n\n\nTip: at the top of your Rmd file, create a chunk that loads all of the R packages you want to use in that file.\nUse the library() command to load each required package.\nPackages need to be reloaded every time you open Rstudio.\n\n\n\nlibrary(dplyr)    # run this every time you open Rstudio\n\n\n\nYou can use a function without loading the package with PackageName::CommandName\n\n\n\ndplyr::arrange(iris, Petal.Width)   # what does arrange do?\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1            4.9         3.1          1.5         0.1     setosa\n2            4.8         3.0          1.4         0.1     setosa\n3            4.3         3.0          1.1         0.1     setosa\n4            5.2         4.1          1.5         0.1     setosa\n5            4.9         3.6          1.4         0.1     setosa\n6            5.1         3.5          1.4         0.2     setosa\n7            4.9         3.0          1.4         0.2     setosa\n8            4.7         3.2          1.3         0.2     setosa\n9            4.6         3.1          1.5         0.2     setosa\n10           5.0         3.6          1.4         0.2     setosa\n11           5.0         3.4          1.5         0.2     setosa\n12           4.4         2.9          1.4         0.2     setosa\n13           5.4         3.7          1.5         0.2     setosa\n14           4.8         3.4          1.6         0.2     setosa\n15           5.8         4.0          1.2         0.2     setosa\n16           5.4         3.4          1.7         0.2     setosa\n17           4.6         3.6          1.0         0.2     setosa\n18           4.8         3.4          1.9         0.2     setosa\n19           5.0         3.0          1.6         0.2     setosa\n20           5.2         3.5          1.5         0.2     setosa\n21           5.2         3.4          1.4         0.2     setosa\n22           4.7         3.2          1.6         0.2     setosa\n23           4.8         3.1          1.6         0.2     setosa\n24           5.5         4.2          1.4         0.2     setosa\n25           4.9         3.1          1.5         0.2     setosa\n26           5.0         3.2          1.2         0.2     setosa\n27           5.5         3.5          1.3         0.2     setosa\n28           4.4         3.0          1.3         0.2     setosa\n29           5.1         3.4          1.5         0.2     setosa\n30           4.4         3.2          1.3         0.2     setosa\n31           5.1         3.8          1.6         0.2     setosa\n32           4.6         3.2          1.4         0.2     setosa\n33           5.3         3.7          1.5         0.2     setosa\n34           5.0         3.3          1.4         0.2     setosa\n35           4.6         3.4          1.4         0.3     setosa\n36           5.1         3.5          1.4         0.3     setosa\n37           5.7         3.8          1.7         0.3     setosa\n38           5.1         3.8          1.5         0.3     setosa\n39           5.0         3.5          1.3         0.3     setosa\n40           4.5         2.3          1.3         0.3     setosa\n41           4.8         3.0          1.4         0.3     setosa\n42           5.4         3.9          1.7         0.4     setosa\n43           5.7         4.4          1.5         0.4     setosa\n44           5.4         3.9          1.3         0.4     setosa\n45           5.1         3.7          1.5         0.4     setosa\n46           5.0         3.4          1.6         0.4     setosa\n47           5.4         3.4          1.5         0.4     setosa\n48           5.1         3.8          1.9         0.4     setosa\n49           5.1         3.3          1.7         0.5     setosa\n50           5.0         3.5          1.6         0.6     setosa\n51           4.9         2.4          3.3         1.0 versicolor\n52           5.0         2.0          3.5         1.0 versicolor\n53           6.0         2.2          4.0         1.0 versicolor\n54           5.8         2.7          4.1         1.0 versicolor\n55           5.7         2.6          3.5         1.0 versicolor\n56           5.5         2.4          3.7         1.0 versicolor\n57           5.0         2.3          3.3         1.0 versicolor\n58           5.6         2.5          3.9         1.1 versicolor\n59           5.5         2.4          3.8         1.1 versicolor\n60           5.1         2.5          3.0         1.1 versicolor\n61           6.1         2.8          4.7         1.2 versicolor\n62           5.8         2.7          3.9         1.2 versicolor\n63           5.5         2.6          4.4         1.2 versicolor\n64           5.8         2.6          4.0         1.2 versicolor\n65           5.7         3.0          4.2         1.2 versicolor\n66           5.5         2.3          4.0         1.3 versicolor\n67           5.7         2.8          4.5         1.3 versicolor\n68           6.6         2.9          4.6         1.3 versicolor\n69           5.6         2.9          3.6         1.3 versicolor\n70           6.1         2.8          4.0         1.3 versicolor\n71           6.4         2.9          4.3         1.3 versicolor\n72           6.3         2.3          4.4         1.3 versicolor\n73           5.6         3.0          4.1         1.3 versicolor\n74           5.5         2.5          4.0         1.3 versicolor\n75           5.6         2.7          4.2         1.3 versicolor\n76           5.7         2.9          4.2         1.3 versicolor\n77           6.2         2.9          4.3         1.3 versicolor\n78           5.7         2.8          4.1         1.3 versicolor\n79           7.0         3.2          4.7         1.4 versicolor\n80           5.2         2.7          3.9         1.4 versicolor\n81           6.1         2.9          4.7         1.4 versicolor\n82           6.7         3.1          4.4         1.4 versicolor\n83           6.6         3.0          4.4         1.4 versicolor\n84           6.8         2.8          4.8         1.4 versicolor\n85           6.1         3.0          4.6         1.4 versicolor\n86           6.1         2.6          5.6         1.4  virginica\n87           6.4         3.2          4.5         1.5 versicolor\n88           6.9         3.1          4.9         1.5 versicolor\n89           6.5         2.8          4.6         1.5 versicolor\n90           5.9         3.0          4.2         1.5 versicolor\n91           5.6         3.0          4.5         1.5 versicolor\n92           6.2         2.2          4.5         1.5 versicolor\n93           6.3         2.5          4.9         1.5 versicolor\n94           6.0         2.9          4.5         1.5 versicolor\n95           5.4         3.0          4.5         1.5 versicolor\n96           6.7         3.1          4.7         1.5 versicolor\n97           6.0         2.2          5.0         1.5  virginica\n98           6.3         2.8          5.1         1.5  virginica\n99           6.3         3.3          4.7         1.6 versicolor\n100          6.0         2.7          5.1         1.6 versicolor\n101          6.0         3.4          4.5         1.6 versicolor\n102          7.2         3.0          5.8         1.6  virginica\n103          6.7         3.0          5.0         1.7 versicolor\n104          4.9         2.5          4.5         1.7  virginica\n105          5.9         3.2          4.8         1.8 versicolor\n106          6.3         2.9          5.6         1.8  virginica\n107          7.3         2.9          6.3         1.8  virginica\n108          6.7         2.5          5.8         1.8  virginica\n109          6.5         3.0          5.5         1.8  virginica\n110          6.3         2.7          4.9         1.8  virginica\n111          7.2         3.2          6.0         1.8  virginica\n112          6.2         2.8          4.8         1.8  virginica\n113          6.1         3.0          4.9         1.8  virginica\n114          6.4         3.1          5.5         1.8  virginica\n115          6.0         3.0          4.8         1.8  virginica\n116          5.9         3.0          5.1         1.8  virginica\n117          5.8         2.7          5.1         1.9  virginica\n118          6.4         2.7          5.3         1.9  virginica\n119          7.4         2.8          6.1         1.9  virginica\n120          5.8         2.7          5.1         1.9  virginica\n121          6.3         2.5          5.0         1.9  virginica\n122          6.5         3.2          5.1         2.0  virginica\n123          5.7         2.5          5.0         2.0  virginica\n124          5.6         2.8          4.9         2.0  virginica\n125          7.7         2.8          6.7         2.0  virginica\n126          7.9         3.8          6.4         2.0  virginica\n127          6.5         3.0          5.2         2.0  virginica\n128          7.1         3.0          5.9         2.1  virginica\n129          7.6         3.0          6.6         2.1  virginica\n130          6.8         3.0          5.5         2.1  virginica\n131          6.7         3.3          5.7         2.1  virginica\n132          6.4         2.8          5.6         2.1  virginica\n133          6.9         3.1          5.4         2.1  virginica\n134          6.5         3.0          5.8         2.2  virginica\n135          7.7         3.8          6.7         2.2  virginica\n136          6.4         2.8          5.6         2.2  virginica\n137          6.4         3.2          5.3         2.3  virginica\n138          7.7         2.6          6.9         2.3  virginica\n139          6.9         3.2          5.7         2.3  virginica\n140          7.7         3.0          6.1         2.3  virginica\n141          6.9         3.1          5.1         2.3  virginica\n142          6.8         3.2          5.9         2.3  virginica\n143          6.7         3.0          5.2         2.3  virginica\n144          6.2         3.4          5.4         2.3  virginica\n145          5.8         2.8          5.1         2.4  virginica\n146          6.3         3.4          5.6         2.4  virginica\n147          6.7         3.1          5.6         2.4  virginica\n148          6.3         3.3          6.0         2.5  virginica\n149          7.2         3.6          6.1         2.5  virginica\n150          6.7         3.3          5.7         2.5  virginica"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#install-the-packages-listed-below-before-day-3",
    "href": "slides_md/Day02_bsta511_md.html#install-the-packages-listed-below-before-day-3",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Install the packages listed below before Day 3",
    "text": "Install the packages listed below before Day 3\n\n\nknitr\n\nthis might actually already be installed\ncheck your packages list\n\ntidyverse\n\nthis is actually a bundle of packages\nWarning: it will take a while to install!!!\nsee more info at https://tidyverse.tidyverse.org/\n\nrstatix\n\nfor summary statistics of a dataset\n\njanitor\n\nfor cleaning and exploring data\n\nggridges\n\nfor creating ridgeline plots\n\ndevtools\n\nused to create R packages\nfor our purposes, needed to install some packages\n\noi_biostat_data\n\nthis package is on github\nsee the next slide for directions on how to install oi_biostat_data"
  },
  {
    "objectID": "slides_md/Day02_bsta511_md.html#directions-for-installing-package-oibiostat",
    "href": "slides_md/Day02_bsta511_md.html#directions-for-installing-package-oibiostat",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Directions for installing package oibiostat",
    "text": "Directions for installing package oibiostat\n\n\nThe textbook’s datasets are in the R package oibiostat\nExplanation of code below\n\nInstallation of oibiostat package requires first installing devtools package\nThe code devtools::install_github() tells R to use the command install_github() from the devtools package without loading the entire package and all of its commands (which library(devtools) would do).\n\n\n\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"OI-Biostat/oi_biostat_data\", force = TRUE)\n\n\n\nAfter running the code above, put # in front of the commands so that RStudio doesn’t evaluate them when rendering.\nNow load the oibiostat package\n\nthe code below needs to be run every time you restart R or knit an Rmd file\n\n\n\n\nlibrary(oibiostat)"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html",
    "href": "slides_md/Day03_bsta511_md.html",
    "title": "Day 3: Data visualization",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) (Sections 1.4, 1.5, 1.6, 1.7.1)\n\nData visualization with ggplot\n\nnumerical & categorical variables, and relationships between variables\n\nSummarizing numerical data\nFrequency (two-way) tables\n\nSome data wrangling techniques along the way\n\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#goals-for-today",
    "href": "slides_md/Day03_bsta511_md.html#goals-for-today",
    "title": "Day 3: Data visualization",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) (Sections 1.4, 1.5, 1.6, 1.7.1)\n\nData visualization with ggplot\n\nnumerical & categorical variables, and relationships between variables\n\nSummarizing numerical data\nFrequency (two-way) tables\n\nSome data wrangling techniques along the way\n\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#international-day-of-women-in-statistics-and-data-science",
    "href": "slides_md/Day03_bsta511_md.html#international-day-of-women-in-statistics-and-data-science",
    "title": "Day 3: Data visualization",
    "section": "International Day of Women in Statistics and Data Science",
    "text": "International Day of Women in Statistics and Data Science\nTuesday, October 8, 2024\n12 am - 11:59 pm UTC (5pm 10/7 to 4:59 pm 10/8 here)\n\n\n\nInternational Day of Women in Statistics and Data Science"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#mimis-tip-of-the-day-sending-messages-in-slack",
    "href": "slides_md/Day03_bsta511_md.html#mimis-tip-of-the-day-sending-messages-in-slack",
    "title": "Day 3: Data visualization",
    "section": "Mimi’s tip of the day: sending messages in Slack",
    "text": "Mimi’s tip of the day: sending messages in Slack\n\n\nAre you frustrated that Slack sends a message when you press Enter? You can change that!"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#recap-of-last-time",
    "href": "slides_md/Day03_bsta511_md.html#recap-of-last-time",
    "title": "Day 3: Data visualization",
    "section": "Recap of last time",
    "text": "Recap of last time\n\n(1.3) Data collection principles\n\nPopulation vs. sample\nSampling methods\nExperiments vs. Observational studies\n\n(1.2) Intro to Data\n\nData types\n\nNumerical: discrete (integer in R), continuous (double or numeric in R)\nCategorical: ordinal, nominal\n\ncharacter or factor in R\n\n\nHow are data stored in R? data frames, tibbles\nWorking with data in R: dim(), nrow(), ncol(), names(), str(), summary(), head(), tail(), $\n\n(1.4) Summarizing numerical data\n\nmean(), median(), sd(), quantile()"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#from-last-time-install-the-pacakges-listed-below",
    "href": "slides_md/Day03_bsta511_md.html#from-last-time-install-the-pacakges-listed-below",
    "title": "Day 3: Data visualization",
    "section": "From last time: Install the pacakges listed below",
    "text": "From last time: Install the pacakges listed below\n\n\nknitr\n\nthis might actually already be installed\ncheck your packages list\n\ntidyverse\n\nthis is actually a bundle of packages\nWarning: it will take a while to install!!!\nsee more info at https://tidyverse.tidyverse.org/\n\nrstatix\n\nfor summary statistics of a dataset\n\njanitor\n\nfor cleaning and exploring data\n\nggridges\n\nfor creating ridgeline plots\n\ndevtools\n\nused to create R packages\nfor our purposes, needed to install some packages\n\noi_biostat_data\n\nthis package is on github\nsee the next slide for directions on how to install oi_biostat_data"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#directions-for-installing-package-oibiostat",
    "href": "slides_md/Day03_bsta511_md.html#directions-for-installing-package-oibiostat",
    "title": "Day 3: Data visualization",
    "section": "Directions for installing package oibiostat",
    "text": "Directions for installing package oibiostat\n\n\nThe textbook’s datasets are in the R package oibiostat\nExplanation of code below\n\nInstallation of oibiostat package requires first installing devtools package\nThe code devtools::install_github() tells R to use the command install_github() from the devtools package without loading the entire package and all of its commands (which library(devtools) would do).\n\n\n\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"OI-Biostat/oi_biostat_data\", force = TRUE)\n\n\n\nAfter running the code above, put # in front of the commands so that RStudio doesn’t evaluate them when rendering.\nNow load the oibiostat package\n\nthe code below needs to be run every time you restart R or render a Qmd file\n\n\n\n\nlibrary(oibiostat)"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#load-packages-with-library-command",
    "href": "slides_md/Day03_bsta511_md.html#load-packages-with-library-command",
    "title": "Day 3: Data visualization",
    "section": "Load packages with library() command",
    "text": "Load packages with library() command\n\n\nTip: at the top of your Qmd file, create a chunk that loads all of the R packages you want to use in that file.\nUse the library() command to load each required package.\n\nPackages need to be reloaded every time you open Rstudio.\nlibrary() commands to load needed packages must be in the Qmd file\n\n\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(ggridges)\nlibrary(janitor)\nlibrary(rstatix)\n\n\nAttaching package: 'rstatix'\n\n\nThe following object is masked from 'package:janitor':\n\n    make_clean_names\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(knitr)\nlibrary(gtsummary) # NEW!!\n\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#case-study-description",
    "href": "slides_md/Day03_bsta511_md.html#case-study-description",
    "title": "Day 3: Data visualization",
    "section": "Case Study Description",
    "text": "Case Study Description\n\n\nIn the US, individuals with developmental disabilities typically receive services and support from state governments.\n\nCalifornia allocates funds to developmentally disabled residents through the Department of Developmental Services (DDS)\nRecipients of DDS funds are referred to as “consumers.”\n\nDataset dds.discr\n\nsample of 1,000 DDS consumers (out of a total of ~ 250,000)\ndata include age, gender, race/ethnicity, and annual DDS financial support per consumer\n\nPrevious research\n\nResearchers examined expenditures on consumers by ethnicity\nFound that the mean annual expenditure on Hispanics was less than that on White non-Hispanics.\n\nResult: an allegation of ethnic discrimination was brought against the California DDS.\nQuestion: Are the data sufficient evidence of ethnic discrimination?\nSee Section 1.7.1 in the textbook for more details"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#load-dds.discr-dataset-from-oibiostat-package",
    "href": "slides_md/Day03_bsta511_md.html#load-dds.discr-dataset-from-oibiostat-package",
    "title": "Day 3: Data visualization",
    "section": "Load dds.discr dataset from oibiostat package",
    "text": "Load dds.discr dataset from oibiostat package\n\n\nThe textbook’s datasets are in the R package oibiostat\nMake sure the oibiostat package is installed before running the code below.\nLoad the oibiostat package and the dataset dds.discr\n\nthe code below needs to be run every time you restart R or render a Qmd file\n\nlibrary(oibiostat)\ndata(\"dds.discr\")\n\n\nAfter loading the dataset dds.discr using data(\"dds.discr\"), you will see dds.discr in the Data list of the Environment window."
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#getting-to-know-the-dataset",
    "href": "slides_md/Day03_bsta511_md.html#getting-to-know-the-dataset",
    "title": "Day 3: Data visualization",
    "section": "Getting to know the dataset",
    "text": "Getting to know the dataset\n\ndim(dds.discr)\n\n[1] 1000    6\n\nnames(dds.discr)\n\n[1] \"id\"           \"age.cohort\"   \"age\"          \"gender\"       \"expenditures\"\n[6] \"ethnicity\"   \n\nlength(unique(dds.discr$id)) # How many unique id's are there?\n\n[1] 1000"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#str-structure",
    "href": "slides_md/Day03_bsta511_md.html#str-structure",
    "title": "Day 3: Data visualization",
    "section": "str() structure",
    "text": "str() structure\n\n\nWe previously used the base R structure command str() to get information about variable types in a dataset.\nNote this dataset is a tibble instead of a data.frame\n\n\n\nstr(dds.discr)      # base R\n\ntibble [1,000 × 6] (S3: tbl_df/tbl/data.frame)\n $ id          : int [1:1000] 10210 10409 10486 10538 10568 10690 10711 10778 10820 10823 ...\n $ age.cohort  : Factor w/ 6 levels \"0-5\",\"6-12\",\"13-17\",..: 3 5 1 4 3 3 3 3 3 3 ...\n $ age         : int [1:1000] 17 37 3 19 13 15 13 17 14 13 ...\n $ gender      : Factor w/ 2 levels \"Female\",\"Male\": 1 2 2 1 2 1 1 2 1 2 ...\n $ expenditures: int [1:1000] 2113 41924 1454 6400 4412 4566 3915 3873 5021 2887 ...\n $ ethnicity   : Factor w/ 8 levels \"American Indian\",..: 8 8 4 4 8 4 8 3 8 4 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   ID = col_integer(),\n  ..   `Age Cohort` = col_character(),\n  ..   Age = col_integer(),\n  ..   Gender = col_character(),\n  ..   Expenditures = col_integer(),\n  ..   Ethnicity = col_character()\n  .. )"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#glimpse",
    "href": "slides_md/Day03_bsta511_md.html#glimpse",
    "title": "Day 3: Data visualization",
    "section": "glimpse()",
    "text": "glimpse()\n\nNew: glimpse()\n\nUse glimpse() from the tidyverse package (technically it’s from the dplyr package) to get information about variable types.\nglimpse() tends to have nicer output for tibbles than str()\n\n\n\nlibrary(tidyverse)\nglimpse(dds.discr)  # from tidyverse package (dplyr)\n\nRows: 1,000\nColumns: 6\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ gender       &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ ethnicity    &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#summary",
    "href": "slides_md/Day03_bsta511_md.html#summary",
    "title": "Day 3: Data visualization",
    "section": "summary()",
    "text": "summary()\n\n\nWe previously used the base R structure command summary() to get summary information about variables\n\n\n\nsummary(dds.discr)      # base R\n\n       id        age.cohort       age          gender     expenditures  \n Min.   :10210   0-5  : 82   Min.   : 0.0   Female:503   Min.   :  222  \n 1st Qu.:31809   6-12 :175   1st Qu.:12.0   Male  :497   1st Qu.: 2899  \n Median :55384   13-17:212   Median :18.0                Median : 7026  \n Mean   :54663   18-21:199   Mean   :22.8                Mean   :18066  \n 3rd Qu.:76135   22-50:226   3rd Qu.:26.0                3rd Qu.:37713  \n Max.   :99898   51+  :106   Max.   :95.0                Max.   :75098  \n                                                                        \n              ethnicity  \n White not Hispanic:401  \n Hispanic          :376  \n Asian             :129  \n Black             : 59  \n Multi Race        : 26  \n American Indian   :  4  \n (Other)           :  5"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#tbl_summary-summary-table",
    "href": "slides_md/Day03_bsta511_md.html#tbl_summary-summary-table",
    "title": "Day 3: Data visualization",
    "section": "tbl_summary(): summary table",
    "text": "tbl_summary(): summary table\n\n\n\n\nNew: Use tbl_summary() from the gtsummary package to get summary information\n\n\n\n# library(gtsummary)\ntbl_summary(dds.discr)"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#what-data-variables-are-included-in-the-plot-below",
    "href": "slides_md/Day03_bsta511_md.html#what-data-variables-are-included-in-the-plot-below",
    "title": "Day 3: Data visualization",
    "section": "What data (variables) are included in the plot below?",
    "text": "What data (variables) are included in the plot below?"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#basics-of-a-ggplot",
    "href": "slides_md/Day03_bsta511_md.html#basics-of-a-ggplot",
    "title": "Day 3: Data visualization",
    "section": "Basics of a ggplot",
    "text": "Basics of a ggplot"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#grammar-of-ggplot2",
    "href": "slides_md/Day03_bsta511_md.html#grammar-of-ggplot2",
    "title": "Day 3: Data visualization",
    "section": "Grammar of ggplot2",
    "text": "Grammar of ggplot2\n\n\n\nKieran Healy"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#histograms",
    "href": "slides_md/Day03_bsta511_md.html#histograms",
    "title": "Day 3: Data visualization",
    "section": "Histograms",
    "text": "Histograms\nWhat is being measured on the vertical axes?\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram() \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = expenditures)) +\n  geom_histogram() \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#histograms-showing-proportions",
    "href": "slides_md/Day03_bsta511_md.html#histograms-showing-proportions",
    "title": "Day 3: Data visualization",
    "section": "Histograms showing proportions",
    "text": "Histograms showing proportions\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram(\n    aes(y = stat(density)))  \n\nWarning: `stat(density)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram(\n    aes(y = stat(density))) +  \n  scale_y_continuous(labels =   \n      scales::percent_format())  \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#distribution-shapes",
    "href": "slides_md/Day03_bsta511_md.html#distribution-shapes",
    "title": "Day 3: Data visualization",
    "section": "Distribution shapes",
    "text": "Distribution shapes"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#density-plots",
    "href": "slides_md/Day03_bsta511_md.html#density-plots",
    "title": "Day 3: Data visualization",
    "section": "Density plots",
    "text": "Density plots\nWhat is being measured on the vertical axes?\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_density() \n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram() \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#dot-plots",
    "href": "slides_md/Day03_bsta511_md.html#dot-plots",
    "title": "Day 3: Data visualization",
    "section": "Dot plots",
    "text": "Dot plots\n\n\nBetter for smaller samples\nWhat is being measured on the vertical axes?\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_dotplot(binwidth =1) \n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram(binwidth =1)"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#boxplots",
    "href": "slides_md/Day03_bsta511_md.html#boxplots",
    "title": "Day 3: Data visualization",
    "section": "Boxplots",
    "text": "Boxplots\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) + \n  geom_boxplot() \n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(y = age)) + \n  geom_boxplot()"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#boxplots-5-number-summary-visualization",
    "href": "slides_md/Day03_bsta511_md.html#boxplots-5-number-summary-visualization",
    "title": "Day 3: Data visualization",
    "section": "Boxplots: 5 number summary visualization",
    "text": "Boxplots: 5 number summary visualization\n\nNo outliers: \nWith outliers:"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#side-by-side-boxplots",
    "href": "slides_md/Day03_bsta511_md.html#side-by-side-boxplots",
    "title": "Day 3: Data visualization",
    "section": "Side-by-side boxplots",
    "text": "Side-by-side boxplots\n\n\n\nggplot(data = dds.discr, \n       aes(x = expenditures,\n           y = ethnicity)) + \n  geom_boxplot() + \n  labs(x = \"Annual Expenditures ($)\", \n       y = \"Race and ethnicity\")  \n\n\nCan you determine the following using boxplots?\n\ndistribution shape\nsample size"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#side-by-side-boxplots-with-data-points",
    "href": "slides_md/Day03_bsta511_md.html#side-by-side-boxplots-with-data-points",
    "title": "Day 3: Data visualization",
    "section": "Side-by-side boxplots with data points",
    "text": "Side-by-side boxplots with data points\n\n\n\nggplot(data = dds.discr, \n       aes(x = expenditures,\n           y = ethnicity)) + \n  geom_boxplot(color=\"darkgrey\") + \n  labs(x = \"Annual Expenditures ($)\",\n       y = \"Race and ethnicity\") +\n  geom_jitter(     \n    aes(color = ethnicity),      \n    alpha = 0.3,      \n    show.legend = FALSE,      \n    position = position_jitter(     \n      height = 0.4))      \n\n\nCan you determine the following using boxplots?\n\ndistribution shape\nsample size"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#density-plots-by-group",
    "href": "slides_md/Day03_bsta511_md.html#density-plots-by-group",
    "title": "Day 3: Data visualization",
    "section": "Density plots by group",
    "text": "Density plots by group\n\nggplot(data = dds.discr, \n       aes(x = expenditures,\n           color = ethnicity)) + \n  geom_density() + \n  labs(x = \"Annual Expenditures ($)\")"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#ridgeline-plot",
    "href": "slides_md/Day03_bsta511_md.html#ridgeline-plot",
    "title": "Day 3: Data visualization",
    "section": "Ridgeline plot",
    "text": "Ridgeline plot\n\n\n\n# library(ggridges)\nggplot(data = dds.discr,\n       aes(x = expenditures,\n           y = ethnicity,      \n           fill = ethnicity)      \n       ) + \n  geom_density_ridges(      \n    alpha = 0.3,      \n    show.legend = FALSE) +      \n  labs(x = \"Annual Expenditures ($)\",\n       y = \"Race and ethnicity\",\n       title =        \n\"Expenditures by race and \n       \\nethnicity\")       \n\n\n\n\nPicking joint bandwidth of 5520"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#transforming-data-1.4.5",
    "href": "slides_md/Day03_bsta511_md.html#transforming-data-1.4.5",
    "title": "Day 3: Data visualization",
    "section": "Transforming data (1.4.5)",
    "text": "Transforming data (1.4.5)\n\n\nWe sometimes apply a transformation to highly skewed data to make it more symmetric\nLog transformations are often used for skewed right data\n\n\n\n\nx = expenditures\n\nggplot(data = dds.discr, \n       aes(x = expenditures)) +  \n  geom_density() \n\n\n\n\n\nx = log(expenditures)\n\nggplot(data = dds.discr, \n       aes(x = log(expenditures))) +  \n  geom_density()"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#scatterplots",
    "href": "slides_md/Day03_bsta511_md.html#scatterplots",
    "title": "Day 3: Data visualization",
    "section": "Scatterplots",
    "text": "Scatterplots\n\n\n\nggplot(data = dds.discr, \n       aes(x = age,\n           y = expenditures)) + \n  geom_point() +       \n  labs(x = \"Age\",\n       y = \"Annual Expenditures ($)\") \n\n\nResponse vs. explanatory variables (Section 1.2.3)\n\nA response variable measures the outcome of interest in a study\nA study will typically examine whether the values of a response variable differ as values of an explanatory variable change\n\n\n\n\n\n\n\n\n\n\n\nDescribe the association between the variables"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#describing-associations-between-2-numerical-variables",
    "href": "slides_md/Day03_bsta511_md.html#describing-associations-between-2-numerical-variables",
    "title": "Day 3: Data visualization",
    "section": "Describing associations between 2 numerical variables",
    "text": "Describing associations between 2 numerical variables\n\n\n\nTwo variables \\(x\\) and \\(y\\) are\n\npositively associated if \\(y\\) increases as \\(x\\) increases.\nnegatively associated if \\(y\\) decreases as \\(x\\) increases.\nIf there is no association between the variables, then we say they are uncorrelated or independent.\n\n\n\n\n\n\n\n\n\n\n\n\nThe term “association” is a very general term.\n\nCan be used for numerical or categorical variables\nNot specifically referring to linear associations"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#pearson-correlation-coefficient-r",
    "href": "slides_md/Day03_bsta511_md.html#pearson-correlation-coefficient-r",
    "title": "Day 3: Data visualization",
    "section": "(Pearson) Correlation coefficient \\(r\\)",
    "text": "(Pearson) Correlation coefficient \\(r\\)\n\n\n\\(r = -1\\) indicates a perfect negative linear relationship: As one variable increases, the value of the other variable tends to go down, following a straight line.\n\\(r = 0\\) indicates no linear relationship: The values of both variables go up/down independently of each other.\n\\(r = 1\\) indicates a perfect positive linear relationship: As the value of one variable goes up, the value of the other variable tends to go up as well in a linear fashion.\nThe closer \\(r\\) is to ±1, the stronger the linear association."
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#pearson-correlation-coefficient-r-formula",
    "href": "slides_md/Day03_bsta511_md.html#pearson-correlation-coefficient-r-formula",
    "title": "Day 3: Data visualization",
    "section": "(Pearson) Correlation coefficient (r): formula",
    "text": "(Pearson) Correlation coefficient (r): formula\n\nThe (Peasron) correlation coefficient of variables \\(x\\) and \\(y\\) can be computed using the formula \\[r = \\frac{1}{n-1}\\sum_{i=1}^{n}\\Big(\\frac{x_i - \\bar{x}}{s_x}\\Big)\\Big(\\frac{y_i - \\bar{y}}{s_y}\\Big)\\] where\n\n\\((x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\) are the \\(n\\) paired values of the variables \\(x\\) and \\(y\\)\n\\(s_x\\) and \\(s_y\\) are the sample standard deviations of the variables \\(x\\) and \\(y\\), respectively\n\n\n\ncor(dds.discr$age, dds.discr$expenditures)\n\n[1] 0.8432422"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#guess-the-correlation-game",
    "href": "slides_md/Day03_bsta511_md.html#guess-the-correlation-game",
    "title": "Day 3: Data visualization",
    "section": "Guess the correlation game!",
    "text": "Guess the correlation game!\n\n\n\nRossman & Chance’s applet\n\n\n\n\n\nTracks performance of guess vs. actual, error vs. actual, and error vs. trial\nhttp://www.rossmanchance.com/applets/GuessCorrelation.html\n\nOr, for the Atari-like experience\n\n\n\nhttp://guessthecorrelation.com/"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#scatterplots-with-color-coded-dots",
    "href": "slides_md/Day03_bsta511_md.html#scatterplots-with-color-coded-dots",
    "title": "Day 3: Data visualization",
    "section": "Scatterplots with color-coded dots",
    "text": "Scatterplots with color-coded dots\nDescribe the association between the variables\n\nggplot(data = dds.discr, \n       aes(x = age, y = expenditures,\n           color = ethnicity)) +   \n  geom_point(alpha = .5) +       \n  labs(x = \"Age\", y = \"Annual Expenditures ($)\") +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#barplots",
    "href": "slides_md/Day03_bsta511_md.html#barplots",
    "title": "Day 3: Data visualization",
    "section": "Barplots",
    "text": "Barplots\n\n\n\nCounts (below) vs.\npercentages (right)\n\n\nggplot(data = dds.discr, \n       aes(x = ethnicity)) +\n  geom_bar() \n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = ethnicity)) +\n  geom_bar(aes(y = stat(prop),  \n               group = 1)) + \n  scale_y_continuous(labels =  \n      scales::percent_format())"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#barplots-with-2-variables-segmented-bar-plots",
    "href": "slides_md/Day03_bsta511_md.html#barplots-with-2-variables-segmented-bar-plots",
    "title": "Day 3: Data visualization",
    "section": "Barplots with 2 variables: segmented bar plots",
    "text": "Barplots with 2 variables: segmented bar plots\n\n\n\nggplot(data = dds.discr, \n       aes(x = ethnicity,\n           fill = age.cohort)) + \n  geom_bar() \n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = ethnicity,\n           fill = age.cohort)) + \n  geom_bar(position = \"fill\")"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#barplots-with-2-variables-side-by-side-bar-plots",
    "href": "slides_md/Day03_bsta511_md.html#barplots-with-2-variables-side-by-side-bar-plots",
    "title": "Day 3: Data visualization",
    "section": "Barplots with 2 variables: side-by-side bar plots",
    "text": "Barplots with 2 variables: side-by-side bar plots\n\nggplot(data = dds.discr, \n       aes(x = ethnicity,\n           fill = age.cohort)) + \n  geom_bar(position = \"dodge\")"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#frequency-tables-count",
    "href": "slides_md/Day03_bsta511_md.html#frequency-tables-count",
    "title": "Day 3: Data visualization",
    "section": "Frequency tables: count()",
    "text": "Frequency tables: count()\n\n\n\n\ncount is from the dplyr package\nthe output is a long tibble, and not a “nice” table\n\n\n\ndds.discr %&gt;% count(ethnicity)\n\n# A tibble: 8 × 2\n  ethnicity              n\n  &lt;fct&gt;              &lt;int&gt;\n1 American Indian        4\n2 Asian                129\n3 Black                 59\n4 Hispanic             376\n5 Multi Race            26\n6 Native Hawaiian        3\n7 Other                  2\n8 White not Hispanic   401\n\n\n\n\ndds.discr %&gt;% \n  count(ethnicity, age.cohort)\n\n# A tibble: 35 × 3\n   ethnicity       age.cohort     n\n   &lt;fct&gt;           &lt;fct&gt;      &lt;int&gt;\n 1 American Indian 13-17          1\n 2 American Indian 22-50          1\n 3 American Indian 51+            2\n 4 Asian           0-5            8\n 5 Asian           6-12          18\n 6 Asian           13-17         20\n 7 Asian           18-21         41\n 8 Asian           22-50         29\n 9 Asian           51+           13\n10 Black           0-5            3\n# ℹ 25 more rows"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#how-to-use-the-pipe",
    "href": "slides_md/Day03_bsta511_md.html#how-to-use-the-pipe",
    "title": "Day 3: Data visualization",
    "section": "How to use the pipe %>%",
    "text": "How to use the pipe %&gt;%\n\nThe pipe operator %&gt;% strings together commands to be performed sequentially\n\n\ndds.discr %&gt;% head(n=3)      # pronounce %&gt;% as \"then\"\n\n# A tibble: 3 × 6\n     id age.cohort   age gender expenditures ethnicity         \n  &lt;int&gt; &lt;fct&gt;      &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt;             \n1 10210 13-17         17 Female         2113 White not Hispanic\n2 10409 22-50         37 Male          41924 White not Hispanic\n3 10486 0-5            3 Male           1454 Hispanic          \n\n\n\n\nAlways first list the tibble that the commands are being applied to\nCan use multiple pipes to run multiple commands in sequence\n\nWhat does the following code do?\n\n\n\n\ndds.discr %&gt;% head(n=3) %&gt;% summary()"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#frequency-tables-janitor-packages-tabyl-function",
    "href": "slides_md/Day03_bsta511_md.html#frequency-tables-janitor-packages-tabyl-function",
    "title": "Day 3: Data visualization",
    "section": "Frequency tables: janitor package’s tabyl function",
    "text": "Frequency tables: janitor package’s tabyl function\n\n\n\n# default table\ndds.discr %&gt;% \n  tabyl(ethnicity)  \n\n          ethnicity   n percent\n    American Indian   4   0.004\n              Asian 129   0.129\n              Black  59   0.059\n           Hispanic 376   0.376\n         Multi Race  26   0.026\n    Native Hawaiian   3   0.003\n              Other   2   0.002\n White not Hispanic 401   0.401\n\n\n\nadorn_ your table!\n\ndds.discr %&gt;% \n  tabyl(ethnicity) %&gt;%\n  adorn_totals(\"row\") %&gt;% \n  adorn_pct_formatting(digits=2)  \n\n          ethnicity    n percent\n    American Indian    4   0.40%\n              Asian  129  12.90%\n              Black   59   5.90%\n           Hispanic  376  37.60%\n         Multi Race   26   2.60%\n    Native Hawaiian    3   0.30%\n              Other    2   0.20%\n White not Hispanic  401  40.10%\n              Total 1000 100.00%"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#relative-frequency-table",
    "href": "slides_md/Day03_bsta511_md.html#relative-frequency-table",
    "title": "Day 3: Data visualization",
    "section": "Relative frequency table",
    "text": "Relative frequency table\n\n\n\n\nA relative frequency table shows proportions (or percentages) instead of counts\nTo the right I removed (deselected) the counts column (n) to create a relative frequency table\n\n\n\n\ndds.discr %&gt;% \n  tabyl(ethnicity) %&gt;%\n  adorn_totals(\"row\") %&gt;% \n  adorn_pct_formatting(digits=2) %&gt;%   \n  select(-n) \n\n          ethnicity percent\n    American Indian   0.40%\n              Asian  12.90%\n              Black   5.90%\n           Hispanic  37.60%\n         Multi Race   2.60%\n    Native Hawaiian   0.30%\n              Other   0.20%\n White not Hispanic  40.10%\n              Total 100.00%"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#contingency-tables-two-way-tables",
    "href": "slides_md/Day03_bsta511_md.html#contingency-tables-two-way-tables",
    "title": "Day 3: Data visualization",
    "section": "Contingency tables (two-way tables)",
    "text": "Contingency tables (two-way tables)\n\n\n\n\nContingency tables summarize data for two categorical variables\n\nwith each value in the table representing the number of times\na particular combination of outcomes occurs\n\nRow & column totals\nare sometimes called marginal totals\n\n\n\n\ndds.discr %&gt;% \n  tabyl(ethnicity, gender) %&gt;%    \n  adorn_totals(c(\"row\", \"col\"))    \n\n          ethnicity Female Male Total\n    American Indian      3    1     4\n              Asian     61   68   129\n              Black     26   33    59\n           Hispanic    192  184   376\n         Multi Race     13   13    26\n    Native Hawaiian      2    1     3\n              Other      1    1     2\n White not Hispanic    205  196   401\n              Total    503  497  1000"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#contingency-tables-with-percentages",
    "href": "slides_md/Day03_bsta511_md.html#contingency-tables-with-percentages",
    "title": "Day 3: Data visualization",
    "section": "Contingency tables with percentages",
    "text": "Contingency tables with percentages\n\ndds.discr %&gt;% \n  tabyl(ethnicity, age.cohort) %&gt;%\n  adorn_totals(c(\"row\")) %&gt;%\n  adorn_percentages(\"row\") %&gt;%   \n  adorn_pct_formatting(digits=0) %&gt;%    \n  adorn_ns()    \n\n          ethnicity      0-5      6-12      13-17     18-21     22-50       51+\n    American Indian  0%  (0)  0%   (0)  25%   (1)  0%   (0) 25%   (1) 50%   (2)\n              Asian  6%  (8) 14%  (18)  16%  (20) 32%  (41) 22%  (29) 10%  (13)\n              Black  5%  (3) 19%  (11)  20%  (12) 15%   (9) 29%  (17) 12%   (7)\n           Hispanic 12% (44) 24%  (91)  27% (103) 21%  (78) 11%  (43)  5%  (17)\n         Multi Race 27%  (7) 35%   (9)  27%   (7)  8%   (2)  4%   (1)  0%   (0)\n    Native Hawaiian  0%  (0)  0%   (0)   0%   (0)  0%   (0) 67%   (2) 33%   (1)\n              Other  0%  (0)  0%   (0) 100%   (2)  0%   (0)  0%   (0)  0%   (0)\n White not Hispanic  5% (20) 11%  (46)  17%  (67) 17%  (69) 33% (133) 16%  (66)\n              Total  8% (82) 18% (175)  21% (212) 20% (199) 23% (226) 11% (106)"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#mean-annual-dds-expenditures-by-raceethnicity",
    "href": "slides_md/Day03_bsta511_md.html#mean-annual-dds-expenditures-by-raceethnicity",
    "title": "Day 3: Data visualization",
    "section": "Mean annual DDS expenditures by race/ethnicity",
    "text": "Mean annual DDS expenditures by race/ethnicity\n\n\n\nmean(dds.discr$expenditures)\n\n[1] 18065.79\n\ndds.discr %&gt;% \n  summarize(\n    ave = mean(expenditures),\n    SD = sd(expenditures),\n    med = median(expenditures))\n\n# A tibble: 1 × 3\n     ave     SD   med\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 18066. 19543.  7026\n\n\n\n\ndds.discr %&gt;% \n  group_by(ethnicity) %&gt;% \n  summarize(\n    ave = mean(expenditures),\n    SD = sd(expenditures),\n    med = median(expenditures))\n\n# A tibble: 8 × 4\n  ethnicity             ave     SD    med\n  &lt;fct&gt;               &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 American Indian    36438. 25694. 41818.\n2 Asian              18392. 19209.  9369 \n3 Black              20885. 20549.  8687 \n4 Hispanic           11066. 15630.  3952 \n5 Multi Race          4457.  7332.  2622 \n6 Native Hawaiian    42782.  6576. 40727 \n7 Other               3316.  1836.  3316.\n8 White not Hispanic 24698. 20604. 15718"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#get_summary_stats-from-rstatix-package",
    "href": "slides_md/Day03_bsta511_md.html#get_summary_stats-from-rstatix-package",
    "title": "Day 3: Data visualization",
    "section": "get_summary_stats() from rstatix package",
    "text": "get_summary_stats() from rstatix package\n\ndds.discr %&gt;% get_summary_stats()\n\n# A tibble: 3 × 13\n  variable         n   min   max median     q1     q3   iqr    mad   mean     sd\n  &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 id            1000 10210 99898 55384. 31809. 76135. 44326 3.27e4 5.47e4 2.56e4\n2 age           1000     0    95    18     12     26     14 1.04e1 2.28e1 1.85e1\n3 expenditures  1000   222 75098  7026   2899. 37713. 34814 7.76e3 1.81e4 1.95e4\n# ℹ 2 more variables: se &lt;dbl&gt;, ci &lt;dbl&gt;\n\ndds.discr %&gt;% \n  group_by(ethnicity) %&gt;%\n  get_summary_stats(expenditures, type = \"common\")\n\n# A tibble: 8 × 11\n  ethnicity variable     n   min   max median    iqr   mean     sd     se     ci\n  &lt;fct&gt;     &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 American… expendi…     4  3726 58392 41818. 34085. 36438. 25694. 12847. 40885.\n2 Asian     expendi…   129   374 75098  9369  30892  18392. 19209.  1691.  3346.\n3 Black     expendi…    59   240 60808  8687  37987  20885. 20549.  2675.  5355.\n4 Hispanic  expendi…   376   222 65581  3952   7961. 11066. 15630.   806.  1585.\n5 Multi Ra… expendi…    26   669 38619  2622   2060.  4457.  7332.  1438.  2962.\n6 Native H… expendi…     3 37479 50141 40727   6331  42782.  6576.  3797. 16337.\n7 Other     expendi…     2  2018  4615  3316.  1298.  3316.  1836.  1298. 16499.\n8 White no… expendi…   401   340 68890 15718  39157  24698. 20604.  1029.  2023."
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#how-to-force-all-output-to-be-shown-12",
    "href": "slides_md/Day03_bsta511_md.html#how-to-force-all-output-to-be-shown-12",
    "title": "Day 3: Data visualization",
    "section": "How to force all output to be shown? (1/2)",
    "text": "How to force all output to be shown? (1/2)\nUse kable() from the knitr package.\n\ndds.discr %&gt;% get_summary_stats() %&gt;% kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nn\nmin\nmax\nmedian\nq1\nq3\niqr\nmad\nmean\nsd\nse\nci\n\n\n\n\nid\n1000\n10210\n99898\n55384.5\n31808.75\n76134.75\n44326\n32734.325\n54662.85\n25643.673\n810.924\n1591.310\n\n\nage\n1000\n0\n95\n18.0\n12.00\n26.00\n14\n10.378\n22.80\n18.462\n0.584\n1.146\n\n\nexpenditures\n1000\n222\n75098\n7026.0\n2898.75\n37712.75\n34814\n7760.670\n18065.79\n19542.831\n617.999\n1212.724"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#how-to-force-all-output-to-be-shown-knitr-22",
    "href": "slides_md/Day03_bsta511_md.html#how-to-force-all-output-to-be-shown-knitr-22",
    "title": "Day 3: Data visualization",
    "section": "How to force all output to be shown? knitr (2/2)",
    "text": "How to force all output to be shown? knitr (2/2)\nUse kable() from the knitr package.\n\ndds.discr %&gt;% \n  group_by(ethnicity) %&gt;%\n  get_summary_stats(expenditures, type = \"common\") %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nethnicity\nvariable\nn\nmin\nmax\nmedian\niqr\nmean\nsd\nse\nci\n\n\n\n\nAmerican Indian\nexpenditures\n4\n3726\n58392\n41817.5\n34085.25\n36438.250\n25693.912\n12846.956\n40884.748\n\n\nAsian\nexpenditures\n129\n374\n75098\n9369.0\n30892.00\n18392.372\n19209.225\n1691.278\n3346.482\n\n\nBlack\nexpenditures\n59\n240\n60808\n8687.0\n37987.00\n20884.593\n20549.274\n2675.288\n5355.170\n\n\nHispanic\nexpenditures\n376\n222\n65581\n3952.0\n7961.25\n11065.569\n15629.847\n806.048\n1584.940\n\n\nMulti Race\nexpenditures\n26\n669\n38619\n2622.0\n2059.75\n4456.731\n7332.135\n1437.950\n2961.514\n\n\nNative Hawaiian\nexpenditures\n3\n37479\n50141\n40727.0\n6331.00\n42782.333\n6576.462\n3796.922\n16336.838\n\n\nOther\nexpenditures\n2\n2018\n4615\n3316.5\n1298.50\n3316.500\n1836.356\n1298.500\n16499.007\n\n\nWhite not Hispanic\nexpenditures\n401\n340\n68890\n15718.0\n39157.00\n24697.549\n20604.376\n1028.933\n2022.793"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md.html#case-study-discrimination-in-developmental-disability-support-1.7.1",
    "href": "slides_md/Day03_bsta511_md.html#case-study-discrimination-in-developmental-disability-support-1.7.1",
    "title": "Day 3: Data visualization",
    "section": "Case study: discrimination in developmental disability support (1.7.1)",
    "text": "Case study: discrimination in developmental disability support (1.7.1)\n\nPrevious research\n\nResearchers examined DDS expenditures for developmentally disabled residents by ethnicity\nFound that the mean annual expenditures on Hispanics was less than that on White non-Hispanics.\n\nResult: an allegation of ethnic discrimination was brought against the California DDS.\nQuestion: Are the data sufficient evidence of ethnic discrimination?"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html",
    "href": "slides_md/Day01_bsta511_md.html",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "",
    "text": "Artwork by @allison_horst\n\n\n\n\n\n\n\nA programming language\nFocus on statistical modeling and data analysis\n\nimport data, manipulate data, run statistics, make plots\n\nUseful for data science\nGreat visualizations\nAlso useful for most anything else you’d want to tell a computer to do\nInterfaces with other languages i.e. python, C++, bash\n\n\n\n\n\nFor the history and details: Wikipedia\n\nan interpreted language (run it through a command line)\nprocedural programming with functions\nWhy “R”?? Scheme inspired S (invented at Bell Labs in 1976) which inspired R since 1st letters of original authors (free and open source! in 2000)\n\n\n\n\n\n\nR is a programming language\n\nRStudio is an integrated development environment (IDE)\n= an interface to use R (with perks!)\n\n\n\n\n\nModern Dive\n\n\n\n\n\n\n\n\nModern Dive\n\n\n\n\n\n\n\n\nEmma Rand\n\n\nRead more about RStudio’s layout in Section 3.4 of “Getting Used to R, RStudio, and R Markdown” (Ismay and Kennedy 2016)"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#what-is-r",
    "href": "slides_md/Day01_bsta511_md.html#what-is-r",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "",
    "text": "A programming language\nFocus on statistical modeling and data analysis\n\nimport data, manipulate data, run statistics, make plots\n\nUseful for data science\nGreat visualizations\nAlso useful for most anything else you’d want to tell a computer to do\nInterfaces with other languages i.e. python, C++, bash\n\n\n\n\n\nFor the history and details: Wikipedia\n\nan interpreted language (run it through a command line)\nprocedural programming with functions\nWhy “R”?? Scheme inspired S (invented at Bell Labs in 1976) which inspired R since 1st letters of original authors (free and open source! in 2000)"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#what-is-rstudio",
    "href": "slides_md/Day01_bsta511_md.html#what-is-rstudio",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "",
    "text": "R is a programming language\n\nRStudio is an integrated development environment (IDE)\n= an interface to use R (with perks!)\n\n\n\n\n\nModern Dive"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#open-rstudio-on-your-computer-not-r",
    "href": "slides_md/Day01_bsta511_md.html#open-rstudio-on-your-computer-not-r",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "",
    "text": "Modern Dive"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#rstudio-anatomy",
    "href": "slides_md/Day01_bsta511_md.html#rstudio-anatomy",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "",
    "text": "Emma Rand\n\n\nRead more about RStudio’s layout in Section 3.4 of “Getting Used to R, RStudio, and R Markdown” (Ismay and Kennedy 2016)"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#coding-in-the-console",
    "href": "slides_md/Day01_bsta511_md.html#coding-in-the-console",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Coding in the console",
    "text": "Coding in the console\n\n\nWhen you first open R, the console should be empty.\n\n\n\n\n\n\nTyping and executing code in the console \n\nType code in the console (blue text)\nPress return to execute the code\nOutput shown below in black"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#math-calculations-using-r",
    "href": "slides_md/Day01_bsta511_md.html#math-calculations-using-r",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Math calculations using R",
    "text": "Math calculations using R\n\nRules for order of operations are followed\nSpaces between numbers and characters are ignored\n\n\n\n\n10^2\n\n[1] 100\n\n3 ^ 7\n\n[1] 2187\n\n6/9\n\n[1] 0.6666667\n\n9-43\n\n[1] -34\n\n\n\n\n4^3-2* 7+9 /2\n\n[1] 54.5\n\n\nThe equation above is computed as \\[4^3 − (2 \\cdot 7) + \\frac{9}{2}\\]"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#variables-saved-r-objects",
    "href": "slides_md/Day01_bsta511_md.html#variables-saved-r-objects",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Variables (saved R objects)",
    "text": "Variables (saved R objects)\nVariables are used to store data, figures, model output, etc.\n\n\n\nCan assign a variable using either = or &lt;-\n\nUsing &lt;- is preferable\n\n\nAssign just one value:\n\nx = 5\nx\n\n[1] 5\n\nx &lt;- 5\nx\n\n[1] 5\n\n\n\n\n\nAssign a vector of values\n\nConsecutive integers using :\n\n\na &lt;- 3:10\na\n\n[1]  3  4  5  6  7  8  9 10\n\n\n\nConcatenate a string of numbers\n\n\nb &lt;- c(5, 12, 2, 100, 8)\nb\n\n[1]   5  12   2 100   8"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#doing-math-with-variables",
    "href": "slides_md/Day01_bsta511_md.html#doing-math-with-variables",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Doing math with variables",
    "text": "Doing math with variables\n\n\nMath using variables with just one value\n\nx &lt;- 5\nx\n\n[1] 5\n\nx + 3\n\n[1] 8\n\ny &lt;- x^2\ny\n\n[1] 25\n\n\n\nMath on vectors of values:\nelement-wise computation\n\na &lt;- 3:6\na\n\n[1] 3 4 5 6\n\na+2; a*3\n\n[1] 5 6 7 8\n\n\n[1]  9 12 15 18\n\na*a\n\n[1]  9 16 25 36"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#variables-can-include-text-characters",
    "href": "slides_md/Day01_bsta511_md.html#variables-can-include-text-characters",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Variables can include text (characters)",
    "text": "Variables can include text (characters)\n\nhi &lt;- \"hello\"\nhi\n\n[1] \"hello\"\n\ngreetings &lt;- c(\"Guten Tag\", \"Hola\", hi)\ngreetings\n\n[1] \"Guten Tag\" \"Hola\"      \"hello\""
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#using-functions",
    "href": "slides_md/Day01_bsta511_md.html#using-functions",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Using functions",
    "text": "Using functions\n\nmean() is an example of a function\nfunctions have “arguments” that can be specified within the ()\n?mean in console will show help file for mean()\n\n\n\nFunction arguments specified by name:\n\nmean(x = 1:4)\n\n[1] 2.5\n\n\n\nseq(from = 1, to = 12, by = 3)\n\n[1]  1  4  7 10\n\nseq(by = 3, to = 12, from = 1)\n\n[1]  1  4  7 10\n\n\n\nFunction arguments not specified, but listed in order:\n\nmean(1:4)\n\n[1] 2.5\n\n\n\nseq(1, 12, 3)\n\n[1]  1  4  7 10"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#common-console-errors-12",
    "href": "slides_md/Day01_bsta511_md.html#common-console-errors-12",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Common console errors (1/2)",
    "text": "Common console errors (1/2)\nIncomplete commands\n\n\n\nWhen the console is waiting for a new command, the prompt line begins with &gt;\n\nIf the console prompt is +, then a previous command is incomplete\nYou can finish typing the command in the console window\n\n\n\nExample:\n\n&gt; 3 + (2*6\n+ )\n\n[1] 15"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#common-console-errors-22",
    "href": "slides_md/Day01_bsta511_md.html#common-console-errors-22",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Common console errors (2/2)",
    "text": "Common console errors (2/2)\nObject is not found\n\nThis happens when text is entered for a non-existent variable (object)\n\nExample:\n\nhello\n\nError in eval(expr, envir, enclos): object 'hello' not found\n\n\n\nCan be due to missing quotes\n\n\ninstall.packages(dplyr) \n\nError in install.packages(dplyr): object 'dplyr' not found\n\n# correct code is: install.packages(\"dplyr\")"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#example-creating-an-html-file",
    "href": "slides_md/Day01_bsta511_md.html#example-creating-an-html-file",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Example: creating an html file",
    "text": "Example: creating an html file\n\n\n.qmd file \n\nhtml output"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#quarto-.qmd-file-code-text",
    "href": "slides_md/Day01_bsta511_md.html#quarto-.qmd-file-code-text",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Quarto = .qmd file = Code + text",
    "text": "Quarto = .qmd file = Code + text\nknitr is a package that converts .qmd files containing code + markdown syntax to a plain text .md markdown file, and then to other formats (html, pdf, Word, etc)\n\n\n\nArtwork from “Hello, Quarto” keynote by Julia Lowndes and Mine Çetinkaya-Rundel, presented at RStudio Conference 2022. Illustrated by Allison Horst."
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#create-a-quarto-file-.qmd",
    "href": "slides_md/Day01_bsta511_md.html#create-a-quarto-file-.qmd",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "1. Create a Quarto file (.qmd)",
    "text": "1. Create a Quarto file (.qmd)\nTwo options:\n\nclick on File \\(\\rightarrow\\) New File \\(\\rightarrow\\) Quarto Document…\\(\\rightarrow\\) OK,\nor in upper left corner of RStudio click on  \\(\\rightarrow\\) \n\n\n\nPop-up window selections:\n\nEnter a title and your name\nSelect HTML output format (default)\nEngine: select Knitr\nEditor: Select Use visual markdown editor\nClick Create"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#create-a-quarto-file-.qmd-1",
    "href": "slides_md/Day01_bsta511_md.html#create-a-quarto-file-.qmd-1",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "2. Create a Quarto file (.qmd)",
    "text": "2. Create a Quarto file (.qmd)\n\nAfter clicking on Create, you should then see the following in your editor window:"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#save-the-quarto-file-.qmd",
    "href": "slides_md/Day01_bsta511_md.html#save-the-quarto-file-.qmd",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "3. Save the Quarto file (.qmd)",
    "text": "3. Save the Quarto file (.qmd)\n\nSave the file by\n\nselecting File -&gt; Save,\nor clicking on  (towards the left above the scripting window),\nor keyboard shortcut\n\nPC: Ctrl + s\nMac: Command + s\n\n\nYou will need to specify\n\na filename to save the file as\n\nALWAYS use .qmd as the filename extension for Quarto files\n\nthe folder to save the file in"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#create-html-file",
    "href": "slides_md/Day01_bsta511_md.html#create-html-file",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "4. Create html file",
    "text": "4. Create html file\nWe create the html file by rendering the .qmd file.\nTwo options:\n\nclick on the Render icon  at the top of the editor window,\nor use keyboard shortcuts\n\nMac: Command+Shift+K\nPC: Ctrl+Shift+K\n\n\n\nA new window will open with the html output.\nYou will now see both .qmd and .html files in the folder where you saved the .qmd file.\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe template .qmd file that RStudio creates will render to an html file by default.\nThe output format can be changed to create a Word doc, pdf, slides, etc."
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#qmd-file-vs.-its-html-output",
    "href": "slides_md/Day01_bsta511_md.html#qmd-file-vs.-its-html-output",
    "title": "Day 1: Intro to R & Rstudio",
    "section": ".qmd file vs. its html output",
    "text": ".qmd file vs. its html output\n\n\n.qmd file \n\nhtml output"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#formatting-text",
    "href": "slides_md/Day01_bsta511_md.html#formatting-text",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Formatting text",
    "text": "Formatting text\n\nbold, italics, superscripts & subscripts, strikethrough, verbatim, etc.\n\n\nText is formatted through a markup language called Markdown (Wikipedia)\n\nOther markup languages include html (webapges) and LaTeX (math)\nAll text formatting is specified via code\n“Markdown is a plain text format that is designed to be easy to write, and, even more importantly, easy to read” 1\n\nNewer versions of RStudio include a Visual editor as well that makes formatting text similar to using a word processor."
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#formatting-text-visual-editor",
    "href": "slides_md/Day01_bsta511_md.html#formatting-text-visual-editor",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Formatting text: Visual editor",
    "text": "Formatting text: Visual editor\n\nUsing the Visual editor is similar to using a wordprocessor, such as Word\nKeyboard shortcuts usually work as well (shown for Mac below)"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#practice",
    "href": "slides_md/Day01_bsta511_md.html#practice",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Practice",
    "text": "Practice\n\nPart 1\n\nUsing the visual editor, practice formatting text in your qmd file, such as making text bold, italicized, and in code format.\nAdd 1st, 2nd, and 3rd level headers\nAdd a list with a\n\nsub-list (bullet and/or numbered)\n\nAdd a table\nAdd whatever else you are interested in!\n\nPart 2\n\nSwitch back to the Source editor and examine the markdown code that was used for the formatting.\n\n\nQuestions:\n\nWhat went smoothly?\nWhat hurdles did you encounter?"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#formatting-text-markdown",
    "href": "slides_md/Day01_bsta511_md.html#formatting-text-markdown",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Formatting text: Markdown",
    "text": "Formatting text: Markdown\n\n\n\n\n\n\n\nMarkdown:\nOutput:\n\n\n\n\n*This text is in italics*, but _so is this text_.\nThis text is in italics, but so is this text.\n\n\n**Bold** also has __2 options__\nBold also has 2 options\n\n\n~~Should this be deleted?~~\nShould this be deleted?\n\n\nNeed^super^ or~sub~ scripts?\nNeedsuper orsub scripts?\n\n\n`Code is often formatted as verbatim`\nCode is often formatted as verbatim\n\n\n&gt;This is a block quote.\n\nThis is a block quote."
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#headers",
    "href": "slides_md/Day01_bsta511_md.html#headers",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Headers",
    "text": "Headers\n\nOrganize your documents using headers to create sections and subsections\nUse # at the beginning of the line to create headers\n\n\n\nText in editor:\n\n\n\n\n\n\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nMake sure there is no space before the #, and there IS a space after the # in order for the header to work properly."
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#rstudio-tip",
    "href": "slides_md/Day01_bsta511_md.html#rstudio-tip",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "RStudio tip",
    "text": "RStudio tip\nYou can easily navigate through your .qmd file if you use headers to outline your text"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#code-chunks",
    "href": "slides_md/Day01_bsta511_md.html#code-chunks",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Code chunks",
    "text": "Code chunks\n\n\n.qmd file \n\nhtml output"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#create-a-code-chunk",
    "href": "slides_md/Day01_bsta511_md.html#create-a-code-chunk",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Create a code chunk",
    "text": "Create a code chunk\n3 options to create a code chunk\n\nClick on  at top right of the editor window, or\nKeyboard shortcut\n\n\n\n\nMac\nCommand + Option + I\n\n\nPC\nCtrl + Alt + I\n\n\n\n\nVisual editor: Select Insert -&gt; Executable Cell -&gt; R"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#what-does-a-code-chunk-look-like",
    "href": "slides_md/Day01_bsta511_md.html#what-does-a-code-chunk-look-like",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "What does a code chunk look like?",
    "text": "What does a code chunk look like?\nAn empty code chunk looks like this:\nVisual editor\n\nSource editor\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that a code chunks start with ```{r} and ends with ```. Make sure there is no space before ```."
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#enter-and-run-code-1n",
    "href": "slides_md/Day01_bsta511_md.html#enter-and-run-code-1n",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Enter and run code (1/n)",
    "text": "Enter and run code (1/n)\n\nType R code inside code chunks\nSelect code you want to run, by\n\nplacing the cursor in the line of code you want to run,\nor highlighting the code you want to run\n\n\n\n\n\nRun selected code by\n\nclicking on the  button in the top right corner of the scripting window and choosing Run Selected Line(s),\nor typing one of the following key combinations:\n\n\n\n\n\nMac\nctrl + return\n\n\nPC\ncommand + return\n\n\n\n\nWhere does the output appear?"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#enter-and-run-code-2n",
    "href": "slides_md/Day01_bsta511_md.html#enter-and-run-code-2n",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Enter and run code (2/n)",
    "text": "Enter and run code (2/n)\n\n\nRun all code in a chunk by\n\nby clicking the play button in the top right corner of the chunk\n\nThe code output appears below the code chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe output should also appear in the Console.\nSettings can be changed so that the output appears only in the Console and not below the code chunk:\n\nSelect  (to right of Render button) and then Chunk Output in Console."
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#useful-keyboard-shortcuts",
    "href": "slides_md/Day01_bsta511_md.html#useful-keyboard-shortcuts",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Useful keyboard shortcuts",
    "text": "Useful keyboard shortcuts\nFull list of keyboard shortcuts\n \n\n\n\n\n\n\n\n\naction\nmac\nwindows/linux\n\n\n\n\nRun code in qmd (or script)\ncmd + enter\nctrl + enter\n\n\n&lt;-\noption + -\nalt + -\n\n\ninterrupt currently running command\nesc\nesc\n\n\nin console, retrieve previously run code\nup/down\nup/down\n\n\nkeyboard shortcut help\noption + shift + k\nalt + shift + k\n\n\n\n\n\n\nPractice\nTry typing code below in your qmd (with shortcut) and evaluating it:\n\ny &lt;- 5\ny"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#yaml-metadata",
    "href": "slides_md/Day01_bsta511_md.html#yaml-metadata",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "YAML metadata",
    "text": "YAML metadata\nMany output options can be set in the YAML metadata, which is the first set of code in the file starting and ending with ---.\n\nIt sets the configuration specifications for the output file\nYAML is an acronym for\n\nyet another markup language, or\nYAML ain’t markup language"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#simple-yaml-example",
    "href": "slides_md/Day01_bsta511_md.html#simple-yaml-example",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Simple YAML example",
    "text": "Simple YAML example\n\n\nThe default YAML includes a title and author that appear at the top of the output file. In the example below, I also added in a date option\n\n\n\n\nYAML:\n\n---\ntitle: \"My first Quarto file\"\nauthor: \"Meike\"\ndate: \"9/25/2023\"\nformat: html\neditor: visual\n---\n\n\nOutput:\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nThe YAML must start and end with 3 dashes ---.\nThe first set of --- must be on the very first line."
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#change-the-output-file-type",
    "href": "slides_md/Day01_bsta511_md.html#change-the-output-file-type",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Change the output file type",
    "text": "Change the output file type\n\n\n\nThe YAML specifies the format of the output file:\n\nhtml, Word, pdf, slides, website, book, etc.\n\nThis is done by changing the format: option\n\n\n\n\n\nIllustration by Alison Hill and Allison Horst, for RStudio.\n\n\n\n\n\n\n\n---\ntitle: \"My first Quarto file\"\nauthor: \"Meike\"\ndate: \"9/25/2023\"\nformat: html\neditor: visual\n---\n\n\n\n\n\n\n\n\n\nOutput format\nYAML\n\n\n\n\nhtml\nformat: html\n\n\nWord\nformat: docx\n\n\npdf2\nformat: pdf\n\n\nhtml slides\nformat: revealjs\n\n\nPPT slides\nformat: pptx"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#section",
    "href": "slides_md/Day01_bsta511_md.html#section",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "",
    "text": "Artwork by @allison_horst"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#you-will-get-frustrated-while-learning-r",
    "href": "slides_md/Day01_bsta511_md.html#you-will-get-frustrated-while-learning-r",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "You WILL get frustrated while learning R!",
    "text": "You WILL get frustrated while learning R!\nFrom Garrett Grolemund’s Prologue of his book Hands-On Programming with R3:\n\nAs you learn to program, you are going to get frustrated. You are learning a new language, and it will take time to become fluent. But frustration is not just natural, it’s actually a positive sign that you should watch for. Frustration is your brain’s way of being lazy; it’s trying to get you to quit and go do something easy or fun. If you want to get physically fitter, you need to push your body even though it complains. If you want to get better at programming, you’ll need to push your brain. Recognize when you get frustrated and see it as a good thing: you’re now stretching yourself. Push yourself a little further every day, and you’ll soon be a confident programmer."
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#resources",
    "href": "slides_md/Day01_bsta511_md.html#resources",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Resources",
    "text": "Resources\n\nOfficial Quarto guide: https://quarto.org/docs/guide/\n\nMarkdown basics: https://quarto.org/docs/authoring/markdown-basics.html\n\nText formatting, headings, linnks, images, lists, tables, equations, diagrams, page breaks, keyboard shortcuts, and more!\n\nCode blocks: https://quarto.org/docs/computations/r.html#code-blocks\n\nChunk options: https://quarto.org/docs/computations/r.html#chunk-options\n\n\nMine Çetinkaya-Rundel’s Quarto tip a day: https://mine-cetinkaya-rundel.github.io/quarto-tip-a-day/\n\nHadley Wickham’s R for Data Science: https://r4ds.hadley.nz/ _ See Chapter 29 for Quarto"
  },
  {
    "objectID": "slides_md/Day01_bsta511_md.html#footnotes",
    "href": "slides_md/Day01_bsta511_md.html#footnotes",
    "title": "Day 1: Intro to R & Rstudio",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFrom Quarto’s Markdown Basics webpage, https://quarto.org/docs/authoring/markdown-basics.html↩︎\nrequires LaTeX installation↩︎\nGrolemund, Garrett. 2014. Hands-on Programming with R. O’Reilly. https://rstudio-education.github.io/hopr/↩︎"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md_part2.html",
    "href": "slides_md/Day03_bsta511_md_part2.html",
    "title": "Day 3: Data visualization - Part 2",
    "section": "",
    "text": "Previous research\n\nResearchers examined DDS expenditures for developmentally disabled residents by ethnicity\nFound that the mean annual expenditures on Hispanics was less than that on White non-Hispanics.\n\nResult: an allegation of ethnic discrimination was brought against the California DDS.\nQuestion: Are the data sufficient evidence of ethnic discrimination?\n\n\n\n\n\n\nThe textbook’s datasets are in the R package oibiostat\nMake sure the oibiostat package is installed before running the code below.\nLoad the oibiostat package and the dataset dds.discr\n\nthe code below needs to be run every time you restart R or render a Qmd file\n\nlibrary(oibiostat)\ndata(\"dds.discr\")\n\n\nAfter loading the dataset dds.discr using data(\"dds.discr\"), you will see dds.discr in the Data list of the Environment window.\n\n\n\n\n\n\nNew: glimpse()\n\nUse glimpse() from the tidyverse package (technically it’s from the dplyr package) to get information about variable types.\nglimpse() tends to have nicer output for tibbles than str()\n\n\n\nlibrary(tidyverse)\nglimpse(dds.discr)  # from tidyverse package (dplyr)\n\nRows: 1,000\nColumns: 6\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ gender       &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ ethnicity    &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nethnicity, age, and expenditures (code on next slide)\n\n\n\n\n\n\n\n\n\nPlot on previous slide\n\n\ndds.discr_Hips_WhnH &lt;- dds.discr %&gt;%  \n  filter(ethnicity == \"White not Hispanic\" | ethnicity == \"Hispanic\" ) %&gt;% \n  droplevels()   # remove empty factor levels\n\nggplot(data = dds.discr_Hips_WhnH,   \n       aes(x = expenditures,\n           y = age.cohort)) + \n  geom_boxplot(color=\"darkgrey\") + \n  facet_grid(rows = \"ethnicity\") +   \n  labs(x = \"Annual Expenditures ($)\",\n       y = \"Race and ethnicity\") +\n  geom_jitter(     \n    aes(color = ethnicity),      \n    alpha = 0.3,      \n    show.legend = FALSE,      \n    position = position_jitter(     \n      height = 0.4))      \n\n\n\n\n\n\n\nmean_expend &lt;- \n  dds.discr_Hips_WhnH %&gt;% \n  group_by(\n    ethnicity, age.cohort)%&gt;% \n  summarize(\n    ave = mean(expenditures))\n\n`summarise()` has grouped output by 'ethnicity'. You can override using the\n`.groups` argument.\n\n\n\n\nmean_expend\n\n# A tibble: 12 × 3\n# Groups:   ethnicity [2]\n   ethnicity          age.cohort    ave\n   &lt;fct&gt;              &lt;fct&gt;       &lt;dbl&gt;\n 1 Hispanic           0-5         1393.\n 2 Hispanic           6-12        2312.\n 3 Hispanic           13-17       3955.\n 4 Hispanic           18-21       9960.\n 5 Hispanic           22-50      40924.\n 6 Hispanic           51+        55585 \n 7 White not Hispanic 0-5         1367.\n 8 White not Hispanic 6-12        2052.\n 9 White not Hispanic 13-17       3904.\n10 White not Hispanic 18-21      10133.\n11 White not Hispanic 22-50      40188.\n12 White not Hispanic 51+        52670.\n\n\n\n\n\n\n\n\n\n\nmean_expend_wide &lt;- \n  mean_expend %&gt;% \n  pivot_wider(\n    names_from = ethnicity,\n    values_from = ave)\n\n\n\nmean_expend_wide\n\n# A tibble: 6 × 3\n  age.cohort Hispanic `White not Hispanic`\n  &lt;fct&gt;         &lt;dbl&gt;                &lt;dbl&gt;\n1 0-5           1393.                1367.\n2 6-12          2312.                2052.\n3 13-17         3955.                3904.\n4 18-21         9960.               10133.\n5 22-50        40924.               40188.\n6 51+          55585                52670.\n\n\n\n\n\n\n\n\nmean_expend_wide &lt;- mean_expend_wide %&gt;% \n  mutate(diff_mean = `White not Hispanic` - Hispanic)\n\nmean_expend_wide\n\n# A tibble: 6 × 4\n  age.cohort Hispanic `White not Hispanic` diff_mean\n  &lt;fct&gt;         &lt;dbl&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n1 0-5           1393.                1367.     -26.3\n2 6-12          2312.                2052.    -260. \n3 13-17         3955.                3904.     -50.9\n4 18-21         9960.               10133.     173. \n5 22-50        40924.               40188.    -736. \n6 51+          55585                52670.   -2915. \n\n\n\nQuestion: Are the data sufficient evidence of ethnic discrimination in DDS expenditures when comparing Hispanics with White non-Hispanics?\n\n\n\n\n\n\nThis case study is an example of confounding known as Simpson’s paradox\nSimpson’s paradox happens when an association observed in several groups disappears or reverses direction when the groups are combined.\nIn other words, an association between two variables \\(X\\) and \\(Y\\) may disappear or reverse direction once data are partitioned into subpopulations based on a third variable \\(Z\\) (i.e., a confounding variable)."
  },
  {
    "objectID": "slides_md/Day03_bsta511_md_part2.html#case-study-discrimination-in-developmental-disability-support-1.7.1",
    "href": "slides_md/Day03_bsta511_md_part2.html#case-study-discrimination-in-developmental-disability-support-1.7.1",
    "title": "Day 3: Data visualization - Part 2",
    "section": "",
    "text": "Previous research\n\nResearchers examined DDS expenditures for developmentally disabled residents by ethnicity\nFound that the mean annual expenditures on Hispanics was less than that on White non-Hispanics.\n\nResult: an allegation of ethnic discrimination was brought against the California DDS.\nQuestion: Are the data sufficient evidence of ethnic discrimination?"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md_part2.html#load-dds.discr-dataset-from-oibiostat-package",
    "href": "slides_md/Day03_bsta511_md_part2.html#load-dds.discr-dataset-from-oibiostat-package",
    "title": "Day 3: Data visualization - Part 2",
    "section": "",
    "text": "The textbook’s datasets are in the R package oibiostat\nMake sure the oibiostat package is installed before running the code below.\nLoad the oibiostat package and the dataset dds.discr\n\nthe code below needs to be run every time you restart R or render a Qmd file\n\nlibrary(oibiostat)\ndata(\"dds.discr\")\n\n\nAfter loading the dataset dds.discr using data(\"dds.discr\"), you will see dds.discr in the Data list of the Environment window."
  },
  {
    "objectID": "slides_md/Day03_bsta511_md_part2.html#glimpse",
    "href": "slides_md/Day03_bsta511_md_part2.html#glimpse",
    "title": "Day 3: Data visualization - Part 2",
    "section": "",
    "text": "New: glimpse()\n\nUse glimpse() from the tidyverse package (technically it’s from the dplyr package) to get information about variable types.\nglimpse() tends to have nicer output for tibbles than str()\n\n\n\nlibrary(tidyverse)\nglimpse(dds.discr)  # from tidyverse package (dplyr)\n\nRows: 1,000\nColumns: 6\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ gender       &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ ethnicity    &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md_part2.html#visualize-in-more-detail",
    "href": "slides_md/Day03_bsta511_md_part2.html#visualize-in-more-detail",
    "title": "Day 3: Data visualization - Part 2",
    "section": "",
    "text": "ethnicity, age, and expenditures (code on next slide)"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md_part2.html#code-for-visualize-in-more-detail-ethnicity-age-and-expenditures",
    "href": "slides_md/Day03_bsta511_md_part2.html#code-for-visualize-in-more-detail-ethnicity-age-and-expenditures",
    "title": "Day 3: Data visualization - Part 2",
    "section": "",
    "text": "Plot on previous slide\n\n\ndds.discr_Hips_WhnH &lt;- dds.discr %&gt;%  \n  filter(ethnicity == \"White not Hispanic\" | ethnicity == \"Hispanic\" ) %&gt;% \n  droplevels()   # remove empty factor levels\n\nggplot(data = dds.discr_Hips_WhnH,   \n       aes(x = expenditures,\n           y = age.cohort)) + \n  geom_boxplot(color=\"darkgrey\") + \n  facet_grid(rows = \"ethnicity\") +   \n  labs(x = \"Annual Expenditures ($)\",\n       y = \"Race and ethnicity\") +\n  geom_jitter(     \n    aes(color = ethnicity),      \n    alpha = 0.3,      \n    show.legend = FALSE,      \n    position = position_jitter(     \n      height = 0.4))"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md_part2.html#mean-annual-dds-expenditures-by-raceethnicity-default-long-format",
    "href": "slides_md/Day03_bsta511_md_part2.html#mean-annual-dds-expenditures-by-raceethnicity-default-long-format",
    "title": "Day 3: Data visualization - Part 2",
    "section": "",
    "text": "mean_expend &lt;- \n  dds.discr_Hips_WhnH %&gt;% \n  group_by(\n    ethnicity, age.cohort)%&gt;% \n  summarize(\n    ave = mean(expenditures))\n\n`summarise()` has grouped output by 'ethnicity'. You can override using the\n`.groups` argument.\n\n\n\n\nmean_expend\n\n# A tibble: 12 × 3\n# Groups:   ethnicity [2]\n   ethnicity          age.cohort    ave\n   &lt;fct&gt;              &lt;fct&gt;       &lt;dbl&gt;\n 1 Hispanic           0-5         1393.\n 2 Hispanic           6-12        2312.\n 3 Hispanic           13-17       3955.\n 4 Hispanic           18-21       9960.\n 5 Hispanic           22-50      40924.\n 6 Hispanic           51+        55585 \n 7 White not Hispanic 0-5         1367.\n 8 White not Hispanic 6-12        2052.\n 9 White not Hispanic 13-17       3904.\n10 White not Hispanic 18-21      10133.\n11 White not Hispanic 22-50      40188.\n12 White not Hispanic 51+        52670."
  },
  {
    "objectID": "slides_md/Day03_bsta511_md_part2.html#mean-annual-dds-expenditures-by-raceethnicity-wide-format",
    "href": "slides_md/Day03_bsta511_md_part2.html#mean-annual-dds-expenditures-by-raceethnicity-wide-format",
    "title": "Day 3: Data visualization - Part 2",
    "section": "",
    "text": "mean_expend_wide &lt;- \n  mean_expend %&gt;% \n  pivot_wider(\n    names_from = ethnicity,\n    values_from = ave)\n\n\n\nmean_expend_wide\n\n# A tibble: 6 × 3\n  age.cohort Hispanic `White not Hispanic`\n  &lt;fct&gt;         &lt;dbl&gt;                &lt;dbl&gt;\n1 0-5           1393.                1367.\n2 6-12          2312.                2052.\n3 13-17         3955.                3904.\n4 18-21         9960.               10133.\n5 22-50        40924.               40188.\n6 51+          55585                52670."
  },
  {
    "objectID": "slides_md/Day03_bsta511_md_part2.html#differences-in-mean-annual-dds-expenditures-by-age-cohort-and-raceethnicity",
    "href": "slides_md/Day03_bsta511_md_part2.html#differences-in-mean-annual-dds-expenditures-by-age-cohort-and-raceethnicity",
    "title": "Day 3: Data visualization - Part 2",
    "section": "",
    "text": "mean_expend_wide &lt;- mean_expend_wide %&gt;% \n  mutate(diff_mean = `White not Hispanic` - Hispanic)\n\nmean_expend_wide\n\n# A tibble: 6 × 4\n  age.cohort Hispanic `White not Hispanic` diff_mean\n  &lt;fct&gt;         &lt;dbl&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n1 0-5           1393.                1367.     -26.3\n2 6-12          2312.                2052.    -260. \n3 13-17         3955.                3904.     -50.9\n4 18-21         9960.               10133.     173. \n5 22-50        40924.               40188.    -736. \n6 51+          55585                52670.   -2915. \n\n\n\nQuestion: Are the data sufficient evidence of ethnic discrimination in DDS expenditures when comparing Hispanics with White non-Hispanics?"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md_part2.html#simpsons-paradox",
    "href": "slides_md/Day03_bsta511_md_part2.html#simpsons-paradox",
    "title": "Day 3: Data visualization - Part 2",
    "section": "",
    "text": "This case study is an example of confounding known as Simpson’s paradox\nSimpson’s paradox happens when an association observed in several groups disappears or reverses direction when the groups are combined.\nIn other words, an association between two variables \\(X\\) and \\(Y\\) may disappear or reverse direction once data are partitioned into subpopulations based on a third variable \\(Z\\) (i.e., a confounding variable)."
  },
  {
    "objectID": "slides_md/Day03_bsta511_md_part2.html#tools-for-wrangling-data",
    "href": "slides_md/Day03_bsta511_md_part2.html#tools-for-wrangling-data",
    "title": "Day 3: Data visualization - Part 2",
    "section": "Tools for wrangling data",
    "text": "Tools for wrangling data\n\n\ntidyverse functions\n\ntidyverse is a suite of packages that implement tidy methods for data importing, cleaning, wrangling, and visualizing\nload the tidyverse packages by running the code library(tidyverse)\n\nDon’t forget to first install tidyverse!\n\n\nFunctions to easily work with rows and columns, such as\n\nsubset rows/columns\nadd new rows/columns\njoin together different data sets\nmake data long or wide\n\nThere are often many steps to tidy data\n\nwe string together commands\nto be performed sequentially\nusing pipes %&gt;%"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md_part2.html#summary-of-data-wrangling-so-far",
    "href": "slides_md/Day03_bsta511_md_part2.html#summary-of-data-wrangling-so-far",
    "title": "Day 3: Data visualization - Part 2",
    "section": "Summary of data wrangling so far",
    "text": "Summary of data wrangling so far\n\n\nThe pipe %&gt;% to string together commands in sequence\nmutate() to add a new variable to a dataset\nselect() to select columns (or deselect columns with -variable)\nfilter() to select specific rows\npivot_wider() to reshape a dataset from a long to a wide format\n\nSummarizing data\n\ntabyl() from janitor package to make frequency tables of categorical variables\nsummarize() to get summary statistics of variables\ngroup_by() to group data by categorical variables before finding summaries"
  },
  {
    "objectID": "slides_md/Day03_bsta511_md_part2.html#what-packages-are-included-in-the-tidyverse",
    "href": "slides_md/Day03_bsta511_md_part2.html#what-packages-are-included-in-the-tidyverse",
    "title": "Day 3: Data visualization - Part 2",
    "section": "What packages are included in the tidyverse?",
    "text": "What packages are included in the tidyverse?\n\n\nCore packages\nThese automatically load when loading the tidyverse package\n\n\n\nhttps://www.tidyverse.org/\n\n\n\nList of all packages:\n\ntidyverse_packages(include_self = TRUE)\n\n [1] \"broom\"         \"conflicted\"    \"cli\"           \"dbplyr\"       \n [5] \"dplyr\"         \"dtplyr\"        \"forcats\"       \"ggplot2\"      \n [9] \"googledrive\"   \"googlesheets4\" \"haven\"         \"hms\"          \n[13] \"httr\"          \"jsonlite\"      \"lubridate\"     \"magrittr\"     \n[17] \"modelr\"        \"pillar\"        \"purrr\"         \"ragg\"         \n[21] \"readr\"         \"readxl\"        \"reprex\"        \"rlang\"        \n[25] \"rstudioapi\"    \"rvest\"         \"stringr\"       \"tibble\"       \n[29] \"tidyr\"         \"xml2\"          \"tidyverse\"    \n\n\n\nPackages not a part of the core get installed with the tidyverse suite, but need to be loaded separately.\n\nSee https://www.tidyverse.org/packages/ for more info."
  },
  {
    "objectID": "rubric.html",
    "href": "rubric.html",
    "title": "Grading rubric",
    "section": "",
    "text": "Grading Rubric (10 pts max):\nAssignments will be graded using the rubric below.\n\nAnswers: 4 pts \nDemonstrating process: 3 pts\nProviding context and relevance: 3 pts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\nAnswers\n(4 pts max)\nAnswers are correct at least 90% of the time\nAnswers are usually correct (75-90%)\nAnswers are sometimes correct (50-74%)\nAnswers are generally incorrect (&lt;50%)\nAnswers are incorrect and no work was shown (demonstrating process has 0 pts)\n\n\nDemonstrating process\n(3 pts max)\n\nAll relevant work is shown, including all steps for figuring out answers.\nR code and output are provided for every question for which R was used.\nRelevant work and steps for figuring out answers are generally provided but incomplete.\nR code and output are generally provided but incomplete for questions that use R.\nRelevant work and steps for figuring out answers are generally missing.\nR code and output are generally missing for questions that use R.\nNo relevant work is shown, including steps for figuring out answers.\nR code and output are not provided for questions for which R was used.\n\n\nProviding context and relevance\n(3 pts max)\n\nAnswers are given in complete sentences with all relevant information for the question as appropriate (context of research question, units, descriptive statistics, explanation of what data visualization is showing, confidence intervals, p-values, test- statistics, etc.), and interpretation of results.\nAnswers are given in complete sentences. Some relevant information may be provided, but much is missing. Context of research question only sometimes provided.\nAnswers are rarely given in complete sentences. Relevant information is not provided. Little to no context is provided.\nAnswers are not in complete sentences. Relevant information and context are not provided."
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html",
    "href": "slides_code/Day02_bsta511_code.html",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "",
    "text": "(1.3) Data collection principles - Day 2 in F24\n\nPopulation vs. sample\nSampling methods\nExperiments vs. Observational studies\n\n(1.2) Intro to Data - Day 2 in F24\n\nData types\nHow are data stored in R?\nWorking with data in R\n\n(1.4) Summarizing numerical data - Day 3 in F24\n\nMean, median, mode, SD, IQR, range, 5 number summary\nEmpirical Rule\nrobust statistics\n\nR packages - Day 4 in F24 -&gt; install for Day 5!!!"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#useful-keyboard-shortcuts",
    "href": "slides_code/Day02_bsta511_code.html#useful-keyboard-shortcuts",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Useful keyboard shortcuts",
    "text": "Useful keyboard shortcuts\nFull list of keyboard shortcuts\n\n\n\n\n\n\n\n\naction\nmac\nwindows/linux\n\n\n\n\nRun code in qmd (or script)\ncmd + enter\nctrl + enter\n\n\n&lt;-\noption + -\nalt + -\n\n\ninterrupt currently running command\nesc\nesc\n\n\nin console, retrieve previously run code\nup/down\nup/down\n\n\nkeyboard shortcut help\noption + shift + k\nalt + shift + k\n\n\n\n\nPractice\nTry typing code below in your qmd (with shortcut) and evaluating it:\n\ny &lt;- 5\ny"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#another-resource-for-an-introduction-to-r",
    "href": "slides_code/Day02_bsta511_code.html#another-resource-for-an-introduction-to-r",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Another resource for an introduction to R",
    "text": "Another resource for an introduction to R\n\nIf you would like another perspective on what we covered the first week, you might find Danielle Navarro’s online book Learning Statistics with R to be helpful.\nDownload free pdf: https://learningstatisticswithr.com/\nSee Sections 3.1-3.7.1 for some of the topics we covered on first day"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#population-vs.-sample",
    "href": "slides_code/Day02_bsta511_code.html#population-vs.-sample",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Population vs. sample",
    "text": "Population vs. sample\n\n(Target) Population\n\ngroup of interest being studied\ngroup from which the sample is selected\n\nstudies often have inclusion and/or exclusion criteria\n\n\n\n\nSample\n\ngroup on which data are collected\noften a small subset of the population"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#sampling-methods-14",
    "href": "slides_code/Day02_bsta511_code.html#sampling-methods-14",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Sampling methods (1/4)",
    "text": "Sampling methods (1/4)\nGoal is to get a representative sample of the population:\nthe characteristics of the sample are similar to the characteristics of the population\nSimple random sample (SRS)\n\neach individual of a population has the same chance of being sampled\nrandomly sampled\nconsidered best way to sample\n\nConvenience sample\n\neasily accessible individuals are more likely to be included in the sample than other individuals\na common “pitfall”\n\n\nSampling methods (2/4)\nGood sampling plans don’t guarantee samples representative of the population\nNon-response bias\n\nnon-response rates can be high\nare all groups within a population being reached?\nunrepresentative sample\n=&gt; skewed results\n\n“Random” samples can be unrepresentative by random chance\n\nIn a SRS each case in the population has an equal chance of being included in the sample\nBut by random chance alone a random sample might contain a higher proportion of one group over another\nEx: a SRS might by chance include 70% men (unlikely, but theoretically possible)\n\n\n\nSampling methods (3/4)\n\nSimple random sample (SRS)\n\neach individual of a population has the same chance of being sampled\nstatistical methods taught in this class assume a SRS!\n\nStratified sampling\n\ndivide population into groups (strata) before selecting cases within each stratum (often via SRS)\nusually cases within a strata are similar, but are different from other strata with respect to the outcome of interest, such as gender or age groups\n\n\n\n\nSampling methods (4/4)\n\nCluster sample\n\nfirst divide population into groups (clusters)\nthen sample a fixed number of clusters, and include all observations from chosen clusters\nclusters are often hospitals, clinicians, schools, etc., where each cluster will have similar services/ policies/ etc.\ncases within clusters usually very diverse\n\nMultistage sample\n\nsimilar to a cluster sample, but select a random sample within each selected cluster instead of all individuals"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#experiments-12",
    "href": "slides_code/Day02_bsta511_code.html#experiments-12",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Experiments (1/2)",
    "text": "Experiments (1/2)\n\nResearchers assign individuals to different treatment or intervention groups\n\ncontrol group: often receive a placebo or usual care\ndifferent treatment groups are often called study arms\n\nRandomization\n\ngroup assignment is usually random to ensure similar (balanced) study arms for all variables (observed and unobserved)\nrandomization allows study arm differences in outcomes to be attributed to treatment rather than variability in patient characteristics\n\ntreatment is the only systematic difference between groups\nestablish causality\n\nblocking (stratification): group individuals into blocks (strata) before randomizing if there are certain characteristics that may influence the outcome other than treatment (i.e. gender, age group)\n\n\n\nExperiments (2/2)\n\nReplication\n\naccomplished by collecting a sufficiently large sample\nresults usually more reliable with a large sample size\n\noften less variability\nmore likely to be representative of population\n\n\nSome studies are not ethical to carry out as experiments"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#observational-studies",
    "href": "slides_code/Day02_bsta511_code.html#observational-studies",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Observational studies",
    "text": "Observational studies\n\ndata are observed and recorded without interference\noften done via surveys, electronic health records, or medical chart reviews\ncohorts\nassociations between variables can be established, but not causality\n\nIndividuals with different characteristics may also differ in other ways that influence response\n\nconfounding variables (lurking variable)\n\nvariables associated with both the explanatory and response variables\n\nprospective vs. retrospective studies"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#comparing-study-designs",
    "href": "slides_code/Day02_bsta511_code.html#comparing-study-designs",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Comparing study designs",
    "text": "Comparing study designs\nScience Media Centre"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#systematic-reviews-example",
    "href": "slides_code/Day02_bsta511_code.html#systematic-reviews-example",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Systematic Reviews example",
    "text": "Systematic Reviews example\nSTEM: Systematically Testing the Evidence on Marijuana\nSTEM is a collaborative project between the US Department of Veterans Affairs and the Center for Evidence-based Policy at Oregon Health & Science University.\nThe project is funded by the US Department of Veterans Affairs: Office of Rural Health."
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#how-are-data-stored-how-do-we-use-them",
    "href": "slides_code/Day02_bsta511_code.html#how-are-data-stored-how-do-we-use-them",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "How are data stored, how do we use them?",
    "text": "How are data stored, how do we use them?\n\nOften, data are in an Excel sheet, or a plain text file (.csv, .txt)\n.csv files open in Excel automatically, but actually are plain text\nUsually, columns are variables/measures and rows are observations (i.e. a person’s measurements)\n\n\nData in R\n\nWe can import data from many file types, including .csv, .txt., and .xlsx\n\nWe will cover this on a later date\n\nOnce imported, R typically stores data as data frames, or tibbles if using the tidyverse package (more on this later).\n\nFor our purposes, these are essentially the same, and I will tend to use the terms interchangeably.\nThese are examples of what we call object types in R."
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#data-frame-example",
    "href": "slides_code/Day02_bsta511_code.html#data-frame-example",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Data frame example",
    "text": "Data frame example\n\ndf &lt;- data.frame(\n  IDs=1:3, \n  gender=c(\"male\", \"female\", \"Male\"), \n  age=c(28, 35.5, 31),\n  trt = c(\"control\", \"1\", \"1\"),\n  Veteran = c(FALSE, TRUE, TRUE)\n  )\ndf\n\n  IDs gender  age     trt Veteran\n1   1   male 28.0 control   FALSE\n2   2 female 35.5       1    TRUE\n3   3   Male 31.0       1    TRUE\n\n\n\nVectors vs. data frames\n\na data frame is a collection (or array or table) of vectors\n\nDifferent columns can be of different data types (i.e. numeric vs. text)\nBoth numeric and text can be stored within a column (stored together as text).\nVectors and data frames are examples of objects in R.\n\nThere are other types of R objects to store data, such as matrices, lists."
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#observations-variables",
    "href": "slides_code/Day02_bsta511_code.html#observations-variables",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Observations & variables",
    "text": "Observations & variables\n\ndf\n\n  IDs gender  age     trt Veteran\n1   1   male 28.0 control   FALSE\n2   2 female 35.5       1    TRUE\n3   3   Male 31.0       1    TRUE\n\n\n\nBook refers to a dataset as a data matrix\nRows are usually observations\nColumns are usually variables\nHow many observations are in this dataset?\nWhat are the variable types in this dataset?"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#variable-column-types",
    "href": "slides_code/Day02_bsta511_code.html#variable-column-types",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Variable (column) types",
    "text": "Variable (column) types\n\n\n\n\n\n\n\n\nR type\nvariable type\ndescription\n\n\n\n\ninteger\ndiscrete\ninteger-valued numbers\n\n\ndouble or numeric\ncontinuous\nnumbers that are decimals\n\n\nfactor\ncategorical\ncategorical variables stored with levels (groups)\n\n\ncharacter\ncategorical\ntext, “strings”\n\n\nlogical\ncategorical\nboolean (TRUE, FALSE)\n\n\n\n\nView the structure of our data frame to see what the variable types are:\n\n\nstr(df)\n\n'data.frame':   3 obs. of  5 variables:\n $ IDs    : int  1 2 3\n $ gender : chr  \"male\" \"female\" \"Male\"\n $ age    : num  28 35.5 31\n $ trt    : chr  \"control\" \"1\" \"1\"\n $ Veteran: logi  FALSE TRUE TRUE"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#fishers-or-andersons-iris-data-set",
    "href": "slides_code/Day02_bsta511_code.html#fishers-or-andersons-iris-data-set",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Fisher’s (or Anderson’s) Iris data set",
    "text": "Fisher’s (or Anderson’s) Iris data set\nData description:\n\nn = 150\n3 species of Iris flowers (Setosa, Virginica, and Versicolour)\n\n50 measurements of each type of Iris\n\nvariables:\n\nsepal length, sepal width, petal length, petal width, and species\n\n\nCan the iris species be determined by these variables?\n\n\n\nGareth Duffy"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#view-the-iris-dataset",
    "href": "slides_code/Day02_bsta511_code.html#view-the-iris-dataset",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "View the iris dataset",
    "text": "View the iris dataset\n\nThe iris dataset is already pre-loaded in base R and ready to use.\nType the following command in the console window\n\nWarning: this command cannot be rendered. It will give an error.\n\n\n\nView(iris)\n\nA new tab in the scripting window should appear with the iris dataset."
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#data-structure",
    "href": "slides_code/Day02_bsta511_code.html#data-structure",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Data structure",
    "text": "Data structure\n\nWhat are the different variable types in this data set?\n\n\nstr(iris)   # structure of data\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ..."
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#data-set-summary",
    "href": "slides_code/Day02_bsta511_code.html#data-set-summary",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Data set summary",
    "text": "Data set summary\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#data-set-info",
    "href": "slides_code/Day02_bsta511_code.html#data-set-info",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Data set info",
    "text": "Data set info\n\ndim(iris)\n\n[1] 150   5\n\nnrow(iris)\n\n[1] 150\n\nncol(iris)\n\n[1] 5\n\nnames(iris)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\""
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#view-the-beginning-or-end-of-a-dataset",
    "href": "slides_code/Day02_bsta511_code.html#view-the-beginning-or-end-of-a-dataset",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "View the beginning or end of a dataset",
    "text": "View the beginning or end of a dataset\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\ntail(iris)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n145          6.7         3.3          5.7         2.5 virginica\n146          6.7         3.0          5.2         2.3 virginica\n147          6.3         2.5          5.0         1.9 virginica\n148          6.5         3.0          5.2         2.0 virginica\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#specify-how-many-rows-to-view-at-beginning-or-end-of-a-dataset",
    "href": "slides_code/Day02_bsta511_code.html#specify-how-many-rows-to-view-at-beginning-or-end-of-a-dataset",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Specify how many rows to view at beginning or end of a dataset",
    "text": "Specify how many rows to view at beginning or end of a dataset\n\nhead(iris, 3)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n\ntail(iris, 2)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#the",
    "href": "slides_code/Day02_bsta511_code.html#the",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "The $",
    "text": "The $\n\nSuppose we want to single out the column of petal width values.\nOne way to do this is to use the $\n\nDatSetName$VariableName\n\n\n\niris$Petal.Width\n\n  [1] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 0.2 0.2 0.1 0.1 0.2 0.4 0.4 0.3\n [19] 0.3 0.3 0.2 0.4 0.2 0.5 0.2 0.2 0.4 0.2 0.2 0.2 0.2 0.4 0.1 0.2 0.2 0.2\n [37] 0.2 0.1 0.2 0.2 0.3 0.3 0.2 0.6 0.4 0.3 0.2 0.2 0.2 0.2 1.4 1.5 1.5 1.3\n [55] 1.5 1.3 1.6 1.0 1.3 1.4 1.0 1.5 1.0 1.4 1.3 1.4 1.5 1.0 1.5 1.1 1.8 1.3\n [73] 1.5 1.2 1.3 1.4 1.4 1.7 1.5 1.0 1.1 1.0 1.2 1.6 1.5 1.6 1.5 1.3 1.3 1.3\n [91] 1.2 1.4 1.2 1.0 1.3 1.2 1.3 1.3 1.1 1.3 2.5 1.9 2.1 1.8 2.2 2.1 1.7 1.8\n[109] 1.8 2.5 2.0 1.9 2.1 2.0 2.4 2.3 1.8 2.2 2.3 1.5 2.3 2.0 2.0 1.8 2.1 1.8\n[127] 1.8 1.8 2.1 1.6 1.9 2.0 2.2 1.5 1.4 2.3 2.4 1.8 1.8 2.1 2.4 2.3 1.9 2.3\n[145] 2.5 2.3 1.9 2.0 2.3 1.8"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#example-using-the",
    "href": "slides_code/Day02_bsta511_code.html#example-using-the",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Example using the $",
    "text": "Example using the $\nThe $ is helpful if you want to create a new dataset for just that one variable, or, more commonly, if you want to calculate summary statistics for that one variable.\n\nmean(iris$Petal.Width)\n\n[1] 1.199333\n\nsd(iris$Petal.Width)\n\n[1] 0.7622377\n\nmedian(iris$Petal.Width)\n\n[1] 1.3"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#inline-code",
    "href": "slides_code/Day02_bsta511_code.html#inline-code",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Inline code",
    "text": "Inline code\n\nWith markdown you can also report R code output inline with the text instead of using a chunk.\n\nText in editor:\nThe mean petal width for all 3 species combined is 1.2 (SD = 0.8) cm.\n\nReporting summary statistics this way in a report, makes the numbers computationally reproducible.\nFor example, if this were for an abstract and a year later you are wondering where the numbers came from, your R code will tell you exactly which dataset was used to calculate the values."
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#table-1-example",
    "href": "slides_code/Day02_bsta511_code.html#table-1-example",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Table 1 example",
    "text": "Table 1 example\nAre We on the Same Page?: A Cross-Sectional Study of Patient-Clinician Goal Concordance in Rheumatoid Arthritis\nJ Barton et al.\nArthritis Care & Research.\n2021 Sep 27 https://pubmed.ncbi.nlm.nih.gov/34569172/"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#measures-of-center-mean",
    "href": "slides_code/Day02_bsta511_code.html#measures-of-center-mean",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of center: mean",
    "text": "Measures of center: mean\nSample mean: the average value of observations\n\\[\\overline{x} = \\frac{x_1+x_2+\\cdots+x_n}{n} = \\sum_{i=1}^{n}\\frac{x_i}{n}\\]\nwhere \\(x_1, x_2, \\ldots, x_n\\) represent the \\(n\\) observed values in a sample\nExample: What is the mean age in the toy dataset df defined earlier?\n\ndf\n\n  IDs gender  age     trt Veteran\n1   1   male 28.0 control   FALSE\n2   2 female 35.5       1    TRUE\n3   3   Male 31.0       1    TRUE\n\nmean(df$age)\n\n[1] 31.5"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#measures-of-center-median",
    "href": "slides_code/Day02_bsta511_code.html#measures-of-center-median",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of center: median",
    "text": "Measures of center: median\n\nThe median is the middle value of the observations in a sample.\nThe median is the 50th percentile, meaning\n\n50% of observations lie below and\n50% of observations lie above the median.\n\nIf the number of observations is\n\nodd: the median is the middle observed value\neven: the median is the average of the two middle observed values\n\n\n\ndf$age\n\n[1] 28.0 35.5 31.0\n\nmedian(df$age)\n\n[1] 31\n\nmedian(c(df$age, 67))\n\n[1] 33.25"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#measures-of-center-mean-vs.-median",
    "href": "slides_code/Day02_bsta511_code.html#measures-of-center-mean-vs.-median",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of center: mean vs. median",
    "text": "Measures of center: mean vs. median\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#measures-of-center-mode",
    "href": "slides_code/Day02_bsta511_code.html#measures-of-center-mode",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of center: mode",
    "text": "Measures of center: mode\nmode: the most frequent value in a dataset\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#measures-of-spread-standard-deviation-sd-13",
    "href": "slides_code/Day02_bsta511_code.html#measures-of-spread-standard-deviation-sd-13",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of spread: standard deviation (SD) (1/3)",
    "text": "Measures of spread: standard deviation (SD) (1/3)\nstandard deviation is (approximately) the average distance between a typical observation and the mean\n\nAn observation’s deviation is the distance between its value \\(x\\) and the sample mean \\(\\overline{x}\\): deviation = \\(x - \\overline{x}\\).\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nMeasures of spread: SD (2/3)\n\nThe sample variance \\(s^2\\) is the sum of squared deviations divided by the number of observations minus 1. \\[s^2 = \\frac{(x_1 - \\overline{x})^2+(x_2 - \\overline{x})^2+\\cdots+(x_n - \\overline{x})^2}{n-1} = \\sum_{i=1}^{n}\\frac{(x_i - \\overline{x})^2}{n-1}\\] where \\(x_1, x_2, \\dots, x_n\\) represent the \\(n\\) observed values.\nThe standard deviation \\(s\\) is the square root of the variance. \\[s = \\sqrt{\\frac{({x_1 - \\overline{x})}^{2}+({x_2 - \\overline{x})}^{2}+\\cdots+({x_n - \\overline{x})}^{2}}{n-1}} = \\sqrt{\\sum_{i=1}^{n}\\frac{(x_i - \\overline{x})^2}{n-1}}\\]\n\n\n\nMeasures of spread: SD (3/3)\nLet’s calculate the sample standard deviation for our toy example\n\ndf$age\n\n[1] 28.0 35.5 31.0\n\n\n\nmean(df$age)\n\n[1] 31.5\n\nsd(df$age)\n\n[1] 3.774917\n\n\n\\(s = \\sqrt{\\sum_{i=1}^{n}\\frac{(x_i - \\overline{x})^2}{n-1}} =\\)"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#empirical-rule-one-way-to-think-about-the-sd-12",
    "href": "slides_code/Day02_bsta511_code.html#empirical-rule-one-way-to-think-about-the-sd-12",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Empirical Rule: one way to think about the SD (1/2)",
    "text": "Empirical Rule: one way to think about the SD (1/2)\nFor symmetric bell-shaped data, about\n\n68% of the data are within 1 SD of the mean\n95% of the data are within 2 SD’s of the mean\n99.7% of the data are within 3 SD’s of the mean\n\nThese percentages are based off of percentages of a true normal distribution.\n\nEmpirical Rule: one way to think about the SD (2/2)\n\nhist(iris$Sepal.Width)\n\n\n\n\n\nmean(iris$Sepal.Width)\n\n[1] 3.057333\n\nsd(iris$Sepal.Width)\n\n[1] 0.4358663"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#measures-of-spread-interquartile-range-iqr-12",
    "href": "slides_code/Day02_bsta511_code.html#measures-of-spread-interquartile-range-iqr-12",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of spread: interquartile range (IQR) (1/2)",
    "text": "Measures of spread: interquartile range (IQR) (1/2)\nThe \\(p^{th}\\) percentile is the observation such that \\(p\\%\\) of the remaining observations fall below this observation.\n\nThe first quartile \\(Q_1\\) is the \\(25^{th}\\) percentile.\nThe second quartile \\(Q_2\\), i.e., the median, is the \\(50^{th}\\) percentile.\nThe third quartile \\(Q_3\\) is the \\(75^{th}\\) percentile.\n\nThe interquartile range (IQR) is the distance between the third and first quartiles. \\[IQR = Q_3 - Q_1\\]\n\nIQR is the width of the middle half of the data"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#measures-of-spread-iqr-22",
    "href": "slides_code/Day02_bsta511_code.html#measures-of-spread-iqr-22",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Measures of spread: IQR (2/2)",
    "text": "Measures of spread: IQR (2/2)\n5 number summary\n\nsummary(iris$Sepal.Width)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.000   2.800   3.000   3.057   3.300   4.400 \n\n\n\n\n\n\n\nWhat is the IQR of the sepal widths?\n\nquantile(iris$Sepal.Width, c(.25, .75))\n\n25% 75% \n2.8 3.3 \n\ndiff(quantile(iris$Sepal.Width, c(.25, .75)))\n\n75% \n0.5 \n\nIQR(iris$Sepal.Width)\n\n[1] 0.5"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#robust-estimates",
    "href": "slides_code/Day02_bsta511_code.html#robust-estimates",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Robust estimates",
    "text": "Robust estimates\nSummary statistics are called robust estimates if extreme observations have little effect on their values\n\n\n\nestimate\nrobust?\n\n\n\n\nmean\n\n\n\nmedian\n\n\n\nmode\n\n\n\nstandard deviation\n\n\n\nIQR\n\n\n\nrange"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#installing-packages",
    "href": "slides_code/Day02_bsta511_code.html#installing-packages",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Installing packages",
    "text": "Installing packages\n\nPackages contain additional functions and data\n\nTwo options to install packages:\n\ninstall.packages() or\nThe “Packages” tab in Files/Plots/Packages/Help/Viewer window\n\n\ninstall.packages(\"dplyr\")   # only do this ONCE, use quotes\n\n\nOnly install packages once (unless you want to update them)\nInstalled from Comprehensive R Archive Network (CRAN) = package mothership"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#video-on-installing-packages",
    "href": "slides_code/Day02_bsta511_code.html#video-on-installing-packages",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Video on installing packages",
    "text": "Video on installing packages\n\nDanielle Navarro’s YouTube video on Installing and loading R packages: https://www.youtube.com/watch?v=kpHZVyDvEhQ"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#load-packages-with-library-command",
    "href": "slides_code/Day02_bsta511_code.html#load-packages-with-library-command",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Load packages with library() command",
    "text": "Load packages with library() command\n\nTip: at the top of your Rmd file, create a chunk that loads all of the R packages you want to use in that file.\nUse the library() command to load each required package.\nPackages need to be reloaded every time you open Rstudio.\n\n\nlibrary(dplyr)    # run this every time you open Rstudio\n\n\nYou can use a function without loading the package with PackageName::CommandName\n\n\ndplyr::arrange(iris, Petal.Width)   # what does arrange do?\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1            4.9         3.1          1.5         0.1     setosa\n2            4.8         3.0          1.4         0.1     setosa\n3            4.3         3.0          1.1         0.1     setosa\n4            5.2         4.1          1.5         0.1     setosa\n5            4.9         3.6          1.4         0.1     setosa\n6            5.1         3.5          1.4         0.2     setosa\n7            4.9         3.0          1.4         0.2     setosa\n8            4.7         3.2          1.3         0.2     setosa\n9            4.6         3.1          1.5         0.2     setosa\n10           5.0         3.6          1.4         0.2     setosa\n11           5.0         3.4          1.5         0.2     setosa\n12           4.4         2.9          1.4         0.2     setosa\n13           5.4         3.7          1.5         0.2     setosa\n14           4.8         3.4          1.6         0.2     setosa\n15           5.8         4.0          1.2         0.2     setosa\n16           5.4         3.4          1.7         0.2     setosa\n17           4.6         3.6          1.0         0.2     setosa\n18           4.8         3.4          1.9         0.2     setosa\n19           5.0         3.0          1.6         0.2     setosa\n20           5.2         3.5          1.5         0.2     setosa\n21           5.2         3.4          1.4         0.2     setosa\n22           4.7         3.2          1.6         0.2     setosa\n23           4.8         3.1          1.6         0.2     setosa\n24           5.5         4.2          1.4         0.2     setosa\n25           4.9         3.1          1.5         0.2     setosa\n26           5.0         3.2          1.2         0.2     setosa\n27           5.5         3.5          1.3         0.2     setosa\n28           4.4         3.0          1.3         0.2     setosa\n29           5.1         3.4          1.5         0.2     setosa\n30           4.4         3.2          1.3         0.2     setosa\n31           5.1         3.8          1.6         0.2     setosa\n32           4.6         3.2          1.4         0.2     setosa\n33           5.3         3.7          1.5         0.2     setosa\n34           5.0         3.3          1.4         0.2     setosa\n35           4.6         3.4          1.4         0.3     setosa\n36           5.1         3.5          1.4         0.3     setosa\n37           5.7         3.8          1.7         0.3     setosa\n38           5.1         3.8          1.5         0.3     setosa\n39           5.0         3.5          1.3         0.3     setosa\n40           4.5         2.3          1.3         0.3     setosa\n41           4.8         3.0          1.4         0.3     setosa\n42           5.4         3.9          1.7         0.4     setosa\n43           5.7         4.4          1.5         0.4     setosa\n44           5.4         3.9          1.3         0.4     setosa\n45           5.1         3.7          1.5         0.4     setosa\n46           5.0         3.4          1.6         0.4     setosa\n47           5.4         3.4          1.5         0.4     setosa\n48           5.1         3.8          1.9         0.4     setosa\n49           5.1         3.3          1.7         0.5     setosa\n50           5.0         3.5          1.6         0.6     setosa\n51           4.9         2.4          3.3         1.0 versicolor\n52           5.0         2.0          3.5         1.0 versicolor\n53           6.0         2.2          4.0         1.0 versicolor\n54           5.8         2.7          4.1         1.0 versicolor\n55           5.7         2.6          3.5         1.0 versicolor\n56           5.5         2.4          3.7         1.0 versicolor\n57           5.0         2.3          3.3         1.0 versicolor\n58           5.6         2.5          3.9         1.1 versicolor\n59           5.5         2.4          3.8         1.1 versicolor\n60           5.1         2.5          3.0         1.1 versicolor\n61           6.1         2.8          4.7         1.2 versicolor\n62           5.8         2.7          3.9         1.2 versicolor\n63           5.5         2.6          4.4         1.2 versicolor\n64           5.8         2.6          4.0         1.2 versicolor\n65           5.7         3.0          4.2         1.2 versicolor\n66           5.5         2.3          4.0         1.3 versicolor\n67           5.7         2.8          4.5         1.3 versicolor\n68           6.6         2.9          4.6         1.3 versicolor\n69           5.6         2.9          3.6         1.3 versicolor\n70           6.1         2.8          4.0         1.3 versicolor\n71           6.4         2.9          4.3         1.3 versicolor\n72           6.3         2.3          4.4         1.3 versicolor\n73           5.6         3.0          4.1         1.3 versicolor\n74           5.5         2.5          4.0         1.3 versicolor\n75           5.6         2.7          4.2         1.3 versicolor\n76           5.7         2.9          4.2         1.3 versicolor\n77           6.2         2.9          4.3         1.3 versicolor\n78           5.7         2.8          4.1         1.3 versicolor\n79           7.0         3.2          4.7         1.4 versicolor\n80           5.2         2.7          3.9         1.4 versicolor\n81           6.1         2.9          4.7         1.4 versicolor\n82           6.7         3.1          4.4         1.4 versicolor\n83           6.6         3.0          4.4         1.4 versicolor\n84           6.8         2.8          4.8         1.4 versicolor\n85           6.1         3.0          4.6         1.4 versicolor\n86           6.1         2.6          5.6         1.4  virginica\n87           6.4         3.2          4.5         1.5 versicolor\n88           6.9         3.1          4.9         1.5 versicolor\n89           6.5         2.8          4.6         1.5 versicolor\n90           5.9         3.0          4.2         1.5 versicolor\n91           5.6         3.0          4.5         1.5 versicolor\n92           6.2         2.2          4.5         1.5 versicolor\n93           6.3         2.5          4.9         1.5 versicolor\n94           6.0         2.9          4.5         1.5 versicolor\n95           5.4         3.0          4.5         1.5 versicolor\n96           6.7         3.1          4.7         1.5 versicolor\n97           6.0         2.2          5.0         1.5  virginica\n98           6.3         2.8          5.1         1.5  virginica\n99           6.3         3.3          4.7         1.6 versicolor\n100          6.0         2.7          5.1         1.6 versicolor\n101          6.0         3.4          4.5         1.6 versicolor\n102          7.2         3.0          5.8         1.6  virginica\n103          6.7         3.0          5.0         1.7 versicolor\n104          4.9         2.5          4.5         1.7  virginica\n105          5.9         3.2          4.8         1.8 versicolor\n106          6.3         2.9          5.6         1.8  virginica\n107          7.3         2.9          6.3         1.8  virginica\n108          6.7         2.5          5.8         1.8  virginica\n109          6.5         3.0          5.5         1.8  virginica\n110          6.3         2.7          4.9         1.8  virginica\n111          7.2         3.2          6.0         1.8  virginica\n112          6.2         2.8          4.8         1.8  virginica\n113          6.1         3.0          4.9         1.8  virginica\n114          6.4         3.1          5.5         1.8  virginica\n115          6.0         3.0          4.8         1.8  virginica\n116          5.9         3.0          5.1         1.8  virginica\n117          5.8         2.7          5.1         1.9  virginica\n118          6.4         2.7          5.3         1.9  virginica\n119          7.4         2.8          6.1         1.9  virginica\n120          5.8         2.7          5.1         1.9  virginica\n121          6.3         2.5          5.0         1.9  virginica\n122          6.5         3.2          5.1         2.0  virginica\n123          5.7         2.5          5.0         2.0  virginica\n124          5.6         2.8          4.9         2.0  virginica\n125          7.7         2.8          6.7         2.0  virginica\n126          7.9         3.8          6.4         2.0  virginica\n127          6.5         3.0          5.2         2.0  virginica\n128          7.1         3.0          5.9         2.1  virginica\n129          7.6         3.0          6.6         2.1  virginica\n130          6.8         3.0          5.5         2.1  virginica\n131          6.7         3.3          5.7         2.1  virginica\n132          6.4         2.8          5.6         2.1  virginica\n133          6.9         3.1          5.4         2.1  virginica\n134          6.5         3.0          5.8         2.2  virginica\n135          7.7         3.8          6.7         2.2  virginica\n136          6.4         2.8          5.6         2.2  virginica\n137          6.4         3.2          5.3         2.3  virginica\n138          7.7         2.6          6.9         2.3  virginica\n139          6.9         3.2          5.7         2.3  virginica\n140          7.7         3.0          6.1         2.3  virginica\n141          6.9         3.1          5.1         2.3  virginica\n142          6.8         3.2          5.9         2.3  virginica\n143          6.7         3.0          5.2         2.3  virginica\n144          6.2         3.4          5.4         2.3  virginica\n145          5.8         2.8          5.1         2.4  virginica\n146          6.3         3.4          5.6         2.4  virginica\n147          6.7         3.1          5.6         2.4  virginica\n148          6.3         3.3          6.0         2.5  virginica\n149          7.2         3.6          6.1         2.5  virginica\n150          6.7         3.3          5.7         2.5  virginica"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#install-the-packages-listed-below-before-day-3",
    "href": "slides_code/Day02_bsta511_code.html#install-the-packages-listed-below-before-day-3",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Install the packages listed below before Day 3",
    "text": "Install the packages listed below before Day 3\n\nknitr\n\nthis might actually already be installed\ncheck your packages list\n\ntidyverse\n\nthis is actually a bundle of packages\nWarning: it will take a while to install!!!\nsee more info at https://tidyverse.tidyverse.org/\n\nrstatix\n\nfor summary statistics of a dataset\n\njanitor\n\nfor cleaning and exploring data\n\nggridges\n\nfor creating ridgeline plots\n\ndevtools\n\nused to create R packages\nfor our purposes, needed to install some packages\n\noi_biostat_data\n\nthis package is on github\nsee the next slide for directions on how to install oi_biostat_data"
  },
  {
    "objectID": "slides_code/Day02_bsta511_code.html#directions-for-installing-package-oibiostat",
    "href": "slides_code/Day02_bsta511_code.html#directions-for-installing-package-oibiostat",
    "title": "Day 2: Data collection & numerical summaries",
    "section": "Directions for installing package oibiostat",
    "text": "Directions for installing package oibiostat\n\nThe textbook’s datasets are in the R package oibiostat\nExplanation of code below\n\nInstallation of oibiostat package requires first installing devtools package\nThe code devtools::install_github() tells R to use the command install_github() from the devtools package without loading the entire package and all of its commands (which library(devtools) would do).\n\n\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"OI-Biostat/oi_biostat_data\", force = TRUE)\n\n\nAfter running the code above, put # in front of the commands so that RStudio doesn’t evaluate them when rendering.\nNow load the oibiostat package\n\nthe code below needs to be run every time you restart R or knit an Rmd file\n\n\n\nlibrary(oibiostat)"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html",
    "href": "slides_code/Day03_bsta511_code.html",
    "title": "Day 3 code: Data visualization",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(ggridges)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary) # NEW!!\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#load-packages",
    "href": "slides_code/Day03_bsta511_code.html#load-packages",
    "title": "Day 3 code: Data visualization",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(ggridges)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary) # NEW!!\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#load-dds.discr-dataset-from-oibiostat-package",
    "href": "slides_code/Day03_bsta511_code.html#load-dds.discr-dataset-from-oibiostat-package",
    "title": "Day 3 code: Data visualization",
    "section": "Load dds.discr dataset from oibiostat package",
    "text": "Load dds.discr dataset from oibiostat package\n\nThe textbook’s datasets are in the R package oibiostat\nMake sure the oibiostat package is installed before running the code below.\nLoad the oibiostat package and the dataset dds.discr\n\nthe code below needs to be run every time you restart R or render a Qmd file\n\nlibrary(oibiostat)\ndata(\"dds.discr\")\n\n\nAfter loading the dataset dds.discr using data(\"dds.discr\"), you will see dds.discr in the Data list of the Environment window."
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#getting-to-know-the-dataset",
    "href": "slides_code/Day03_bsta511_code.html#getting-to-know-the-dataset",
    "title": "Day 3 code: Data visualization",
    "section": "Getting to know the dataset",
    "text": "Getting to know the dataset\n\ndim(dds.discr)\n\n[1] 1000    6\n\nnames(dds.discr)\n\n[1] \"id\"           \"age.cohort\"   \"age\"          \"gender\"       \"expenditures\"\n[6] \"ethnicity\"   \n\nlength(unique(dds.discr$id)) # How many unique id's are there?\n\n[1] 1000\n\n\n\nstr()\n\nWe previously used the base R structure command str() to get information about variable types in a dataset.\nNote this dataset is a tibble instead of a data.frame\n\n\nstr(dds.discr)      # base R\n\ntibble [1,000 × 6] (S3: tbl_df/tbl/data.frame)\n $ id          : int [1:1000] 10210 10409 10486 10538 10568 10690 10711 10778 10820 10823 ...\n $ age.cohort  : Factor w/ 6 levels \"0-5\",\"6-12\",\"13-17\",..: 3 5 1 4 3 3 3 3 3 3 ...\n $ age         : int [1:1000] 17 37 3 19 13 15 13 17 14 13 ...\n $ gender      : Factor w/ 2 levels \"Female\",\"Male\": 1 2 2 1 2 1 1 2 1 2 ...\n $ expenditures: int [1:1000] 2113 41924 1454 6400 4412 4566 3915 3873 5021 2887 ...\n $ ethnicity   : Factor w/ 8 levels \"American Indian\",..: 8 8 4 4 8 4 8 3 8 4 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   ID = col_integer(),\n  ..   `Age Cohort` = col_character(),\n  ..   Age = col_integer(),\n  ..   Gender = col_character(),\n  ..   Expenditures = col_integer(),\n  ..   Ethnicity = col_character()\n  .. )\n\n\n\n\nglimpse()\nNew: glimpse()\n\nUse glimpse() from the tidyverse package (technically it’s from the dplyr package) to get information about variable types.\nglimpse() tends to have nicer output for tibbles than str()\n\n\nlibrary(tidyverse)\nglimpse(dds.discr)  # from tidyverse package (dplyr)\n\nRows: 1,000\nColumns: 6\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ gender       &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ ethnicity    &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…\n\n\n\n\nsummary()\n\nWe previously used the base R structure command summary() to get summary information about variables\n\n\nsummary(dds.discr)      # base R\n\n       id        age.cohort       age          gender     expenditures  \n Min.   :10210   0-5  : 82   Min.   : 0.0   Female:503   Min.   :  222  \n 1st Qu.:31809   6-12 :175   1st Qu.:12.0   Male  :497   1st Qu.: 2899  \n Median :55384   13-17:212   Median :18.0                Median : 7026  \n Mean   :54663   18-21:199   Mean   :22.8                Mean   :18066  \n 3rd Qu.:76135   22-50:226   3rd Qu.:26.0                3rd Qu.:37713  \n Max.   :99898   51+  :106   Max.   :95.0                Max.   :75098  \n                                                                        \n              ethnicity  \n White not Hispanic:401  \n Hispanic          :376  \n Asian             :129  \n Black             : 59  \n Multi Race        : 26  \n American Indian   :  4  \n (Other)           :  5"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#getting-to-know-the-dataset-tbl_summary",
    "href": "slides_code/Day03_bsta511_code.html#getting-to-know-the-dataset-tbl_summary",
    "title": "Day 3 code: Data visualization",
    "section": "Getting to know the dataset: tbl_summary()",
    "text": "Getting to know the dataset: tbl_summary()\n\nNew: Use tbl_summary() from the gtsummary package to get summary information\n\n\n# library(gtsummary)\ntbl_summary(dds.discr)  # from package gtsummary\n\n\n\n\n\n  \n    \n      Characteristic\n\n      N = 1,000\n1\n    \n  \n  \n    id\n55,385 (31,759, 76,205)\n    age.cohort\n\n        0-5\n82 (8.2%)\n        6-12\n175 (18%)\n        13-17\n212 (21%)\n        18-21\n199 (20%)\n        22-50\n226 (23%)\n        51+\n106 (11%)\n    age\n18 (12, 26)\n    gender\n\n        Female\n503 (50%)\n        Male\n497 (50%)\n    expenditures\n7,026 (2,898, 37,718)\n    ethnicity\n\n        American Indian\n4 (0.4%)\n        Asian\n129 (13%)\n        Black\n59 (5.9%)\n        Hispanic\n376 (38%)\n        Multi Race\n26 (2.6%)\n        Native Hawaiian\n3 (0.3%)\n        Other\n2 (0.2%)\n        White not Hispanic\n401 (40%)\n  \n  \n  \n    \n      1 Median (Q1, Q3); n (%)"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#histograms",
    "href": "slides_code/Day03_bsta511_code.html#histograms",
    "title": "Day 3 code: Data visualization",
    "section": "Histograms",
    "text": "Histograms\nWhat is being measured on the vertical axes?\n\nggplot(data = dds.discr, aes(x = age)) +\n  geom_histogram() \n\n\n\n\n\nggplot(data = dds.discr, aes(x = expenditures)) +\n  geom_histogram()"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#histograms-showing-proportions",
    "href": "slides_code/Day03_bsta511_code.html#histograms-showing-proportions",
    "title": "Day 3 code: Data visualization",
    "section": "Histograms showing proportions",
    "text": "Histograms showing proportions\n\nggplot(data = dds.discr, aes(x = age)) +\n  geom_histogram(aes(y = stat(density)))  \n\nWarning: `stat(density)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\nggplot(data = dds.discr, aes(x = age)) +\n  geom_histogram(aes(y = stat(density))) +  \n  scale_y_continuous(labels = scales::percent_format())"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#density-plots",
    "href": "slides_code/Day03_bsta511_code.html#density-plots",
    "title": "Day 3 code: Data visualization",
    "section": "Density plots",
    "text": "Density plots\nWhat is being measured on the vertical axes?\n\nggplot(data = dds.discr, aes(x = age)) +\n  geom_density() \n\n\n\n\n\nggplot(data = dds.discr, aes(x = age)) +\n  geom_histogram()"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#dot-plots-better-for-smaller-samples",
    "href": "slides_code/Day03_bsta511_code.html#dot-plots-better-for-smaller-samples",
    "title": "Day 3 code: Data visualization",
    "section": "Dot plots (better for smaller samples)",
    "text": "Dot plots (better for smaller samples)\nWhat is being measured on the vertical axes?\n\nggplot(data = dds.discr, aes(x = age)) +\n  geom_dotplot(binwidth =1) \n\n\n\n\n\nggplot(data = dds.discr, aes(x = age)) +\n  geom_histogram(binwidth =1)"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#boxplots",
    "href": "slides_code/Day03_bsta511_code.html#boxplots",
    "title": "Day 3 code: Data visualization",
    "section": "Boxplots",
    "text": "Boxplots\n\nggplot(data = dds.discr, aes(x = age)) + \n  geom_boxplot() \n\n\n\n\n\nggplot(data = dds.discr, aes(y = age)) + \n  geom_boxplot()"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#side-by-side-boxplots",
    "href": "slides_code/Day03_bsta511_code.html#side-by-side-boxplots",
    "title": "Day 3 code: Data visualization",
    "section": "Side-by-side boxplots",
    "text": "Side-by-side boxplots\n\nggplot(data = dds.discr, \n       aes(x = expenditures, y = ethnicity)) + \n  geom_boxplot() + \n  labs(x = \"Annual Expenditures ($)\", \n       y = \"Race and ethnicity\")  \n\n\n\n\nCan you determine the following using boxplots?\n\ndistribution shape\nsample size"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#side-by-side-boxplots-with-data-points",
    "href": "slides_code/Day03_bsta511_code.html#side-by-side-boxplots-with-data-points",
    "title": "Day 3 code: Data visualization",
    "section": "Side-by-side boxplots with data points",
    "text": "Side-by-side boxplots with data points\n\nggplot(data = dds.discr, \n       aes(x = expenditures, y = ethnicity)) + \n  geom_boxplot(color=\"darkgrey\") + \n  labs(x = \"Annual Expenditures ($)\",\n       y = \"Race and ethnicity\") +\n  geom_jitter(aes(color = ethnicity),\n              alpha = 0.3,\n              show.legend = FALSE,\n              position = position_jitter(height = 0.4)\n              )      \n\n\n\n\nCan you determine the following using boxplots?\n\ndistribution shape\nsample size"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#density-plots-by-group",
    "href": "slides_code/Day03_bsta511_code.html#density-plots-by-group",
    "title": "Day 3 code: Data visualization",
    "section": "Density plots by group",
    "text": "Density plots by group\n\nggplot(data = dds.discr, \n       aes(x = expenditures,\n           color = ethnicity)) + \n  geom_density() + \n  labs(x = \"Annual Expenditures ($)\")"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#ridgeline-plot",
    "href": "slides_code/Day03_bsta511_code.html#ridgeline-plot",
    "title": "Day 3 code: Data visualization",
    "section": "Ridgeline plot",
    "text": "Ridgeline plot\n\n# library(ggridges)\nggplot(data = dds.discr,\n       aes(x = expenditures, y = ethnicity,      \n           fill = ethnicity)) + \n  geom_density_ridges(\n    alpha = 0.3,      \n    show.legend = FALSE) +\n  labs(x = \"Annual Expenditures ($)\",\n       y = \"Race and ethnicity\",\n       title = \"Expenditures by race and ethnicity\")"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#transforming-data-1.4.5",
    "href": "slides_code/Day03_bsta511_code.html#transforming-data-1.4.5",
    "title": "Day 3 code: Data visualization",
    "section": "Transforming data (1.4.5)",
    "text": "Transforming data (1.4.5)\n\nWe sometimes apply a transformation to highly skewed data to make it more symmetric\nLog transformations are often used for skewed right data\n\nx = expenditures\n\nggplot(data = dds.discr, \n       aes(x = expenditures)) +  \n  geom_density() \n\n\n\n\nx = log(expenditures)\n\nggplot(data = dds.discr, \n       aes(x = log(expenditures))) +  \n  geom_density()"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#scatterplots",
    "href": "slides_code/Day03_bsta511_code.html#scatterplots",
    "title": "Day 3 code: Data visualization",
    "section": "Scatterplots",
    "text": "Scatterplots\n\nggplot(data = dds.discr, \n       aes(x = age, y = expenditures)) + \n  geom_point() +       \n  labs(x = \"Age\",\n       y = \"Annual Expenditures ($)\") \n\n\n\n\nResponse vs. explanatory variables (Section 1.2.3) - A response variable measures the outcome of interest in a study - A study will typically examine whether the values of a response variable differ as values of an explanatory variable change\nDescribe the association between the variables"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#pearson-correlation-coefficient-r",
    "href": "slides_code/Day03_bsta511_code.html#pearson-correlation-coefficient-r",
    "title": "Day 3 code: Data visualization",
    "section": "(Pearson) Correlation coefficient \\(r\\)",
    "text": "(Pearson) Correlation coefficient \\(r\\)\nThe (Peasron) correlation coefficient of variables \\(x\\) and \\(y\\) can be computed using the formula \\[r = \\frac{1}{n-1}\\sum_{i=1}^{n}\\Big(\\frac{x_i - \\bar{x}}{s_x}\\Big)\\Big(\\frac{y_i - \\bar{y}}{s_y}\\Big)\\] where * \\((x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\) are the \\(n\\) paired values of the variables \\(x\\) and \\(y\\) * \\(s_x\\) and \\(s_y\\) are the sample standard deviations of the variables \\(x\\) and \\(y\\), respectively\n\ncor(dds.discr$age, dds.discr$expenditures)\n\n[1] 0.8432422"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#scatterplots-with-color-coded-dots",
    "href": "slides_code/Day03_bsta511_code.html#scatterplots-with-color-coded-dots",
    "title": "Day 3 code: Data visualization",
    "section": "Scatterplots with color-coded dots",
    "text": "Scatterplots with color-coded dots\n\nggplot(data = dds.discr, \n       aes(x = age, y = expenditures,\n           color = ethnicity)) +   \n  geom_point(alpha = .5) +       \n  labs(x = \"Age\",\n       y = \"Annual Expenditures ($)\")"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#barplots",
    "href": "slides_code/Day03_bsta511_code.html#barplots",
    "title": "Day 3 code: Data visualization",
    "section": "Barplots",
    "text": "Barplots\nCounts\n\nggplot(data = dds.discr, \n       aes(x = ethnicity)) +\n  geom_bar() \n\n\n\n\npercentages\n\nggplot(data = dds.discr, \n       aes(x = ethnicity)) +\n  geom_bar(aes(y = stat(prop), group = 1)) + \n  scale_y_continuous(labels = scales::percent_format())"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#barplots-with-2-variables-segmented-bar-plots",
    "href": "slides_code/Day03_bsta511_code.html#barplots-with-2-variables-segmented-bar-plots",
    "title": "Day 3 code: Data visualization",
    "section": "Barplots with 2 variables: segmented bar plots",
    "text": "Barplots with 2 variables: segmented bar plots\n\nggplot(data = dds.discr, \n       aes(x = ethnicity,\n           fill = age.cohort)) + \n  geom_bar() \n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = ethnicity,\n           fill = age.cohort)) + \n  geom_bar(position = \"fill\")"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#barplots-with-2-variables-side-by-side-bar-plots",
    "href": "slides_code/Day03_bsta511_code.html#barplots-with-2-variables-side-by-side-bar-plots",
    "title": "Day 3 code: Data visualization",
    "section": "Barplots with 2 variables: side-by-side bar plots",
    "text": "Barplots with 2 variables: side-by-side bar plots\nSide-by-side bar plot\n\nggplot(data = dds.discr, \n       aes(x = ethnicity,\n           fill = age.cohort)) + \n  geom_bar(position = \"dodge\")"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#frequency-tables-count",
    "href": "slides_code/Day03_bsta511_code.html#frequency-tables-count",
    "title": "Day 3 code: Data visualization",
    "section": "Frequency tables: count()",
    "text": "Frequency tables: count()\n\ncount is from the dplyr package\nthe output is a long tibble, and not a “nice” table\n\n\ndds.discr %&gt;% count(ethnicity)\n\n# A tibble: 8 × 2\n  ethnicity              n\n  &lt;fct&gt;              &lt;int&gt;\n1 American Indian        4\n2 Asian                129\n3 Black                 59\n4 Hispanic             376\n5 Multi Race            26\n6 Native Hawaiian        3\n7 Other                  2\n8 White not Hispanic   401\n\n\n\ndds.discr %&gt;% \n  count(ethnicity, age.cohort)\n\n# A tibble: 35 × 3\n   ethnicity       age.cohort     n\n   &lt;fct&gt;           &lt;fct&gt;      &lt;int&gt;\n 1 American Indian 13-17          1\n 2 American Indian 22-50          1\n 3 American Indian 51+            2\n 4 Asian           0-5            8\n 5 Asian           6-12          18\n 6 Asian           13-17         20\n 7 Asian           18-21         41\n 8 Asian           22-50         29\n 9 Asian           51+           13\n10 Black           0-5            3\n# ℹ 25 more rows"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#how-to-use-the-pipe",
    "href": "slides_code/Day03_bsta511_code.html#how-to-use-the-pipe",
    "title": "Day 3 code: Data visualization",
    "section": "How to use the pipe %>%",
    "text": "How to use the pipe %&gt;%\nThe pipe operator %&gt;% strings together commands to be performed sequentially\n\ndds.discr %&gt;% head(n=3)      # pronounce %&gt;% as \"then\"\n\n# A tibble: 3 × 6\n     id age.cohort   age gender expenditures ethnicity         \n  &lt;int&gt; &lt;fct&gt;      &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt;             \n1 10210 13-17         17 Female         2113 White not Hispanic\n2 10409 22-50         37 Male          41924 White not Hispanic\n3 10486 0-5            3 Male           1454 Hispanic          \n\n\n\nAlways first list the tibble that the commands are being applied to\nCan use multiple pipes to run multiple commands in sequence\n\nWhat does the following code do?\n\n\n\ndds.discr %&gt;% head(n=3) %&gt;% summary()\n\n       id        age.cohort      age        gender   expenditures  \n Min.   :10210   0-5  :1    Min.   : 3   Female:1   Min.   : 1454  \n 1st Qu.:10310   6-12 :0    1st Qu.:10   Male  :2   1st Qu.: 1784  \n Median :10409   13-17:1    Median :17              Median : 2113  \n Mean   :10368   18-21:0    Mean   :19              Mean   :15164  \n 3rd Qu.:10448   22-50:1    3rd Qu.:27              3rd Qu.:22018  \n Max.   :10486   51+  :0    Max.   :37              Max.   :41924  \n                                                                   \n              ethnicity\n White not Hispanic:2  \n Hispanic          :1  \n American Indian   :0  \n Asian             :0  \n Black             :0  \n Multi Race        :0  \n (Other)           :0"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#frequency-tables-janitor-packages-tabyl-function",
    "href": "slides_code/Day03_bsta511_code.html#frequency-tables-janitor-packages-tabyl-function",
    "title": "Day 3 code: Data visualization",
    "section": "Frequency tables: janitor package’s tabyl function",
    "text": "Frequency tables: janitor package’s tabyl function\n\n# default table\ndds.discr %&gt;% \n  tabyl(ethnicity)  \n\n          ethnicity   n percent\n    American Indian   4   0.004\n              Asian 129   0.129\n              Black  59   0.059\n           Hispanic 376   0.376\n         Multi Race  26   0.026\n    Native Hawaiian   3   0.003\n              Other   2   0.002\n White not Hispanic 401   0.401\n\n\nadorn_ your table!\n\ndds.discr %&gt;% \n  tabyl(ethnicity) %&gt;%\n  adorn_totals(\"row\") %&gt;% \n  adorn_pct_formatting(digits=2)  \n\n          ethnicity    n percent\n    American Indian    4   0.40%\n              Asian  129  12.90%\n              Black   59   5.90%\n           Hispanic  376  37.60%\n         Multi Race   26   2.60%\n    Native Hawaiian    3   0.30%\n              Other    2   0.20%\n White not Hispanic  401  40.10%\n              Total 1000 100.00%\n\n\n\nRelative frequency table\n\nA relative frequency table shows proportions (or percentages) instead of counts\nBelow I removed (deselected) the counts column (n) to create a relative frequency table\n\n\ndds.discr %&gt;% \n  tabyl(ethnicity) %&gt;%\n  adorn_totals(\"row\") %&gt;% \n  adorn_pct_formatting(digits=2) %&gt;%   \n  select(-n) # remove column with variable name n\n\n          ethnicity percent\n    American Indian   0.40%\n              Asian  12.90%\n              Black   5.90%\n           Hispanic  37.60%\n         Multi Race   2.60%\n    Native Hawaiian   0.30%\n              Other   0.20%\n White not Hispanic  40.10%\n              Total 100.00%"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#contingency-tables-two-way-tables",
    "href": "slides_code/Day03_bsta511_code.html#contingency-tables-two-way-tables",
    "title": "Day 3 code: Data visualization",
    "section": "Contingency tables (two-way tables)",
    "text": "Contingency tables (two-way tables)\n\nContingency tables summarize data for two categorical variables\n\nwith each value in the table representing the number of times\na particular combination of outcomes occurs\n\nRow & column totals\nare sometimes called marginal totals\n\n\ndds.discr %&gt;% \n  tabyl(ethnicity, gender) %&gt;%    \n  adorn_totals(c(\"row\", \"col\"))    \n\n          ethnicity Female Male Total\n    American Indian      3    1     4\n              Asian     61   68   129\n              Black     26   33    59\n           Hispanic    192  184   376\n         Multi Race     13   13    26\n    Native Hawaiian      2    1     3\n              Other      1    1     2\n White not Hispanic    205  196   401\n              Total    503  497  1000\n\n\n\nContingency tables with percentages\n\ndds.discr %&gt;% \n  tabyl(ethnicity, age.cohort) %&gt;%\n  adorn_totals(c(\"row\")) %&gt;%\n  adorn_percentages(\"row\") %&gt;%   \n  adorn_pct_formatting(digits=0) %&gt;%    \n  adorn_ns()    \n\n          ethnicity      0-5      6-12      13-17     18-21     22-50       51+\n    American Indian  0%  (0)  0%   (0)  25%   (1)  0%   (0) 25%   (1) 50%   (2)\n              Asian  6%  (8) 14%  (18)  16%  (20) 32%  (41) 22%  (29) 10%  (13)\n              Black  5%  (3) 19%  (11)  20%  (12) 15%   (9) 29%  (17) 12%   (7)\n           Hispanic 12% (44) 24%  (91)  27% (103) 21%  (78) 11%  (43)  5%  (17)\n         Multi Race 27%  (7) 35%   (9)  27%   (7)  8%   (2)  4%   (1)  0%   (0)\n    Native Hawaiian  0%  (0)  0%   (0)   0%   (0)  0%   (0) 67%   (2) 33%   (1)\n              Other  0%  (0)  0%   (0) 100%   (2)  0%   (0)  0%   (0)  0%   (0)\n White not Hispanic  5% (20) 11%  (46)  17%  (67) 17%  (69) 33% (133) 16%  (66)\n              Total  8% (82) 18% (175)  21% (212) 20% (199) 23% (226) 11% (106)"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#mean-annual-dds-expenditures-by-raceethnicity",
    "href": "slides_code/Day03_bsta511_code.html#mean-annual-dds-expenditures-by-raceethnicity",
    "title": "Day 3 code: Data visualization",
    "section": "Mean annual DDS expenditures by race/ethnicity",
    "text": "Mean annual DDS expenditures by race/ethnicity\n\nmean(dds.discr$expenditures)\n\n[1] 18065.79\n\ndds.discr %&gt;% \n  summarize(\n    ave = mean(expenditures),\n    SD = sd(expenditures),\n    med = median(expenditures))\n\n# A tibble: 1 × 3\n     ave     SD   med\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 18066. 19543.  7026\n\n\n\ndds.discr %&gt;% \n  group_by(ethnicity) %&gt;% \n  summarize(\n    ave = mean(expenditures),\n    SD = sd(expenditures),\n    med = median(expenditures))\n\n# A tibble: 8 × 4\n  ethnicity             ave     SD    med\n  &lt;fct&gt;               &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 American Indian    36438. 25694. 41818.\n2 Asian              18392. 19209.  9369 \n3 Black              20885. 20549.  8687 \n4 Hispanic           11066. 15630.  3952 \n5 Multi Race          4457.  7332.  2622 \n6 Native Hawaiian    42782.  6576. 40727 \n7 Other               3316.  1836.  3316.\n8 White not Hispanic 24698. 20604. 15718"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#get_summary_stats-from-rstatix-package",
    "href": "slides_code/Day03_bsta511_code.html#get_summary_stats-from-rstatix-package",
    "title": "Day 3 code: Data visualization",
    "section": "get_summary_stats() from rstatix package",
    "text": "get_summary_stats() from rstatix package\n\ndds.discr %&gt;% get_summary_stats()\n\n# A tibble: 3 × 13\n  variable         n   min   max median     q1     q3   iqr    mad   mean     sd\n  &lt;fct&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 id            1000 10210 99898 55384. 31809. 76135. 44326 3.27e4 5.47e4 2.56e4\n2 age           1000     0    95    18     12     26     14 1.04e1 2.28e1 1.85e1\n3 expenditures  1000   222 75098  7026   2899. 37713. 34814 7.76e3 1.81e4 1.95e4\n# ℹ 2 more variables: se &lt;dbl&gt;, ci &lt;dbl&gt;\n\ndds.discr %&gt;% \n  group_by(ethnicity) %&gt;%\n  get_summary_stats(expenditures, type = \"common\")\n\n# A tibble: 8 × 11\n  ethnicity variable     n   min   max median    iqr   mean     sd     se     ci\n  &lt;fct&gt;     &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 American… expendi…     4  3726 58392 41818. 34085. 36438. 25694. 12847. 40885.\n2 Asian     expendi…   129   374 75098  9369  30892  18392. 19209.  1691.  3346.\n3 Black     expendi…    59   240 60808  8687  37987  20885. 20549.  2675.  5355.\n4 Hispanic  expendi…   376   222 65581  3952   7961. 11066. 15630.   806.  1585.\n5 Multi Ra… expendi…    26   669 38619  2622   2060.  4457.  7332.  1438.  2962.\n6 Native H… expendi…     3 37479 50141 40727   6331  42782.  6576.  3797. 16337.\n7 Other     expendi…     2  2018  4615  3316.  1298.  3316.  1836.  1298. 16499.\n8 White no… expendi…   401   340 68890 15718  39157  24698. 20604.  1029.  2023."
  },
  {
    "objectID": "slides_code/Day03_bsta511_code.html#how-to-force-all-output-to-be-shown",
    "href": "slides_code/Day03_bsta511_code.html#how-to-force-all-output-to-be-shown",
    "title": "Day 3 code: Data visualization",
    "section": "How to force all output to be shown?",
    "text": "How to force all output to be shown?\nUse kable() from the knitr package.\n\ndds.discr %&gt;% get_summary_stats() %&gt;% kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nn\nmin\nmax\nmedian\nq1\nq3\niqr\nmad\nmean\nsd\nse\nci\n\n\n\n\nid\n1000\n10210\n99898\n55384.5\n31808.75\n76134.75\n44326\n32734.325\n54662.85\n25643.673\n810.924\n1591.310\n\n\nage\n1000\n0\n95\n18.0\n12.00\n26.00\n14\n10.378\n22.80\n18.462\n0.584\n1.146\n\n\nexpenditures\n1000\n222\n75098\n7026.0\n2898.75\n37712.75\n34814\n7760.670\n18065.79\n19542.831\n617.999\n1212.724\n\n\n\n\n\n\ndds.discr %&gt;% \n  group_by(ethnicity) %&gt;%\n  get_summary_stats(expenditures, type = \"common\") %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nethnicity\nvariable\nn\nmin\nmax\nmedian\niqr\nmean\nsd\nse\nci\n\n\n\n\nAmerican Indian\nexpenditures\n4\n3726\n58392\n41817.5\n34085.25\n36438.250\n25693.912\n12846.956\n40884.748\n\n\nAsian\nexpenditures\n129\n374\n75098\n9369.0\n30892.00\n18392.372\n19209.225\n1691.278\n3346.482\n\n\nBlack\nexpenditures\n59\n240\n60808\n8687.0\n37987.00\n20884.593\n20549.274\n2675.288\n5355.170\n\n\nHispanic\nexpenditures\n376\n222\n65581\n3952.0\n7961.25\n11065.569\n15629.847\n806.048\n1584.940\n\n\nMulti Race\nexpenditures\n26\n669\n38619\n2622.0\n2059.75\n4456.731\n7332.135\n1437.950\n2961.514\n\n\nNative Hawaiian\nexpenditures\n3\n37479\n50141\n40727.0\n6331.00\n42782.333\n6576.462\n3796.922\n16336.838\n\n\nOther\nexpenditures\n2\n2018\n4615\n3316.5\n1298.50\n3316.500\n1836.356\n1298.500\n16499.007\n\n\nWhite not Hispanic\nexpenditures\n401\n340\n68890\n15718.0\n39157.00\n24697.549\n20604.376\n1028.933\n2022.793"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part2.html",
    "href": "slides_code/Day03_bsta511_code_part2.html",
    "title": "Day 3 code - Part 2: Data visualization",
    "section": "",
    "text": "Previous research\n\nResearchers examined DDS expenditures for developmentally disabled residents by ethnicity\nFound that the mean annual expenditures on Hispanics was less than that on White non-Hispanics.\n\nResult: an allegation of ethnic discrimination was brought against the California DDS.\nQuestion: Are the data sufficient evidence of ethnic discrimination?\n\n\n\n\n\nThe textbook’s datasets are in the R package oibiostat\nMake sure the oibiostat package is installed before running the code below.\nLoad the oibiostat package and the dataset dds.discr\n\nthe code below needs to be run every time you restart R or render a Qmd file\n\nlibrary(oibiostat)\ndata(\"dds.discr\")\n\n\nAfter loading the dataset dds.discr using data(\"dds.discr\"), you will see dds.discr in the Data list of the Environment window.\n\n\n\nNew: glimpse()\n\nUse glimpse() from the tidyverse package (technically it’s from the dplyr package) to get information about variable types.\nglimpse() tends to have nicer output for tibbles than str()\n\n\nlibrary(tidyverse)\nglimpse(dds.discr)  # from tidyverse package (dplyr)\n\nRows: 1,000\nColumns: 6\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ gender       &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ ethnicity    &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…\n\n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = expenditures,\n           y = ethnicity)) + \n  geom_boxplot(color=\"darkgrey\") + \n  labs(x = \"Annual Expenditures ($)\",\n       y = \"Race and ethnicity\") +\n  geom_jitter(     \n    aes(color = ethnicity),      \n    alpha = 0.3,      \n    show.legend = FALSE,      \n    position = position_jitter(height = 0.4))      \n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = age,\n           y = expenditures)) + \n  geom_point() +       \n  labs(x = \"Age\",\n       y = \"Annual Expenditures ($)\") \n\n\n\n\n\n\n\n\ndds.discr_Hips_WhnH &lt;- dds.discr %&gt;%  \n  filter(ethnicity == \"White not Hispanic\" | ethnicity == \"Hispanic\" ) %&gt;% \n  droplevels()   # remove empty factor levels\n  \n\nggplot(data = dds.discr_Hips_WhnH,   \n       aes(x = expenditures,\n           y = age.cohort)) + \n  geom_boxplot(color=\"darkgrey\") + \n  facet_grid(rows = \"ethnicity\") +   \n  labs(x = \"Annual Expenditures ($)\",\n       y = \"Race and ethnicity\") +\n  geom_jitter(     \n    aes(color = ethnicity),      \n    alpha = 0.3,      \n    show.legend = FALSE,      \n    position = position_jitter(     \n      height = 0.4))      \n\n\n\n\n\n\n\n\n\n\nmean_expend &lt;- dds.discr_Hips_WhnH %&gt;% \n  group_by(ethnicity, age.cohort)%&gt;% \n  summarize(ave = mean(expenditures))\n\nmean_expend\n\n# A tibble: 12 × 3\n# Groups:   ethnicity [2]\n   ethnicity          age.cohort    ave\n   &lt;fct&gt;              &lt;fct&gt;       &lt;dbl&gt;\n 1 Hispanic           0-5         1393.\n 2 Hispanic           6-12        2312.\n 3 Hispanic           13-17       3955.\n 4 Hispanic           18-21       9960.\n 5 Hispanic           22-50      40924.\n 6 Hispanic           51+        55585 \n 7 White not Hispanic 0-5         1367.\n 8 White not Hispanic 6-12        2052.\n 9 White not Hispanic 13-17       3904.\n10 White not Hispanic 18-21      10133.\n11 White not Hispanic 22-50      40188.\n12 White not Hispanic 51+        52670.\n\n\n\n\n\n\nmean_expend_wide &lt;- mean_expend %&gt;% \n  pivot_wider(names_from = ethnicity,\n              values_from = ave)\n\nmean_expend_wide\n\n# A tibble: 6 × 3\n  age.cohort Hispanic `White not Hispanic`\n  &lt;fct&gt;         &lt;dbl&gt;                &lt;dbl&gt;\n1 0-5           1393.                1367.\n2 6-12          2312.                2052.\n3 13-17         3955.                3904.\n4 18-21         9960.               10133.\n5 22-50        40924.               40188.\n6 51+          55585                52670.\n\n\n\n\n\n\n\nmean_expend_wide &lt;- mean_expend_wide %&gt;% \n  mutate(diff_mean = `White not Hispanic` - Hispanic)\n\nmean_expend_wide\n\n# A tibble: 6 × 4\n  age.cohort Hispanic `White not Hispanic` diff_mean\n  &lt;fct&gt;         &lt;dbl&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n1 0-5           1393.                1367.     -26.3\n2 6-12          2312.                2052.    -260. \n3 13-17         3955.                3904.     -50.9\n4 18-21         9960.               10133.     173. \n5 22-50        40924.               40188.    -736. \n6 51+          55585                52670.   -2915. \n\n\nQuestion: Are the data sufficient evidence of ethnic discrimination in DDS expenditures when comparing Hispanics with White non-Hispanics?"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part2.html#case-study-discrimination-in-developmental-disability-support-1.7.1",
    "href": "slides_code/Day03_bsta511_code_part2.html#case-study-discrimination-in-developmental-disability-support-1.7.1",
    "title": "Day 3 code - Part 2: Data visualization",
    "section": "",
    "text": "Previous research\n\nResearchers examined DDS expenditures for developmentally disabled residents by ethnicity\nFound that the mean annual expenditures on Hispanics was less than that on White non-Hispanics.\n\nResult: an allegation of ethnic discrimination was brought against the California DDS.\nQuestion: Are the data sufficient evidence of ethnic discrimination?"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part2.html#load-dds.discr-dataset-from-oibiostat-package",
    "href": "slides_code/Day03_bsta511_code_part2.html#load-dds.discr-dataset-from-oibiostat-package",
    "title": "Day 3 code - Part 2: Data visualization",
    "section": "",
    "text": "The textbook’s datasets are in the R package oibiostat\nMake sure the oibiostat package is installed before running the code below.\nLoad the oibiostat package and the dataset dds.discr\n\nthe code below needs to be run every time you restart R or render a Qmd file\n\nlibrary(oibiostat)\ndata(\"dds.discr\")\n\n\nAfter loading the dataset dds.discr using data(\"dds.discr\"), you will see dds.discr in the Data list of the Environment window.\n\n\n\nNew: glimpse()\n\nUse glimpse() from the tidyverse package (technically it’s from the dplyr package) to get information about variable types.\nglimpse() tends to have nicer output for tibbles than str()\n\n\nlibrary(tidyverse)\nglimpse(dds.discr)  # from tidyverse package (dplyr)\n\nRows: 1,000\nColumns: 6\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ gender       &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ ethnicity    &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part2.html#recall-previous-data-viz",
    "href": "slides_code/Day03_bsta511_code_part2.html#recall-previous-data-viz",
    "title": "Day 3 code - Part 2: Data visualization",
    "section": "",
    "text": "ggplot(data = dds.discr, \n       aes(x = expenditures,\n           y = ethnicity)) + \n  geom_boxplot(color=\"darkgrey\") + \n  labs(x = \"Annual Expenditures ($)\",\n       y = \"Race and ethnicity\") +\n  geom_jitter(     \n    aes(color = ethnicity),      \n    alpha = 0.3,      \n    show.legend = FALSE,      \n    position = position_jitter(height = 0.4))      \n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = age,\n           y = expenditures)) + \n  geom_point() +       \n  labs(x = \"Age\",\n       y = \"Annual Expenditures ($)\")"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part2.html#visualize-in-more-detail-ethnicity-age-and-expenditures",
    "href": "slides_code/Day03_bsta511_code_part2.html#visualize-in-more-detail-ethnicity-age-and-expenditures",
    "title": "Day 3 code - Part 2: Data visualization",
    "section": "",
    "text": "dds.discr_Hips_WhnH &lt;- dds.discr %&gt;%  \n  filter(ethnicity == \"White not Hispanic\" | ethnicity == \"Hispanic\" ) %&gt;% \n  droplevels()   # remove empty factor levels\n  \n\nggplot(data = dds.discr_Hips_WhnH,   \n       aes(x = expenditures,\n           y = age.cohort)) + \n  geom_boxplot(color=\"darkgrey\") + \n  facet_grid(rows = \"ethnicity\") +   \n  labs(x = \"Annual Expenditures ($)\",\n       y = \"Race and ethnicity\") +\n  geom_jitter(     \n    aes(color = ethnicity),      \n    alpha = 0.3,      \n    show.legend = FALSE,      \n    position = position_jitter(     \n      height = 0.4))"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part2.html#mean-annual-dds-expenditures-by-raceethnicity",
    "href": "slides_code/Day03_bsta511_code_part2.html#mean-annual-dds-expenditures-by-raceethnicity",
    "title": "Day 3 code - Part 2: Data visualization",
    "section": "",
    "text": "mean_expend &lt;- dds.discr_Hips_WhnH %&gt;% \n  group_by(ethnicity, age.cohort)%&gt;% \n  summarize(ave = mean(expenditures))\n\nmean_expend\n\n# A tibble: 12 × 3\n# Groups:   ethnicity [2]\n   ethnicity          age.cohort    ave\n   &lt;fct&gt;              &lt;fct&gt;       &lt;dbl&gt;\n 1 Hispanic           0-5         1393.\n 2 Hispanic           6-12        2312.\n 3 Hispanic           13-17       3955.\n 4 Hispanic           18-21       9960.\n 5 Hispanic           22-50      40924.\n 6 Hispanic           51+        55585 \n 7 White not Hispanic 0-5         1367.\n 8 White not Hispanic 6-12        2052.\n 9 White not Hispanic 13-17       3904.\n10 White not Hispanic 18-21      10133.\n11 White not Hispanic 22-50      40188.\n12 White not Hispanic 51+        52670.\n\n\n\n\n\n\nmean_expend_wide &lt;- mean_expend %&gt;% \n  pivot_wider(names_from = ethnicity,\n              values_from = ave)\n\nmean_expend_wide\n\n# A tibble: 6 × 3\n  age.cohort Hispanic `White not Hispanic`\n  &lt;fct&gt;         &lt;dbl&gt;                &lt;dbl&gt;\n1 0-5           1393.                1367.\n2 6-12          2312.                2052.\n3 13-17         3955.                3904.\n4 18-21         9960.               10133.\n5 22-50        40924.               40188.\n6 51+          55585                52670."
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part2.html#differences-in-mean-annual-dds-expenditures-by-age-cohort-and-raceethnicity",
    "href": "slides_code/Day03_bsta511_code_part2.html#differences-in-mean-annual-dds-expenditures-by-age-cohort-and-raceethnicity",
    "title": "Day 3 code - Part 2: Data visualization",
    "section": "",
    "text": "mean_expend_wide &lt;- mean_expend_wide %&gt;% \n  mutate(diff_mean = `White not Hispanic` - Hispanic)\n\nmean_expend_wide\n\n# A tibble: 6 × 4\n  age.cohort Hispanic `White not Hispanic` diff_mean\n  &lt;fct&gt;         &lt;dbl&gt;                &lt;dbl&gt;     &lt;dbl&gt;\n1 0-5           1393.                1367.     -26.3\n2 6-12          2312.                2052.    -260. \n3 13-17         3955.                3904.     -50.9\n4 18-21         9960.               10133.     173. \n5 22-50        40924.               40188.    -736. \n6 51+          55585                52670.   -2915. \n\n\nQuestion: Are the data sufficient evidence of ethnic discrimination in DDS expenditures when comparing Hispanics with White non-Hispanics?"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part3_data_wrangling.html",
    "href": "slides_code/Day03_bsta511_code_part3_data_wrangling.html",
    "title": "Day 3 Part 3 code: Data wrangling",
    "section": "",
    "text": "library(tidyverse)\nlibrary(janitor)"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#load-dataset-dds.discr-from-package-oibiostat",
    "href": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#load-dataset-dds.discr-from-package-oibiostat",
    "title": "Day 3 Part 3 code: Data wrangling",
    "section": "Load dataset dds.discr from package oibiostat",
    "text": "Load dataset dds.discr from package oibiostat\n\nThe textbook’s datasets are in the R package oibiostat\nIf you haven’t already installed the package oibiostat, then first do so using directions in previous slide.\nLoad the oibiostat package and the dataset dds.discr\n\nthe code below needs to be run every time you restart R or knit an Rmd file\n\n\n\nlibrary(oibiostat)\ndata(\"dds.discr\")\n\n\nAfter loading the dataset dds.discr using data(\"dds.discr\"), you will see dds.discr in the Data list of the Environment window."
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#getting-to-know-the-dataset",
    "href": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#getting-to-know-the-dataset",
    "title": "Day 3 Part 3 code: Data wrangling",
    "section": "Getting to know the dataset",
    "text": "Getting to know the dataset\n\ndim(dds.discr)\n\n[1] 1000    6\n\nnames(dds.discr)\n\n[1] \"id\"           \"age.cohort\"   \"age\"          \"gender\"       \"expenditures\"\n[6] \"ethnicity\"   \n\nlength(unique(dds.discr$id)) # How many unique id's are there?\n\n[1] 1000"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#filter-rows-that-satisfy-specified-conditions",
    "href": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#filter-rows-that-satisfy-specified-conditions",
    "title": "Day 3 Part 3 code: Data wrangling",
    "section": "filter() rows that satisfy specified conditions",
    "text": "filter() rows that satisfy specified conditions\n\nfilter() to select rows\nfilter data based on rows\n\nmath: &gt;, &lt;, &gt;=, &lt;=\ndouble = for “is equal to”: ==\n!= (not equal)\n& (and)\n| (or)\nis.na() to filter based on missing values\n%in% to filter based on group membership\n! in front negates the statement, as in\n\n!is.na(age)\n!(ethnicity %in% c(\"Asian\",\"Black\"))\n\n\n\n# Note: the output from the command below is not being saved \n# since it's not being assigned to a variable using &lt;-\ndds.discr %&gt;% filter(age &gt; 90)   \n\n# A tibble: 4 × 6\n     id age.cohort   age gender expenditures ethnicity\n  &lt;int&gt; &lt;fct&gt;      &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt;    \n1 19250 51+           94 Female        60871 Hispanic \n2 46726 51+           95 Male          55187 Hispanic \n3 55056 51+           95 Female        54680 Black    \n4 87737 51+           91 Male          54481 Asian    \n\n\n\n\nfilter() practice\nWhat do these commands do? Try them out:\n\ndds.discr %&gt;% filter(age &lt; 5)\ndds.discr %&gt;% filter(age/expenditures &lt; 0.5)    # can do math within filter command\ndds.discr %&gt;% filter((age &lt; 15) | (age &gt; 50))\n\n# simultaneously filter on multiple variables\ndds.discr %&gt;% filter(age &lt; 20, expenditures &gt; 1000, gender == \"Male\") \n\ndds.discr %&gt;% filter(id == 10210) # note the use of == instead of just =\ndds.discr %&gt;% filter(gender == \"Female\")\ndds.discr %&gt;% filter(!(age.cohort == \"51+\"))\ndds.discr %&gt;% filter(age.cohort %in% c(\"0-5\", \"6-12\"))\n\ndds.discr %&gt;% filter(is.na(age))\ndds.discr %&gt;% filter(!is.na(age))"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#subset-by-columns",
    "href": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#subset-by-columns",
    "title": "Day 3 Part 3 code: Data wrangling",
    "section": "Subset by columns",
    "text": "Subset by columns\n\nselect() to choose columns\n\nselect columns (variables)\nno quotes needed around variable names\ncan be used to rearrange columns\nuses special syntax that is flexible and has many options\n\n\ndds.discr %&gt;% select(id, expenditures, ethnicity)\n\n# A tibble: 1,000 × 3\n      id expenditures ethnicity         \n   &lt;int&gt;        &lt;int&gt; &lt;fct&gt;             \n 1 10210         2113 White not Hispanic\n 2 10409        41924 White not Hispanic\n 3 10486         1454 Hispanic          \n 4 10538         6400 Hispanic          \n 5 10568         4412 White not Hispanic\n 6 10690         4566 Hispanic          \n 7 10711         3915 White not Hispanic\n 8 10778         3873 Black             \n 9 10820         5021 White not Hispanic\n10 10823         2887 Hispanic          \n# ℹ 990 more rows\n\n\n\n\nColumn selection syntax options\nThere are many ways to select a set of variable names (columns):\n\nvar1:var20: all columns from var1 to var20\none_of(c(\"a\", \"b\", \"c\")): all columns with names in the specified character vector of names\nRemoving columns\n\n-var1: remove the columnvar1\n-(var1:var20): remove all columns from var1 to var20\n\nSelect by specifying text within column names\n\ncontains(\"date\"), contains(\"_\"): all variable names that contain the specified string of characters\nstarts_with(\"a\") or ends_with(\"last\"): all variable names that start or end with the specified string\n\n\nSee other examples in the data transformation cheatsheet.\n\n\nselect() practice\nWhich columns are selected & in what order using these commands?\nFirst guess and then try them out.\n\ndds.discr %&gt;% select(id:gender)\ndds.discr %&gt;% select(one_of(c(\"age\",\"expenditures\", \"notindata\")))\n\ndds.discr %&gt;% select(-age.cohort,-gender)\ndds.discr %&gt;% select(-(id:gender))\n\ndds.discr %&gt;% select(contains(\"age\"))\ndds.discr %&gt;% select(starts_with(\"a\"))\ndds.discr %&gt;% select(-contains(\"a\"))"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#relocate-to-change-order-of-columns",
    "href": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#relocate-to-change-order-of-columns",
    "title": "Day 3 Part 3 code: Data wrangling",
    "section": "relocate() to change order of columns",
    "text": "relocate() to change order of columns\n\nchange the order of columns in dataset\nspecified column names get put first,\n\nand unspecified column names after that in original order\n\nno quotes needed around variable names\nsimilar options as with select(),\n\nplus special ones such as .before and .after\n\n\n\ndds.discr %&gt;% relocate(age.cohort, ethnicity)\n\n# A tibble: 1,000 × 6\n   age.cohort ethnicity             id   age gender expenditures\n   &lt;fct&gt;      &lt;fct&gt;              &lt;int&gt; &lt;int&gt; &lt;fct&gt;         &lt;int&gt;\n 1 13-17      White not Hispanic 10210    17 Female         2113\n 2 22-50      White not Hispanic 10409    37 Male          41924\n 3 0-5        Hispanic           10486     3 Male           1454\n 4 18-21      Hispanic           10538    19 Female         6400\n 5 13-17      White not Hispanic 10568    13 Male           4412\n 6 13-17      Hispanic           10690    15 Female         4566\n 7 13-17      White not Hispanic 10711    13 Female         3915\n 8 13-17      Black              10778    17 Male           3873\n 9 13-17      White not Hispanic 10820    14 Female         5021\n10 13-17      Hispanic           10823    13 Male           2887\n# ℹ 990 more rows"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#relocate-practice",
    "href": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#relocate-practice",
    "title": "Day 3 Part 3 code: Data wrangling",
    "section": "relocate() practice",
    "text": "relocate() practice\nWhat order are the columns in using these commands?\nFirst guess and then try them out.\n\ndds.discr %&gt;% relocate(age:ethnicity)\n\ndds.discr %&gt;% relocate(where(is.numeric))\ndds.discr %&gt;% relocate(where(is.factor))\n# note: the next command doesn't do anything \n# since there are no character type variables in the dataset\ndds.discr %&gt;% relocate(where(is.character))\n\ndds.discr %&gt;% relocate(age,.before = ethnicity)\ndds.discr %&gt;% relocate(ethnicity, .after = age.cohort)\ndds.discr %&gt;% relocate(age, .after = last_col())"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#mutate",
    "href": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#mutate",
    "title": "Day 3 Part 3 code: Data wrangling",
    "section": "mutate()",
    "text": "mutate()\nUse mutate() to add new columns to a tibble * Many options in how to define new column of data\n\n# use = to define new a variable within mutate (not &lt;- or ==) \nnewdata &lt;- dds.discr %&gt;% \n  mutate(\n    log_expenditures = log(expenditures),\n    expend_per_yearage = expenditures / age)   \n\n\nnewdata %&gt;% select(id, age, expenditures, log_expenditures, expend_per_yearage)\n\n# A tibble: 1,000 × 5\n      id   age expenditures log_expenditures expend_per_yearage\n   &lt;int&gt; &lt;int&gt;        &lt;int&gt;            &lt;dbl&gt;              &lt;dbl&gt;\n 1 10210    17         2113             7.66               124.\n 2 10409    37        41924            10.6               1133.\n 3 10486     3         1454             7.28               485.\n 4 10538    19         6400             8.76               337.\n 5 10568    13         4412             8.39               339.\n 6 10690    15         4566             8.43               304.\n 7 10711    13         3915             8.27               301.\n 8 10778    17         3873             8.26               228.\n 9 10820    14         5021             8.52               359.\n10 10823    13         2887             7.97               222.\n# ℹ 990 more rows\n\n\n\nmutate() practice\nWhat do the following commands do?\nFirst guess and then try them out.\n\ndds.discr %&gt;% mutate(age_young = (age &lt; 18))\n\ndds.discr %&gt;% mutate(male = (gender == \"Male\"))\ndds.discr %&gt;% mutate(male = 1 * (gender == \"Male\"))"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#case_when-to-create-multi-valued-variables",
    "href": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#case_when-to-create-multi-valued-variables",
    "title": "Day 3 Part 3 code: Data wrangling",
    "section": "case_when() to create multi-valued variables",
    "text": "case_when() to create multi-valued variables\n\nExample: create age groups based off of the age variable\n\n\ndds.discr2 &lt;- dds.discr %&gt;%\n  mutate(\n    age_group = case_when(\n      age &lt; 6 ~ \"0 to 5\",               # condition ~ new_value\n      age &gt;= 6 & age &lt; 13 ~ \"6 to 12\",\n      age &gt;= 13 & age &lt; 18 ~ \"13 to 17\",\n      age &gt;= 18 & age &lt; 22 ~ \"18 to 21\",\n      age &gt;= 22 & age &lt; 51 ~ \"22 to 50\",\n      age &gt;= 51 ~ \"51 plus\")\n    )\n\ndds.discr2 %&gt;% select(age, age_group) %&gt;% head()\n\n# A tibble: 6 × 2\n    age age_group\n  &lt;int&gt; &lt;chr&gt;    \n1    17 13 to 17 \n2    37 22 to 50 \n3     3 0 to 5   \n4    19 18 to 21 \n5    13 13 to 17 \n6    15 13 to 17 \n\ndds.discr2 %&gt;% tabyl(age.cohort, age_group)\n\n age.cohort 0 to 5 13 to 17 18 to 21 22 to 50 51 plus 6 to 12\n        0-5     82        0        0        0       0       0\n       6-12      0        0        0        0       0     175\n      13-17      0      212        0        0       0       0\n      18-21      0        0      199        0       0       0\n      22-50      0        0        0      226       0       0\n        51+      0        0        0        0     106       0"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#wide-vs.-long-data",
    "href": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#wide-vs.-long-data",
    "title": "Day 3 Part 3 code: Data wrangling",
    "section": "Wide vs. long data",
    "text": "Wide vs. long data\n\nWide data has one row per individual,\n\nwith multiple columns for their repeated measurements\n\nLong data has multiple rows per individual,\n\nwith one column for the measurement variable and\nanother indicating from when/where the repeated measures are from"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#dds-example",
    "href": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#dds-example",
    "title": "Day 3 Part 3 code: Data wrangling",
    "section": "DDS example",
    "text": "DDS example\nMean expenditures by ethnicity and age cohort (from Day 3 slides)"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#example-wide-toy-dataset",
    "href": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#example-wide-toy-dataset",
    "title": "Day 3 Part 3 code: Data wrangling",
    "section": "Example wide toy dataset",
    "text": "Example wide toy dataset\nCopy and paste the code below into R to create this example dataset\n\nSBP_wide &lt;- tibble(id = letters[1:4],\n                     sex = c(\"F\", \"M\", \"M\", \"F\"),\n                     SBP_v1 = c(130, 120, 130, 119),\n                     SBP_v2 = c(110, 116, 136, 106),\n                     SBP_v3 = c(112, 122, 138, 118))\nSBP_wide\n\n# A tibble: 4 × 5\n  id    sex   SBP_v1 SBP_v2 SBP_v3\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 a     F        130    110    112\n2 b     M        120    116    122\n3 c     M        130    136    138\n4 d     F        119    106    118\n\n\n\nWhat do you think the data in the table are measures of?\nHow can we tell the data are wide?"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#wide-to-long-pivot_longer",
    "href": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#wide-to-long-pivot_longer",
    "title": "Day 3 Part 3 code: Data wrangling",
    "section": "Wide to long: pivot_longer()",
    "text": "Wide to long: pivot_longer()\n\nSBP_wide\n\n# A tibble: 4 × 5\n  id    sex   SBP_v1 SBP_v2 SBP_v3\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 a     F        130    110    112\n2 b     M        120    116    122\n3 c     M        130    136    138\n4 d     F        119    106    118\n\n\nFor pivot_longer we need to specify: - cols: which columns to make long - names_to: the name of the variable that will be created from the data stored in the column names - values_to: the name of the variable that will be created from the data stored in the cell values\n\nSBP_long &lt;- SBP_wide %&gt;% \n  pivot_longer(\n    cols=c(SBP_v1,SBP_v2,SBP_v3), \n    names_to = \"visit\", \n    values_to = \"SBP\")\nSBP_long\n\n# A tibble: 12 × 4\n   id    sex   visit    SBP\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1 a     F     SBP_v1   130\n 2 a     F     SBP_v2   110\n 3 a     F     SBP_v3   112\n 4 b     M     SBP_v1   120\n 5 b     M     SBP_v2   116\n 6 b     M     SBP_v3   122\n 7 c     M     SBP_v1   130\n 8 c     M     SBP_v2   136\n 9 c     M     SBP_v3   138\n10 d     F     SBP_v1   119\n11 d     F     SBP_v2   106\n12 d     F     SBP_v3   118"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#long-to-wide-pivot_wider",
    "href": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#long-to-wide-pivot_wider",
    "title": "Day 3 Part 3 code: Data wrangling",
    "section": "Long to wide: pivot_wider()",
    "text": "Long to wide: pivot_wider()\n\nSBP_long\n\n# A tibble: 12 × 4\n   id    sex   visit    SBP\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1 a     F     SBP_v1   130\n 2 a     F     SBP_v2   110\n 3 a     F     SBP_v3   112\n 4 b     M     SBP_v1   120\n 5 b     M     SBP_v2   116\n 6 b     M     SBP_v3   122\n 7 c     M     SBP_v1   130\n 8 c     M     SBP_v2   136\n 9 c     M     SBP_v3   138\n10 d     F     SBP_v1   119\n11 d     F     SBP_v2   106\n12 d     F     SBP_v3   118\n\n\nFor pivot_wider we need to specify: - names_from: which column contains the names for the new columns - values_from: which column contains the values that will fill in the cell values\n\nSBP_wide2 &lt;- SBP_long %&gt;% \n  pivot_wider(names_from = \"visit\", \n              values_from = \"SBP\")\nSBP_wide2\n\n# A tibble: 4 × 5\n  id    sex   SBP_v1 SBP_v2 SBP_v3\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 a     F        130    110    112\n2 b     M        120    116    122\n3 c     M        130    136    138\n4 d     F        119    106    118\n\n\n\nClean up visit column in the long data (1/3)\n\nSBP_long\n\n# A tibble: 12 × 4\n   id    sex   visit    SBP\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1 a     F     SBP_v1   130\n 2 a     F     SBP_v2   110\n 3 a     F     SBP_v3   112\n 4 b     M     SBP_v1   120\n 5 b     M     SBP_v2   116\n 6 b     M     SBP_v3   122\n 7 c     M     SBP_v1   130\n 8 c     M     SBP_v2   136\n 9 c     M     SBP_v3   138\n10 d     F     SBP_v1   119\n11 d     F     SBP_v2   106\n12 d     F     SBP_v3   118\n\n\nGoal: remove the string “SBP_” from the visit variable’s values.\nMethod #1: tidy the visit column after making the data long\nMethod #2: tidy the visit column while making the data long\n\n\nClean up visit column in the long data (2/3)\nMethod #1: tidy the visit column after making the data long\n\nSBP_long2 &lt;- SBP_long %&gt;% \n  mutate(\n    visit = str_replace(\n      visit,\n      pattern = \"SBP_\",\n      replacement = \"\")\n    ) \nSBP_long2\n\n# A tibble: 12 × 4\n   id    sex   visit   SBP\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n 1 a     F     v1      130\n 2 a     F     v2      110\n 3 a     F     v3      112\n 4 b     M     v1      120\n 5 b     M     v2      116\n 6 b     M     v3      122\n 7 c     M     v1      130\n 8 c     M     v2      136\n 9 c     M     v3      138\n10 d     F     v1      119\n11 d     F     v2      106\n12 d     F     v3      118\n\n\n\n\nNote that mutate is replacing the existing visit column with new values\nIf I wanted to keep the original visit column instead of overwriting it, I would call the new column something else, such as visit_clean\nWithin str_replace(), double quotes need to be used around the characters specifying 1) the string of text to replace (\"SBP_\") and 2) what to replace the text with, where \"\" is used for no text\nCould instead use str_remove(): mutate(visit = str_remove(visit,\"SBP_\"))\n\n\n\nClean up visit column in the long data (3/3)\nMethod #2: tidy the visit column while making the data long\n\nSBP_long3 &lt;- SBP_wide %&gt;% \n pivot_longer(cols = c(SBP_v1, SBP_v2,SBP_v3), \n              names_to = \"visit\", \n              names_prefix = \"SBP_\", #&lt;&lt;\n              values_to = \"SBP\")\nSBP_long3\n\n# A tibble: 12 × 4\n   id    sex   visit   SBP\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n 1 a     F     v1      130\n 2 a     F     v2      110\n 3 a     F     v3      112\n 4 b     M     v1      120\n 5 b     M     v2      116\n 6 b     M     v3      122\n 7 c     M     v1      130\n 8 c     M     v2      136\n 9 c     M     v3      138\n10 d     F     v1      119\n11 d     F     v2      106\n12 d     F     v3      118\n\n\nRemarks:\n\nNote the new parameter names_prefix specifying what the prefix is that needs to be stripped.\nMore complex pivot_longer() examples are shown at https://tidyr.tidyverse.org/articles/pivot.html\n(the end of the url is articles/pivot.html)"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#specifying-cols-in-pivot_longer",
    "href": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#specifying-cols-in-pivot_longer",
    "title": "Day 3 Part 3 code: Data wrangling",
    "section": "Specifying cols in pivot_longer",
    "text": "Specifying cols in pivot_longer\nIn the example creating SBP_long, the columns to make the tibble longer by were explicitly listed using cols=c(SBP_v1,SBP_v2,SBP_v3).\n\nSBP_long &lt;- SBP_wide %&gt;% \n pivot_longer(cols = c(SBP_v1, SBP_v2,SBP_v3), #&lt;&lt;\n              names_to = \"visit\", \n              values_to = \"SBP\")\nSBP_long\n\n# A tibble: 12 × 4\n   id    sex   visit    SBP\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1 a     F     SBP_v1   130\n 2 a     F     SBP_v2   110\n 3 a     F     SBP_v3   112\n 4 b     M     SBP_v1   120\n 5 b     M     SBP_v2   116\n 6 b     M     SBP_v3   122\n 7 c     M     SBP_v1   130\n 8 c     M     SBP_v2   136\n 9 c     M     SBP_v3   138\n10 d     F     SBP_v1   119\n11 d     F     SBP_v2   106\n12 d     F     SBP_v3   118\n\n\nHowever, we can specify the columns in many different ways, just like with select():\n\ncols = c(SBP_v1:SBP_v3)\ncols = c(-id, -sex)\ncols = starts_wth(\"SBP\")\ncols = contains(\"SBP\")"
  },
  {
    "objectID": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#notes-on-pivot_-commands",
    "href": "slides_code/Day03_bsta511_code_part3_data_wrangling.html#notes-on-pivot_-commands",
    "title": "Day 3 Part 3 code: Data wrangling",
    "section": "Notes on pivot_*() commands",
    "text": "Notes on pivot_*() commands\n\npivot_longer() and pivot_wider() are relatively new commands\npreviously we used gather() and spread(),\n\nwhich have different function parameters and are less intuitive to use\n\nif you search for help making data longer or wider,\n\nyou might still see references for gather() and spread()\n\nsee my workshop slides for gather() and spread() usage\nsee https://tidyr.tidyverse.org/articles/pivot.html for more pivot examples"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Week\nDates\nDay\nTopics\nChapter sections\n\n\n\n\n1\nM 9/30\n\nNo class:  OHSU New Student Orientation\n\n\n\n\nW 10/2\n1\nIntroduction to R & RStudio\n\n\n\n2\nM 10/7\n2\nIntro to Data, Data collection\n1.1-1.3\n\n\n\nW 10/9\n3\nSummarizing numerical data,  Probability\n1.4, 2.1-2.2.4\n\n\n\nSa 10/12\n\nHW 1 Due (Days 1 - 3)\n\n\n\n3\nM 10/14\n4\nBayes’ Theorem, R Packages\n2.2.5, extra\n\n\n\nW 10/16\n5\nData visualization,  exploratory data analysis (EDA),  and summarizing categorical data\n1.5-1.7, extra\n\n\n\nSa 10/19\n\nHW 2 Due (Days 4 - 5)\n\n\n\n4\nM 10/21\n6\nRandom variables,  Binomial distribution\n3.1-3.2\n\n\n\nW 10/23\n7\nNormal and Poisson distributions\n3.3-3.4\n\n\n\nSa 10/26\n\nHW 3 Due (Days 6 - 7)\n\n\n\n5\nM 10/28\n8\nVariability in estimates\n4.1\n\n\n\nW 10/30\n\nExam 1: Days 1-7\nCh 1, 2, 3.1-3.4\n\n\n6\nM 11/4\n9\nConfidence Intervals\n4.2\n\n\n\nW 11/6\n10\nHypothesis Testing:  Single-sample with t-distribution  & Two-sample paired data\n4.3, 5.1, 5.2\n\n\n\nSa 11/9\n\nHW 4 Due (Days 8 - 10)\n\n\n\n7\nM 11/11\n11\nHypothesis Testing:  Two-sample independent data;  Power\n5.3-5.4, extra\n\n\n\nW 11/13\n12\nInference for a single proportion  or difference in proportions\n8.1-8.2\n\n\n\nSa 11/16\n\nHW 5 Due (Days 11 - 12)\n\n\n\n8\nM 11/18\n13\nChi-squared tests,  Fishers exact test\n8.3-8.4\n\n\n\nW 11/20\n14\nComparing Means with ANOVA.\n5.5, extra\n\n\n\nSa 11/23\n\nHW 6 Due (Days 13 - 14)\n\n\n\n9\nM 11/25\n\nExam 2: Days 8-13\nCh 4, 5.1-5.4;  extra, 8.1-8.4\n\n\n\nW 11/27\n15\nSimple Linear Regression\n6.1-6.2\n\n\n10\nM 12/2\n16\nSimple Linear Regression\n6.3-6.4\n\n\n\nW 12/4\n17\nNon-parametric tests  (sign, signed-rank, rank-sum,  Kruskal-Wallis)\nExtra\n\n\n\nSa 12/7\n\nHW 7 Due (Days 15 - 17)\n\n\n\n11\nM 12/9\n\nReview - catch up\n\n\n\n\nW 12/11\n\nExam 3 due (take home):  Cumulative with a focus on Days 14-17\nCh 5.5, 6,  non-parametric"
  },
  {
    "objectID": "slides_code/Probability_distributions_in_R_bsta511.html",
    "href": "slides_code/Probability_distributions_in_R_bsta511.html",
    "title": "Probability distributions in R for BSTA 511/611",
    "section": "",
    "text": "I recommend installing the tinytex R package to help with rendering LaTeX equations if you do not have a LaTeX program installed on your computer."
  },
  {
    "objectID": "slides_code/Probability_distributions_in_R_bsta511.html#probability-calculations-pxx",
    "href": "slides_code/Probability_distributions_in_R_bsta511.html#probability-calculations-pxx",
    "title": "Probability distributions in R for BSTA 511/611",
    "section": "1.1 Probability calculations: \\(P(X=x)\\)",
    "text": "1.1 Probability calculations: \\(P(X=x)\\)\n\n1.1.1 Using formula\nCalculate \\(P(X=4)\\) for a Bin(n=10, p=0.25) distribution.\nIn general, for a binomial random variable, \\[P(X=x) = \\binom{n}{x}p^x q^{n-x}\\] Thus \\(P(X=4)\\) for a Bin(n=10, p=0.25) random variable, \\[P(X=4) = \\binom{10}{4}0.25^4 0.75^{10-4} = \\frac{10!}{4!(10-4)!}0.25^4 0.75^{10-4}\\] Calculate “directly” in R:\n\nchoose(10,4) * 0.25^4 * 0.75^6\n\n[1] 0.145998\n\n# using factorials instead of the choose function:\nfactorial(10)/(factorial(4)*factorial(6)) * 0.25^4 * 0.75^6\n\n[1] 0.145998\n\n\n\n\n1.1.2 Using dbinom()\nCalculate \\(P(X=4)\\) for a Bin(n=10, p=0.25) distribution.\n\n# P(X = 4) for Bin(n=10, p=0.25) random variable\ndbinom(x = 4, size = 10, prob = 0.25) \n\n[1] 0.145998\n\n\nCalculate \\(P(X=x)\\) for all possible values \\(x\\) for a Bin(n=10, p=0.25) distribution.\n\n# Entire probability distribution:\n# P(X = x) for all x=0,1,2,...,10 for Bin(n=10, p=0.25) random variable\ndbinom(x = 0:10, size = 10, prob = 0.25) \n\n [1] 5.631351e-02 1.877117e-01 2.815676e-01 2.502823e-01 1.459980e-01\n [6] 5.839920e-02 1.622200e-02 3.089905e-03 3.862381e-04 2.861023e-05\n[11] 9.536743e-07\n\n\nIn the next section we visualize these probability distribution values."
  },
  {
    "objectID": "slides_code/Probability_distributions_in_R_bsta511.html#histogram-of-probability-distribution",
    "href": "slides_code/Probability_distributions_in_R_bsta511.html#histogram-of-probability-distribution",
    "title": "Probability distributions in R for BSTA 511/611",
    "section": "1.2 Histogram of probability distribution",
    "text": "1.2 Histogram of probability distribution\n\nFirst, create a data frame of the distribution’s possible x values and their respective probabilities:\n\n\n# Bin(n=10, p=0.25) random variable\n\nbinom_probs &lt;- data.frame(x_values = 0:10, \n                          px = dbinom(x = 0:10, size = 10, prob = 0.25)) \nbinom_probs\n\n   x_values           px\n1         0 5.631351e-02\n2         1 1.877117e-01\n3         2 2.815676e-01\n4         3 2.502823e-01\n5         4 1.459980e-01\n6         5 5.839920e-02\n7         6 1.622200e-02\n8         7 3.089905e-03\n9         8 3.862381e-04\n10        9 2.861023e-05\n11       10 9.536743e-07\n\n\n\n1.2.1 Histogram\nThe histogram below is actually created as a bar plot in ggplot:\n\nggplot(binom_probs, \n       aes(x = x_values, y = px))  + \n  geom_col(fill =\"cornflowerblue\") +\n  labs(x = \"x, number of successes\",\n       y = \"P(X=x)\") +\n  scale_x_continuous(breaks=0:10) +\n  labs(title = \"Bin(n = 10, p = 0.25) distribution\")"
  },
  {
    "objectID": "slides_code/Probability_distributions_in_R_bsta511.html#probability-calculations-px-leq-x",
    "href": "slides_code/Probability_distributions_in_R_bsta511.html#probability-calculations-px-leq-x",
    "title": "Probability distributions in R for BSTA 511/611",
    "section": "1.3 Probability calculations: \\(P(X \\leq x)\\)",
    "text": "1.3 Probability calculations: \\(P(X \\leq x)\\)\n\\[P(X\\leq k) = \\sum_{x=0}^{k}\\binom{n}{x}p^xq^{n-x}\\]\n\n1.3.1 Using formula\nCalculate \\(P(X \\leq 3)\\) for a Bin(n=10, p=0.25) distribution:\n\\[P(X\\leq 3) = \\sum_{x=0}^{3}\\binom{10}{x}0.2.5^x 0.75^{10-x}\\] Calculate “directly” in R:\n\n# vector of x values whose probabilities need to be added\nx &lt;- 0:3  \n# vector of respective binomial prob's of x values\n(binom_prob_0_3 &lt;- choose(10,x) * 0.25^x * 0.75^(10-x))\n\n[1] 0.05631351 0.18771172 0.28156757 0.25028229\n\n# add up the probabilities\nsum(binom_prob_0_3)\n\n[1] 0.7758751\n\n\n\n\n1.3.2 Using pbinom()\n\n\\(P(X\\leq k)\\) = pbinom(q = k, size = n, prob = p, lower.tail = TRUE)\n\nCalculate \\(P(X \\leq 3)\\) for a Bin(n=10, p=0.25) distribution.\n\n# P(X &lt;= 3) for Bin(n=10, p=0.25) random variable\npbinom(q = 3, size = 10, prob = 0.25, lower.tail = TRUE)\n\n[1] 0.7758751\n\n# Note: setting TRUE for the lower.tail option is the default value\n# This means that if we do not specify this option, \n# it will give the same result as above:\npbinom(q = 3, size = 10, prob = 0.25) \n\n[1] 0.7758751"
  },
  {
    "objectID": "slides_code/Probability_distributions_in_R_bsta511.html#probability-calculations-px-geq-x",
    "href": "slides_code/Probability_distributions_in_R_bsta511.html#probability-calculations-px-geq-x",
    "title": "Probability distributions in R for BSTA 511/611",
    "section": "1.4 Probability calculations: \\(P(X \\geq x)\\)",
    "text": "1.4 Probability calculations: \\(P(X \\geq x)\\)\n\\[P(X\\geq k) = \\sum_{x=k}^{n}\\binom{n}{x}p^xq^{n-x}\\]\n\n1.4.1 Using formula\nCalculate \\(P(X \\geq 5)\\) for a Bin(n=10, p=0.25) distribution:\n\\[P(X \\geq 5) = \\sum_{x=5}^{10}\\binom{10}{x}0.2.5^x 0.75^{10-x}\\] Calculate “directly” in R:\n\n# vector of x values whose probabilities need to be added\nx &lt;- 5:10  \n# vector of respective binomial prob's of x values\n(binom_prob_5_10 &lt;- choose(10,x) * 0.25^x * 0.75^(10-x))\n\n[1] 5.839920e-02 1.622200e-02 3.089905e-03 3.862381e-04 2.861023e-05\n[6] 9.536743e-07\n\n# add up the probabilities\nsum(binom_prob_5_10)\n\n[1] 0.07812691\n\n\n\n\n1.4.2 Using pbinom() with lower.tail = TRUE\nCalculate \\(P(X \\geq 5)\\) for a Bin(n=10, p=0.25) distribution.\n\\[P(X \\geq 5) = 1 - P(X \\leq 4) =1 - \\sum_{x=0}^{4}\\binom{10}{x}0.2.5^x 0.75^{10-x}\\]\n\n# P(X &gt;= 5) for Bin(n=10, p=0.25) random variable\n# P(X &gt;= 5) = 1 - P(X &lt;= 4)\n1 - pbinom(q = 4, size = 10, prob = 0.25, lower.tail = TRUE)\n\n[1] 0.07812691\n\n\n\n\n1.4.3 Using pbinom() with lower.tail = FALSE\n\n\\(P(X \\geq k) = P(X &gt; k)\\) = pbinom(q = k, size = n, prob = p, lower.tail = FALSE)\n\nCalculate \\(P(X \\geq 5)\\) for a Bin(n=10, p=0.25) distribution.\n\\[P(X \\geq 5) = P(X &gt; 4)\\]\n\n# P(X &gt;= 5) for Bin(n=10, p=0.25) random variable\n# P(X &gt;= 5) = P(X &gt; 4)\npbinom(q = 4, size = 10, prob = 0.25, lower.tail = FALSE)\n\n[1] 0.07812691"
  },
  {
    "objectID": "slides_code/Probability_distributions_in_R_bsta511.html#probability-calculations-px-x",
    "href": "slides_code/Probability_distributions_in_R_bsta511.html#probability-calculations-px-x",
    "title": "Probability distributions in R for BSTA 511/611",
    "section": "2.1 Probability calculations: \\(P(X < x)\\)",
    "text": "2.1 Probability calculations: \\(P(X &lt; x)\\)\n\\[\nP(X &lt; x) = P\\Big(Z &lt; \\frac{x-\\mu}{\\sigma}\\Big)\n\\] Calculate \\(P(X &lt; 10)\\), for \\(X \\sim N(\\mu = 8, \\sigma = 2)\\).\n\\[\nP(X &lt; 10) = P\\Big(Z &lt; \\frac{10-8}{2}\\Big) = P\\Big(Z &lt; 1\\Big)\n\\]\n\n# P(X &lt; 10) without using z-scores\npnorm(q = 10, mean = 8, sd = 2, lower.tail = TRUE)\n\n[1] 0.8413447\n\n# P(X &lt; 10) = P(Z &lt; 1) using z-scores\npnorm(q = 1, mean = 0, sd = 1, lower.tail = TRUE)\n\n[1] 0.8413447\n\n# this works too for a standard normal distribution:\npnorm(1)\n\n[1] 0.8413447"
  },
  {
    "objectID": "slides_code/Probability_distributions_in_R_bsta511.html#probability-calculations-px-x-1",
    "href": "slides_code/Probability_distributions_in_R_bsta511.html#probability-calculations-px-x-1",
    "title": "Probability distributions in R for BSTA 511/611",
    "section": "2.2 Probability calculations: \\(P(X > x)\\)",
    "text": "2.2 Probability calculations: \\(P(X &gt; x)\\)\n\\[\nP(X &gt; x) = P\\Big(Z &gt; \\frac{x-\\mu}{\\sigma}\\Big) = 1-P\\Big(Z \\leq  \\frac{x-\\mu}{\\sigma}\\Big)\n\\] Calculate \\(P(X &gt; 10)\\), for \\(X \\sim N(\\mu = 8, \\sigma = 2)\\).\n\\[\nP(X &gt; 10) = P\\Big(Z &gt; \\frac{10-8}{2}\\Big) = 1 - P\\Big(Z \\leq 1\\Big)\n\\]\n\n# P(X &gt; 10) without using z-scores\n1 - pnorm(q = 10, mean = 8, sd = 2, lower.tail = TRUE)\n\n[1] 0.1586553\n\n# using lower.tail = FALSE:\npnorm(q = 10, mean = 8, sd = 2, lower.tail = FALSE)\n\n[1] 0.1586553\n\n# P(X &gt; 10) = P(Z &gt; 1) using z-scores\n1 - pnorm(q = 1, mean = 0, sd = 1, lower.tail = TRUE)\n\n[1] 0.1586553\n\n# using lower.tail = FALSE:\npnorm(q = 1, mean = 0, sd = 1, lower.tail = FALSE)\n\n[1] 0.1586553\n\n# this works too for a standard normal distribution:\npnorm(1, lower.tail = FALSE)\n\n[1] 0.1586553"
  },
  {
    "objectID": "slides_code/Probability_distributions_in_R_bsta511.html#normal-curve-figure-with-region-shaded-in",
    "href": "slides_code/Probability_distributions_in_R_bsta511.html#normal-curve-figure-with-region-shaded-in",
    "title": "Probability distributions in R for BSTA 511/611",
    "section": "2.3 Normal curve figure with region shaded in",
    "text": "2.3 Normal curve figure with region shaded in\n\nR code to create a figure of the normal distribution curve with the probability region of interest shaded in.\n\nShade in the region representing \\(P(X&gt;10)\\), for \\(X \\sim N(\\mu = 8, \\sigma = 2)\\).\n\n# This code shades in the probability P(X &gt; 10) \n# for X ~ N(mu = 8, sigma = 2)\n# Note that I set the upper and lower bounds of the normal curve to be from mu - 4*sigma to mu + 4*sigma since these bounds include almost 100% of the area under the normal curve\n\nmu &lt;- 8   # specify the mean of the normal distribution\nstd &lt;- 2  # specify the standard deviation of the normal distribution\n\n# specify upper and lower bounds of shaded region below\nggplot(data.frame(x = c(mu-4*std, mu+4*std)), aes(x = x)) + \n  stat_function(fun = dnorm, \n                args = list(mean = mu, sd = std)) + \n  stat_function(fun = dnorm, \n                args = list(mean = mu, sd = std), \n          # specify the upper and lower bounds of the shaded region:\n                xlim = c(10, mu+4*std),             \n                geom = \"area\", fill = \"darkblue\") +\n  # the breaks values below might need to be adjusted \n  # if there are too many values showing on the x-axis\n  scale_x_continuous(breaks=(mu-4*std):(mu+4*std)) +\n  labs(y = \"\") +\n  labs(title = \"P(X &gt;10) for a N(mu=8, sigma=2) distribution\")"
  },
  {
    "objectID": "slides_code/Probability_distributions_in_R_bsta511.html#probability-calculations-pxx-1",
    "href": "slides_code/Probability_distributions_in_R_bsta511.html#probability-calculations-pxx-1",
    "title": "Probability distributions in R for BSTA 511/611",
    "section": "3.1 Probability calculations: \\(P(X=x)\\)",
    "text": "3.1 Probability calculations: \\(P(X=x)\\)\n\n3.1.1 Using formula\nCalculate \\(P(X=3)\\) for a \\(Pois(\\lambda = 5)\\) distribution.\nIn general, for a Poisson random variable, \\[P(X=x) = \\frac{e^{-\\lambda}\\lambda^x}{x!}\\] Thus the \\(P(X=3)\\) for a \\(Pois(\\lambda = 5)\\) random variable is \\[P(X=3) = \\frac{e^{-5}5^3}{3!}\\] Calculate “directly” in R:\n\nexp(-5)*(5^3)/factorial(3)\n\n[1] 0.1403739\n\n\n\n\n3.1.2 Using dpois()\nCalculate \\(P(X=3)\\) for a \\(Pois(\\lambda = 5)\\) distribution.\n\n# P(X = 3) for Pois(lambda = 5) random variable\ndpois(x = 3, lambda = 5) \n\n[1] 0.1403739\n\n\n\nCalculate \\(P(X=x)\\) for many possible values \\(x\\) for a \\(Pois(\\lambda = 5)\\) distribution.\nThe possible values of \\(x\\) for a Poisson distribution are \\(x=0, 1, 2, \\ldots\\), i.e., \\(x\\) can be infinitely large.\nBelow we look at probabilities just for \\(x=0, 1, 2, \\ldots, 20\\).\n\n\n# P(X = x) for x=0,1,2,...,100 for Pois(\\lambda = 5) random variable\ndpois(x = 0:20, lambda = 5) \n\n [1] 6.737947e-03 3.368973e-02 8.422434e-02 1.403739e-01 1.754674e-01\n [6] 1.754674e-01 1.462228e-01 1.044449e-01 6.527804e-02 3.626558e-02\n[11] 1.813279e-02 8.242177e-03 3.434240e-03 1.320862e-03 4.717363e-04\n[16] 1.572454e-04 4.913920e-05 1.445271e-05 4.014640e-06 1.056484e-06\n[21] 2.641211e-07\n\n# although we didn't calculate the probabilities for all possible values of x, \n# below we see that the probabilities for the first 21 values almost add up to 1\nsum(dpois(x = 0:20, lambda = 5) )\n\n[1] 0.9999999\n\n\nIn the next section we visualize these probability distribution values."
  },
  {
    "objectID": "slides_code/Probability_distributions_in_R_bsta511.html#histogram-of-probability-distribution-1",
    "href": "slides_code/Probability_distributions_in_R_bsta511.html#histogram-of-probability-distribution-1",
    "title": "Probability distributions in R for BSTA 511/611",
    "section": "3.2 Histogram of probability distribution",
    "text": "3.2 Histogram of probability distribution\n\nFirst, create a data frame of the distribution’s possible x values and their respective probabilities:\n\n\n# Pois(lambda = 5) random variable\n\nPoisson_probs &lt;- data.frame(x_values = 0:20, \n                          px = dpois(x = 0:20, lambda = 5)) \nPoisson_probs\n\n   x_values           px\n1         0 6.737947e-03\n2         1 3.368973e-02\n3         2 8.422434e-02\n4         3 1.403739e-01\n5         4 1.754674e-01\n6         5 1.754674e-01\n7         6 1.462228e-01\n8         7 1.044449e-01\n9         8 6.527804e-02\n10        9 3.626558e-02\n11       10 1.813279e-02\n12       11 8.242177e-03\n13       12 3.434240e-03\n14       13 1.320862e-03\n15       14 4.717363e-04\n16       15 1.572454e-04\n17       16 4.913920e-05\n18       17 1.445271e-05\n19       18 4.014640e-06\n20       19 1.056484e-06\n21       20 2.641211e-07\n\n\n\n3.2.1 Histogram\nThe histogram below is actually created as a bar plot in ggplot:\n\nggplot(Poisson_probs, \n       aes(x = x_values, y = px))  + \n  geom_col(fill =\"cornflowerblue\") +\n  labs(x = \"x, number of successes\",\n       y = \"P(X=x)\") +\n  scale_x_continuous(breaks=0:20) +  # chose 20 since prob's are already tiny for this distribution\n  labs(title = \"Pois(lambda = 5) distribution\")"
  },
  {
    "objectID": "slides_code/Probability_distributions_in_R_bsta511.html#probability-calculations-px-leq-x-1",
    "href": "slides_code/Probability_distributions_in_R_bsta511.html#probability-calculations-px-leq-x-1",
    "title": "Probability distributions in R for BSTA 511/611",
    "section": "3.3 Probability calculations: \\(P(X \\leq x)\\)",
    "text": "3.3 Probability calculations: \\(P(X \\leq x)\\)\n\\[P(X\\leq k) = \\sum_{x=0}^{k}\\frac{e^{-\\lambda}\\lambda^x}{x!}\\]\n\n3.3.1 Using formula\nCalculate \\(P(X \\leq 12)\\) for a \\(Pois(\\lambda = 5)\\) distribution:\n\\[P(X\\leq 12) = \\sum_{x=0}^{12} \\frac{e^{-5}5^x}{x!}\\] Calculate “directly” in R:\n\n# vector of x values whose probabilities need to be added\nx &lt;- 0:12  \n# vector of respective Poisson prob's of x values\n(Poisson_prob_0_12 &lt;- exp(-5)*(5^x)/factorial(x))\n\n [1] 0.006737947 0.033689735 0.084224337 0.140373896 0.175467370 0.175467370\n [7] 0.146222808 0.104444863 0.065278039 0.036265577 0.018132789 0.008242177\n[13] 0.003434240\n\n# add up the probabilities\nsum(Poisson_prob_0_12)\n\n[1] 0.9979811\n\n\n\n\n3.3.2 Using ppois()\n\n\\(P(X\\leq k)\\) = ppois(q = k, lambda = , lower.tail = TRUE)\n\nCalculate \\(P(X \\leq 12)\\) for a \\(Pois(\\lambda = 5)\\) distribution.\n\n# P(X &lt;= 3) for Pois(lambda = 5) random variable\nppois(q = 12, lambda = 5, lower.tail = TRUE)\n\n[1] 0.9979811\n\n# Note: setting TRUE for the lower.tail option is the default value\n# This means that if we do not specify this option, \n# it will give the same result as above:\nppois(q = 12, lambda = 5) \n\n[1] 0.9979811"
  },
  {
    "objectID": "slides_code/Probability_distributions_in_R_bsta511.html#probability-calculations-px-geq-x-1",
    "href": "slides_code/Probability_distributions_in_R_bsta511.html#probability-calculations-px-geq-x-1",
    "title": "Probability distributions in R for BSTA 511/611",
    "section": "3.4 Probability calculations: \\(P(X \\geq x)\\)",
    "text": "3.4 Probability calculations: \\(P(X \\geq x)\\)\n\\[P(X\\geq k) = \\sum_{x=k}^{\\infty} \\frac{e^{-\\lambda}\\lambda^x}{x!}\\]\n\n3.4.1 Using formula\nCalculate \\(P(X \\geq 13)\\) for a \\(Pois(\\lambda = 5)\\) distribution:\n\\[P(X \\geq 13) = \\sum_{x=13}^{\\infty} \\frac{e^{-5}5^x}{x!}\\] Calculate “directly” in R:\n\n# vector of x values whose probabilities need to be added\nx &lt;- 13:100 # choose a big number; big enough so that the probabilities for the high x values below are tiny\n# vector of respective Poisson prob's of x values\n(Poisson_prob_13_100 &lt;- exp(-5)*(5^x)/factorial(x))\n\n [1] 1.320862e-03 4.717363e-04 1.572454e-04 4.913920e-05 1.445271e-05\n [6] 4.014640e-06 1.056484e-06 2.641211e-07 6.288597e-08 1.429227e-08\n[11] 3.107014e-09 6.472947e-10 1.294589e-10 2.489595e-11 4.610361e-12\n[16] 8.232787e-13 1.419446e-13 2.365743e-14 3.815715e-15 5.962055e-16\n[21] 9.033417e-17 1.328444e-17 1.897777e-18 2.635801e-19 3.561893e-20\n[26] 4.686701e-21 6.008592e-22 7.510739e-23 9.159438e-24 1.090409e-24\n[31] 1.267918e-25 1.440816e-26 1.600906e-27 1.740116e-28 1.851187e-29\n[36] 1.928320e-30 1.967673e-31 1.967673e-32 1.929091e-33 1.854895e-34\n[41] 1.749901e-35 1.620279e-36 1.472981e-37 1.315162e-38 1.153650e-39\n[46] 9.945263e-41 8.428189e-42 7.023491e-43 5.756959e-44 4.642709e-45\n[51] 3.684690e-46 2.878664e-47 2.214357e-48 1.677543e-49 1.251898e-50\n[56] 9.205131e-52 6.670385e-53 4.764561e-54 3.355324e-55 2.330086e-56\n[61] 1.595950e-57 1.078344e-58 7.188962e-60 4.729580e-61 3.071156e-62\n[66] 1.968690e-63 1.246006e-64 7.787539e-66 4.807123e-67 2.931172e-68\n[71] 1.765766e-69 1.051051e-70 6.182656e-72 3.594567e-73 2.065843e-74\n[76] 1.173775e-75 6.594239e-77 3.663466e-78 2.012894e-79 1.093964e-80\n[81] 5.881526e-82 3.128471e-83 1.646564e-84 8.575854e-86 4.420543e-87\n[86] 2.255379e-88 1.139080e-89 5.695402e-91\n\n# add up the probabilities\nsum(Poisson_prob_13_100)\n\n[1] 0.002018852\n\n\n\n\n3.4.2 Using ppois() with lower.tail = TRUE\nCalculate \\(P(X \\geq 13)\\) for a \\(Pois(\\lambda = 5)\\) distribution.\n\\[P(X \\geq 13) = 1 - P(X \\leq 12) =1 - \\sum_{x=0}^{12} \\frac{e^{-5}5^x}{x!}\\]\n\n# P(X &gt;= 13) for Pois(lambda = 5) random variable\n# P(X &gt;= 13) = 1 - P(X &lt;= 12)\n1 - ppois(q = 12, lambda = 5, lower.tail = TRUE)\n\n[1] 0.002018852\n\n\n\n\n3.4.3 Using ppois() with lower.tail = FALSE\n\n\\(P(X \\geq k) = P(X &gt; k)\\) = ppois(q = k, size = n, prob = p, lower.tail = FALSE)\n\nCalculate \\(P(X \\geq 13)\\) for a \\(Pois(\\lambda = 5)\\) distribution.\n\\[P(X \\geq 13) = P(X &gt; 12)\\]\n\n# P(X &gt;= 13) for Pois(lambda = 5) random variable\n# P(X &gt;= 13) = P(X &gt; 12)\nppois(q = 12, lambda = 5, lower.tail = FALSE)\n\n[1] 0.002018852"
  },
  {
    "objectID": "homework/HW_3_F24_bsta511.html",
    "href": "homework/HW_3_F24_bsta511.html",
    "title": "HW 3: BSTA 511/611 F24",
    "section": "",
    "text": "Due 10/26/24\nDownload the .qmd file for this assignment from https://github.com/niederhausen/BSTA_511_F24/blob/main/homework/HW_3_F24_bsta511.qmd"
  },
  {
    "objectID": "homework/HW_3_F24_bsta511.html#graded-exercises",
    "href": "homework/HW_3_F24_bsta511.html#graded-exercises",
    "title": "HW 3: BSTA 511/611 F24",
    "section": "Graded exercises",
    "text": "Graded exercises\nThe exercises listed below will be graded for this assignment. You are strongly encouraged to complete the entire assignment. You will receive feedback on exercises you turn in that are not being graded.\n\nBook exercises\n\n3.4, 3.8, 3.22, 3.32, 3.40\n\nNon-book exercise\n\nNB 1 - I recommend completing this problem after completing book questions 3.4-3.6."
  },
  {
    "objectID": "homework/HW_3_F24_bsta511.html#directions",
    "href": "homework/HW_3_F24_bsta511.html#directions",
    "title": "HW 3: BSTA 511/611 F24",
    "section": "Directions",
    "text": "Directions\n\n\n\n\n\n\nImportant\n\n\n\n\nMake sure to check out the calculating probabilities in R code file: qmd, html\nAll exercises in this assignment may be completed by hand (such as on paper or using a tablet) not using Quarto.\nIf you complete the assignment by hand (not using Quarto):\n\nUpload ONE pdf on Sakai with your work instead of qmd & html files.\nSome problems involve R code to calculate a probability, but the code is brief and you can write out the code and the answer by hand.\nIf you are completing the homework on paper, you can use a scanning app, such as Adobe Scan, to create one pdf of your assignment.\n\nIf you complete the assignment using Quarto:\n\nI highly recommend using LaTeX to format the equations.\n\nFor instructions on creating equations in the Visual editor, check out https://quarto.org/docs/get-started/authoring/rstudio.html#equations.\nYou can see examples of LaTeX formatting in the calculating probabilities in R code file: qmd, html\nAlso check out examples of LaTeX formatting for statistics created by recent biostats alum Ariel Weingarten.\nIf you have difficulty rendering the LaTeX equations, I recommend installing and running the R package tinytex. See this website for instructions.\n\n\n\n\n\n\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file (or just your pdf if completing the assignment by hand).\n\nUse the assignment .qmd file linked to above as a template for your own assignment.\n\nPlease always use the following naming convention for submitting your files:\n\nLastname_Firstname_HWx.qmd, such as Niederhausen_Meike_HW2.qmd\nLastname_Firstname_HWx.html, such as Niederhausen_Meike_HW2.html\n\nFor each question, make sure to show all of your work.\n\nThis includes all code and resulting output in the html file to support your answers for exercises requiring work done in R (including any arithmetic calculations).\nFor non-calculation questions, this includes an explanation of your answer (why did you choose your answer?).\n\nFor each question, include a sentence summarizing the answer for that question in the context of the research question.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your Qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_3_F24_bsta511.html#baggage-fees",
    "href": "homework/HW_3_F24_bsta511.html#baggage-fees",
    "title": "HW 3: BSTA 511/611 F24",
    "section": "3.4 Baggage fees",
    "text": "3.4 Baggage fees"
  },
  {
    "objectID": "homework/HW_3_F24_bsta511.html#gull-clutch-size",
    "href": "homework/HW_3_F24_bsta511.html#gull-clutch-size",
    "title": "HW 3: BSTA 511/611 F24",
    "section": "3.5 Gull clutch size",
    "text": "3.5 Gull clutch size\nReview the solution in the back of the book for exercise 3.5.\n\n(a)\nFor part (a), the answer is correct, but there is an error in the work and the notation is sloppy. Correct the error and rewrite the solution with proper notation.\n\n\n(b)\nFor part (b), the answer is not correct, as a result of the error in the work with sloppy notation. Give a correct solution with proper notation."
  },
  {
    "objectID": "homework/HW_3_F24_bsta511.html#scooping-ice-cream",
    "href": "homework/HW_3_F24_bsta511.html#scooping-ice-cream",
    "title": "HW 3: BSTA 511/611 F24",
    "section": "3.6 Scooping ice cream",
    "text": "3.6 Scooping ice cream"
  },
  {
    "objectID": "homework/HW_3_F24_bsta511.html#chickenpox-part-i.",
    "href": "homework/HW_3_F24_bsta511.html#chickenpox-part-i.",
    "title": "HW 3: BSTA 511/611 F24",
    "section": "3.8 Chickenpox, Part I.",
    "text": "3.8 Chickenpox, Part I.\nFor #3.8, calculate the binomial probabilities two ways:\n(1) using the formula (can use R to calculate factorials or the choose function) and\n(2) using R functions for binomial distribution probabilities."
  },
  {
    "objectID": "homework/HW_3_F24_bsta511.html#chickenpox-part-ii.",
    "href": "homework/HW_3_F24_bsta511.html#chickenpox-part-ii.",
    "title": "HW 3: BSTA 511/611 F24",
    "section": "3.10 Chickenpox, Part II.",
    "text": "3.10 Chickenpox, Part II.\nFor #3.10, you can use R functions to calculate the binomial probabilities instead of directly using the formula. However, include the mathematical formulas that would be used to calculate the probabilities."
  },
  {
    "objectID": "homework/HW_3_F24_bsta511.html#instructions-for-normal-probability-exercises",
    "href": "homework/HW_3_F24_bsta511.html#instructions-for-normal-probability-exercises",
    "title": "HW 3: BSTA 511/611 F24",
    "section": "!!! Instructions for Normal probability exercises !!!",
    "text": "!!! Instructions for Normal probability exercises !!!\n\n\n\n\n\n\nImportant\n\n\n\nAdditional Instructions - IMPORTANT!!!\n\nFor ALL normal distribution exercises:\n\nmake a sketch of the normal distribution curve with the mean and 1 sd away from the mean clearly labeled, and the area representing probability of interest shaded in.\ncalculate probabilities using both\n\nz-table\nR"
  },
  {
    "objectID": "homework/HW_3_F24_bsta511.html#area-under-the-curve-part-ii",
    "href": "homework/HW_3_F24_bsta511.html#area-under-the-curve-part-ii",
    "title": "HW 3: BSTA 511/611 F24",
    "section": "3.20 Area under the curve, Part II",
    "text": "3.20 Area under the curve, Part II"
  },
  {
    "objectID": "homework/HW_3_F24_bsta511.html#triathlon-times",
    "href": "homework/HW_3_F24_bsta511.html#triathlon-times",
    "title": "HW 3: BSTA 511/611 F24",
    "section": "3.22 Triathlon times",
    "text": "3.22 Triathlon times"
  },
  {
    "objectID": "homework/HW_3_F24_bsta511.html#arsenic-poisoning",
    "href": "homework/HW_3_F24_bsta511.html#arsenic-poisoning",
    "title": "HW 3: BSTA 511/611 F24",
    "section": "3.28 Arsenic poisoning",
    "text": "3.28 Arsenic poisoning"
  },
  {
    "objectID": "homework/HW_3_F24_bsta511.html#find-the-sd",
    "href": "homework/HW_3_F24_bsta511.html#find-the-sd",
    "title": "HW 3: BSTA 511/611 F24",
    "section": "3.30 Find the SD",
    "text": "3.30 Find the SD"
  },
  {
    "objectID": "homework/HW_3_F24_bsta511.html#chickenpox-part-iii",
    "href": "homework/HW_3_F24_bsta511.html#chickenpox-part-iii",
    "title": "HW 3: BSTA 511/611 F24",
    "section": "3.32 Chickenpox, Part III",
    "text": "3.32 Chickenpox, Part III"
  },
  {
    "objectID": "homework/HW_3_F24_bsta511.html#stenographers-typos",
    "href": "homework/HW_3_F24_bsta511.html#stenographers-typos",
    "title": "HW 3: BSTA 511/611 F24",
    "section": "3.38 Stenographer’s typos",
    "text": "3.38 Stenographer’s typos"
  },
  {
    "objectID": "homework/HW_3_F24_bsta511.html#osteosarcoma-in-nyc",
    "href": "homework/HW_3_F24_bsta511.html#osteosarcoma-in-nyc",
    "title": "HW 3: BSTA 511/611 F24",
    "section": "3.40 Osteosarcoma in NYC",
    "text": "3.40 Osteosarcoma in NYC"
  },
  {
    "objectID": "homework/HW_3_F24_bsta511.html#nb-1-clinician-time-with-patients",
    "href": "homework/HW_3_F24_bsta511.html#nb-1-clinician-time-with-patients",
    "title": "HW 3: BSTA 511/611 F24",
    "section": "NB 1: Clinician time with patients",
    "text": "NB 1: Clinician time with patients\nSuppose a clinician schedules 20 minutes to spend with each of their patients. However, they sometimes run over or end earlier. Based on past data, the mean “extra” time they spend with a patient is 3 minutes with a standard deviation of 2 minutes. Suppose they see 13 patients today and the extra times they spend with patients are independent from patient to patient.\n\n(a) Expected total time\nFind the expected total time they will spend with all of their patients today.\n\n\n(b) SD of total time\nFind the standard deviation of the total time they will spend with all of their patients today."
  },
  {
    "objectID": "weeks/week_04.html#homework",
    "href": "weeks/week_04.html#homework",
    "title": "Week 4",
    "section": "Homework",
    "text": "Homework\n\nHW 3 due on Sat, 10/26\nMake sure to check out the calculating probabilities in R code file: qmd, html\nNote: the answer to exercise 3.31(b) in the textbook (not assigned) is incorrect.\n\nCheck out this spreadsheet for more typos in book."
  },
  {
    "objectID": "weeks/week_04.html#exam-1-information",
    "href": "weeks/week_04.html#exam-1-information",
    "title": "Week 4",
    "section": "Exam 1 information",
    "text": "Exam 1 information\n\nExam 1 will be on Wed, Oct. 30th\nSamples of past exam questions and answers (on Sakai)\nMaterial will cover Days 1-7,\n\nwhich is approximately Chapters 1, 2, 3.1-3.4 from the textbook.\n\nThe exam will be in-class and handwritten. Bring a calculator.\nYou may bring one page of notes on an 8.5” x 11” sheet of paper.\n\nYou may use both sides.\nYou may type your notes.\nYou may not use screenshots of the notes or textbook\nYou will be turning in the page of notes with the exam, so please add your name to the sheet."
  },
  {
    "objectID": "weeks/week_05.html",
    "href": "weeks/week_05.html",
    "title": "Week 5",
    "section": "",
    "text": "Variability in estimates: Section 4.1\n\n\n\n\n\nSee the Exam 1 information section below for more information"
  },
  {
    "objectID": "weeks/week_05.html#overview-of-week-5",
    "href": "weeks/week_05.html#overview-of-week-5",
    "title": "Week 5",
    "section": "",
    "text": "Variability in estimates: Section 4.1\n\n\n\n\n\nSee the Exam 1 information section below for more information"
  },
  {
    "objectID": "weeks/week_05.html#slides-recordings",
    "href": "weeks/week_05.html#slides-recordings",
    "title": "Week 5",
    "section": "Slides & Recordings",
    "text": "Slides & Recordings\n\nPre-recorded lessons are on Echo Cloud (aka echo 360 or echo video).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nTopic\nSlides: html\nSlides: pdf\nSlides: web-page\nSlides with notes\nRecording Link\nDuration\nCode: qmd\nCode: html\n\n\n\n\n8\nVariability in estimates: Section 4.1\nDay 8: slides 1-13\n\n\n\nDay 8 Part 1\n26 min\n\n\n\n\n\n4.1 con’td\nDay 8: slides 14-29\nsame\n\nsame\nDay 8 Part 2\n37 min\nsame\n\n\n\n\nR code demo\n\n\n\n\nDay 8 Part 3\n11 min\nsame"
  },
  {
    "objectID": "weeks/week_05.html#class-discussion",
    "href": "weeks/week_05.html#class-discussion",
    "title": "Week 5",
    "section": "Class discussion",
    "text": "Class discussion\nDuring class you will be working in groups discussing the following:\n\nDay 8\n\nSlide 21: match figures to distribution (Sampling high schoolers’ weights)\nProblems from Homework 4:\n\nBook exercise: 4.2 (warm-up)\nR1: Youth weights (YRBSS)\nNon-book exercise: Ethan Allen"
  },
  {
    "objectID": "weeks/week_05.html#homework",
    "href": "weeks/week_05.html#homework",
    "title": "Week 5",
    "section": "Homework",
    "text": "Homework\n\nHW 4 covers Days 8 -10 and is due on Sat, 11/9\nCheck out this spreadsheet for typos in book."
  },
  {
    "objectID": "weeks/week_05.html#sec-Exam1info",
    "href": "weeks/week_05.html#sec-Exam1info",
    "title": "Week 5",
    "section": "Exam 1 information",
    "text": "Exam 1 information\n\nExam 1 will be on Wed, Oct. 30th\nSamples of past exam questions and answers (on Sakai)\nMaterial will cover Days 1-7,\n\nwhich is approximately Chapters 1, 2, 3.1-3.4 from the textbook.\n\nThe exam will be in-class and handwritten. Bring a calculator.\nYou may bring one page of notes on an 8.5” x 11” sheet of paper.\n\nYou may use both sides.\nYou may type your notes.\nYou may not use screenshots of the notes or textbook\nYou will be turning in the page of notes with the exam, so please add your name to the sheet."
  },
  {
    "objectID": "weeks/week_05.html#homework---coming-soon",
    "href": "weeks/week_05.html#homework---coming-soon",
    "title": "Week 5",
    "section": "Homework - coming soon!",
    "text": "Homework - coming soon!\n\nHW 4 covers Days 8 -10 and is due on Sat, 11/9\nCheck out this spreadsheet for typos in book."
  },
  {
    "objectID": "slides/Day08_bsta511.html#where-are-we",
    "href": "slides/Day08_bsta511.html#where-are-we",
    "title": "Day 8: Variability in estimates",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "slides/Day08_bsta511.html#goals-for-today",
    "href": "slides/Day08_bsta511.html#goals-for-today",
    "title": "Day 8: Variability in estimates",
    "section": "Goals for today",
    "text": "Goals for today\nSection 4.1\n\nSampling from a population\n\npopulation parameters vs. point estimates\nsampling variation\n\nSampling distribution of the mean\n\nCentral Limit Theorem\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "slides/Day08_bsta511.html#moritzs-tip-of-the-day-add-a-code-pane-in-rstudio",
    "href": "slides/Day08_bsta511.html#moritzs-tip-of-the-day-add-a-code-pane-in-rstudio",
    "title": "Day 8: Variability in estimates",
    "section": "MoRitz’s tip of the day: add a code pane in RStudio",
    "text": "MoRitz’s tip of the day: add a code pane in RStudio\nDo you want to be able to view two code files side-by-side?\nYou can do that by adding a column to the RStudio layout.\n\nSee https://posit.co/blog/rstudio-1-4-preview-multiple-source-columns/ for more information."
  },
  {
    "objectID": "slides/Day08_bsta511.html#population-vs.-sample-from-section-1.3",
    "href": "slides/Day08_bsta511.html#population-vs.-sample-from-section-1.3",
    "title": "Day 8: Variability in estimates",
    "section": "Population vs. sample (from section 1.3)",
    "text": "Population vs. sample (from section 1.3)\n\n\n(Target) Population\n\ngroup of interest being studied\ngroup from which the sample is selected\n\nstudies often have inclusion and/or exclusion criteria\n\n\nSample\n\ngroup on which data are collected\noften a small subset of the population\n\n\nSimple random sample (SRS)\n\neach individual of a population has the same chance of being sampled\nrandomly sampled\nconsidered best way to sample"
  },
  {
    "objectID": "slides/Day08_bsta511.html#population-parameters-vs.-sample-statistics",
    "href": "slides/Day08_bsta511.html#population-parameters-vs.-sample-statistics",
    "title": "Day 8: Variability in estimates",
    "section": "Population parameters vs. sample statistics",
    "text": "Population parameters vs. sample statistics\n\n\nPopulation parameter\n\nSample statistic (point estimate)"
  },
  {
    "objectID": "slides/Day08_bsta511.html#our-hypothetical-population-yrbss",
    "href": "slides/Day08_bsta511.html#our-hypothetical-population-yrbss",
    "title": "Day 8: Variability in estimates",
    "section": "Our hypothetical population: YRBSS",
    "text": "Our hypothetical population: YRBSS\nYouth Risk Behavior Surveillance System (YRBSS)\n\nYearly survey conducted by the US Centers for Disease Control (CDC)\n“A set of surveys that track behaviors that can lead to poor health in students grades 9 through 12.”1\nDataset yrbss from oibiostat pacakge contains responses from n = 13,583 participants in 2013 for a subset of the variables included in the complete survey data\n\n\n\n\nlibrary(oibiostat)\ndata(\"yrbss\")  #load the data\n# ?yrbss\n\n\n\ndim(yrbss)\n\n[1] 13583    13\n\n\n\n\n\nnames(yrbss)\n\n [1] \"age\"                      \"gender\"                  \n [3] \"grade\"                    \"hispanic\"                \n [5] \"race\"                     \"height\"                  \n [7] \"weight\"                   \"helmet.12m\"              \n [9] \"text.while.driving.30d\"   \"physically.active.7d\"    \n[11] \"hours.tv.per.school.day\"  \"strength.training.7d\"    \n[13] \"school.night.hours.sleep\"\n\n\nYouth Risk Behavior Surveillance System https://www.cdc.gov/healthyyouth/data/yrbs/index.htm (YRBSS)"
  },
  {
    "objectID": "slides/Day08_bsta511.html#getting-to-know-the-dataset-glimpse",
    "href": "slides/Day08_bsta511.html#getting-to-know-the-dataset-glimpse",
    "title": "Day 8: Variability in estimates",
    "section": "Getting to know the dataset: glimpse()",
    "text": "Getting to know the dataset: glimpse()\n\nglimpse(yrbss)  # from tidyverse package (dplyr)\n\nRows: 13,583\nColumns: 13\n$ age                      &lt;int&gt; 14, 14, 15, 15, 15, 15, 15, 14, 15, 15, 15, 1…\n$ gender                   &lt;chr&gt; \"female\", \"female\", \"female\", \"female\", \"fema…\n$ grade                    &lt;chr&gt; \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", …\n$ hispanic                 &lt;chr&gt; \"not\", \"not\", \"hispanic\", \"not\", \"not\", \"not\"…\n$ race                     &lt;chr&gt; \"Black or African American\", \"Black or Africa…\n$ height                   &lt;dbl&gt; NA, NA, 1.73, 1.60, 1.50, 1.57, 1.65, 1.88, 1…\n$ weight                   &lt;dbl&gt; NA, NA, 84.37, 55.79, 46.72, 67.13, 131.54, 7…\n$ helmet.12m               &lt;chr&gt; \"never\", \"never\", \"never\", \"never\", \"did not …\n$ text.while.driving.30d   &lt;chr&gt; \"0\", NA, \"30\", \"0\", \"did not drive\", \"did not…\n$ physically.active.7d     &lt;int&gt; 4, 2, 7, 0, 2, 1, 4, 4, 5, 0, 0, 0, 4, 7, 7, …\n$ hours.tv.per.school.day  &lt;chr&gt; \"5+\", \"5+\", \"5+\", \"2\", \"3\", \"5+\", \"5+\", \"5+\",…\n$ strength.training.7d     &lt;int&gt; 0, 0, 0, 0, 1, 0, 2, 0, 3, 0, 3, 0, 0, 7, 7, …\n$ school.night.hours.sleep &lt;chr&gt; \"8\", \"6\", \"&lt;5\", \"6\", \"9\", \"8\", \"9\", \"6\", \"&lt;5\"…"
  },
  {
    "objectID": "slides/Day08_bsta511.html#height-weight-variables",
    "href": "slides/Day08_bsta511.html#height-weight-variables",
    "title": "Day 8: Variability in estimates",
    "section": "Height & weight variables",
    "text": "Height & weight variables\n\n\n\nyrbss %&gt;% \n  select(height, weight) %&gt;% \n  summary()\n\n     height          weight      \n Min.   :1.270   Min.   : 29.94  \n 1st Qu.:1.600   1st Qu.: 56.25  \n Median :1.680   Median : 64.41  \n Mean   :1.691   Mean   : 67.91  \n 3rd Qu.:1.780   3rd Qu.: 76.20  \n Max.   :2.110   Max.   :180.99  \n NA's   :1004    NA's   :1004    \n\n\n\n\nggplot(data = yrbss, \n       aes(x = height)) +\n  geom_histogram()"
  },
  {
    "objectID": "slides/Day08_bsta511.html#transform-height-weight-from-metric-to-to-standard",
    "href": "slides/Day08_bsta511.html#transform-height-weight-from-metric-to-to-standard",
    "title": "Day 8: Variability in estimates",
    "section": "Transform height & weight from metric to to standard",
    "text": "Transform height & weight from metric to to standard\nAlso, drop missing values and add a column of id values\n\nyrbss2 &lt;- yrbss %&gt;%                 # save new dataset with new name\n  mutate(                           # add variables for \n    height.ft = 3.28084*height,     #     height in feet\n    weight.lb = 2.20462*weight      #     weight in pounds\n  ) %&gt;% \n  drop_na(height.ft, weight.lb) %&gt;% # drop rows w/ missing height/weight values\n  mutate(id = 1:nrow(.)) %&gt;%        # add id column\n  select(id, height.ft, weight.lb)  # restrict dataset to columns of interest\n\nhead(yrbss2)  \n\n  id height.ft weight.lb\n1  1  5.675853  186.0038\n2  2  5.249344  122.9957\n3  3  4.921260  102.9998\n4  4  5.150919  147.9961\n5  5  5.413386  289.9957\n6  6  6.167979  157.0130\n\ndim(yrbss2)\n\n[1] 12579     3\n\n# number of rows deleted that had missing values for height and/or weight:\nnrow(yrbss) - nrow(yrbss2) \n\n[1] 1004"
  },
  {
    "objectID": "slides/Day08_bsta511.html#yrbss2-summary",
    "href": "slides/Day08_bsta511.html#yrbss2-summary",
    "title": "Day 8: Variability in estimates",
    "section": "yrbss2 summary",
    "text": "yrbss2 summary\n\nsummary(yrbss2)\n\n       id          height.ft       weight.lb     \n Min.   :    1   Min.   :4.167   Min.   : 66.01  \n 1st Qu.: 3146   1st Qu.:5.249   1st Qu.:124.01  \n Median : 6290   Median :5.512   Median :142.00  \n Mean   : 6290   Mean   :5.549   Mean   :149.71  \n 3rd Qu.: 9434   3rd Qu.:5.840   3rd Qu.:167.99  \n Max.   :12579   Max.   :6.923   Max.   :399.01  \n\n\nAnother summary:\n\nyrbss2 %&gt;% \n  get_summary_stats(type = \"mean_sd\") %&gt;% \n  kable()\n\n\n\n\nvariable\nn\nmean\nsd\n\n\n\n\nid\n12579\n6290.000\n3631.389\n\n\nheight.ft\n12579\n5.549\n0.343\n\n\nweight.lb\n12579\n149.708\n37.254"
  },
  {
    "objectID": "slides/Day08_bsta511.html#random-sample-of-size-n-5-from-yrbss2",
    "href": "slides/Day08_bsta511.html#random-sample-of-size-n-5-from-yrbss2",
    "title": "Day 8: Variability in estimates",
    "section": "Random sample of size n = 5 from yrbss2",
    "text": "Random sample of size n = 5 from yrbss2\n\n\nTake a random sample of size n = 5 from yrbss2:\n\nlibrary(moderndive)\nsamp_n5_rep1 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 5, \n               reps = 1,\n               replace = FALSE)\nsamp_n5_rep1\n\n# A tibble: 5 × 4\n# Groups:   replicate [1]\n  replicate    id height.ft weight.lb\n      &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1         1  5869      5.15      145.\n2         1  6694      5.41      127.\n3         1  2517      5.74      130.\n4         1  5372      6.07      180.\n5         1  5403      6.07      163.\n\n\n\nCalculate the mean of the random sample:\n\nmeans_hght_samp_n5_rep1 &lt;- \n  samp_n5_rep1 %&gt;% \n  summarise(\n    mean_height = mean(height.ft))\n\nmeans_hght_samp_n5_rep1\n\n# A tibble: 1 × 2\n  replicate mean_height\n      &lt;int&gt;       &lt;dbl&gt;\n1         1        5.69\n\n\n\n\nWould we get the same mean height if we took another sample?"
  },
  {
    "objectID": "slides/Day08_bsta511.html#sampling-variation",
    "href": "slides/Day08_bsta511.html#sampling-variation",
    "title": "Day 8: Variability in estimates",
    "section": "Sampling variation",
    "text": "Sampling variation\n\nIf a different random sample is taken, the mean height (point estimate) will likely be different\n\nthis is a result of sampling variation\n\n\n\n\nTake a 2nd random sample of size\nn = 5 from yrbss2:\n\nsamp_n5_rep1 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 5, \n               reps = 1,\n               replace = FALSE)\nsamp_n5_rep1\n\n# A tibble: 5 × 4\n# Groups:   replicate [1]\n  replicate    id height.ft weight.lb\n      &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1         1  2329      6.07      182.\n2         1  8863      5.25      125.\n3         1  8058      5.84      135.\n4         1   335      6.17      235.\n5         1  4698      5.58      124.\n\n\n\nCalculate the mean of the 2nd random sample:\n\nmeans_hght_samp_n5_rep1 &lt;- \n  samp_n5_rep1 %&gt;% \n  summarise(\n    mean_height = mean(height.ft))\n\nmeans_hght_samp_n5_rep1\n\n# A tibble: 1 × 2\n  replicate mean_height\n      &lt;int&gt;       &lt;dbl&gt;\n1         1        5.78\n\n\n\n\nDid we get the same mean height with our 2nd sample?"
  },
  {
    "objectID": "slides/Day08_bsta511.html#random-samples-of-size-n-5-from-yrbss2",
    "href": "slides/Day08_bsta511.html#random-samples-of-size-n-5-from-yrbss2",
    "title": "Day 8: Variability in estimates",
    "section": "100 random samples of size n = 5 from yrbss2",
    "text": "100 random samples of size n = 5 from yrbss2\n\n\nTake 100 random samples of size\nn = 5 from yrbss2:\n\nsamp_n5_rep100 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 5, \n               reps = 100,\n               replace = FALSE)\nsamp_n5_rep100\n\n# A tibble: 500 × 4\n# Groups:   replicate [100]\n   replicate    id height.ft weight.lb\n       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1         1  6483      5.51     145. \n 2         1  9899      4.92      90.0\n 3         1  6103      5.68     118. \n 4         1  2702      5.68     150. \n 5         1 11789      5.35     115. \n 6         2 10164      5.51     140. \n 7         2  5807      5.41     215. \n 8         2  9382      5.15      98.0\n 9         2  4904      6.00     196. \n10         2   229      6.07     101. \n# ℹ 490 more rows\n\n\n\nCalculate the mean for each of the 100 random samples:\n\nmeans_hght_samp_n5_rep100 &lt;- \n  samp_n5_rep100 %&gt;% \n  group_by(replicate) %&gt;% \n  summarise(\n    mean_height = mean(height.ft))\n\nmeans_hght_samp_n5_rep100\n\n# A tibble: 100 × 2\n   replicate mean_height\n       &lt;int&gt;       &lt;dbl&gt;\n 1         1        5.43\n 2         2        5.63\n 3         3        5.34\n 4         4        5.70\n 5         5        5.90\n 6         6        5.37\n 7         7        5.49\n 8         8        5.60\n 9         9        5.50\n10        10        5.68\n# ℹ 90 more rows\n\n\n\n\nHow close are the mean heights for each of the 100 random samples?"
  },
  {
    "objectID": "slides/Day08_bsta511.html#distribution-of-100-sample-mean-heights-n-5",
    "href": "slides/Day08_bsta511.html#distribution-of-100-sample-mean-heights-n-5",
    "title": "Day 8: Variability in estimates",
    "section": "Distribution of 100 sample mean heights (n = 5)",
    "text": "Distribution of 100 sample mean heights (n = 5)\n\n\nDescribe the distribution shape.\n\nggplot(\n  means_hght_samp_n5_rep100, \n  aes(x = mean_height)) + \n  geom_histogram()\n\n\n\n\n\nCalculate the mean and SD of the 100 mean heights from the 100 samples:\n\nstats_means_hght_samp_n5_rep100 &lt;- \n  means_hght_samp_n5_rep100 %&gt;% \n  summarise(\n   mean_mean_height = mean(mean_height),\n   sd_mean_height = sd(mean_height)\n   )\nstats_means_hght_samp_n5_rep100\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.58          0.150\n\n\nIs the mean of the means close to the “center” of the distribution?"
  },
  {
    "objectID": "slides/Day08_bsta511.html#random-samples-of-size-n-5-from-yrbss2-1",
    "href": "slides/Day08_bsta511.html#random-samples-of-size-n-5-from-yrbss2-1",
    "title": "Day 8: Variability in estimates",
    "section": "10,000 random samples of size n = 5 from yrbss2",
    "text": "10,000 random samples of size n = 5 from yrbss2\n\n\nTake 10,000 random samples of size\nn = 5 from yrbss2:\n\nsamp_n5_rep10000 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 5, \n               reps = 10000,\n               replace = FALSE)\nsamp_n5_rep10000\n\n# A tibble: 50,000 × 4\n# Groups:   replicate [10,000]\n   replicate    id height.ft weight.lb\n       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1         1  6383      5.35      126.\n 2         1  4019      5.41      107.\n 3         1  4856      5.25      135.\n 4         1  9988      5.58      120.\n 5         1  2245      6.17      270.\n 6         2 10580      5.68      155.\n 7         2  2254      5.84      159.\n 8         2  8081      5.09      110.\n 9         2 10194      5.35      115.\n10         2  7689      5.35      135.\n# ℹ 49,990 more rows\n\n\n\nCalculate the mean for each of the 10,000 random samples:\n\nmeans_hght_samp_n5_rep10000 &lt;- \n  samp_n5_rep10000 %&gt;% \n  group_by(replicate) %&gt;% \n  summarise(\n    mean_height = mean(height.ft))\n\nmeans_hght_samp_n5_rep10000\n\n# A tibble: 10,000 × 2\n   replicate mean_height\n       &lt;int&gt;       &lt;dbl&gt;\n 1         1        5.55\n 2         2        5.46\n 3         3        5.49\n 4         4        5.60\n 5         5        5.47\n 6         6        5.83\n 7         7        5.68\n 8         8        5.47\n 9         9        5.37\n10        10        5.15\n# ℹ 9,990 more rows\n\n\n\n\nHow close are the mean heights for each of the 10,000 random samples?"
  },
  {
    "objectID": "slides/Day08_bsta511.html#distribution-of-10000-sample-mean-heights-n-5",
    "href": "slides/Day08_bsta511.html#distribution-of-10000-sample-mean-heights-n-5",
    "title": "Day 8: Variability in estimates",
    "section": "Distribution of 10,000 sample mean heights (n = 5)",
    "text": "Distribution of 10,000 sample mean heights (n = 5)\n\n\nDescribe the distribution shape.\n\nggplot(\n  means_hght_samp_n5_rep10000, \n  aes(x = mean_height)) + \n  geom_histogram()\n\n\n\n\n\nCalculate the mean and SD of the 10,000 mean heights from the 10,000 samples:\n\nstats_means_hght_samp_n5_rep10000 &lt;- \n  means_hght_samp_n5_rep10000 %&gt;% \n  summarise(\n   mean_mean_height=mean(mean_height),\n   sd_mean_height = sd(mean_height)\n   )\nstats_means_hght_samp_n5_rep10000\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.55          0.153\n\n\nIs the mean of the means close to the “center” of the distribution?"
  },
  {
    "objectID": "slides/Day08_bsta511.html#samples-of-size-n-30-from-yrbss2",
    "href": "slides/Day08_bsta511.html#samples-of-size-n-30-from-yrbss2",
    "title": "Day 8: Variability in estimates",
    "section": "10,000 samples of size n = 30 from yrbss2",
    "text": "10,000 samples of size n = 30 from yrbss2\n\n\nTake 10,000 random samples of size\nn = 30 from yrbss2:\n\nsamp_n30_rep10000 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 30, \n               reps = 10000,\n               replace = FALSE)\nsamp_n30_rep10000\n\n# A tibble: 300,000 × 4\n# Groups:   replicate [10,000]\n   replicate    id height.ft weight.lb\n       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1         1  3871      5.25      115.\n 2         1 12090      5.15      125.\n 3         1   241      5.58      119.\n 4         1  4570      5.58      140.\n 5         1  4131      5.35      143.\n 6         1 11513      5.35      135.\n 7         1  9663      5.25      125.\n 8         1  3789      5.25      160.\n 9         1   442      5.15      130.\n10         1 11528      5.51      200.\n# ℹ 299,990 more rows\n\n\n\nCalculate the mean for each of the 10,000 random samples:\n\nmeans_hght_samp_n30_rep10000 &lt;- \n  samp_n30_rep10000 %&gt;% \n  group_by(replicate) %&gt;% \n  summarise(mean_height = \n            mean(height.ft))\n\nmeans_hght_samp_n30_rep10000\n\n# A tibble: 10,000 × 2\n   replicate mean_height\n       &lt;int&gt;       &lt;dbl&gt;\n 1         1        5.48\n 2         2        5.63\n 3         3        5.46\n 4         4        5.46\n 5         5        5.51\n 6         6        5.54\n 7         7        5.56\n 8         8        5.51\n 9         9        5.51\n10        10        5.50\n# ℹ 9,990 more rows\n\n\n\n\nHow close are the mean heights for each of the 10,000 random samples?"
  },
  {
    "objectID": "slides/Day08_bsta511.html#distribution-of-10000-sample-mean-heights-n-30",
    "href": "slides/Day08_bsta511.html#distribution-of-10000-sample-mean-heights-n-30",
    "title": "Day 8: Variability in estimates",
    "section": "Distribution of 10,000 sample mean heights (n = 30)",
    "text": "Distribution of 10,000 sample mean heights (n = 30)\n\n\nDescribe the distribution shape.\n\nggplot(\n  means_hght_samp_n30_rep10000, \n  aes(x = mean_height)) + \n  geom_histogram()\n\n\n\n\n\nCalculate the mean and SD of the 10,000 mean heights from the 10,000 samples:\n\nstats_means_hght_samp_n30_rep10000&lt;- \n  means_hght_samp_n30_rep10000 %&gt;% \n  summarise(\n   mean_mean_height=mean(mean_height),\n   sd_mean_height = sd(mean_height)\n   )\nstats_means_hght_samp_n30_rep10000\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.55         0.0623\n\n\nIs the mean of the means close to the “center” of the distribution?"
  },
  {
    "objectID": "slides/Day08_bsta511.html#compare-distributions-of-10000-sample-mean-heights-when-n-5-left-vs.-n-30-right",
    "href": "slides/Day08_bsta511.html#compare-distributions-of-10000-sample-mean-heights-when-n-5-left-vs.-n-30-right",
    "title": "Day 8: Variability in estimates",
    "section": "Compare distributions of 10,000 sample mean heights when n = 5 (left) vs. n = 30 (right)",
    "text": "Compare distributions of 10,000 sample mean heights when n = 5 (left) vs. n = 30 (right)\nHow are the center, shape, and spread similar and/or different?\n\n\n\n\n\n\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.55          0.153\n\n\n\n\n\n\n\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.55         0.0623"
  },
  {
    "objectID": "slides/Day08_bsta511.html#sampling-high-schoolers-weights",
    "href": "slides/Day08_bsta511.html#sampling-high-schoolers-weights",
    "title": "Day 8: Variability in estimates",
    "section": "Sampling high schoolers’ weights",
    "text": "Sampling high schoolers’ weights\n\n\nWhich figure is which?\n\nPopulation distribution of weights\nSampling distribution of mean weights when \\(n=5\\)\nSampling distribution of mean weights when \\(n=30\\).\n\n\n\n\n\nA\n\n\n\n\n\n\nB\n\n\n\n\n\n\nC"
  },
  {
    "objectID": "slides/Day08_bsta511.html#the-sampling-distribution-of-the-mean",
    "href": "slides/Day08_bsta511.html#the-sampling-distribution-of-the-mean",
    "title": "Day 8: Variability in estimates",
    "section": "The sampling distribution of the mean",
    "text": "The sampling distribution of the mean\n\n\n\nThe sampling distribution of the mean is the distribution of sample means calculated from repeated random samples of the same size from the same population\nOur simulations show approximations of the sampling distribution of the mean for various sample sizes\nThe theoretical sampling distribution is based on all possible samples of a given sample size \\(n\\)."
  },
  {
    "objectID": "slides/Day08_bsta511.html#the-central-limit-theorem-clt",
    "href": "slides/Day08_bsta511.html#the-central-limit-theorem-clt",
    "title": "Day 8: Variability in estimates",
    "section": "The Central Limit Theorem (CLT)",
    "text": "The Central Limit Theorem (CLT)\n\nFor “large” sample sizes ( \\(n\\geq 30\\) ),\n\nthe sampling distribution of the sample mean\ncan be approximated by a normal distribution,with\n\nmean equal to the population mean value \\(\\mu\\), and\nstandard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\n\n\n\n  \n\nFor small sample sizes, if the population is known to be normally distributed, then\n\nthe sampling distribution of the sample mean\nis a normal distribution, with\n\nmean equal to the population mean value \\(\\mu\\), and\nstandard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\)"
  },
  {
    "objectID": "slides/Day08_bsta511.html#the-cutest-statistics-video-on-youtube",
    "href": "slides/Day08_bsta511.html#the-cutest-statistics-video-on-youtube",
    "title": "Day 8: Variability in estimates",
    "section": "The cutest statistics video on YouTube",
    "text": "The cutest statistics video on YouTube\n\nBunnies, Dragons and the ‘Normal’ World: Central Limit Theorem\n\nCreature Cast from the New York Times\nhttps://www.youtube.com/watch?v=jvoxEYmQHNM&feature=youtu.be"
  },
  {
    "objectID": "slides/Day08_bsta511.html#sampling-distribution-of-mean-heights-when-n-30-12",
    "href": "slides/Day08_bsta511.html#sampling-distribution-of-mean-heights-when-n-30-12",
    "title": "Day 8: Variability in estimates",
    "section": "Sampling distribution of mean heights when n = 30 (1/2)",
    "text": "Sampling distribution of mean heights when n = 30 (1/2)\n\n\n\nggplot(\n  means_hght_samp_n30_rep10000, \n  aes(x = mean_height)) + \n  geom_histogram()\n\n\n\n\n\nCLT tells us that we can model the sampling distribution of mean heights using a normal distribution."
  },
  {
    "objectID": "slides/Day08_bsta511.html#sampling-distribution-of-mean-heights-when-n-30-22",
    "href": "slides/Day08_bsta511.html#sampling-distribution-of-mean-heights-when-n-30-22",
    "title": "Day 8: Variability in estimates",
    "section": "Sampling distribution of mean heights when n = 30 (2/2)",
    "text": "Sampling distribution of mean heights when n = 30 (2/2)\n\n\nMean and SD of population:\n\n(mean_height.ft &lt;- mean(yrbss2$height.ft))\n\n[1] 5.548691\n\n(sd_height.ft &lt;- sd(yrbss2$height.ft))\n\n[1] 0.3434949\n\nsd_height.ft/sqrt(30)\n\n[1] 0.06271331\n\n\nMean and SD of simulated sampling distribution:\n\nstats_means_hght_samp_n30_rep10000&lt;- \n  means_hght_samp_n30_rep10000 %&gt;% \n  summarise(\n   mean_mean_height=mean(mean_height),\n   sd_mean_height = sd(mean_height)\n   )\nstats_means_hght_samp_n30_rep10000\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.55         0.0623"
  },
  {
    "objectID": "slides/Day08_bsta511.html#why-is-the-mean-mu-the-standard-error-fracsigmasqrtn",
    "href": "slides/Day08_bsta511.html#why-is-the-mean-mu-the-standard-error-fracsigmasqrtn",
    "title": "Day 8: Variability in estimates",
    "section": "Why is the mean \\(\\mu\\) & the standard error \\(\\frac{\\sigma}{\\sqrt{n}}\\) ?",
    "text": "Why is the mean \\(\\mu\\) & the standard error \\(\\frac{\\sigma}{\\sqrt{n}}\\) ?"
  },
  {
    "objectID": "slides/Day08_bsta511.html#applying-the-clt",
    "href": "slides/Day08_bsta511.html#applying-the-clt",
    "title": "Day 8: Variability in estimates",
    "section": "Applying the CLT",
    "text": "Applying the CLT\nWhat is the probability that for a random sample of 30 high schoolers, that their mean height is greater than 5.6 ft?"
  },
  {
    "objectID": "slides/Day08_bsta511.html#class-discussion",
    "href": "slides/Day08_bsta511.html#class-discussion",
    "title": "Day 8: Variability in estimates",
    "section": "Class Discussion",
    "text": "Class Discussion\n\nSlide 21: match figures to distribution (Sampling high schoolers’ weights)\n\nProblems from Homework 4:\n\nR1: Youth weights (YRBSS)\nBook exercise: 4.2\nNon-book exercise: Ethan Allen"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#goals-for-today",
    "href": "slides_md/Day08_bsta511_md.html#goals-for-today",
    "title": "Day 8: Variability in estimates",
    "section": "Goals for today",
    "text": "Goals for today\nSection 4.1\n\nSampling from a population\n\npopulation parameters vs. point estimates\nsampling variation\n\nSampling distribution of the mean\n\nCentral Limit Theorem\n\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#moritzs-tip-of-the-day-add-a-code-pane-in-rstudio",
    "href": "slides_md/Day08_bsta511_md.html#moritzs-tip-of-the-day-add-a-code-pane-in-rstudio",
    "title": "Day 8: Variability in estimates",
    "section": "MoRitz’s tip of the day: add a code pane in RStudio",
    "text": "MoRitz’s tip of the day: add a code pane in RStudio\nDo you want to be able to view two code files side-by-side?\nYou can do that by adding a column to the RStudio layout.\n\n\n\n\n\nSee https://posit.co/blog/rstudio-1-4-preview-multiple-source-columns/ for more information."
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#population-vs.-sample-from-section-1.3",
    "href": "slides_md/Day08_bsta511_md.html#population-vs.-sample-from-section-1.3",
    "title": "Day 8: Variability in estimates",
    "section": "Population vs. sample (from section 1.3)",
    "text": "Population vs. sample (from section 1.3)\n\n\n(Target) Population\n\ngroup of interest being studied\ngroup from which the sample is selected\n\nstudies often have inclusion and/or exclusion criteria\n\n\nSample\n\ngroup on which data are collected\noften a small subset of the population\n\n\nSimple random sample (SRS)\n\neach individual of a population has the same chance of being sampled\nrandomly sampled\nconsidered best way to sample"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#population-parameters-vs.-sample-statistics",
    "href": "slides_md/Day08_bsta511_md.html#population-parameters-vs.-sample-statistics",
    "title": "Day 8: Variability in estimates",
    "section": "Population parameters vs. sample statistics",
    "text": "Population parameters vs. sample statistics\n\n\nPopulation parameter\n\nSample statistic (point estimate)"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#our-hypothetical-population-yrbss",
    "href": "slides_md/Day08_bsta511_md.html#our-hypothetical-population-yrbss",
    "title": "Day 8: Variability in estimates",
    "section": "Our hypothetical population: YRBSS",
    "text": "Our hypothetical population: YRBSS\nYouth Risk Behavior Surveillance System (YRBSS)\n\nYearly survey conducted by the US Centers for Disease Control (CDC)\n“A set of surveys that track behaviors that can lead to poor health in students grades 9 through 12.”1\nDataset yrbss from oibiostat pacakge contains responses from n = 13,572 participants in 2013 for a subset of the variables included in the complete survey data\n\n\n\n\nlibrary(oibiostat)\ndata(\"yrbss\")  #load the data\n# ?yrbss\n\n\n\ndim(yrbss)\n\n[1] 13583    13\n\n\n\n\n\nnames(yrbss)\n\n [1] \"age\"                      \"gender\"                  \n [3] \"grade\"                    \"hispanic\"                \n [5] \"race\"                     \"height\"                  \n [7] \"weight\"                   \"helmet.12m\"              \n [9] \"text.while.driving.30d\"   \"physically.active.7d\"    \n[11] \"hours.tv.per.school.day\"  \"strength.training.7d\"    \n[13] \"school.night.hours.sleep\""
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#getting-to-know-the-dataset-glimpse",
    "href": "slides_md/Day08_bsta511_md.html#getting-to-know-the-dataset-glimpse",
    "title": "Day 8: Variability in estimates",
    "section": "Getting to know the dataset: glimpse()",
    "text": "Getting to know the dataset: glimpse()\n\nglimpse(yrbss)  # from tidyverse package (dplyr)\n\nRows: 13,583\nColumns: 13\n$ age                      &lt;int&gt; 14, 14, 15, 15, 15, 15, 15, 14, 15, 15, 15, 1…\n$ gender                   &lt;chr&gt; \"female\", \"female\", \"female\", \"female\", \"fema…\n$ grade                    &lt;chr&gt; \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", …\n$ hispanic                 &lt;chr&gt; \"not\", \"not\", \"hispanic\", \"not\", \"not\", \"not\"…\n$ race                     &lt;chr&gt; \"Black or African American\", \"Black or Africa…\n$ height                   &lt;dbl&gt; NA, NA, 1.73, 1.60, 1.50, 1.57, 1.65, 1.88, 1…\n$ weight                   &lt;dbl&gt; NA, NA, 84.37, 55.79, 46.72, 67.13, 131.54, 7…\n$ helmet.12m               &lt;chr&gt; \"never\", \"never\", \"never\", \"never\", \"did not …\n$ text.while.driving.30d   &lt;chr&gt; \"0\", NA, \"30\", \"0\", \"did not drive\", \"did not…\n$ physically.active.7d     &lt;int&gt; 4, 2, 7, 0, 2, 1, 4, 4, 5, 0, 0, 0, 4, 7, 7, …\n$ hours.tv.per.school.day  &lt;chr&gt; \"5+\", \"5+\", \"5+\", \"2\", \"3\", \"5+\", \"5+\", \"5+\",…\n$ strength.training.7d     &lt;int&gt; 0, 0, 0, 0, 1, 0, 2, 0, 3, 0, 3, 0, 0, 7, 7, …\n$ school.night.hours.sleep &lt;chr&gt; \"8\", \"6\", \"&lt;5\", \"6\", \"9\", \"8\", \"9\", \"6\", \"&lt;5\"…"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#height-weight-variables",
    "href": "slides_md/Day08_bsta511_md.html#height-weight-variables",
    "title": "Day 8: Variability in estimates",
    "section": "Height & weight variables",
    "text": "Height & weight variables\n\n\n\nyrbss %&gt;% \n  select(height, weight) %&gt;% \n  summary()\n\n     height          weight      \n Min.   :1.270   Min.   : 29.94  \n 1st Qu.:1.600   1st Qu.: 56.25  \n Median :1.680   Median : 64.41  \n Mean   :1.691   Mean   : 67.91  \n 3rd Qu.:1.780   3rd Qu.: 76.20  \n Max.   :2.110   Max.   :180.99  \n NA's   :1004    NA's   :1004    \n\n\n\n\nggplot(data = yrbss, \n       aes(x = height)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 1004 rows containing non-finite values (`stat_bin()`)."
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#transform-height-weight-from-metric-to-to-standard",
    "href": "slides_md/Day08_bsta511_md.html#transform-height-weight-from-metric-to-to-standard",
    "title": "Day 8: Variability in estimates",
    "section": "Transform height & weight from metric to to standard",
    "text": "Transform height & weight from metric to to standard\nAlso, drop missing values and add a column of id values\n\nyrbss2 &lt;- yrbss %&gt;%                 # save new dataset with new name\n  mutate(                           # add variables for \n    height.ft = 3.28084*height,     #     height in feet\n    weight.lb = 2.20462*weight      #     weight in pounds\n  ) %&gt;% \n  drop_na(height.ft, weight.lb) %&gt;% # drop rows w/ missing height/weight values\n  mutate(id = 1:nrow(.)) %&gt;%        # add id column\n  select(id, height.ft, weight.lb)  # restrict dataset to columns of interest\n\nhead(yrbss2)  \n\n  id height.ft weight.lb\n1  1  5.675853  186.0038\n2  2  5.249344  122.9957\n3  3  4.921260  102.9998\n4  4  5.150919  147.9961\n5  5  5.413386  289.9957\n6  6  6.167979  157.0130\n\ndim(yrbss2)\n\n[1] 12579     3\n\n# number of rows deleted that had missing values for height and/or weight:\nnrow(yrbss) - nrow(yrbss2) \n\n[1] 1004"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#yrbss2-summary",
    "href": "slides_md/Day08_bsta511_md.html#yrbss2-summary",
    "title": "Day 8: Variability in estimates",
    "section": "yrbss2 summary",
    "text": "yrbss2 summary\n\nsummary(yrbss2)\n\n       id          height.ft       weight.lb     \n Min.   :    1   Min.   :4.167   Min.   : 66.01  \n 1st Qu.: 3146   1st Qu.:5.249   1st Qu.:124.01  \n Median : 6290   Median :5.512   Median :142.00  \n Mean   : 6290   Mean   :5.549   Mean   :149.71  \n 3rd Qu.: 9434   3rd Qu.:5.840   3rd Qu.:167.99  \n Max.   :12579   Max.   :6.923   Max.   :399.01  \n\n\nAnother summary:\n\nyrbss2 %&gt;% \n  get_summary_stats(type = \"mean_sd\") %&gt;% \n  kable()\n\n\n\n\nvariable\nn\nmean\nsd\n\n\n\n\nid\n12579\n6290.000\n3631.389\n\n\nheight.ft\n12579\n5.549\n0.343\n\n\nweight.lb\n12579\n149.708\n37.254"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#random-sample-of-size-n-5-from-yrbss2",
    "href": "slides_md/Day08_bsta511_md.html#random-sample-of-size-n-5-from-yrbss2",
    "title": "Day 8: Variability in estimates",
    "section": "Random sample of size n = 5 from yrbss2",
    "text": "Random sample of size n = 5 from yrbss2\n\n\nTake a random sample of size n = 5 from yrbss2:\n\nlibrary(moderndive)\nsamp_n5_rep1 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 5, \n               reps = 1,\n               replace = FALSE)\nsamp_n5_rep1\n\n# A tibble: 5 × 4\n# Groups:   replicate [1]\n  replicate    id height.ft weight.lb\n      &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1         1  5869      5.15      145.\n2         1  6694      5.41      127.\n3         1  2517      5.74      130.\n4         1  5372      6.07      180.\n5         1  5403      6.07      163.\n\n\n\nCalculate the mean of the random sample:\n\nmeans_hght_samp_n5_rep1 &lt;- \n  samp_n5_rep1 %&gt;% \n  summarise(\n    mean_height = mean(height.ft))\n\nmeans_hght_samp_n5_rep1\n\n# A tibble: 1 × 2\n  replicate mean_height\n      &lt;int&gt;       &lt;dbl&gt;\n1         1        5.69\n\n\n\n\nWould we get the same mean height if we took another sample?"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#sampling-variation",
    "href": "slides_md/Day08_bsta511_md.html#sampling-variation",
    "title": "Day 8: Variability in estimates",
    "section": "Sampling variation",
    "text": "Sampling variation\n\nIf a different random sample is taken, the mean height (point estimate) will likely be different\n\nthis is a result of sampling variation\n\n\n\n\nTake a 2nd random sample of size\nn = 5 from yrbss2:\n\nsamp_n5_rep1 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 5, \n               reps = 1,\n               replace = FALSE)\nsamp_n5_rep1\n\n# A tibble: 5 × 4\n# Groups:   replicate [1]\n  replicate    id height.ft weight.lb\n      &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1         1  2329      6.07      182.\n2         1  8863      5.25      125.\n3         1  8058      5.84      135.\n4         1   335      6.17      235.\n5         1  4698      5.58      124.\n\n\n\nCalculate the mean of the 2nd random sample:\n\nmeans_hght_samp_n5_rep1 &lt;- \n  samp_n5_rep1 %&gt;% \n  summarise(\n    mean_height = mean(height.ft))\n\nmeans_hght_samp_n5_rep1\n\n# A tibble: 1 × 2\n  replicate mean_height\n      &lt;int&gt;       &lt;dbl&gt;\n1         1        5.78\n\n\n\n\nDid we get the same mean height with our 2nd sample?"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#random-samples-of-size-n-5-from-yrbss2",
    "href": "slides_md/Day08_bsta511_md.html#random-samples-of-size-n-5-from-yrbss2",
    "title": "Day 8: Variability in estimates",
    "section": "100 random samples of size n = 5 from yrbss2",
    "text": "100 random samples of size n = 5 from yrbss2\n\n\nTake 100 random samples of size\nn = 5 from yrbss2:\n\nsamp_n5_rep100 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 5, \n               reps = 100,\n               replace = FALSE)\nsamp_n5_rep100\n\n# A tibble: 500 × 4\n# Groups:   replicate [100]\n   replicate    id height.ft weight.lb\n       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1         1  6483      5.51     145. \n 2         1  9899      4.92      90.0\n 3         1  6103      5.68     118. \n 4         1  2702      5.68     150. \n 5         1 11789      5.35     115. \n 6         2 10164      5.51     140. \n 7         2  5807      5.41     215. \n 8         2  9382      5.15      98.0\n 9         2  4904      6.00     196. \n10         2   229      6.07     101. \n# ℹ 490 more rows\n\n\n\nCalculate the mean for each of the 100 random samples:\n\nmeans_hght_samp_n5_rep100 &lt;- \n  samp_n5_rep100 %&gt;% \n  group_by(replicate) %&gt;% \n  summarise(\n    mean_height = mean(height.ft))\n\nmeans_hght_samp_n5_rep100\n\n# A tibble: 100 × 2\n   replicate mean_height\n       &lt;int&gt;       &lt;dbl&gt;\n 1         1        5.43\n 2         2        5.63\n 3         3        5.34\n 4         4        5.70\n 5         5        5.90\n 6         6        5.37\n 7         7        5.49\n 8         8        5.60\n 9         9        5.50\n10        10        5.68\n# ℹ 90 more rows\n\n\n\n\nHow close are the mean heights for each of the 100 random samples?"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#distribution-of-100-sample-mean-heights-n-5",
    "href": "slides_md/Day08_bsta511_md.html#distribution-of-100-sample-mean-heights-n-5",
    "title": "Day 8: Variability in estimates",
    "section": "Distribution of 100 sample mean heights (n = 5)",
    "text": "Distribution of 100 sample mean heights (n = 5)\n\n\nDescribe the distribution shape.\n\nggplot(\n  means_hght_samp_n5_rep100, \n  aes(x = mean_height)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nCalculate the mean and SD of the 100 mean heights from the 100 samples:\n\nstats_means_hght_samp_n5_rep100 &lt;- \n  means_hght_samp_n5_rep100 %&gt;% \n  summarise(\n   mean_mean_height = mean(mean_height),\n   sd_mean_height = sd(mean_height)\n   )\nstats_means_hght_samp_n5_rep100\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.58          0.150\n\n\nIs the mean of the means close to the “center” of the distribution?"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#random-samples-of-size-n-5-from-yrbss2-1",
    "href": "slides_md/Day08_bsta511_md.html#random-samples-of-size-n-5-from-yrbss2-1",
    "title": "Day 8: Variability in estimates",
    "section": "10,000 random samples of size n = 5 from yrbss2",
    "text": "10,000 random samples of size n = 5 from yrbss2\n\n\nTake 10,000 random samples of size\nn = 5 from yrbss2:\n\nsamp_n5_rep10000 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 5, \n               reps = 10000,\n               replace = FALSE)\nsamp_n5_rep10000\n\n# A tibble: 50,000 × 4\n# Groups:   replicate [10,000]\n   replicate    id height.ft weight.lb\n       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1         1  6383      5.35      126.\n 2         1  4019      5.41      107.\n 3         1  4856      5.25      135.\n 4         1  9988      5.58      120.\n 5         1  2245      6.17      270.\n 6         2 10580      5.68      155.\n 7         2  2254      5.84      159.\n 8         2  8081      5.09      110.\n 9         2 10194      5.35      115.\n10         2  7689      5.35      135.\n# ℹ 49,990 more rows\n\n\n\nCalculate the mean for each of the 10,000 random samples:\n\nmeans_hght_samp_n5_rep10000 &lt;- \n  samp_n5_rep10000 %&gt;% \n  group_by(replicate) %&gt;% \n  summarise(\n    mean_height = mean(height.ft))\n\nmeans_hght_samp_n5_rep10000\n\n# A tibble: 10,000 × 2\n   replicate mean_height\n       &lt;int&gt;       &lt;dbl&gt;\n 1         1        5.55\n 2         2        5.46\n 3         3        5.49\n 4         4        5.60\n 5         5        5.47\n 6         6        5.83\n 7         7        5.68\n 8         8        5.47\n 9         9        5.37\n10        10        5.15\n# ℹ 9,990 more rows\n\n\n\n\nHow close are the mean heights for each of the 10,000 random samples?"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#distribution-of-10000-sample-mean-heights-n-5",
    "href": "slides_md/Day08_bsta511_md.html#distribution-of-10000-sample-mean-heights-n-5",
    "title": "Day 8: Variability in estimates",
    "section": "Distribution of 10,000 sample mean heights (n = 5)",
    "text": "Distribution of 10,000 sample mean heights (n = 5)\n\n\nDescribe the distribution shape.\n\nggplot(\n  means_hght_samp_n5_rep10000, \n  aes(x = mean_height)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nCalculate the mean and SD of the 10,000 mean heights from the 10,000 samples:\n\nstats_means_hght_samp_n5_rep10000 &lt;- \n  means_hght_samp_n5_rep10000 %&gt;% \n  summarise(\n   mean_mean_height=mean(mean_height),\n   sd_mean_height = sd(mean_height)\n   )\nstats_means_hght_samp_n5_rep10000\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.55          0.153\n\n\nIs the mean of the means close to the “center” of the distribution?"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#samples-of-size-n-30-from-yrbss2",
    "href": "slides_md/Day08_bsta511_md.html#samples-of-size-n-30-from-yrbss2",
    "title": "Day 8: Variability in estimates",
    "section": "10,000 samples of size n = 30 from yrbss2",
    "text": "10,000 samples of size n = 30 from yrbss2\n\n\nTake 10,000 random samples of size\nn = 30 from yrbss2:\n\nsamp_n30_rep10000 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 30, \n               reps = 10000,\n               replace = FALSE)\nsamp_n30_rep10000\n\n# A tibble: 300,000 × 4\n# Groups:   replicate [10,000]\n   replicate    id height.ft weight.lb\n       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1         1  3871      5.25      115.\n 2         1 12090      5.15      125.\n 3         1   241      5.58      119.\n 4         1  4570      5.58      140.\n 5         1  4131      5.35      143.\n 6         1 11513      5.35      135.\n 7         1  9663      5.25      125.\n 8         1  3789      5.25      160.\n 9         1   442      5.15      130.\n10         1 11528      5.51      200.\n# ℹ 299,990 more rows\n\n\n\nCalculate the mean for each of the 10,000 random samples:\n\nmeans_hght_samp_n30_rep10000 &lt;- \n  samp_n30_rep10000 %&gt;% \n  group_by(replicate) %&gt;% \n  summarise(mean_height = \n            mean(height.ft))\n\nmeans_hght_samp_n30_rep10000\n\n# A tibble: 10,000 × 2\n   replicate mean_height\n       &lt;int&gt;       &lt;dbl&gt;\n 1         1        5.48\n 2         2        5.63\n 3         3        5.46\n 4         4        5.46\n 5         5        5.51\n 6         6        5.54\n 7         7        5.56\n 8         8        5.51\n 9         9        5.51\n10        10        5.50\n# ℹ 9,990 more rows\n\n\n\n\nHow close are the mean heights for each of the 10,000 random samples?"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#distribution-of-10000-sample-mean-heights-n-30",
    "href": "slides_md/Day08_bsta511_md.html#distribution-of-10000-sample-mean-heights-n-30",
    "title": "Day 8: Variability in estimates",
    "section": "Distribution of 10,000 sample mean heights (n = 30)",
    "text": "Distribution of 10,000 sample mean heights (n = 30)\n\n\nDescribe the distribution shape.\n\nggplot(\n  means_hght_samp_n30_rep10000, \n  aes(x = mean_height)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nCalculate the mean and SD of the 10,000 mean heights from the 10,000 samples:\n\nstats_means_hght_samp_n30_rep10000&lt;- \n  means_hght_samp_n30_rep10000 %&gt;% \n  summarise(\n   mean_mean_height=mean(mean_height),\n   sd_mean_height = sd(mean_height)\n   )\nstats_means_hght_samp_n30_rep10000\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.55         0.0623\n\n\nIs the mean of the means close to the “center” of the distribution?"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#compare-distributions-of-10000-sample-mean-heights-when-n-5-left-vs.-n-30-right",
    "href": "slides_md/Day08_bsta511_md.html#compare-distributions-of-10000-sample-mean-heights-when-n-5-left-vs.-n-30-right",
    "title": "Day 8: Variability in estimates",
    "section": "Compare distributions of 10,000 sample mean heights when n = 5 (left) vs. n = 30 (right)",
    "text": "Compare distributions of 10,000 sample mean heights when n = 5 (left) vs. n = 30 (right)\nHow are the center, shape, and spread similar and/or different?\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.55          0.153\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.55         0.0623"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#sampling-high-schoolers-weights",
    "href": "slides_md/Day08_bsta511_md.html#sampling-high-schoolers-weights",
    "title": "Day 8: Variability in estimates",
    "section": "Sampling high schoolers’ weights",
    "text": "Sampling high schoolers’ weights\n\n\nWhich figure is which?\n\nPopulation distribution of weights\nSampling distribution of mean weights when \\(n=5\\)\nSampling distribution of mean weights when \\(n=30\\).\n\n\n\n\n\nA\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nB\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nC\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#the-sampling-distribution-of-the-mean",
    "href": "slides_md/Day08_bsta511_md.html#the-sampling-distribution-of-the-mean",
    "title": "Day 8: Variability in estimates",
    "section": "The sampling distribution of the mean",
    "text": "The sampling distribution of the mean\n\n\n\nThe sampling distribution of the mean is the distribution of sample means calculated from repeated random samples of the same size from the same population\nOur simulations show approximations of the sampling distribution of the mean for various sample sizes\nThe theoretical sampling distribution is based on all possible samples of a given sample size \\(n\\).\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`)."
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#the-central-limit-theorem-clt",
    "href": "slides_md/Day08_bsta511_md.html#the-central-limit-theorem-clt",
    "title": "Day 8: Variability in estimates",
    "section": "The Central Limit Theorem (CLT)",
    "text": "The Central Limit Theorem (CLT)\n\nFor “large” sample sizes ( \\(n\\geq 30\\) ),\n\nthe sampling distribution of the sample mean\ncan be approximated by a normal distribution,with\n\nmean equal to the population mean value \\(\\mu\\), and\nstandard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\n\n\n\n  \n\nFor small sample sizes, if the population is known to be normally distributed, then\n\nthe sampling distribution of the sample mean\nis a normal distribution, with\n\nmean equal to the population mean value \\(\\mu\\), and\nstandard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\)"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#the-cutest-statistics-video-on-youtube",
    "href": "slides_md/Day08_bsta511_md.html#the-cutest-statistics-video-on-youtube",
    "title": "Day 8: Variability in estimates",
    "section": "The cutest statistics video on YouTube",
    "text": "The cutest statistics video on YouTube\n\nBunnies, Dragons and the ‘Normal’ World: Central Limit Theorem\n\nCreature Cast from the New York Times\nhttps://www.youtube.com/watch?v=jvoxEYmQHNM&feature=youtu.be"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#sampling-distribution-of-mean-heights-when-n-30-12",
    "href": "slides_md/Day08_bsta511_md.html#sampling-distribution-of-mean-heights-when-n-30-12",
    "title": "Day 8: Variability in estimates",
    "section": "Sampling distribution of mean heights when n = 30 (1/2)",
    "text": "Sampling distribution of mean heights when n = 30 (1/2)\n\n\n\nggplot(\n  means_hght_samp_n30_rep10000, \n  aes(x = mean_height)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nCLT tells us that we can model the sampling distribution of mean heights using a normal distribution."
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#sampling-distribution-of-mean-heights-when-n-30-22",
    "href": "slides_md/Day08_bsta511_md.html#sampling-distribution-of-mean-heights-when-n-30-22",
    "title": "Day 8: Variability in estimates",
    "section": "Sampling distribution of mean heights when n = 30 (2/2)",
    "text": "Sampling distribution of mean heights when n = 30 (2/2)\n\n\nMean and SD of population:\n\n(mean_height.ft &lt;- mean(yrbss2$height.ft))\n\n[1] 5.548691\n\n(sd_height.ft &lt;- sd(yrbss2$height.ft))\n\n[1] 0.3434949\n\nsd_height.ft/sqrt(30)\n\n[1] 0.06271331\n\n\nMean and SD of simulated sampling distribution:\n\nstats_means_hght_samp_n30_rep10000&lt;- \n  means_hght_samp_n30_rep10000 %&gt;% \n  summarise(\n   mean_mean_height=mean(mean_height),\n   sd_mean_height = sd(mean_height)\n   )\nstats_means_hght_samp_n30_rep10000\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.55         0.0623"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#why-is-the-mean-mu-the-standard-error-fracsigmasqrtn",
    "href": "slides_md/Day08_bsta511_md.html#why-is-the-mean-mu-the-standard-error-fracsigmasqrtn",
    "title": "Day 8: Variability in estimates",
    "section": "Why is the mean \\(\\mu\\) & the standard error \\(\\frac{\\sigma}{\\sqrt{n}}\\) ?",
    "text": "Why is the mean \\(\\mu\\) & the standard error \\(\\frac{\\sigma}{\\sqrt{n}}\\) ?"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#applying-the-clt",
    "href": "slides_md/Day08_bsta511_md.html#applying-the-clt",
    "title": "Day 8: Variability in estimates",
    "section": "Applying the CLT",
    "text": "Applying the CLT\nWhat is the probability that for a random sample of 30 high schoolers, that their mean height is greater than 5.6 ft?"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#class-discussion",
    "href": "slides_md/Day08_bsta511_md.html#class-discussion",
    "title": "Day 8: Variability in estimates",
    "section": "Class Discussion",
    "text": "Class Discussion\n\nSlide 21: match figures to distribution (Sampling high schoolers’ weights)\n\nProblems from Homework 4:\n\nR1: Youth weights (YRBSS)\nBook exercise: 4.2\nNon-book exercise: Ethan Allen"
  },
  {
    "objectID": "slides_md/Day08_bsta511_md.html#footnotes",
    "href": "slides_md/Day08_bsta511_md.html#footnotes",
    "title": "Day 8: Variability in estimates",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYouth Risk Behavior Surveillance System https://www.cdc.gov/healthyyouth/data/yrbs/index.htm (YRBSS)↩︎"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html",
    "href": "slides_code/Day08_bsta511_code.html",
    "title": "Day 8: Variability in estimates",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(ggridges)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive) # NEW!!\n\nset.seed(456)\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#load-packages",
    "href": "slides_code/Day08_bsta511_code.html#load-packages",
    "title": "Day 8: Variability in estimates",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(ggridges)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive) # NEW!!\n\nset.seed(456)\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#moritzs-tip-of-the-day-add-a-code-pane-in-rstudio",
    "href": "slides_code/Day08_bsta511_code.html#moritzs-tip-of-the-day-add-a-code-pane-in-rstudio",
    "title": "Day 8: Variability in estimates",
    "section": "MoRitz’s tip of the day: add a code pane in RStudio",
    "text": "MoRitz’s tip of the day: add a code pane in RStudio\nDo you want to be able to view two code files side-by-side?\nYou can do that by adding a column to the RStudio layout.\nSee https://posit.co/blog/rstudio-1-4-preview-multiple-source-columns/ for more information."
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#getting-to-know-the-dataset-glimpse",
    "href": "slides_code/Day08_bsta511_code.html#getting-to-know-the-dataset-glimpse",
    "title": "Day 8: Variability in estimates",
    "section": "Getting to know the dataset: glimpse()",
    "text": "Getting to know the dataset: glimpse()\n\nglimpse(yrbss)  # from tidyverse package (dplyr)\n\nRows: 13,583\nColumns: 13\n$ age                      &lt;int&gt; 14, 14, 15, 15, 15, 15, 15, 14, 15, 15, 15, 1…\n$ gender                   &lt;chr&gt; \"female\", \"female\", \"female\", \"female\", \"fema…\n$ grade                    &lt;chr&gt; \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", \"9\", …\n$ hispanic                 &lt;chr&gt; \"not\", \"not\", \"hispanic\", \"not\", \"not\", \"not\"…\n$ race                     &lt;chr&gt; \"Black or African American\", \"Black or Africa…\n$ height                   &lt;dbl&gt; NA, NA, 1.73, 1.60, 1.50, 1.57, 1.65, 1.88, 1…\n$ weight                   &lt;dbl&gt; NA, NA, 84.37, 55.79, 46.72, 67.13, 131.54, 7…\n$ helmet.12m               &lt;chr&gt; \"never\", \"never\", \"never\", \"never\", \"did not …\n$ text.while.driving.30d   &lt;chr&gt; \"0\", NA, \"30\", \"0\", \"did not drive\", \"did not…\n$ physically.active.7d     &lt;int&gt; 4, 2, 7, 0, 2, 1, 4, 4, 5, 0, 0, 0, 4, 7, 7, …\n$ hours.tv.per.school.day  &lt;chr&gt; \"5+\", \"5+\", \"5+\", \"2\", \"3\", \"5+\", \"5+\", \"5+\",…\n$ strength.training.7d     &lt;int&gt; 0, 0, 0, 0, 1, 0, 2, 0, 3, 0, 3, 0, 0, 7, 7, …\n$ school.night.hours.sleep &lt;chr&gt; \"8\", \"6\", \"&lt;5\", \"6\", \"9\", \"8\", \"9\", \"6\", \"&lt;5\"…"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#height-weight-variables",
    "href": "slides_code/Day08_bsta511_code.html#height-weight-variables",
    "title": "Day 8: Variability in estimates",
    "section": "Height & weight variables",
    "text": "Height & weight variables\n\nyrbss %&gt;% \n  select(height, weight) %&gt;% \n  summary()\n\n     height          weight      \n Min.   :1.270   Min.   : 29.94  \n 1st Qu.:1.600   1st Qu.: 56.25  \n Median :1.680   Median : 64.41  \n Mean   :1.691   Mean   : 67.91  \n 3rd Qu.:1.780   3rd Qu.: 76.20  \n Max.   :2.110   Max.   :180.99  \n NA's   :1004    NA's   :1004    \n\n\n\nggplot(data = yrbss, \n       aes(x = height)) +\n  geom_histogram() #&lt;&lt;\n\nWarning: Removed 1004 rows containing non-finite values (`stat_bin()`)."
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#transform-height-weight-from-metric-to-to-standard",
    "href": "slides_code/Day08_bsta511_code.html#transform-height-weight-from-metric-to-to-standard",
    "title": "Day 8: Variability in estimates",
    "section": "Transform height & weight from metric to to standard",
    "text": "Transform height & weight from metric to to standard\nAlso, drop missing values and add a column of id values\n\nyrbss2 &lt;- yrbss %&gt;%                 # save new dataset with new name\n  mutate(                           # add variables for \n    height.ft = 3.28084*height,     #     height in feet\n    weight.lb = 2.20462*weight      #     weight in pounds\n  ) %&gt;% \n  drop_na(height.ft, weight.lb) %&gt;% # drop rows w/ missing height/weight values\n  mutate(id = 1:nrow(.)) %&gt;%        # add id column\n  select(id, height.ft, weight.lb)  # restrict dataset to columns of interest\n\nhead(yrbss2)  \n\n  id height.ft weight.lb\n1  1  5.675853  186.0038\n2  2  5.249344  122.9957\n3  3  4.921260  102.9998\n4  4  5.150919  147.9961\n5  5  5.413386  289.9957\n6  6  6.167979  157.0130\n\ndim(yrbss2)\n\n[1] 12579     3\n\n# number of rows deleted that had missing values for height and/or weight:\nnrow(yrbss) - nrow(yrbss2) \n\n[1] 1004"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#yrbss2-summary",
    "href": "slides_code/Day08_bsta511_code.html#yrbss2-summary",
    "title": "Day 8: Variability in estimates",
    "section": "yrbss2 summary",
    "text": "yrbss2 summary\n\nsummary(yrbss2)\n\n       id          height.ft       weight.lb     \n Min.   :    1   Min.   :4.167   Min.   : 66.01  \n 1st Qu.: 3146   1st Qu.:5.249   1st Qu.:124.01  \n Median : 6290   Median :5.512   Median :142.00  \n Mean   : 6290   Mean   :5.549   Mean   :149.71  \n 3rd Qu.: 9434   3rd Qu.:5.840   3rd Qu.:167.99  \n Max.   :12579   Max.   :6.923   Max.   :399.01  \n\n\nAnother summary:\n\nyrbss2 %&gt;% \n  get_summary_stats(type = \"mean_sd\") %&gt;% \n  kable()\n\n\n\n\nvariable\nn\nmean\nsd\n\n\n\n\nid\n12579\n6290.000\n3631.389\n\n\nheight.ft\n12579\n5.549\n0.343\n\n\nweight.lb\n12579\n149.708\n37.254"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#random-sample-of-size-n-5-from-yrbss2",
    "href": "slides_code/Day08_bsta511_code.html#random-sample-of-size-n-5-from-yrbss2",
    "title": "Day 8: Variability in estimates",
    "section": "Random sample of size n = 5 from yrbss2",
    "text": "Random sample of size n = 5 from yrbss2\nTake a random sample of size n = 5 from yrbss2:\n\nlibrary(moderndive)\nsamp_n5_rep1 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 5, \n               reps = 1,\n               replace = FALSE)\nsamp_n5_rep1\n\n# A tibble: 5 × 4\n# Groups:   replicate [1]\n  replicate    id height.ft weight.lb\n      &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1         1  5869      5.15      145.\n2         1  6694      5.41      127.\n3         1  2517      5.74      130.\n4         1  5372      6.07      180.\n5         1  5403      6.07      163.\n\n\nCalculate the mean of the random sample:\n\nmeans_hght_samp_n5_rep1 &lt;- \n  samp_n5_rep1 %&gt;% \n  summarise(mean_height = mean(height.ft))\n\nmeans_hght_samp_n5_rep1\n\n# A tibble: 1 × 2\n  replicate mean_height\n      &lt;int&gt;       &lt;dbl&gt;\n1         1        5.69\n\n\nWould we get the same mean height if we took another sample?"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#random-samples-of-size-n-5-from-yrbss2",
    "href": "slides_code/Day08_bsta511_code.html#random-samples-of-size-n-5-from-yrbss2",
    "title": "Day 8: Variability in estimates",
    "section": "100 random samples of size n = 5 from yrbss2",
    "text": "100 random samples of size n = 5 from yrbss2\nTake 100 random samples of size n = 5 from yrbss2:\n\nsamp_n5_rep100 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 5, \n               reps = 100,\n               replace = FALSE)\nsamp_n5_rep100\n\n# A tibble: 500 × 4\n# Groups:   replicate [100]\n   replicate    id height.ft weight.lb\n       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1         1  6483      5.51     145. \n 2         1  9899      4.92      90.0\n 3         1  6103      5.68     118. \n 4         1  2702      5.68     150. \n 5         1 11789      5.35     115. \n 6         2 10164      5.51     140. \n 7         2  5807      5.41     215. \n 8         2  9382      5.15      98.0\n 9         2  4904      6.00     196. \n10         2   229      6.07     101. \n# ℹ 490 more rows\n\n\nCalculate the mean for each of the 100 random samples:\n\nmeans_hght_samp_n5_rep100 &lt;- \n  samp_n5_rep100 %&gt;% \n  group_by(replicate) %&gt;% \n  summarise(mean_height = mean(height.ft))\n\nmeans_hght_samp_n5_rep100\n\n# A tibble: 100 × 2\n   replicate mean_height\n       &lt;int&gt;       &lt;dbl&gt;\n 1         1        5.43\n 2         2        5.63\n 3         3        5.34\n 4         4        5.70\n 5         5        5.90\n 6         6        5.37\n 7         7        5.49\n 8         8        5.60\n 9         9        5.50\n10        10        5.68\n# ℹ 90 more rows\n\n\nHow close are the mean heights for each of the 100 random samples?"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#distribution-of-100-sample-mean-heights-n-5",
    "href": "slides_code/Day08_bsta511_code.html#distribution-of-100-sample-mean-heights-n-5",
    "title": "Day 8: Variability in estimates",
    "section": "Distribution of 100 sample mean heights (n = 5)",
    "text": "Distribution of 100 sample mean heights (n = 5)\nDescribe the distribution shape.\n\nggplot(\n  means_hght_samp_n5_rep100, \n  aes(x = mean_height)) + \n  geom_histogram()\n\n\n\n\nCalculate the mean and SD of the 100 mean heights from the 100 samples:\n\nstats_means_hght_samp_n5_rep100 &lt;- \n  means_hght_samp_n5_rep100 %&gt;% \n  summarise(\n   mean_mean_height = mean(mean_height),\n   sd_mean_height = sd(mean_height)\n   )\nstats_means_hght_samp_n5_rep100\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.58          0.150\n\n\nIs the mean of the means close to the “center” of the distribution?"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#samples-of-size-n-5-from-yrbss2",
    "href": "slides_code/Day08_bsta511_code.html#samples-of-size-n-5-from-yrbss2",
    "title": "Day 8: Variability in estimates",
    "section": "10,000 samples of size n = 5 from yrbss2",
    "text": "10,000 samples of size n = 5 from yrbss2\nTake 10,000 random samples of size\nn = 5 from yrbss2:\n\nsamp_n5_rep10000 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 5, \n               reps = 10000,\n               replace = FALSE)\nsamp_n5_rep10000\n\n# A tibble: 50,000 × 4\n# Groups:   replicate [10,000]\n   replicate    id height.ft weight.lb\n       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1         1  6383      5.35      126.\n 2         1  4019      5.41      107.\n 3         1  4856      5.25      135.\n 4         1  9988      5.58      120.\n 5         1  2245      6.17      270.\n 6         2 10580      5.68      155.\n 7         2  2254      5.84      159.\n 8         2  8081      5.09      110.\n 9         2 10194      5.35      115.\n10         2  7689      5.35      135.\n# ℹ 49,990 more rows\n\n\nCalculate the mean for each of the 10,000 random samples:\n\nmeans_hght_samp_n5_rep10000 &lt;- \n  samp_n5_rep10000 %&gt;% \n  group_by(replicate) %&gt;% \n  summarise(mean_height = \n            mean(height.ft))\n\nmeans_hght_samp_n5_rep10000\n\n# A tibble: 10,000 × 2\n   replicate mean_height\n       &lt;int&gt;       &lt;dbl&gt;\n 1         1        5.55\n 2         2        5.46\n 3         3        5.49\n 4         4        5.60\n 5         5        5.47\n 6         6        5.83\n 7         7        5.68\n 8         8        5.47\n 9         9        5.37\n10        10        5.15\n# ℹ 9,990 more rows\n\n\nHow close are the mean heights for each of the 10,000 random samples?"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#distribution-of-10000-sample-mean-heights-n-5",
    "href": "slides_code/Day08_bsta511_code.html#distribution-of-10000-sample-mean-heights-n-5",
    "title": "Day 8: Variability in estimates",
    "section": "Distribution of 10,000 sample mean heights (n = 5)",
    "text": "Distribution of 10,000 sample mean heights (n = 5)\nDescribe the distribution shape.\n\nggplot(\n  means_hght_samp_n5_rep10000, \n  aes(x = mean_height)) + \n  geom_histogram()\n\n\n\n\nCalculate the mean and SD of the 10,000 mean heights from the 10,000 samples:\n\nstats_means_hght_samp_n5_rep10000 &lt;- \n  means_hght_samp_n5_rep10000 %&gt;% \n  summarise(\n   mean_mean_height=mean(mean_height),\n   sd_mean_height = sd(mean_height)\n   )\nstats_means_hght_samp_n5_rep10000\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.55          0.153\n\n\nIs the mean of the means close to the “center” of the distribution?"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#samples-of-size-n-30-from-yrbss2",
    "href": "slides_code/Day08_bsta511_code.html#samples-of-size-n-30-from-yrbss2",
    "title": "Day 8: Variability in estimates",
    "section": "10,000 samples of size n = 30 from yrbss2",
    "text": "10,000 samples of size n = 30 from yrbss2\nTake 10,000 random samples of size\nn = 30 from yrbss2:\n\nsamp_n30_rep10000 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 30, \n               reps = 10000,\n               replace = FALSE)\nsamp_n30_rep10000\n\n# A tibble: 300,000 × 4\n# Groups:   replicate [10,000]\n   replicate    id height.ft weight.lb\n       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1         1  3871      5.25      115.\n 2         1 12090      5.15      125.\n 3         1   241      5.58      119.\n 4         1  4570      5.58      140.\n 5         1  4131      5.35      143.\n 6         1 11513      5.35      135.\n 7         1  9663      5.25      125.\n 8         1  3789      5.25      160.\n 9         1   442      5.15      130.\n10         1 11528      5.51      200.\n# ℹ 299,990 more rows\n\n\nCalculate the mean for each of the 10,000 random samples:\n\nmeans_hght_samp_n30_rep10000 &lt;- \n  samp_n30_rep10000 %&gt;% \n  group_by(replicate) %&gt;% \n  summarise(mean_height = \n            mean(height.ft))\n\nmeans_hght_samp_n30_rep10000\n\n# A tibble: 10,000 × 2\n   replicate mean_height\n       &lt;int&gt;       &lt;dbl&gt;\n 1         1        5.48\n 2         2        5.63\n 3         3        5.46\n 4         4        5.46\n 5         5        5.51\n 6         6        5.54\n 7         7        5.56\n 8         8        5.51\n 9         9        5.51\n10        10        5.50\n# ℹ 9,990 more rows\n\n\nHow close are the mean heights for each of the 10,000 random samples?"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#distribution-of-10000-sample-mean-heights-n-30",
    "href": "slides_code/Day08_bsta511_code.html#distribution-of-10000-sample-mean-heights-n-30",
    "title": "Day 8: Variability in estimates",
    "section": "Distribution of 10,000 sample mean heights (n = 30)",
    "text": "Distribution of 10,000 sample mean heights (n = 30)\nDescribe the distribution shape.\n\nggplot(\n  means_hght_samp_n30_rep10000, \n  aes(x = mean_height)) + \n  geom_histogram()\n\n\n\n\nCalculate the mean and SD of the 10,000 mean heights from the 10,000 samples:\n\nstats_means_hght_samp_n30_rep10000&lt;- \n  means_hght_samp_n30_rep10000 %&gt;% \n  summarise(\n   mean_mean_height=mean(mean_height),\n   sd_mean_height = sd(mean_height)\n   )\nstats_means_hght_samp_n30_rep10000\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.55         0.0623\n\n\nIs the mean of the means close to the “center” of the distribution?"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#compare-distributions-of-10000-sample-mean-heights-when-n-5-left-vs.-n-30-right",
    "href": "slides_code/Day08_bsta511_code.html#compare-distributions-of-10000-sample-mean-heights-when-n-5-left-vs.-n-30-right",
    "title": "Day 8: Variability in estimates",
    "section": "Compare distributions of 10,000 sample mean heights when n = 5 (left) vs. n = 30 (right)",
    "text": "Compare distributions of 10,000 sample mean heights when n = 5 (left) vs. n = 30 (right)\nHow are the center, shape, and spread similar and/or different?\n\n\n\n\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.55          0.153\n\n\n\n\n\n\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.55         0.0623"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#sampling-high-schoolers-weights",
    "href": "slides_code/Day08_bsta511_code.html#sampling-high-schoolers-weights",
    "title": "Day 8: Variability in estimates",
    "section": "Sampling high schoolers’ weights",
    "text": "Sampling high schoolers’ weights\nWhich figure is which?\n\nPopulation distribution of weights\nSampling distribution of mean weights when \\(n=5\\)\nSampling distribution of mean weights when \\(n=30\\).\n\nA\n\n\n\n\n\nB\n\n\n\n\n\nC"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#the-sampling-distribution-of-the-mean",
    "href": "slides_code/Day08_bsta511_code.html#the-sampling-distribution-of-the-mean",
    "title": "Day 8: Variability in estimates",
    "section": "The sampling distribution of the mean",
    "text": "The sampling distribution of the mean\n\nThe sampling distribution of the mean is the distribution of sample means calculated from repeated random samples of the same size from the same population\nOur simulations show approximations of the sampling distribution of the mean for various sample sizes\nThe theoretical sampling distribution is based on all possible samples of a given sample size \\(n\\).\n\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`)."
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#the-central-limit-theorem-clt",
    "href": "slides_code/Day08_bsta511_code.html#the-central-limit-theorem-clt",
    "title": "Day 8: Variability in estimates",
    "section": "The Central Limit Theorem (CLT)",
    "text": "The Central Limit Theorem (CLT)\n\nFor “large” sample sizes ( \\(n\\geq 30\\) ),\n\nthe sampling distribution of the sample mean\ncan be approximated by a normal distribution,with\n\nmean equal to the population mean value \\(\\mu\\), and\nstandard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\n\nFor small sample sizes, if the population is known to be normally distributed, then\n\nthe sampling distribution of the sample mean\nis a normal distribution, with\n\nmean equal to the population mean value \\(\\mu\\), and\nstandard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\)"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#the-cutest-statistics-video-on-youtube",
    "href": "slides_code/Day08_bsta511_code.html#the-cutest-statistics-video-on-youtube",
    "title": "Day 8: Variability in estimates",
    "section": "The cutest statistics video on YouTube",
    "text": "The cutest statistics video on YouTube\n\nBunnies, Dragons and the ‘Normal’ World: Central Limit Theorem\n\nCreature Cast from the New York Times\nhttps://www.youtube.com/watch?v=jvoxEYmQHNM&feature=youtu.be"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#sampling-distribution-of-mean-heights-when-n-30",
    "href": "slides_code/Day08_bsta511_code.html#sampling-distribution-of-mean-heights-when-n-30",
    "title": "Day 8: Variability in estimates",
    "section": "Sampling distribution of mean heights when n = 30",
    "text": "Sampling distribution of mean heights when n = 30\n\nggplot(\n  means_hght_samp_n30_rep10000, \n  aes(x = mean_height)) + \n  geom_histogram()\n\n\n\n\nCLT tells us that we can model the sampling distribution of mean heights using a normal distribution.\n\nPlot of theoretical sampling distribution\n\nmu &lt;- 5.55\nSE &lt;- 0.34/sqrt(30)\nsig &lt;- round(SE, 2)\n\ngg_samp_dist_heights &lt;- ggplot(data.frame(x = c(mu-4*sig, mu+4*sig)), aes(x = x)) + \n  stat_function(fun = dnorm, \n                args = list(mean = mu, sd = sig)) + \n  stat_function(fun = dnorm, \n                args = list(mean = mu, sd = sig), \n                xlim = c(mu-4*sig, mu-2*sig), \n                geom = \"area\", fill = \"darkblue\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mu, sd = sig), \n                xlim = c(mu+2*sig, mu+4*sig), \n                geom = \"area\", fill = \"darkblue\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mu, sd = sig), \n                xlim = c(mu-2*sig, mu-1*sig), \n                geom = \"area\", fill = \"darkgreen\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mu, sd = sig), \n                xlim = c(mu+1*sig, mu+2*sig), \n                geom = \"area\", fill = \"darkgreen\")+\n  stat_function(fun = dnorm, \n                args = list(mean = mu, sd = sig), \n                xlim = c(mu-1*sig, mu+1*sig), \n                geom = \"area\", fill = \"orange\") +\n  scale_x_continuous(name =\"mean height (ft)\", \n                    breaks=c(mu-2*sig,mu-1*sig,mu, mu+1*sig, mu+2*sig)) +\n  labs(title = \"Sampling distribution\", y = \"\") +\n  scale_y_continuous(labels = NULL, breaks = NULL)\n\ngg_samp_dist_heights   \n\n\n\n\n\n\nMean and SD of population\n\n(mean_height.ft&lt;-mean(yrbss2$height.ft))\n\n[1] 5.548691\n\n(sd_height.ft &lt;- sd(yrbss2$height.ft))\n\n[1] 0.3434949\n\nsd_height.ft/sqrt(30)\n\n[1] 0.06271331\n\n\n\n\nMean and SD of simulated sampling distribution\n\nstats_means_hght_samp_n30_rep10000&lt;- \n  means_hght_samp_n30_rep10000 %&gt;% \n  summarise(\n   mean_mean_height=mean(mean_height),\n   sd_mean_height = sd(mean_height)\n   )\nstats_means_hght_samp_n30_rep10000\n\n# A tibble: 1 × 2\n  mean_mean_height sd_mean_height\n             &lt;dbl&gt;          &lt;dbl&gt;\n1             5.55         0.0623"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#why-is-the-mean-mu-the-standard-error-fracsigmasqrtn",
    "href": "slides_code/Day08_bsta511_code.html#why-is-the-mean-mu-the-standard-error-fracsigmasqrtn",
    "title": "Day 8: Variability in estimates",
    "section": "Why is the mean \\(\\mu\\) & the standard error \\(\\frac{\\sigma}{\\sqrt{n}}\\) ?",
    "text": "Why is the mean \\(\\mu\\) & the standard error \\(\\frac{\\sigma}{\\sqrt{n}}\\) ?"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#applying-the-clt",
    "href": "slides_code/Day08_bsta511_code.html#applying-the-clt",
    "title": "Day 8: Variability in estimates",
    "section": "Applying the CLT",
    "text": "Applying the CLT\nWhat is the probability that for a random sample of 30 high schoolers, that their mean height is greater than 5.6 ft?"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#class-discussion",
    "href": "slides_code/Day08_bsta511_code.html#class-discussion",
    "title": "Day 8: Variability in estimates",
    "section": "Class Discussion",
    "text": "Class Discussion\n\nSlide 21: match figures to distribution (Sampling high schoolers’ weights)\n\nProblems from Homework 4:\n\nR1: Youth weights (YRBSS)\nBook exercise: 4.2\nNon-book exercise: Ethan Allen"
  },
  {
    "objectID": "slides_code/Day08_bsta511_code.html#footnotes",
    "href": "slides_code/Day08_bsta511_code.html#footnotes",
    "title": "Day 8: Variability in estimates",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYouth Risk Behavior Surveillance System https://www.cdc.gov/healthyyouth/data/yrbs/index.htm (YRBSS)↩︎"
  },
  {
    "objectID": "homework/HW_4_F24_bsta511.html",
    "href": "homework/HW_4_F24_bsta511.html",
    "title": "HW 4: BSTA 511/611 F24",
    "section": "",
    "text": "Due 11/9/24\nDownload the .qmd file for this assignment from https://github.com/niederhausen/BSTA_511_F24/blob/main/homework/HW_4_F24_bsta511.qmd"
  },
  {
    "objectID": "homework/HW_4_F24_bsta511.html#graded-exercises",
    "href": "homework/HW_4_F24_bsta511.html#graded-exercises",
    "title": "HW 4: BSTA 511/611 F24",
    "section": "Graded exercises",
    "text": "Graded exercises\nThe exercises listed below will be graded for this assignment. You are strongly encouraged to complete the entire assignment. You will receive feedback on exercises you turn in that are not being graded.\n\nBook exercises\n\n5.6, 5.12\n\nNon-book exercise\n\nNB1: The Ethan Allen\n\nR exercises\n\nR1: Youth weights - Part 1\nR2: Youth weights - Part 2\nR3: Swim times"
  },
  {
    "objectID": "homework/HW_4_F24_bsta511.html#directions",
    "href": "homework/HW_4_F24_bsta511.html#directions",
    "title": "HW 4: BSTA 511/611 F24",
    "section": "Directions",
    "text": "Directions\n\n\n\n\n\n\nImportant\n\n\n\n\nComplete all exercises in this assignment using Quarto.\nI highly recommend using LaTeX to format equations.\n\nSee the .qmd files from class notes for LaTeX code to make it easier to show your work in computations.\nFor instructions on creating equations in the Visual editor, check out https://quarto.org/docs/get-started/authoring/rstudio.html#equations. html\nAlso check out examples of LaTeX formatting for statistics created by recent biostats alum Ariel Weingarten.\nIf you have difficulty rendering the LaTeX equations, I recommend installing and running the R package tinytex. See this website for instructions.\n\n\n\n\n\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file.\n\nUse the assignment .qmd file linked to above as a template for your own assignment.\n\nPlease always use the following naming convention for submitting your files:\n\nLastname_Firstname_HWx.qmd, such as Niederhausen_Meike_HW2.qmd\nLastname_Firstname_HWx.html, such as Niederhausen_Meike_HW2.html\n\nFor each question, make sure to show all of your work.\n\nThis includes all code and resulting output in the html file to support your answers for exercises requiring work done in R (including any arithmetic calculations).\nFor non-calculation questions, this includes an explanation of your answer (why did you choose your answer?).\n\nFor each question, include a sentence summarizing the answer for that question in the context of the research question.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your Qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_4_F24_bsta511.html#heights-of-adults",
    "href": "homework/HW_4_F24_bsta511.html#heights-of-adults",
    "title": "HW 4: BSTA 511/611 F24",
    "section": "4.2 Heights of adults",
    "text": "4.2 Heights of adults"
  },
  {
    "objectID": "homework/HW_4_F24_bsta511.html#mental-health-part-i",
    "href": "homework/HW_4_F24_bsta511.html#mental-health-part-i",
    "title": "HW 4: BSTA 511/611 F24",
    "section": "4.4 Mental health, Part I",
    "text": "4.4 Mental health, Part I"
  },
  {
    "objectID": "homework/HW_4_F24_bsta511.html#thanksgiving-spending-part-i",
    "href": "homework/HW_4_F24_bsta511.html#thanksgiving-spending-part-i",
    "title": "HW 4: BSTA 511/611 F24",
    "section": "4.6 Thanksgiving spending, Part I",
    "text": "4.6 Thanksgiving spending, Part I"
  },
  {
    "objectID": "homework/HW_4_F24_bsta511.html#age-at-first-marriage-part-i",
    "href": "homework/HW_4_F24_bsta511.html#age-at-first-marriage-part-i",
    "title": "HW 4: BSTA 511/611 F24",
    "section": "4.8 Age at first marriage, Part I",
    "text": "4.8 Age at first marriage, Part I"
  },
  {
    "objectID": "homework/HW_4_F24_bsta511.html#working-backwards-part-ii",
    "href": "homework/HW_4_F24_bsta511.html#working-backwards-part-ii",
    "title": "HW 4: BSTA 511/611 F24",
    "section": "5.6 Working backwards, Part II",
    "text": "5.6 Working backwards, Part II"
  },
  {
    "objectID": "homework/HW_4_F24_bsta511.html#t-vs.-z",
    "href": "homework/HW_4_F24_bsta511.html#t-vs.-z",
    "title": "HW 4: BSTA 511/611 F24",
    "section": "5.10 t⋆ vs. z⋆",
    "text": "5.10 t⋆ vs. z⋆"
  },
  {
    "objectID": "homework/HW_4_F24_bsta511.html#auto-exhaust-and-lead-exposure",
    "href": "homework/HW_4_F24_bsta511.html#auto-exhaust-and-lead-exposure",
    "title": "HW 4: BSTA 511/611 F24",
    "section": "5.12 Auto exhaust and lead exposure",
    "text": "5.12 Auto exhaust and lead exposure"
  },
  {
    "objectID": "homework/HW_4_F24_bsta511.html#paired-or-not-part-ii",
    "href": "homework/HW_4_F24_bsta511.html#paired-or-not-part-ii",
    "title": "HW 4: BSTA 511/611 F24",
    "section": "5.16 Paired or not, Part II",
    "text": "5.16 Paired or not, Part II"
  },
  {
    "objectID": "homework/HW_4_F24_bsta511.html#ddt-exposure",
    "href": "homework/HW_4_F24_bsta511.html#ddt-exposure",
    "title": "HW 4: BSTA 511/611 F24",
    "section": "5.22 DDT exposure",
    "text": "5.22 DDT exposure"
  },
  {
    "objectID": "homework/HW_4_F24_bsta511.html#nb1-the-ethan-allen",
    "href": "homework/HW_4_F24_bsta511.html#nb1-the-ethan-allen",
    "title": "HW 4: BSTA 511/611 F24",
    "section": "NB1: The Ethan Allen",
    "text": "NB1: The Ethan Allen\nOn October 5, 2005, a tour boat named the Ethan Allen capsized on Lake George in New York with 47 passengers aboard. In the inquiries that followed, it was suggested that the tour operators should have realized that the combined weight of so many passengers was likely to exceed the weight capacity of the boat. Could they have predicted this?\n\nThe maximum weight capacity of passengers that the Ethan Allen could accommodate was estimated to be 7500 pounds.\nData from the Centers for Disease Control and Prevention indicate that weights of American adults in 2005 had a mean of 167 pounds and a standard deviation of 35 pounds.\n\nIf the tour boat company consistently accepted 47 passengers, what we want to know is the probability that the combined weight of the 47 passengers would exceed this capacity.\n\n(a) Maximum average weight\nWith 47 passengers on board, what is the maximum average weight that the Ethan Allen could accommodate?\n\n\n(b) Probability of an individual\nAssuming that the weights of American adults in 2005 can be modeled with a normal distribution, find the probability that an individual weighs more than the maximum average weight the Ethan Allen can accommodate.\n\n\n(c) Probability a random sample\nCalculate the probability that a random sample of 47 American adults has an average weight greater than the maximum average weight the Ethan Allen can accommodate.\n\n\n(d) Theorem used?\nWhat theorem did you use in the previous part, and why were you able to apply it to this problem?\n\n\n(e) Could this have been predicted?\nCould the tour operators have predicted that the combined weight of so many passengers was likely to exceed the weight capacity of the Ethan Allen?"
  },
  {
    "objectID": "homework/HW_4_F24_bsta511.html#load-packages",
    "href": "homework/HW_4_F24_bsta511.html#load-packages",
    "title": "HW 4: BSTA 511/611 F24",
    "section": "Load packages",
    "text": "Load packages\nLoad all the packages you need in the first code chunk of the file that starts with #| label: \"setup\"."
  },
  {
    "objectID": "homework/HW_4_F24_bsta511.html#r1-youth-weights---part-1",
    "href": "homework/HW_4_F24_bsta511.html#r1-youth-weights---part-1",
    "title": "HW 4: BSTA 511/611 F24",
    "section": "R1: Youth weights - Part 1",
    "text": "R1: Youth weights - Part 1\nIn this exercise you will use the YRBSS dataset we used in class on Day 8, to simulate the distribution of mean weights from repeated samples. Use the code from class where we simulated mean heights, and apply it to the weights (in pounds) as directed below.\n\n\n\n\n\n\nImportant\n\n\n\nYou will need to install and load the moderndive R package to use the rep_sample_n() command from the class notes.\n\n\n\n(a) set.seed()\nUse the set.seed() command to set a randomization seed. Use whatever number you want for the seed.\n\n\n(b) 1000 random samples of size 10\nTake 1000 random samples of size 10 and save the tibble with the random samples. Show the first 20 lines of this tibble.\n\n\n(c) Mean weights from the 1000 random samples\nCreate a tibble with mean weights from the 1000 random samples. Show the first 10 rows of this tibble.\n\n\n(d) Histogram of the 1000 mean weights\nMake a histogram of the 1000 mean weights. What do we call this distribution? Describe the shape of the distribution.\n\n\n(e) Mean and standard deviation of the 1000 sample mean weights\nCalculate the mean and standard deviation of the 1000 sample mean weights. What is another name for this standard deviation?\n\n\n(f) Theoretical values for mean and standard deviation\nWhat are the theoretical values for mean and standard deviation of the sampling distribution from the CLT, and how do your simulated values compare to the theoretical values?"
  },
  {
    "objectID": "homework/HW_4_F24_bsta511.html#r2-youth-weights---part-2",
    "href": "homework/HW_4_F24_bsta511.html#r2-youth-weights---part-2",
    "title": "HW 4: BSTA 511/611 F24",
    "section": "R2: Youth weights - Part 2",
    "text": "R2: Youth weights - Part 2\nIn this exercise you will use the YRBSS dataset again that we used in class on Days 8-9, to simulate the distribution of mean weights from repeated samples.\n\n(a) CI\nSuppose you took a random sample of size 50 from the YRBSS data, that has mean weight 130 pounds. Calculate and interpret a 90% confidence interval using the standard deviation of weights from the YRBSS “population.”\n\n\n(b) Another CI\nCalculate and interpret a 90% confidence interval assuming the standard deviation of weights from the random sample is 40."
  },
  {
    "objectID": "homework/HW_4_F24_bsta511.html#r3-swim-times",
    "href": "homework/HW_4_F24_bsta511.html#r3-swim-times",
    "title": "HW 4: BSTA 511/611 F24",
    "section": "R3: Swim times",
    "text": "R3: Swim times\n\nIn these exercises you will use R to work through the swim times example from Section 5.2 in the textbook.\nThe data are in the oibiostats package and called swim.\n\n\n(a) Mean & SD of differences\nCalculate the mean and standard deviation for the differences in swim times, and compare them to the ones in the book. Which order were the differences calculated, wet suit - swim suit or the opposite? Were all the differences positive?\n\n\n(b) Dot plot of differences\nCreate a dot plot of the differences in swim times and comment on the distribution shape.\n\n\n(c) Hypothesis test\nRun the appropriate statistical test in R as both a one-sample t-test and a paired t-test to verify the test statistic, p-value, and CI in the text. Use inline R code to pull these values from the test output when writing up your comparison of these values to the book’s values."
  },
  {
    "objectID": "slides/Day09_bsta511.html#last-time---goals-for-today",
    "href": "slides/Day09_bsta511.html#last-time---goals-for-today",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Last time -> Goals for today",
    "text": "Last time -&gt; Goals for today\n\n\nDay 8: Section 4.1\n\nSampling from a population\n\npopulation parameters vs. point estimates\nsampling variation\n\n\n\n\nSampling distribution of a mean\nCentral Limit Theorem\n\n\n\n\nDay 9: Section 4.2\nWhat are Confidence Intervals?\n\nHow to calculate CI’s?\nHow to interpret & NOT interpret CI’s?\n\n\n\nWhat if we don’t know \\(\\sigma\\)?\nStudent’s t-distribution"
  },
  {
    "objectID": "slides/Day09_bsta511.html#where-are-we",
    "href": "slides/Day09_bsta511.html#where-are-we",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "slides/Day09_bsta511.html#where-are-we-continuous-outcome-zoomed-in",
    "href": "slides/Day09_bsta511.html#where-are-we-continuous-outcome-zoomed-in",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Where are we? Continuous outcome zoomed in",
    "text": "Where are we? Continuous outcome zoomed in"
  },
  {
    "objectID": "slides/Day09_bsta511.html#our-hypothetical-population-yrbss",
    "href": "slides/Day09_bsta511.html#our-hypothetical-population-yrbss",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Our hypothetical population: YRBSS",
    "text": "Our hypothetical population: YRBSS\nYouth Risk Behavior Surveillance System (YRBSS)\n\nYearly survey conducted by the US Centers for Disease Control (CDC)\n“A set of surveys that track behaviors that can lead to poor health in students grades 9 through 12.”1\nDataset yrbss from oibiostat pacakge contains responses from n = 13,583 participants in 2013 for a subset of the variables included in the complete survey data\n\n\n\n\nlibrary(oibiostat)\ndata(\"yrbss\")  #load the data\n# ?yrbss\n\n\n\ndim(yrbss)\n\n[1] 13583    13\n\n\n\n\n\nnames(yrbss)\n\n [1] \"age\"                      \"gender\"                  \n [3] \"grade\"                    \"hispanic\"                \n [5] \"race\"                     \"height\"                  \n [7] \"weight\"                   \"helmet.12m\"              \n [9] \"text.while.driving.30d\"   \"physically.active.7d\"    \n[11] \"hours.tv.per.school.day\"  \"strength.training.7d\"    \n[13] \"school.night.hours.sleep\"\n\n\nYouth Risk Behavior Surveillance System https://www.cdc.gov/healthyyouth/data/yrbs/index.htm (YRBSS)"
  },
  {
    "objectID": "slides/Day09_bsta511.html#transform-height-weight-from-metric-to-to-standard",
    "href": "slides/Day09_bsta511.html#transform-height-weight-from-metric-to-to-standard",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Transform height & weight from metric to to standard",
    "text": "Transform height & weight from metric to to standard\nAlso, drop missing values and add a column of id values\n\nyrbss2 &lt;- yrbss %&gt;%                 # save new dataset with new name\n  mutate(                           # add variables for \n    height.ft = 3.28084*height,     #     height in feet\n    weight.lb = 2.20462*weight      #     weight in pounds\n  ) %&gt;% \n  drop_na(height.ft, weight.lb) %&gt;% # drop rows w/ missing height/weight values\n  mutate(id = 1:nrow(.)) %&gt;%        # add id column\n  select(id, height.ft, weight.lb)  # restrict dataset to columns of interest\n\nhead(yrbss2)  \n\n  id height.ft weight.lb\n1  1  5.675853  186.0038\n2  2  5.249344  122.9957\n3  3  4.921260  102.9998\n4  4  5.150919  147.9961\n5  5  5.413386  289.9957\n6  6  6.167979  157.0130\n\ndim(yrbss2)\n\n[1] 12579     3\n\n# number of rows deleted that had missing values for height and/or weight:\nnrow(yrbss) - nrow(yrbss2) \n\n[1] 1004"
  },
  {
    "objectID": "slides/Day09_bsta511.html#yrbss2-stats-for-height-in-feet",
    "href": "slides/Day09_bsta511.html#yrbss2-stats-for-height-in-feet",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "yrbss2: stats for height in feet",
    "text": "yrbss2: stats for height in feet\n\nsummary(yrbss2)\n\n       id          height.ft       weight.lb     \n Min.   :    1   Min.   :4.167   Min.   : 66.01  \n 1st Qu.: 3146   1st Qu.:5.249   1st Qu.:124.01  \n Median : 6290   Median :5.512   Median :142.00  \n Mean   : 6290   Mean   :5.549   Mean   :149.71  \n 3rd Qu.: 9434   3rd Qu.:5.840   3rd Qu.:167.99  \n Max.   :12579   Max.   :6.923   Max.   :399.01  \n\n(mean_height.ft &lt;- mean(yrbss2$height.ft))\n\n[1] 5.548691\n\n(sd_height.ft &lt;- sd(yrbss2$height.ft))\n\n[1] 0.3434949"
  },
  {
    "objectID": "slides/Day09_bsta511.html#samples-of-size-n-30-from-yrbss2",
    "href": "slides/Day09_bsta511.html#samples-of-size-n-30-from-yrbss2",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "10,000 samples of size n = 30 from yrbss2",
    "text": "10,000 samples of size n = 30 from yrbss2\n\n\nTake 10,000 random samples of size\nn = 30 from yrbss2:\n\nsamp_n30_rep10000 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 30, \n               reps = 10000,\n               replace = FALSE)\nsamp_n30_rep10000\n\n# A tibble: 300,000 × 4\n# Groups:   replicate [10,000]\n   replicate    id height.ft weight.lb\n       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1         1  5869      5.15      145.\n 2         1  6694      5.41      127.\n 3         1  2517      5.74      130.\n 4         1  5372      6.07      180.\n 5         1  5403      6.07      163.\n 6         1  2329      6.07      182.\n 7         1  8863      5.25      125.\n 8         1  8058      5.84      135.\n 9         1   335      6.17      235.\n10         1  4698      5.58      124.\n# ℹ 299,990 more rows\n\n\n\nCalculate the mean for each of the 10,000 random samples:\n\nmeans_hght_samp_n30_rep10000 &lt;- \n  samp_n30_rep10000 %&gt;% \n  group_by(replicate) %&gt;% \n  summarise(mean_height = \n            mean(height.ft))\n\nmeans_hght_samp_n30_rep10000\n\n# A tibble: 10,000 × 2\n   replicate mean_height\n       &lt;int&gt;       &lt;dbl&gt;\n 1         1        5.59\n 2         2        5.59\n 3         3        5.51\n 4         4        5.65\n 5         5        5.64\n 6         6        5.57\n 7         7        5.61\n 8         8        5.60\n 9         9        5.52\n10        10        5.64\n# ℹ 9,990 more rows\n\n\n\n\nHow close are the mean heights for each of the 10,000 random samples?"
  },
  {
    "objectID": "slides/Day09_bsta511.html#simulated-sampling-distribution-for-n-30-using-10000-sample-mean-heights",
    "href": "slides/Day09_bsta511.html#simulated-sampling-distribution-for-n-30-using-10000-sample-mean-heights",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Simulated sampling distribution for n = 30  using 10,000 sample mean heights",
    "text": "Simulated sampling distribution for n = 30  using 10,000 sample mean heights\n\n\n\nggplot(\n  means_hght_samp_n30_rep10000, \n  aes(x = mean_height)) + \n  geom_histogram() +\n  labs(title = \"Simulated \\n sampling distribution\")\n\n\n\n\n\nCLT tells us that we can model the sampling distribution of mean heights using a normal distribution."
  },
  {
    "objectID": "slides/Day09_bsta511.html#given-barx-what-are-plausible-values-of-mu",
    "href": "slides/Day09_bsta511.html#given-barx-what-are-plausible-values-of-mu",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Given \\(\\bar{x}\\), what are plausible values of \\(\\mu\\)?",
    "text": "Given \\(\\bar{x}\\), what are plausible values of \\(\\mu\\)?"
  },
  {
    "objectID": "slides/Day09_bsta511.html#confidence-interval-c-i-for-the-mean-mu",
    "href": "slides/Day09_bsta511.html#confidence-interval-c-i-for-the-mean-mu",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Confidence interval (C I) for the mean \\(\\mu\\)",
    "text": "Confidence interval (C I) for the mean \\(\\mu\\)\n\n\n\\[\\overline{x}\\ \\pm\\ z^*\\times \\text{SE}\\]\nwhere\n\n\\(SE = \\frac{\\sigma}{\\sqrt{n}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(z^*\\) depends on the confidence level\n\nFor a 95% CI, \\(z^*\\) is chosen such that 95% of the standard normal curve is between \\(-z^*\\) and \\(z^*\\)\n\n\n\n\nqnorm(.975)\n\n[1] 1.959964\n\nqnorm(.995)\n\n[1] 2.575829\n\n\n\n\nWhen can this be applied?"
  },
  {
    "objectID": "slides/Day09_bsta511.html#example-c-i-for-mean-height",
    "href": "slides/Day09_bsta511.html#example-c-i-for-mean-height",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Example: C I for mean height",
    "text": "Example: C I for mean height\n\nA random sample of 30 high schoolers has mean height 5.6 ft.\nFind the 95% confidence interval for the population mean, assuming that the population standard deviation is 0.34 ft."
  },
  {
    "objectID": "slides/Day09_bsta511.html#how-to-interpret-a-c-i-12",
    "href": "slides/Day09_bsta511.html#how-to-interpret-a-c-i-12",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "How to interpret a C I? (1/2)",
    "text": "How to interpret a C I? (1/2)\n\n\nSimulating Confidence Intervals:\n\nhttp://www.rossmanchance.com/applets/ConfSim.html\n\nThe figure shows CI’s from 100 simulations.\n\nThe true value of \\(\\mu\\) = 5.55 is the vertical black line.\nThe horizontal lines are 95% CI’s from 100 samples.\n\nGreen: the CI “captured” the true value of \\(\\mu\\)\nRed: the CI did not “capture” the true value of \\(\\mu\\)\n\n\n\nQuestion:\nWhat percent of CI’s captured the true value of \\(\\mu\\) ?"
  },
  {
    "objectID": "slides/Day09_bsta511.html#how-to-interpret-a-c-i-22",
    "href": "slides/Day09_bsta511.html#how-to-interpret-a-c-i-22",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "How to interpret a C I? (2/2)",
    "text": "How to interpret a C I? (2/2)\nActual interpretation:\n\nIf we were to\n\nrepeatedly take random samples from a population and\ncalculate a 95% CI for each random sample,\n\nthen we would expect 95% of our CI’s to contain the true population parameter \\(\\mu\\).\n\n\n\n\nWhat we typically write as “shorthand”:\n\nWe are 95% confident that (the 95% confidence interval) captures the value of the population parameter.\n\nWRONG interpretation:\n\nThere is a 95% chance that (the 95% confidence interval) captures the value of the population parameter.\n\nFor one CI on its own, it either does or doesn’t contain the population parameter with probability 0 or 1. We just don’t know which!"
  },
  {
    "objectID": "slides/Day09_bsta511.html#what-percent-c-i-was-being-simulated-in-this-figure",
    "href": "slides/Day09_bsta511.html#what-percent-c-i-was-being-simulated-in-this-figure",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "What percent C I was being simulated in this figure?",
    "text": "What percent C I was being simulated in this figure?\n\n\n\n\n\n\n\n\n100 CI’s are shown in the figure."
  },
  {
    "objectID": "slides/Day09_bsta511.html#interpretation-of-the-mean-heights-c-i",
    "href": "slides/Day09_bsta511.html#interpretation-of-the-mean-heights-c-i",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Interpretation of the mean heights C I",
    "text": "Interpretation of the mean heights C I\nCorrect interpretation:\n\nWe are 95% confident that the mean height for high schoolers is between 5.43 and 5.67 feet.\n\nWRONG:\n\nThere is a 95% chance that the mean height for high schoolers is between 5.43 and 5.67 feet."
  },
  {
    "objectID": "slides/Day09_bsta511.html#what-if-we-dont-know-sigma-13",
    "href": "slides/Day09_bsta511.html#what-if-we-dont-know-sigma-13",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "What if we don’t know \\(\\sigma\\) ? (1/3)",
    "text": "What if we don’t know \\(\\sigma\\) ? (1/3)\nSimulating Confidence Intervals: http://www.rossmanchance.com/applets/ConfSim.html\n\nThe normal distribution doesn’t have a 95% “coverage rate”\nwhen using \\(s\\) instead of \\(\\sigma\\)"
  },
  {
    "objectID": "slides/Day09_bsta511.html#what-if-we-dont-know-sigma-23",
    "href": "slides/Day09_bsta511.html#what-if-we-dont-know-sigma-23",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "What if we don’t know \\(\\sigma\\) ? (2/3)",
    "text": "What if we don’t know \\(\\sigma\\) ? (2/3)\n\nIn real life, we don’t know what the population sd is ( \\(\\sigma\\) )\nIf we replace \\(\\sigma\\) with \\(s\\) in the SE formula, we add in additional variability to the SE! \\[\\frac{\\sigma}{\\sqrt{n}} ~~~~\\textrm{vs.} ~~~~ \\frac{s}{\\sqrt{n}}\\]\nThus when using \\(s\\) instead of \\(\\sigma\\) when calculating the SE, we need a different probability distribution with thicker tails than the normal distribution.\n\nIn practice this will mean using a different value than 1.96 when calculating the CI."
  },
  {
    "objectID": "slides/Day09_bsta511.html#what-if-we-dont-know-sigma-33",
    "href": "slides/Day09_bsta511.html#what-if-we-dont-know-sigma-33",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "What if we don’t know \\(\\sigma\\) ? (3/3)",
    "text": "What if we don’t know \\(\\sigma\\) ? (3/3)\n\n\nThe Student’s t-distribution:\n\nIs bell shaped and symmetric with mean = 0.\nIts tails are a thicker than that of a normal distribution\n\nThe “thickness” depends on its degrees of freedom: \\(df = n–1\\) , where n = sample size.\n\nAs the degrees of freedom (sample size) increase,\n\nthe tails are less thick, and\nthe t-distribution is more like a normal distribution\nin theory, with an infinite sample size the t-distribution is a normal distribution."
  },
  {
    "objectID": "slides/Day09_bsta511.html#calculating-the-c-i-for-the-population-mean-using-s",
    "href": "slides/Day09_bsta511.html#calculating-the-c-i-for-the-population-mean-using-s",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Calculating the C I for the population mean using \\(s\\)",
    "text": "Calculating the C I for the population mean using \\(s\\)\nCI for \\(\\mu\\):\n\\[\\bar{x} \\pm t^*\\cdot\\frac{s}{\\sqrt{n}}\\]\nwhere \\(t^*\\) is determined by the t-distribution and dependent on the\ndf = \\(n-1\\) and the confidence level\n\n\n\nqt gives the quartiles for a t-distribution. Need to specify\n\nthe percent under the curve to the left of the quartile\nthe degrees of freedom = n-1\n\nNote in the R output to the right that \\(t^*\\) gets closer to 1.96 as the sample size increases.\n\n\n\nqt(.975, df=9)  # df = n-1\n\n[1] 2.262157\n\nqt(.975, df=49)\n\n[1] 2.009575\n\nqt(.975, df=99)\n\n[1] 1.984217\n\nqt(.975, df=999)\n\n[1] 1.962341"
  },
  {
    "objectID": "slides/Day09_bsta511.html#using-a-t-table-to-get-t",
    "href": "slides/Day09_bsta511.html#using-a-t-table-to-get-t",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Using a \\(t\\)-table to get \\(t^*\\)",
    "text": "Using a \\(t\\)-table to get \\(t^*\\)"
  },
  {
    "objectID": "slides/Day09_bsta511.html#example-c-i-for-mean-height-revisited",
    "href": "slides/Day09_bsta511.html#example-c-i-for-mean-height-revisited",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Example: C I for mean height (revisited)",
    "text": "Example: C I for mean height (revisited)\n\nA random sample of 30 high schoolers has mean height 5.6 ft and standard deviation 0.34 ft.\nFind the 95% confidence interval for the population mean."
  },
  {
    "objectID": "slides/Day09_bsta511.html#z-vs-t-important-comment-about-chapter-4-of-textbook",
    "href": "slides/Day09_bsta511.html#z-vs-t-important-comment-about-chapter-4-of-textbook",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "\\(z\\) vs \\(t\\)??  (& important comment about Chapter 4 of textbook)",
    "text": "\\(z\\) vs \\(t\\)??  (& important comment about Chapter 4 of textbook)\n\n\nTextbook’s rule of thumb\n\n\n(Ch 4) If \\(n \\geq 30\\) and population distribution not strongly skewed:\n\nUse normal distribution\nNo matter if using \\(\\sigma\\) or \\(s\\) for the \\(SE\\)\nIf there is skew or some large outliers, then need \\(n \\geq 50\\)\n\n(Ch 5) If \\(n &lt; 30\\) and data approximately symmetric with no large outliers:\n\nUse Student’s t-distribution\n\n\n\n\n\n\nBSTA 511 rule of thumb\n\nUse normal distribution ONLY if know \\(\\sigma\\)\n\nIf using \\(s\\) for the \\(SE\\), then use the Student’s t-distribution\n\n\nFor either case, can apply if either\n\n\\(n \\geq 30\\) and population distribution not strongly skewed\n\nIf there is skew or some large outliers, then \\(n \\geq 50\\) gives better estimates\n\n\\(n &lt; 30\\) and data approximately symmetric with no large outliers\n\nIf do not know population distribution, then check the distribution of the data."
  },
  {
    "objectID": "weeks/week_06.html",
    "href": "weeks/week_06.html",
    "title": "Week 6",
    "section": "",
    "text": "Confidence intervals for one mean (single-sample):\n\nSections 4.2 and 5.1.1, 5.1.2 (pgs. 238-242)\n\nStudent’s t-distribution\n\n\n\n\n\nHypothesis testing for one mean (single-sample): Section 4.3, 5.1\nCIs vs. hypothesis tests\nInference for two-sample paired data: Section 5.2\nR projects, here::here(), and loading data into R from a file\n\nSee Projects in RStudio file for more information\n\nRunning a t.test in R"
  },
  {
    "objectID": "weeks/week_06.html#overview-of-week-6",
    "href": "weeks/week_06.html#overview-of-week-6",
    "title": "Week 6",
    "section": "",
    "text": "Confidence intervals for one mean (single-sample):\n\nSections 4.2 and 5.1.1, 5.1.2 (pgs. 238-242)\n\nStudent’s t-distribution\n\n\n\n\n\nHypothesis testing for one mean (single-sample): Section 4.3, 5.1\nCIs vs. hypothesis tests\nInference for two-sample paired data: Section 5.2\nR projects, here::here(), and loading data into R from a file\n\nSee Projects in RStudio file for more information\n\nRunning a t.test in R"
  },
  {
    "objectID": "weeks/week_06.html#slides-recordings",
    "href": "weeks/week_06.html#slides-recordings",
    "title": "Week 6",
    "section": "Slides & Recordings",
    "text": "Slides & Recordings\n\nPre-recorded lessons are on Echo Cloud (aka echo 360 or echo video).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nTopic\nSlides: html\nSlides: pdf\nSlides: web-page\nSlides with notes\nRecording Link\nDuration\nCode: qmd\nCode: html\n\n\n\n\n9\nCI’s for 1 mean: 4.2\nDay 9: slides 1-12\n\n\n\nDay 9 Part 1\n17 min\n\n\n\n\n\n4.2 con’td\nDay 9: slides 13-19\nsame\n\nsame\nDay 9 Part 2\n17 min\nsame\nsame\n\n\n\n4.2 con’td, 5.1.1, 5.1.2 (pgs. 238-242)\nDay 9: slides 20-23\nsame\n\nsame\nDay 9 Part 3\n14 min\nsame\nsame\n\n\n10 part 1\nHyp test 1 mean intro\nDay 10 Part 1: slides 1-13\n\n\n\nDay 10 Part 1\n26 min\n\n\n\n\n\nHyp test steps: 4.3, 5.1\nDay 10 Part 1: slides 14-25\nsame\n\nsame\nDay 10 Part 2\n36 min\nsame\n\n\n\n\nR demo\nDay 10 Part 1: slides 26-34\nsame\n\nsame\nDay 10 Part 3\n25 min\nsame\n\n\n\n10 part 2\n\nDay 10 Part 2\n\n\n\nNo recording"
  },
  {
    "objectID": "weeks/week_06.html#class-discussion",
    "href": "weeks/week_06.html#class-discussion",
    "title": "Week 6",
    "section": "Class discussion",
    "text": "Class discussion\nDuring class you will be working in groups discussing the following:\n\nDay 9\n\nSlide 15: What percent CI was being simulated in this figure?\nSlide 22: Calculate CI\nProblems from Homework 4:\n\nBook exercises:\n\nwarm-up: 4.4, 4.6, 5.10\n5.6\n\nR2: Youth weights Part 2 (YRBSS)\n\n\n\n\nDay 10\n\nWork through Day 10 Part 2 slides\n\nThere are specific questions in orange on (most) slides.\n\nProblems from Homework 4:\n\nBook exercises:\n\n5.16 (warm-up)\n5.12\n\nR3: Swim times"
  },
  {
    "objectID": "weeks/week_06.html#homework",
    "href": "weeks/week_06.html#homework",
    "title": "Week 6",
    "section": "Homework",
    "text": "Homework\n\nHW 4 covers Days 8 -10 and is due on Sat, 11/9\nCheck out this spreadsheet for typos in book."
  },
  {
    "objectID": "weeks/week_06.html#sec-Exam1info",
    "href": "weeks/week_06.html#sec-Exam1info",
    "title": "Week 6",
    "section": "Exam 1 information",
    "text": "Exam 1 information\n\nExam 1 will be on Wed, Oct. 30th\nSamples of past exam questions and answers (on Sakai)\nMaterial will cover Days 1-7,\n\nwhich is approximately Chapters 1, 2, 3.1-3.4 from the textbook.\n\nThe exam will be in-class and handwritten. Bring a calculator.\nYou may bring one page of notes on an 8.5” x 11” sheet of paper.\n\nYou may use both sides.\nYou may type your notes.\nYou may not use screenshots of the notes or textbook\nYou will be turning in the page of notes with the exam, so please add your name to the sheet."
  },
  {
    "objectID": "slides_code/Day09_bsta511_code.html",
    "href": "slides_code/Day09_bsta511_code.html",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(ggridges)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive) # NEW!!\n\nset.seed(456)\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day09_bsta511_code.html#load-packages",
    "href": "slides_code/Day09_bsta511_code.html#load-packages",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(ggridges)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive) # NEW!!\n\nset.seed(456)\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day09_bsta511_code.html#day-8-section-4.1",
    "href": "slides_code/Day09_bsta511_code.html#day-8-section-4.1",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Day 8: Section 4.1",
    "text": "Day 8: Section 4.1\n\nSampling from a population\n\npopulation parameters vs. point estimates\nsampling variation\n\n\n\n\nSampling distribution of a mean\nCentral Limit Theorem"
  },
  {
    "objectID": "slides_code/Day09_bsta511_code.html#day-9-section-4.2",
    "href": "slides_code/Day09_bsta511_code.html#day-9-section-4.2",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Day 9: Section 4.2",
    "text": "Day 9: Section 4.2\nWhat are Confidence Intervals?\n\nHow to calculate CI’s?\nHow to interpret & NOT interpret CI’s?\n\n\n\nWhat if we don’t know \\(\\sigma\\)?\nStudent’s t-distribution"
  },
  {
    "objectID": "slides_code/Day09_bsta511_code.html#transform-height-weight-from-metric-to-to-standard",
    "href": "slides_code/Day09_bsta511_code.html#transform-height-weight-from-metric-to-to-standard",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Transform height & weight from metric to to standard",
    "text": "Transform height & weight from metric to to standard\nAlso, drop missing values and add a column of id values\n\nyrbss2 &lt;- yrbss %&gt;%                 # save new dataset with new name\n  mutate(                           # add variables for \n    height.ft = 3.28084*height,     #     height in feet\n    weight.lb = 2.20462*weight      #     weight in pounds\n  ) %&gt;% \n  drop_na(height.ft, weight.lb) %&gt;% # drop rows w/ missing height/weight values\n  mutate(id = 1:nrow(.)) %&gt;%        # add id column\n  select(id, height.ft, weight.lb)  # restrict dataset to columns of interest\n\nhead(yrbss2)  \n\n  id height.ft weight.lb\n1  1  5.675853  186.0038\n2  2  5.249344  122.9957\n3  3  4.921260  102.9998\n4  4  5.150919  147.9961\n5  5  5.413386  289.9957\n6  6  6.167979  157.0130\n\ndim(yrbss2)\n\n[1] 12579     3\n\n# number of rows deleted that had missing values for height and/or weight:\nnrow(yrbss) - nrow(yrbss2) \n\n[1] 1004"
  },
  {
    "objectID": "slides_code/Day09_bsta511_code.html#yrbss2-stats-for-height-in-feet",
    "href": "slides_code/Day09_bsta511_code.html#yrbss2-stats-for-height-in-feet",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "yrbss2: stats for height in feet",
    "text": "yrbss2: stats for height in feet\n\nsummary(yrbss2)\n\n       id          height.ft       weight.lb     \n Min.   :    1   Min.   :4.167   Min.   : 66.01  \n 1st Qu.: 3146   1st Qu.:5.249   1st Qu.:124.01  \n Median : 6290   Median :5.512   Median :142.00  \n Mean   : 6290   Mean   :5.549   Mean   :149.71  \n 3rd Qu.: 9434   3rd Qu.:5.840   3rd Qu.:167.99  \n Max.   :12579   Max.   :6.923   Max.   :399.01  \n\n(mean_height.ft &lt;- mean(yrbss2$height.ft))\n\n[1] 5.548691\n\n(sd_height.ft &lt;- sd(yrbss2$height.ft))\n\n[1] 0.3434949"
  },
  {
    "objectID": "slides_code/Day09_bsta511_code.html#take-10000-samples-of-size-n-30-from-yrbss2",
    "href": "slides_code/Day09_bsta511_code.html#take-10000-samples-of-size-n-30-from-yrbss2",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Take 10,000 samples of size n = 30 from yrbss2",
    "text": "Take 10,000 samples of size n = 30 from yrbss2\n\nsamp_n30_rep10000 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 30, \n               reps = 10000,\n               replace = FALSE)\nsamp_n30_rep10000\n\n# A tibble: 300,000 × 4\n# Groups:   replicate [10,000]\n   replicate    id height.ft weight.lb\n       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1         1  5869      5.15      145.\n 2         1  6694      5.41      127.\n 3         1  2517      5.74      130.\n 4         1  5372      6.07      180.\n 5         1  5403      6.07      163.\n 6         1  2329      6.07      182.\n 7         1  8863      5.25      125.\n 8         1  8058      5.84      135.\n 9         1   335      6.17      235.\n10         1  4698      5.58      124.\n# ℹ 299,990 more rows\n\n\n\nmeans_hght_samp_n30_rep10000 &lt;- \n  samp_n30_rep10000 %&gt;% \n  group_by(replicate) %&gt;% \n  summarise(mean_height = \n            mean(height.ft))\n\nmeans_hght_samp_n30_rep10000\n\n# A tibble: 10,000 × 2\n   replicate mean_height\n       &lt;int&gt;       &lt;dbl&gt;\n 1         1        5.59\n 2         2        5.59\n 3         3        5.51\n 4         4        5.65\n 5         5        5.64\n 6         6        5.57\n 7         7        5.61\n 8         8        5.60\n 9         9        5.52\n10        10        5.64\n# ℹ 9,990 more rows"
  },
  {
    "objectID": "slides_code/Day09_bsta511_code.html#simulated-sampling-distribution-for-n-30-using-10000-sample-mean-heights",
    "href": "slides_code/Day09_bsta511_code.html#simulated-sampling-distribution-for-n-30-using-10000-sample-mean-heights",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Simulated sampling distribution for n = 30 using 10,000 sample mean heights",
    "text": "Simulated sampling distribution for n = 30 using 10,000 sample mean heights\n\nggplot(\n  means_hght_samp_n30_rep10000, \n  aes(x = mean_height)) + \n  geom_histogram()\n\n\n\n\nCLT tells us that we can model the sampling distribution of mean heights using a normal distribution.\n\nmu &lt;- 5.55\nSE &lt;- 0.34/sqrt(30)\nsig &lt;- round(SE, 2)\n\nggplot(data.frame(x = c(mu-4*sig, mu+4*sig)), aes(x = x)) + \n  stat_function(fun = dnorm, \n                args = list(mean = mu, sd = sig)) + \n  stat_function(fun = dnorm, \n                args = list(mean = mu, sd = sig), \n                xlim = c(mu-4*sig, mu-2*sig), \n                geom = \"area\", fill = \"darkblue\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mu, sd = sig), \n                xlim = c(mu+2*sig, mu+4*sig), \n                geom = \"area\", fill = \"darkblue\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mu, sd = sig), \n                xlim = c(mu-2*sig, mu-1*sig), \n                geom = \"area\", fill = \"darkgreen\") +\n  stat_function(fun = dnorm, \n                args = list(mean = mu, sd = sig), \n                xlim = c(mu+1*sig, mu+2*sig), \n                geom = \"area\", fill = \"darkgreen\")+\n  stat_function(fun = dnorm, \n                args = list(mean = mu, sd = sig), \n                xlim = c(mu-1*sig, mu+1*sig), \n                geom = \"area\", fill = \"orange\") +\n  scale_x_continuous(name =\"mean height (ft)\", \n                    breaks=c(mu-2*sig,mu-1*sig,mu, mu+1*sig, mu+2*sig))  +\n  labs(title = \"Sampling distribution\", y = \"\") +\n  scale_y_continuous(labels = NULL, breaks = NULL)"
  },
  {
    "objectID": "slides_code/Day09_bsta511_code.html#example-ci-for-mean-height",
    "href": "slides_code/Day09_bsta511_code.html#example-ci-for-mean-height",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Example: CI for mean height",
    "text": "Example: CI for mean height\n\nA random sample of 30 high schoolers has mean height 5.6 ft.\nFind the 95% confidence interval for the population mean, assuming that the population standard deviation is 0.34 ft."
  },
  {
    "objectID": "slides_code/Day09_bsta511_code.html#how-to-interpret-a-ci",
    "href": "slides_code/Day09_bsta511_code.html#how-to-interpret-a-ci",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "How to interpret a CI?",
    "text": "How to interpret a CI?\nSimulating Confidence Intervals: http://www.rossmanchance.com/applets/ConfSim.html\nActual interpretation:\n\nIf we were to\n\nrepeatedly take random samples from a population and\ncalculate a 95% CI for each random sample,\n\nthen we would expect 95% of our CI’s to contain the true population parameter \\(\\mu\\).\n\nWhat we typically write as “shorthand”:\n\nWe are 95% confident that (the 95% confidence interval) captures the value of the population parameter.\n\nWRONG interpretation:\n\nThere is a 95% chance that (the 95% confidence interval) captures the value of the population parameter.\n\nFor one CI on its own, it either does or doesn’t contain the population parameter with probability 0 or 1. We just don’t know which!"
  },
  {
    "objectID": "slides_code/Day09_bsta511_code.html#interpretation-of-our-heights-ci",
    "href": "slides_code/Day09_bsta511_code.html#interpretation-of-our-heights-ci",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Interpretation of our heights CI",
    "text": "Interpretation of our heights CI\nCorrect interpretation:\n\nWe are 95% confident that the mean height for high schoolers is between 5.43 and 5.67 feet.\n\nWRONG:\n\nThere is a 95% chance that the mean height for high schoolers is between 5.43 and 5.67 feet."
  },
  {
    "objectID": "slides_code/Day09_bsta511_code.html#students-t-distribution",
    "href": "slides_code/Day09_bsta511_code.html#students-t-distribution",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Student’s t-distribution",
    "text": "Student’s t-distribution\nThe Student’s t-distribution:\n\nIs bell shaped and symmetric with mean = 0.\nIts tails are a thicker than that of a normal distribution\n\nThe “thickness” depends on its degrees of freedom: \\(df = n–1\\) , where n = sample size.\n\nAs the degrees of freedom (sample size) increase,\n\nthe tails are less thick, and\nthe t-distribution is more like a normal distribution\nin theory, with an infinite sample size the t-distribution is a normal distribution."
  },
  {
    "objectID": "slides_code/Day09_bsta511_code.html#calculating-the-ci-for-the-population-mean-using-s",
    "href": "slides_code/Day09_bsta511_code.html#calculating-the-ci-for-the-population-mean-using-s",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Calculating the CI for the population mean using \\(s\\)",
    "text": "Calculating the CI for the population mean using \\(s\\)\nCI for \\(\\mu\\):\n\\[\\bar{x} \\pm t^*\\cdot\\frac{s}{\\sqrt{n}}\\]\nwhere \\(t^*\\) is determined by the t-distribution and dependent on the\ndf = \\(n-1\\) and the confidence level\n\nqt gives the quartiles for a t-distribution. Need to specify\n\nthe percent under the curve to the left of the quartile\nthe degrees of freedom = n-1\n\nNote in the R output to the right that \\(t^*\\) gets closer to 1.96 as the sample size increases.\n\n\nqt(.975, df=9)  # df = n-1\n\n[1] 2.262157\n\nqt(.975, df=49)\n\n[1] 2.009575\n\nqt(.975, df=99)\n\n[1] 1.984217\n\nqt(.975, df=999)\n\n[1] 1.962341"
  },
  {
    "objectID": "slides_code/Day09_bsta511_code.html#example-ci-for-mean-height-revisited",
    "href": "slides_code/Day09_bsta511_code.html#example-ci-for-mean-height-revisited",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Example: CI for mean height (revisited)",
    "text": "Example: CI for mean height (revisited)\n\nA random sample of 30 high schoolers has mean height 5.6 ft and standard deviation 0.34 ft.\nFind the 95% confidence interval for the population mean."
  },
  {
    "objectID": "slides_code/Day09_bsta511_code.html#textbooks-rule-of-thumb",
    "href": "slides_code/Day09_bsta511_code.html#textbooks-rule-of-thumb",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Textbook’s rule of thumb",
    "text": "Textbook’s rule of thumb\n\n(Ch 4) If \\(n \\geq 30\\) and population distribution not strongly skewed:\n\nUse normal distribution\nNo matter if using \\(\\sigma\\) or \\(s\\) for the \\(SE\\)\nIf there is skew or some large outliers, then need \\(n \\geq 50\\)\n\n(Ch 5) If \\(n &lt; 30\\) and data approximately symmetric with no large outliers:\n\nUse Student’s t-distribution"
  },
  {
    "objectID": "slides_code/Day09_bsta511_code.html#bsta-511-rule-of-thumb",
    "href": "slides_code/Day09_bsta511_code.html#bsta-511-rule-of-thumb",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "BSTA 511 rule of thumb",
    "text": "BSTA 511 rule of thumb\n\nUse normal distribution ONLY if know \\(\\sigma\\)\n\nIf using \\(s\\) for the \\(SE\\), then use the Student’s t-distribution\n\n\nFor either case, can apply if either\n\n\\(n \\geq 30\\) and population distribution not strongly skewed\n\nIf there is skew or some large outliers, then \\(n \\geq 50\\) gives better estimates\n\n\\(n &lt; 30\\) and data approximately symmetric with no large outliers\n\nIf do not know population distribution, then check the distribution of the data."
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html",
    "href": "slides_md/Day09_bsta511_md.html",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "",
    "text": "Sampling from a population\n\npopulation parameters vs. point estimates\nsampling variation\n\n\n\n\nSampling distribution of a mean\nCentral Limit Theorem\n\n\n\n\n\n\n\nWhat are Confidence Intervals?\n\nHow to calculate CI’s?\nHow to interpret & NOT interpret CI’s?\n\n\n\nWhat if we don’t know \\(\\sigma\\)?\nStudent’s t-distribution"
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#last-time---goals-for-today",
    "href": "slides_md/Day09_bsta511_md.html#last-time---goals-for-today",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "",
    "text": "Sampling from a population\n\npopulation parameters vs. point estimates\nsampling variation\n\n\n\n\nSampling distribution of a mean\nCentral Limit Theorem\n\n\n\n\n\n\n\nWhat are Confidence Intervals?\n\nHow to calculate CI’s?\nHow to interpret & NOT interpret CI’s?\n\n\n\nWhat if we don’t know \\(\\sigma\\)?\nStudent’s t-distribution"
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#where-are-we",
    "href": "slides_md/Day09_bsta511_md.html#where-are-we",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#where-are-we-continuous-outcome-zoomed-in",
    "href": "slides_md/Day09_bsta511_md.html#where-are-we-continuous-outcome-zoomed-in",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Where are we? Continuous outcome zoomed in",
    "text": "Where are we? Continuous outcome zoomed in"
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#our-hypothetical-population-yrbss",
    "href": "slides_md/Day09_bsta511_md.html#our-hypothetical-population-yrbss",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Our hypothetical population: YRBSS",
    "text": "Our hypothetical population: YRBSS\nYouth Risk Behavior Surveillance System (YRBSS)\n\nYearly survey conducted by the US Centers for Disease Control (CDC)\n“A set of surveys that track behaviors that can lead to poor health in students grades 9 through 12.”1\nDataset yrbss from oibiostat pacakge contains responses from n = 13,583 participants in 2013 for a subset of the variables included in the complete survey data\n\n\n\n\nlibrary(oibiostat)\ndata(\"yrbss\")  #load the data\n# ?yrbss\n\n\n\ndim(yrbss)\n\n[1] 13583    13\n\n\n\n\n\nnames(yrbss)\n\n [1] \"age\"                      \"gender\"                  \n [3] \"grade\"                    \"hispanic\"                \n [5] \"race\"                     \"height\"                  \n [7] \"weight\"                   \"helmet.12m\"              \n [9] \"text.while.driving.30d\"   \"physically.active.7d\"    \n[11] \"hours.tv.per.school.day\"  \"strength.training.7d\"    \n[13] \"school.night.hours.sleep\""
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#our-hypothetical-population-yrbss-1",
    "href": "slides_md/Day09_bsta511_md.html#our-hypothetical-population-yrbss-1",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Our hypothetical population: YRBSS",
    "text": "Our hypothetical population: YRBSS\nYouth Risk Behavior Surveillance System (YRBSS)\n\nYearly survey conducted by the US Centers for Disease Control (CDC)\n“A set of surveys that track behaviors that can lead to poor health in students grades 9 through 12.”2\nDataset yrbss from oibiostat pacakge contains responses from n = 13,583 participants in 2013 for a subset of the variables included in the complete survey data\n\n\n\n\nlibrary(oibiostat)\ndata(\"yrbss\")  #load the data\n# ?yrbss\n\n\n\ndim(yrbss)\n\n[1] 13583    13\n\n\n\n\n\nnames(yrbss)\n\n [1] \"age\"                      \"gender\"                  \n [3] \"grade\"                    \"hispanic\"                \n [5] \"race\"                     \"height\"                  \n [7] \"weight\"                   \"helmet.12m\"              \n [9] \"text.while.driving.30d\"   \"physically.active.7d\"    \n[11] \"hours.tv.per.school.day\"  \"strength.training.7d\"    \n[13] \"school.night.hours.sleep\""
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#transform-height-weight-from-metric-to-to-standard",
    "href": "slides_md/Day09_bsta511_md.html#transform-height-weight-from-metric-to-to-standard",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Transform height & weight from metric to to standard",
    "text": "Transform height & weight from metric to to standard\nAlso, drop missing values and add a column of id values\n\nyrbss2 &lt;- yrbss %&gt;%                 # save new dataset with new name\n  mutate(                           # add variables for \n    height.ft = 3.28084*height,     #     height in feet\n    weight.lb = 2.20462*weight      #     weight in pounds\n  ) %&gt;% \n  drop_na(height.ft, weight.lb) %&gt;% # drop rows w/ missing height/weight values\n  mutate(id = 1:nrow(.)) %&gt;%        # add id column\n  select(id, height.ft, weight.lb)  # restrict dataset to columns of interest\n\nhead(yrbss2)  \n\n  id height.ft weight.lb\n1  1  5.675853  186.0038\n2  2  5.249344  122.9957\n3  3  4.921260  102.9998\n4  4  5.150919  147.9961\n5  5  5.413386  289.9957\n6  6  6.167979  157.0130\n\ndim(yrbss2)\n\n[1] 12579     3\n\n# number of rows deleted that had missing values for height and/or weight:\nnrow(yrbss) - nrow(yrbss2) \n\n[1] 1004"
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#yrbss2-stats-for-height-in-feet",
    "href": "slides_md/Day09_bsta511_md.html#yrbss2-stats-for-height-in-feet",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "yrbss2: stats for height in feet",
    "text": "yrbss2: stats for height in feet\n\nsummary(yrbss2)\n\n       id          height.ft       weight.lb     \n Min.   :    1   Min.   :4.167   Min.   : 66.01  \n 1st Qu.: 3146   1st Qu.:5.249   1st Qu.:124.01  \n Median : 6290   Median :5.512   Median :142.00  \n Mean   : 6290   Mean   :5.549   Mean   :149.71  \n 3rd Qu.: 9434   3rd Qu.:5.840   3rd Qu.:167.99  \n Max.   :12579   Max.   :6.923   Max.   :399.01  \n\n(mean_height.ft &lt;- mean(yrbss2$height.ft))\n\n[1] 5.548691\n\n(sd_height.ft &lt;- sd(yrbss2$height.ft))\n\n[1] 0.3434949"
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#samples-of-size-n-30-from-yrbss2",
    "href": "slides_md/Day09_bsta511_md.html#samples-of-size-n-30-from-yrbss2",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "10,000 samples of size n = 30 from yrbss2",
    "text": "10,000 samples of size n = 30 from yrbss2\n\n\nTake 10,000 random samples of size\nn = 30 from yrbss2:\n\nsamp_n30_rep10000 &lt;- yrbss2 %&gt;%\n  rep_sample_n(size = 30, \n               reps = 10000,\n               replace = FALSE)\nsamp_n30_rep10000\n\n# A tibble: 300,000 × 4\n# Groups:   replicate [10,000]\n   replicate    id height.ft weight.lb\n       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1         1  5869      5.15      145.\n 2         1  6694      5.41      127.\n 3         1  2517      5.74      130.\n 4         1  5372      6.07      180.\n 5         1  5403      6.07      163.\n 6         1  2329      6.07      182.\n 7         1  8863      5.25      125.\n 8         1  8058      5.84      135.\n 9         1   335      6.17      235.\n10         1  4698      5.58      124.\n# ℹ 299,990 more rows\n\n\n\nCalculate the mean for each of the 10,000 random samples:\n\nmeans_hght_samp_n30_rep10000 &lt;- \n  samp_n30_rep10000 %&gt;% \n  group_by(replicate) %&gt;% \n  summarise(mean_height = \n            mean(height.ft))\n\nmeans_hght_samp_n30_rep10000\n\n# A tibble: 10,000 × 2\n   replicate mean_height\n       &lt;int&gt;       &lt;dbl&gt;\n 1         1        5.59\n 2         2        5.59\n 3         3        5.51\n 4         4        5.65\n 5         5        5.64\n 6         6        5.57\n 7         7        5.61\n 8         8        5.60\n 9         9        5.52\n10        10        5.64\n# ℹ 9,990 more rows\n\n\n\n\nHow close are the mean heights for each of the 10,000 random samples?"
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#simulated-sampling-distribution-for-n-30-using-10000-sample-mean-heights",
    "href": "slides_md/Day09_bsta511_md.html#simulated-sampling-distribution-for-n-30-using-10000-sample-mean-heights",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Simulated sampling distribution for n = 30  using 10,000 sample mean heights",
    "text": "Simulated sampling distribution for n = 30  using 10,000 sample mean heights\n\n\n\nggplot(\n  means_hght_samp_n30_rep10000, \n  aes(x = mean_height)) + \n  geom_histogram() +\n  labs(title = \"Simulated \\n sampling distribution\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nCLT tells us that we can model the sampling distribution of mean heights using a normal distribution."
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#given-barx-what-are-plausible-values-of-mu",
    "href": "slides_md/Day09_bsta511_md.html#given-barx-what-are-plausible-values-of-mu",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Given \\(\\bar{x}\\), what are plausible values of \\(\\mu\\)?",
    "text": "Given \\(\\bar{x}\\), what are plausible values of \\(\\mu\\)?"
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#confidence-interval-c-i-for-the-mean-mu",
    "href": "slides_md/Day09_bsta511_md.html#confidence-interval-c-i-for-the-mean-mu",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Confidence interval (C I) for the mean \\(\\mu\\)",
    "text": "Confidence interval (C I) for the mean \\(\\mu\\)\n\n\n\\[\\overline{x}\\ \\pm\\ z^*\\times \\text{SE}\\]\nwhere\n\n\\(SE = \\frac{\\sigma}{\\sqrt{n}}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(z^*\\) depends on the confidence level\n\nFor a 95% CI, \\(z^*\\) is chosen such that 95% of the standard normal curve is between \\(-z^*\\) and \\(z^*\\)\n\n\n\n\nqnorm(.975)\n\n[1] 1.959964\n\nqnorm(.995)\n\n[1] 2.575829\n\n\n\n\nWhen can this be applied?"
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#example-c-i-for-mean-height",
    "href": "slides_md/Day09_bsta511_md.html#example-c-i-for-mean-height",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Example: C I for mean height",
    "text": "Example: C I for mean height\n\nA random sample of 30 high schoolers has mean height 5.6 ft.\nFind the 95% confidence interval for the population mean, assuming that the population standard deviation is 0.34 ft."
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#how-to-interpret-a-c-i-12",
    "href": "slides_md/Day09_bsta511_md.html#how-to-interpret-a-c-i-12",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "How to interpret a C I? (1/2)",
    "text": "How to interpret a C I? (1/2)\nSimulating Confidence Intervals:\n\nhttp://www.rossmanchance.com/applets/ConfSim.html\n\nThe figure shows CI’s from 100 simulations.\n\nThe true value of \\(\\mu\\) = 5.55 is the vertical black line.\nThe horizontal lines are 95% CI’s from 100 samples.\n\nGreen: the CI “captured” the true value of \\(\\mu\\)\nRed: the CI did not “capture” the true value of \\(\\mu\\)\n\n\n\n\nQuestion:\nWhat percent of CI’s captured the true value of \\(\\mu\\) ?"
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#how-to-interpret-a-c-i-22",
    "href": "slides_md/Day09_bsta511_md.html#how-to-interpret-a-c-i-22",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "How to interpret a C I? (2/2)",
    "text": "How to interpret a C I? (2/2)\nActual interpretation:\n\nIf we were to\n\nrepeatedly take random samples from a population and\ncalculate a 95% CI for each random sample,\n\nthen we would expect 95% of our CI’s to contain the true population parameter \\(\\mu\\).\n\n\n\n\nWhat we typically write as “shorthand”:\n\nWe are 95% confident that (the 95% confidence interval) captures the value of the population parameter.\n\nWRONG interpretation:\n\nThere is a 95% chance that (the 95% confidence interval) captures the value of the population parameter.\n\nFor one CI on its own, it either does or doesn’t contain the population parameter with probability 0 or 1. We just don’t know which!"
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#what-percent-c-i-was-being-simulated-in-this-figure",
    "href": "slides_md/Day09_bsta511_md.html#what-percent-c-i-was-being-simulated-in-this-figure",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "What percent C I was being simulated in this figure?",
    "text": "What percent C I was being simulated in this figure?\n\n\n\n\n\n\n\n\n100 CI’s are shown in the figure."
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#interpretation-of-the-mean-heights-c-i",
    "href": "slides_md/Day09_bsta511_md.html#interpretation-of-the-mean-heights-c-i",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Interpretation of the mean heights C I",
    "text": "Interpretation of the mean heights C I\nCorrect interpretation:\n\nWe are 95% confident that the mean height for high schoolers is between 5.43 and 5.67 feet.\n\nWRONG:\n\nThere is a 95% chance that the mean height for high schoolers is between 5.43 and 5.67 feet."
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#what-if-we-dont-know-sigma-13",
    "href": "slides_md/Day09_bsta511_md.html#what-if-we-dont-know-sigma-13",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "What if we don’t know \\(\\sigma\\) ? (1/3)",
    "text": "What if we don’t know \\(\\sigma\\) ? (1/3)\nSimulating Confidence Intervals: http://www.rossmanchance.com/applets/ConfSim.html\n\n\n\n\n\nThe normal distribution doesn’t have a 95% “coverage rate”\nwhen using \\(s\\) instead of \\(\\sigma\\)"
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#what-if-we-dont-know-sigma-23",
    "href": "slides_md/Day09_bsta511_md.html#what-if-we-dont-know-sigma-23",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "What if we don’t know \\(\\sigma\\) ? (2/3)",
    "text": "What if we don’t know \\(\\sigma\\) ? (2/3)\n\nIn real life, we don’t know what the population sd is ( \\(\\sigma\\) )\nIf we replace \\(\\sigma\\) with \\(s\\) in the SE formula, we add in additional variability to the SE! \\[\\frac{\\sigma}{\\sqrt{n}} ~~~~\\textrm{vs.} ~~~~ \\frac{s}{\\sqrt{n}}\\]\nThus when using \\(s\\) instead of \\(\\sigma\\) when calculating the SE, we need a different probability distribution with thicker tails than the normal distribution.\n\nIn practice this will mean using a different value than 1.96 when calculating the CI."
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#what-if-we-dont-know-sigma-33",
    "href": "slides_md/Day09_bsta511_md.html#what-if-we-dont-know-sigma-33",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "What if we don’t know \\(\\sigma\\) ? (3/3)",
    "text": "What if we don’t know \\(\\sigma\\) ? (3/3)\n\n\nThe Student’s t-distribution:\n\nIs bell shaped and symmetric with mean = 0.\nIts tails are a thicker than that of a normal distribution\n\nThe “thickness” depends on its degrees of freedom: \\(df = n–1\\) , where n = sample size.\n\nAs the degrees of freedom (sample size) increase,\n\nthe tails are less thick, and\nthe t-distribution is more like a normal distribution\nin theory, with an infinite sample size the t-distribution is a normal distribution."
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#calculating-the-c-i-for-the-population-mean-using-s",
    "href": "slides_md/Day09_bsta511_md.html#calculating-the-c-i-for-the-population-mean-using-s",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Calculating the C I for the population mean using \\(s\\)",
    "text": "Calculating the C I for the population mean using \\(s\\)\nCI for \\(\\mu\\):\n\\[\\bar{x} \\pm t^*\\cdot\\frac{s}{\\sqrt{n}}\\]\nwhere \\(t^*\\) is determined by the t-distribution and dependent on the\ndf = \\(n-1\\) and the confidence level\n\n\n\nqt gives the quartiles for a t-distribution. Need to specify\n\nthe percent under the curve to the left of the quartile\nthe degrees of freedom = n-1\n\nNote in the R output to the right that \\(t^*\\) gets closer to 1.96 as the sample size increases.\n\n\n\nqt(.975, df=9)  # df = n-1\n\n[1] 2.262157\n\nqt(.975, df=49)\n\n[1] 2.009575\n\nqt(.975, df=99)\n\n[1] 1.984217\n\nqt(.975, df=999)\n\n[1] 1.962341"
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#using-a-t-table-to-get-t",
    "href": "slides_md/Day09_bsta511_md.html#using-a-t-table-to-get-t",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Using a \\(t\\)-table to get \\(t^*\\)",
    "text": "Using a \\(t\\)-table to get \\(t^*\\)"
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#example-c-i-for-mean-height-revisited",
    "href": "slides_md/Day09_bsta511_md.html#example-c-i-for-mean-height-revisited",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Example: C I for mean height (revisited)",
    "text": "Example: C I for mean height (revisited)\n\nA random sample of 30 high schoolers has mean height 5.6 ft and standard deviation 0.34 ft.\nFind the 95% confidence interval for the population mean."
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#z-vs-t-important-comment-about-chapter-4-of-textbook",
    "href": "slides_md/Day09_bsta511_md.html#z-vs-t-important-comment-about-chapter-4-of-textbook",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "\\(z\\) vs \\(t\\)??  (& important comment about Chapter 4 of textbook)",
    "text": "\\(z\\) vs \\(t\\)??  (& important comment about Chapter 4 of textbook)\n\n\nTextbook’s rule of thumb\n\n\n(Ch 4) If \\(n \\geq 30\\) and population distribution not strongly skewed:\n\nUse normal distribution\nNo matter if using \\(\\sigma\\) or \\(s\\) for the \\(SE\\)\nIf there is skew or some large outliers, then need \\(n \\geq 50\\)\n\n(Ch 5) If \\(n &lt; 30\\) and data approximately symmetric with no large outliers:\n\nUse Student’s t-distribution\n\n\n\n\n\n\nBSTA 511 rule of thumb\n\nUse normal distribution ONLY if know \\(\\sigma\\)\n\nIf using \\(s\\) for the \\(SE\\), then use the Student’s t-distribution\n\n\nFor either case, can apply if either\n\n\\(n \\geq 30\\) and population distribution not strongly skewed\n\nIf there is skew or some large outliers, then \\(n \\geq 50\\) gives better estimates\n\n\\(n &lt; 30\\) and data approximately symmetric with no large outliers\n\nIf do not know population distribution, then check the distribution of the data."
  },
  {
    "objectID": "slides_md/Day09_bsta511_md.html#footnotes",
    "href": "slides_md/Day09_bsta511_md.html#footnotes",
    "title": "Day 9: Confidence intervals (4.2)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYouth Risk Behavior Surveillance System https://www.cdc.gov/healthyyouth/data/yrbs/index.htm (YRBSS)↩︎\nYouth Risk Behavior Surveillance System https://www.cdc.gov/healthyyouth/data/yrbs/index.htm (YRBSS)↩︎"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#where-are-we",
    "href": "slides/Day10_part1_bsta511.html#where-are-we",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#where-are-we-continuous-outcome-zoomed-in",
    "href": "slides/Day10_part1_bsta511.html#where-are-we-continuous-outcome-zoomed-in",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Where are we? Continuous outcome zoomed in",
    "text": "Where are we? Continuous outcome zoomed in"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#goals-for-today-part-1",
    "href": "slides/Day10_part1_bsta511.html#goals-for-today-part-1",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Goals for today: Part 1",
    "text": "Goals for today: Part 1\n(4.3, 5.1) Hypothesis testing for mean from one sample\n\nIntroduce hypothesis testing using the case of analyzing a mean from one sample (group)\n\n\n\n\nSteps of a hypothesis test:\n\nlevel of significance\nnull ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\ntest statistic\np-value\nconclusion\n\n\n\n\nRun a hypothesis test in R\n\nLoad a dataset - need to specify location of dataset\nR projects\nRun a t-test in R\ntidy() the test output using broom package\n\n\n\n\n(4.3.3) Confidence intervals (CIs) vs. hypothesis tests"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#goals-for-today-part-2---class-discussion",
    "href": "slides/Day10_part1_bsta511.html#goals-for-today-part-2---class-discussion",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Goals for today: Part 2 - Class discussion",
    "text": "Goals for today: Part 2 - Class discussion\n(5.2) Inference for mean difference from dependent/paired 2 samples\n\nInference: CIs and hypothesis testing\nExploratory data analysis (EDA) to visualize data\nRun paired t-test in R\n\nOne-sided CIs\nClass discussion\n\nInference for the mean difference from dependent/paired data is a special case of the inference for the mean from just one sample, that was already covered.\nThus this part will be used for class discussion to practice CIs and hypothesis testing for one mean and apply it in this new setting.\nIn class I will briefly introduce this topic, explain how it is similar and different from what we already covered, and let you work through the slides and code."
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#moritzs-tip-of-the-day-use-r-projects-to-organize-analyses",
    "href": "slides/Day10_part1_bsta511.html#moritzs-tip-of-the-day-use-r-projects-to-organize-analyses",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "MoRitz’s tip of the day: use R projects to organize analyses",
    "text": "MoRitz’s tip of the day: use R projects to organize analyses\n\n\n\n\n\n\n\n\nMoRitz loves using R projects to\n\norganize analyses and\nmake it easier to load data files\nand also save output\n\nOther bonuses include\n\nmaking to it easier to collaborate with others,\nincluding yourself when accessing files from different computers.\n\n\nWe will discuss how to use projects later in today’s slides when loading a dataset.\nSee file Projects in RStudio for more information."
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#is-98.6f-really-the-mean-healthy-body-temperature",
    "href": "slides/Day10_part1_bsta511.html#is-98.6f-really-the-mean-healthy-body-temperature",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Is 98.6°F really the mean “healthy” body temperature?",
    "text": "Is 98.6°F really the mean “healthy” body temperature?\n\nWhere did the 98.6°F value come from?\n\nGerman physician Carl Reinhold August Wunderlich determined 98.6°F (or 37°C) based on temperatures from 25,000 patients in Leipzig in 1851.\n\n1992 JAMA article by Mackowiak, Wasserman, & Levine\n\nThey claim that 98.2°F (36.8°C) is a more accurate average body temp\nSample: n = 148 healthy men and women aged 18 - 40 years\n\nIn January 2020, a group from Stanford published Decreasing human body temperature in the United States since the Industrial Revolution in eLIFE.\n\n“determined that mean body temperature in men and women, after adjusting for age, height, weight and, in some models date and time of day, has decreased monotonically by 0.03°C (0.05°F) per birth decade”\nSeptember 2023 update: Defining Usual Oral Temperature Ranges in Outpatients Using an Unsupervised Learning Algorithm in JAMA Internal Medicine\n\nAverage is 36.64 °C (97.95 °F); “range of mean temperatures for the coolest to the warmest individuals was 36.24 °C to 36.89 °C” (97.23 to 98.40 °F); based 2008-2017 data\n“findings suggest that age, sex, height, weight, and time of day are factors that contribute to variations in individualized normal temperature ranges.”\n\n\nNYT article The Average Human Body Temperature Is Not 98.6 Degrees, Oct 12, 2023, by Dana G. Smith\n\nQuestion: based on the 1992 JAMA data, is there evidence to support that the population mean body temperature is different from 98.6°F?"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#question-based-on-the-1992-jama-data-is-there-evidence-to-support-that-the-population-mean-body-temperature-is-different-from-98.6f",
    "href": "slides/Day10_part1_bsta511.html#question-based-on-the-1992-jama-data-is-there-evidence-to-support-that-the-population-mean-body-temperature-is-different-from-98.6f",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Question: based on the 1992 JAMA data, is there evidence to support that the population mean body temperature is different from 98.6°F?",
    "text": "Question: based on the 1992 JAMA data, is there evidence to support that the population mean body temperature is different from 98.6°F?\nTwo approaches to answer this question:\n\nCreate a confidence interval (CI) for the population mean \\(\\mu\\) and determine whether 98.6°F is inside the CI or not.\n\nis 98.6°F a plausible value?\n\nRun a hypothesis test to see if there is evidence that the population mean \\(\\mu\\) is significantly different from 98.6°F or not.\n\nThis does not give us a range of plausible values for the population mean \\(\\mu\\).\nInstead, we calculate a test statistic and p-value\n\nto see how likely we are to observe the sample mean \\(\\bar{x}\\)\nor a more extreme sample mean\nassuming that the population mean \\(\\mu\\) is 98.6°F."
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#approach-1-create-a-95-c-i-for-the-population-mean-body-temperature",
    "href": "slides/Day10_part1_bsta511.html#approach-1-create-a-95-c-i-for-the-population-mean-body-temperature",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Approach 1: Create a 95% C I for the population mean body temperature",
    "text": "Approach 1: Create a 95% C I for the population mean body temperature\n\nUse data based on the results from the 1992 JAMA study\n\nThe original dataset used in the JAMA article is not available\nHowever, Allen Shoemaker from Calvin College created a dataset with the same summary statistics as in the JAMA article, which we will use:\n\n\n\\[\\bar{x} = 98.25,~s=0.733,~n=130\\] CI for \\(\\mu\\):\n\n\n\\[\\begin{align}\n\\bar{x} &\\pm t^*\\cdot\\frac{s}{\\sqrt{n}}\\\\\n98.25 &\\pm 1.979\\cdot\\frac{0.733}{\\sqrt{130}}\\\\\n98.25 &\\pm 0.127\\\\\n(98.123&, 98.377)\n\\end{align}\\]\n\nUsed \\(t^*\\) = qt(.975, df=129)\nConclusion:\nWe are 95% confident that the (population) mean body temperature is between 98.123°F and 98.377°F.\n\nHow does the CI compare to 98.6°F?"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#approach-2-hypothesis-test",
    "href": "slides/Day10_part1_bsta511.html#approach-2-hypothesis-test",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Approach 2: Hypothesis Test",
    "text": "Approach 2: Hypothesis Test\nFrom before:\n\nRun a hypothesis test to see if there is evidence that the population mean \\(\\mu\\) is significantly different from 98.6°F or not.\n\nThis does not give us a range of plausible values for the population mean \\(\\mu\\).\nInstead, we calculate a test statistic and p-value\n\nto see how likely we are to observe the sample mean \\(\\bar{x}\\)\nor a more extreme sample mean\nassuming that the population mean \\(\\mu\\) is 98.6°F.\n\n\n\nHow do we calculate a test statistic and p-value?"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#recall-the-sampling-distribution-of-the-mean",
    "href": "slides/Day10_part1_bsta511.html#recall-the-sampling-distribution-of-the-mean",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Recall the sampling distribution of the mean",
    "text": "Recall the sampling distribution of the mean\nFrom the Central Limit Theorem (CLT), we know that\n\nFor “large” sample sizes ( \\(n\\geq 30\\) ),\n\nthe sampling distribution of the sample mean\ncan be approximated by a normal distribution,with\n\nmean equal to the population mean value \\(\\mu\\), and\nstandard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\n\n\n\\[\\bar{X}\\sim N\\Big(\\mu_{\\bar{X}} = \\mu, \\sigma_{\\bar{X}}= \\frac{\\sigma}{\\sqrt{n}}\\Big)\\]\n\nFor small sample sizes, if the population is known to be normally distributed, then\n\nthe same result holds"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#case-1-suppose-we-know-the-population-sd-sigma",
    "href": "slides/Day10_part1_bsta511.html#case-1-suppose-we-know-the-population-sd-sigma",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Case 1: suppose we know the population sd \\(\\sigma\\)",
    "text": "Case 1: suppose we know the population sd \\(\\sigma\\)\n\nHow likely we are to observe the sample mean \\(\\bar{x}\\) ,\n\nor a more extreme sample mean,\nassuming that the population mean \\(\\mu\\) is 98.6°F?\n\nUse \\(\\bar{x} = 98.25\\), \\(\\sigma=0.733\\), and \\(n=130\\)"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#case-2-we-dont-know-the-population-sd-sigma",
    "href": "slides/Day10_part1_bsta511.html#case-2-we-dont-know-the-population-sd-sigma",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Case 2: we don’t know the population sd \\(\\sigma\\)",
    "text": "Case 2: we don’t know the population sd \\(\\sigma\\)\n\n\n\nThis is usually the case in real life\nWe estimate \\(\\sigma\\) with the sample standard deviation \\(s\\)\nFrom last time, we know that in this case we need to use the t-distribution with d.f. = n-1, instead of the normal distribution\nQuestion: How likely we are to observe the sample mean \\(\\bar{x}\\) or a more extreme sample mean, assuming that the population mean \\(\\mu\\) is 98.6°F?\nUse \\(\\bar{x} = 98.25\\), \\(s=0.733\\), and \\(n=130\\)"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#steps-in-a-hypothesis-test",
    "href": "slides/Day10_part1_bsta511.html#steps-in-a-hypothesis-test",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Steps in a Hypothesis Test",
    "text": "Steps in a Hypothesis Test\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#step-2-null-alternative-hypotheses-12",
    "href": "slides/Day10_part1_bsta511.html#step-2-null-alternative-hypotheses-12",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 2: Null & Alternative Hypotheses (1/2)",
    "text": "Step 2: Null & Alternative Hypotheses (1/2)\nIn statistics, a hypothesis is a statement about the value of an unknown population parameter.\nA hypothesis test consists of a test between two competing hypotheses:\n\na null hypothesis \\(H_0\\) (pronounced “H-naught”) vs. \nan alternative hypothesis \\(H_A\\) (also denoted \\(H_1\\))\n\nExample of hypotheses in words:\n\\[\\begin{aligned}\nH_0 &: \\text{The population mean body temperature is 98.6°F}\\\\\n\\text{vs. } H_A &: \\text{The population mean body temperature is not 98.6°F}\n\\end{aligned}\\]\n\n\\(H_0\\) is a claim that there is “no effect” or “no difference of interest.”\n\\(H_A\\) is the claim a researcher wants to establish or find evidence to support. It is viewed as a “challenger” hypothesis to the null hypothesis \\(H_0\\)"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#step-2-null-alternative-hypotheses-22",
    "href": "slides/Day10_part1_bsta511.html#step-2-null-alternative-hypotheses-22",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 2: Null & Alternative Hypotheses (2/2)",
    "text": "Step 2: Null & Alternative Hypotheses (2/2)\nNotation for hypotheses:\n\\[\\begin{aligned}\nH_0 &: \\mu = \\mu_0\\\\\n\\text{vs. } H_A&: \\mu \\neq, &lt;, \\textrm{or}, &gt; \\mu_0\n\\end{aligned}\\]\nWe call \\(\\mu_0\\) the null value\n\n\n\\(H_A: \\mu \\neq \\mu_0\\)\n\n\nnot choosing a priori whether we believe the population mean is greater or less than the null value \\(\\mu_0\\)\n\n\n\n\\(H_A: \\mu &lt; \\mu_0\\)\n\n\nbelieve the population mean is less than the null value \\(\\mu_0\\)\n\n\n\n\\(H_A: \\mu &gt; \\mu_0\\)\n\n\nbelieve the population mean is greater than the null value \\(\\mu_0\\)\n\n\n\n\n\n\\(H_A: \\mu \\neq \\mu_0\\) is the most common option, since it’s the most conservative\n\nExample:\n\\[\\begin{aligned}\nH_0 &: \\mu = 98.6\\\\\n\\text{vs. } H_A&: \\mu \\neq 98.6\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#step-3-test-statistic-its-distribution",
    "href": "slides/Day10_part1_bsta511.html#step-3-test-statistic-its-distribution",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 3: Test statistic (& its distribution)",
    "text": "Step 3: Test statistic (& its distribution)\n\n\nCase 1: know population sd \\(\\sigma\\)\n\\[\n\\text{test statistic} = z_{\\bar{x}} = \\frac{\\bar{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\n\\]\n\nStatistical theory tells us that \\(z_{\\bar{x}}\\) follows a Standard Normal distribution \\(N(0,1)\\)\n\n\nCase 2: don’t know population sd \\(\\sigma\\)\n\\[\n\\text{test statistic} = t_{\\bar{x}} = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\n\\]\n\nStatistical theory tells us that \\(t_{\\bar{x}}\\) follows a Student’s t distribution with degrees of freedom (df) = \\(n-1\\)\n\n\n\n\n\\(\\bar{x}\\) = sample mean, \\(\\mu_0\\) = hypothesized population mean from \\(H_0\\),\n\\(\\sigma\\) = population standard deviation, \\(s\\) = sample standard deviation,\n\\(n\\) = sample size\n\nAssumptions: same as CLT\n\nIndependent observations: the observations were collected independently.\nApproximately normal sample or big n: the distribution of the sample should be approximately normal, or the sample size should be at least 30."
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#step-3-test-statistic-calculation",
    "href": "slides/Day10_part1_bsta511.html#step-3-test-statistic-calculation",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 3: Test statistic calculation",
    "text": "Step 3: Test statistic calculation\nRecall that \\(\\bar{x} = 98.25\\), \\(s=0.733\\), and \\(n=130.\\)\nThe test statistic is:\n\\[t_{\\bar{x}} = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\n= \\frac{98.25 - 98.6}{\\frac{0.73}{\\sqrt{130}}}\n= -5.45\\]\n\nStatistical theory tells us that \\(t_{\\bar{x}}\\) follows a Student’s t-distribution with \\(d.f. = n-1 = 129\\).\n\n\n\n\n\n\n\n\n\nAssumptions met?"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#step-4-p-value",
    "href": "slides/Day10_part1_bsta511.html#step-4-p-value",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\n\n\nThe \\(p\\)-value is a quantification of “surprise”\n\nAssuming \\(H_0\\) is true, how surprised are we with the observed results?\nEx: assuming that the true mean body temperature is 98.6°F, how surprised are we to get a sample mean of 98.25°F (or more extreme)?\n\nIf the \\(p\\)-value is “small,” it means there’s a small probability that we would get the observed statistic (or more extreme) when \\(H_0\\) is true."
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#step-4-p-value-calculation",
    "href": "slides/Day10_part1_bsta511.html#step-4-p-value-calculation",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 4: p-value calculation",
    "text": "Step 4: p-value calculation\nCalculate the p-value using the Student’s t-distribution with \\(d.f. = n-1 = 129\\):\n\\[p-value=P(T \\leq -5.45) + P(T \\geq 5.45) = 2.410889 \\times 10^{-07}\\]\n\n# use pt() instead of pnorm()\n# need to specify df\n2*pt(-5.4548, df = 130-1, lower.tail = TRUE)\n\n[1] 2.410889e-07"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#step-4-p-value-estimation-using-t-table",
    "href": "slides/Day10_part1_bsta511.html#step-4-p-value-estimation-using-t-table",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 4: p-value estimation using \\(t\\)-table",
    "text": "Step 4: p-value estimation using \\(t\\)-table\n\n\\(t\\)-table only gives us bounds on the p-value\nRecall from using the \\(t\\)-table for CIs, that the table gives us the cutoff values for varying tail probabilities (1-tail & 2-tail)\nFind the row with the appropriate degrees of freedom\n\nUse next smallest df in table if actual df not shown\nI.e., for df = 129, use df = 100 in table\n\nFigure out where the test statistic’s absolute value is in relation to the values in the columns, i.e. between which columns is the test statistic?\nThe header rows for those columns gives the lower & upper bounds for the p-value\n\nChoosing one-tail vs. two-tail test, depends on the alternative hypothesis \\(H_A\\).\nFor a 2-sided test ( \\(H_A: \\mu \\neq \\mu_0\\) ), use two-tails\nFor a 1-sided test ( \\(H_A: \\mu &lt; \\textrm{or} &gt; \\mu_0\\) ), use one-tail"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#using-a-t-table-to-estimate-p-value",
    "href": "slides/Day10_part1_bsta511.html#using-a-t-table-to-estimate-p-value",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Using a \\(t\\)-table to estimate p-value",
    "text": "Using a \\(t\\)-table to estimate p-value"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#step-1-significance-level-alpha",
    "href": "slides/Day10_part1_bsta511.html#step-1-significance-level-alpha",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 1: Significance Level \\(\\alpha\\)",
    "text": "Step 1: Significance Level \\(\\alpha\\)\n\nBefore doing a hypothesis test, we set a cut-off for how small the \\(p\\)-value should be in order to reject \\(H_0\\).\nWe call this the significance level, denoted by the Greek symbol alpha ( \\(\\alpha\\) )\nTypical \\(\\alpha\\) values are\n\n0.05 - most common by far!!\n0.01 and 0.1\n\nDecision rule:\n\nWhen \\(p\\)-value &lt; \\(\\alpha\\), we “reject the null hypothesis \\(H_0\\).”\nWhen \\(p\\)-value \\(\\geq \\alpha\\), we “fail to reject the null hypothesis \\(H_0\\).”\n\n\n\n\n\n\n\n\nImportant\n\n\n\n“Failing to reject” \\(H_0\\) is NOT the same as “accepting” \\(H_0\\)!\nBy failing to reject \\(H_0\\) we are just saying that we don’t have sufficient evidence to support the alternative \\(H_A\\).\nThis does not imply that \\(H_0\\) is true!!"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#step-5-conclusion-to-hypothesis-test",
    "href": "slides/Day10_part1_bsta511.html#step-5-conclusion-to-hypothesis-test",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\\[\\begin{aligned}\nH_0 &: \\mu = 98.6\\\\\n\\text{vs. } H_A&: \\mu \\neq 98.6\n\\end{aligned}\\]\n\nRecall the \\(p\\)-value = \\(2.410889 \\times 10^{-07}\\)\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nBasic: (“stats class” conclusion)\n\nThere is sufficient evidence that the (population) mean body temperature is discernibly different from 98.6°F ( \\(p\\)-value &lt; 0.001).\n\nBetter: (“manuscript style” conclusion)\n\nThe average body temperature in the sample was 98.25°F (95% CI 98.12, 98.38°F), which is discernibly different from 98.6°F ( \\(p\\)-value &lt; 0.001)."
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#confidence-intervals-vs.-hypothesis-testing",
    "href": "slides/Day10_part1_bsta511.html#confidence-intervals-vs.-hypothesis-testing",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Confidence Intervals vs. Hypothesis Testing",
    "text": "Confidence Intervals vs. Hypothesis Testing\n\nSee also V&H Section 4.3.3"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#working-directory",
    "href": "slides/Day10_part1_bsta511.html#working-directory",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Working directory",
    "text": "Working directory\n\nIn order to load a dataset from a file, you need to tell R where the dataset is located\nTo do this you also need to know the location from which R is working, i.e. your working directory\nYou can figure out your working directory by running the getwd() function.\n\n\ngetwd()\n\n[1] \"/Users/niederha/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/teaching/BSTA 511/F24/0_webpage/BSTA_511_F24\"\n\n\n\nAbove is the working directory of this slides file\n\nIn this case, this is NOT the location of the actual qmd file though!\n\nTo make it easier to juggle the working directory, the location of your qmd file, and the location of the data,\n\nI highly recommend using R Projects!"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#r-projects",
    "href": "slides/Day10_part1_bsta511.html#r-projects",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "R projects",
    "text": "R projects\n\nI highly, highly, HIGHLY recommend using R Projects to organize your analyses and make it easier to load data files and also save output.\nWhen you create an R Project on your computer, the Project is associated with the folder (directory) you created it in.\n\nThis folder becomes the “root” of your working directory, and RStudio’s point of reference from where to load files from and to.\n\nI create separate Projects for every analysis project and every class I teach.\nYou can run multiple sessions of RStudio by opening different Projects, and the environments (or working directory) of each are working independently of each other.\n\n\n\n\n\n\n\nNote\n\n\n\nAlthough we are using Quarto files,\n\nI will show how to set up and use a “regular” R Project\ninstead of “Quarto Project”\n\nQuarto Projects include extra features and thus complexity. Once you are used to how regular R Projects work, you can try out a Quarto Project."
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#how-to-create-an-r-project",
    "href": "slides/Day10_part1_bsta511.html#how-to-create-an-r-project",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "How to create an R Project",
    "text": "How to create an R Project\n\nDemonstration in class recording\nPosit’s (RStudio’s) directions for creating Projects\n\nhttps://support.rstudio.com/hc/en-us/articles/200526207-Using-RStudio-Projects\n\nSee file Projects in RStudio for more information on R Projects."
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#load-the-dataset",
    "href": "slides/Day10_part1_bsta511.html#load-the-dataset",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Load the dataset",
    "text": "Load the dataset\n\nThe data are in a csv file called BodyTemperatures.csv\nYou need to tell R where the dataset is located!\nI recommend saving all datasets in a folder called data.\n\nThe code I will be providing you will be set up this way.\n\nTo make it easier to specify where the dataset is located, I recommend using the here() function from the here package: here::here().\n\n\n# read_csv() is a function from the readr package that is a part of the tidyverse\nlibrary(here)   # first install this package\n\nBodyTemps &lt;- read_csv(here::here(\"data\", \"BodyTemperatures.csv\"))\n#                     location: look in \"data\" folder\n#                               for the file \"BodyTemperatures.csv\"\n\nglimpse(BodyTemps)\n\nRows: 130\nColumns: 3\n$ Temperature &lt;dbl&gt; 96.3, 96.7, 96.9, 97.0, 97.1, 97.1, 97.1, 97.2, 97.3, 97.4…\n$ Gender      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ HeartRate   &lt;dbl&gt; 70, 71, 74, 80, 73, 75, 82, 64, 69, 70, 68, 72, 78, 70, 75…"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#herehere",
    "href": "slides/Day10_part1_bsta511.html#herehere",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "here::here()",
    "text": "here::here()\nGeneral use of here::here()\nhere::here(\"folder_name\", \"filename\")\n\n\nResources for here::here():\n\nhow to use the here package (Jenny Richmond)\nOde to the here package (Jenny Bryan)\n\nProject-oriented workflow (Jenny Bryan)\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#t.test-base-rs-function-for-testing-one-mean",
    "href": "slides/Day10_part1_bsta511.html#t.test-base-rs-function-for-testing-one-mean",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "t.test: base R’s function for testing one mean",
    "text": "t.test: base R’s function for testing one mean\n\nUse the body temperature example with \\(H_A: \\mu \\neq 98.6\\)\nWe called the dataset BodyTemps when we loaded it\n\n\nglimpse(BodyTemps)\n\nRows: 130\nColumns: 3\n$ Temperature &lt;dbl&gt; 96.3, 96.7, 96.9, 97.0, 97.1, 97.1, 97.1, 97.2, 97.3, 97.4…\n$ Gender      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ HeartRate   &lt;dbl&gt; 70, 71, 74, 80, 73, 75, 82, 64, 69, 70, 68, 72, 78, 70, 75…\n\n(temps_ttest &lt;- t.test(x = BodyTemps$Temperature,\n       # alternative = \"two.sided\",  # default\n       mu = 98.6))\n\n\n    One Sample t-test\n\ndata:  BodyTemps$Temperature\nt = -5.4548, df = 129, p-value = 2.411e-07\nalternative hypothesis: true mean is not equal to 98.6\n95 percent confidence interval:\n 98.12200 98.37646\nsample estimates:\nmean of x \n 98.24923 \n\n\nNote that the test output also gives the 95% CI using the t-distribution."
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#tidy-the-t.test-output",
    "href": "slides/Day10_part1_bsta511.html#tidy-the-t.test-output",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "tidy() the t.test output",
    "text": "tidy() the t.test output\n\nUse the tidy() function from the broom package for briefer output in table format that’s stored as a tibble\nCombined with the gt() function from the gt package, we get a nice table\n\n\ntidy(temps_ttest) %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    98.24923\n-5.454823\n2.410632e-07\n129\n98.122\n98.37646\nOne Sample t-test\ntwo.sided\n  \n  \n  \n\n\n\n\n\nSince the tidy() output is a tibble, we can easily pull() specific values from it:\n\n\n\nUsing base R’s $\n\ntidy(temps_ttest)$p.value  \n\n[1] 2.410632e-07\n\n\nAdvantage: quick and easy\n\nOr the tidyverse way: using pull() from dplyr package\n\ntidy(temps_ttest) %&gt;% pull(p.value)\n\n[1] 2.410632e-07\n\n\nAdvantage: can use together with piping (%&gt;%) other functions"
  },
  {
    "objectID": "slides/Day10_part1_bsta511.html#whats-next",
    "href": "slides/Day10_part1_bsta511.html#whats-next",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "What’s next?",
    "text": "What’s next?\nCI’s and hypothesis testing for different scenarios:\n\n\n\n\n\n\n\n\n\n\n\nDay\nSection\nPopulation parameter\nSymbol\nPoint estimate\nSymbol\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\n\n11\n5.3\nDiff in pop means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n\n\n12\n8.2\nDiff in pop prop’s\n\\(p_1-p_2\\)\nDiff in sample prop’s\n\\(\\widehat{p}_1-\\widehat{p}_2\\)"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#where-are-we",
    "href": "slides/Day10_part2_bsta511.html#where-are-we",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#where-are-we-continuous-outcome-zoomed-in",
    "href": "slides/Day10_part2_bsta511.html#where-are-we-continuous-outcome-zoomed-in",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Where are we? Continuous outcome zoomed in",
    "text": "Where are we? Continuous outcome zoomed in"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#what-we-covered-in-day-10-part-1",
    "href": "slides/Day10_part2_bsta511.html#what-we-covered-in-day-10-part-1",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "What we covered in Day 10 Part 1",
    "text": "What we covered in Day 10 Part 1\n(4.3, 5.1) Hypothesis testing for mean from one sample\n\nIntroduce hypothesis testing using the case of analyzing a mean from one sample (group)\n\n\n\n\nSteps of a hypothesis test:\n\nlevel of significance\nnull ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\ntest statistic\np-value\nconclusion\n\n\n\n\nRun a hypothesis test in R\n\nLoad a dataset - need to specify location of dataset\nR projects\nRun a t-test in R\ntidy() the test output using broom package\n\n\n\n\n(4.3.3) Confidence intervals (CIs) vs. hypothesis tests"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#goals-for-today-part-2---class-discussion",
    "href": "slides/Day10_part2_bsta511.html#goals-for-today-part-2---class-discussion",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Goals for today: Part 2 - Class discussion",
    "text": "Goals for today: Part 2 - Class discussion\n(5.2) Inference for mean difference from dependent/paired 2 samples\n\nInference: CIs and hypothesis testing\nExploratory data analysis (EDA) to visualize data\nRun paired t-test in R\n\nOne-sided CIs\nClass discussion\n\nInference for the mean difference from dependent/paired data is a special case of the inference for the mean from just one sample, that was already covered.\nThus this part will be used for class discussion to practice CIs and hypothesis testing for one mean and apply it in this new setting.\nIn class I will briefly introduce this topic, explain how it is similar and different from what we already covered, and let you work through the slides and code."
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#cis-and-hypothesis-tests-for-different-scenarios",
    "href": "slides/Day10_part2_bsta511.html#cis-and-hypothesis-tests-for-different-scenarios",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "CI’s and hypothesis tests for different scenarios:",
    "text": "CI’s and hypothesis tests for different scenarios:\n\n\\[\\text{point estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n???\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#steps-in-a-hypothesis-test",
    "href": "slides/Day10_part2_bsta511.html#steps-in-a-hypothesis-test",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Steps in a Hypothesis Test",
    "text": "Steps in a Hypothesis Test\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#examples-of-paired-designs-two-samples",
    "href": "slides/Day10_part2_bsta511.html#examples-of-paired-designs-two-samples",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Examples of paired designs (two samples)",
    "text": "Examples of paired designs (two samples)\n\nEnroll pairs of identical twins to study a disease\nEnroll father & son pairs to study cholesterol levels\nStudying pairs of eyes\nEnroll people and collect data before & after an intervention (longitudinal data)\nTextbook example: Compare maximal speed of competitive swimmers wearing a wetsuit vs. wearing a regular swimsuit\n\nWIll use these data on homework\n\n\nCome up with 2 more examples of paired study designs."
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#can-a-vegetarian-diet-change-cholesterol-levels",
    "href": "slides/Day10_part2_bsta511.html#can-a-vegetarian-diet-change-cholesterol-levels",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Can a vegetarian diet change cholesterol levels?",
    "text": "Can a vegetarian diet change cholesterol levels?\n\nScenario:\n\n24 non-vegetarian people were enrolled in a study\nThey were instructed to adopt a vegetarian diet\nCholesterol levels were measured before and after the diet\n\nQuestion: Is there evidence to support that cholesterol levels changed after the vegetarian diet?\nHow to answer the question?\n\nFirst, calculate changes (differences) in cholesterol levels\n\nWe usually do after - before if the data are longitudinal\n\n\n\n\n\nCalculate CI for the\nmean difference \\(\\delta\\):\n\\[\\bar{x}_d \\pm t^*\\cdot\\frac{s_d}{\\sqrt{n}}\\]\n\nRun a hypothesis test\nHypotheses\n\\[\\begin{align}\nH_0:& \\delta = \\delta_0 \\\\\nH_A:& \\delta \\neq \\delta_0 \\\\\n(or&~ &lt;, &gt;)\n\\end{align}\\]\n\n Test statistic\n\\[\nt_{\\bar{x}_d} = \\frac{\\bar{x}_d - \\delta_0}{\\frac{s_d}{\\sqrt{n}}}\n\\]"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#eda-explore-the-cholesterol-data",
    "href": "slides/Day10_part2_bsta511.html#eda-explore-the-cholesterol-data",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "EDA: Explore the cholesterol data",
    "text": "EDA: Explore the cholesterol data\n\nScenario:\n\n24 non-vegetarian people were enrolled in a study\nThey were instructed to adopt a vegetarian diet\nCholesterol levels were measured before and after the diet\n\n\n\nchol &lt;- read_csv(here::here(\"data\", \"chol213.csv\"))\nglimpse(chol)\n\nRows: 24\nColumns: 2\n$ Before &lt;dbl&gt; 195, 145, 205, 159, 244, 166, 250, 236, 192, 224, 238, 197, 169…\n$ After  &lt;dbl&gt; 146, 155, 178, 146, 208, 147, 202, 215, 184, 208, 206, 169, 182…\n\nchol %&gt;% \n  get_summary_stats(type = \"common\") %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      variable\n      n\n      min\n      max\n      median\n      iqr\n      mean\n      sd\n      se\n      ci\n    \n  \n  \n    Before\n24\n137\n250\n179\n44.5\n187.792\n33.160\n6.769\n14.002\n    After\n24\n125\n215\n165\n38.0\n168.250\n26.796\n5.470\n11.315\n  \n  \n  \n\n\n\n\nMake sure you are able to load the data on your computer!"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#eda-cholesterol-levels-before-and-after-vegetarian-diet",
    "href": "slides/Day10_part2_bsta511.html#eda-cholesterol-levels-before-and-after-vegetarian-diet",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "EDA: Cholesterol levels before and after vegetarian diet",
    "text": "EDA: Cholesterol levels before and after vegetarian diet\nDescribe the distributions of the before & after data.\n\n\n\nggplot(chol, aes(x=Before)) +\n  geom_density()\n\n\n\nggplot(chol, aes(x=Before)) +\n  geom_boxplot()\n\n\n\n\n\n\nggplot(chol, aes(x=After)) +\n  geom_density()\n\n\n\nggplot(chol, aes(x=After)) +\n  geom_boxplot()"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#eda-spaghetti-plot-of-cholesterol-levels-before-after-diet",
    "href": "slides/Day10_part2_bsta511.html#eda-spaghetti-plot-of-cholesterol-levels-before-after-diet",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "EDA: Spaghetti plot of cholesterol levels before & after diet",
    "text": "EDA: Spaghetti plot of cholesterol levels before & after diet\n\nVisualize the individual before vs. after diet changes in cholesterol levels\n\nWhat does this figure tell us?\n\n\nSee code file for how to wrangle the data and create the figure - you will not be expected to do this yourself."
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#eda-differences-in-cholesterol-levels-after---before-diet",
    "href": "slides/Day10_part2_bsta511.html#eda-differences-in-cholesterol-levels-after---before-diet",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "EDA: Differences in cholesterol levels: After - Before diet",
    "text": "EDA: Differences in cholesterol levels: After - Before diet\nWhat is this code doing?\n\nchol &lt;- chol %&gt;% \n  mutate(DiffChol = After-Before) \nhead(chol, 8)\n\n# A tibble: 8 × 3\n  Before After DiffChol\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1    195   146      -49\n2    145   155       10\n3    205   178      -27\n4    159   146      -13\n5    244   208      -36\n6    166   147      -19\n7    250   202      -48\n8    236   215      -21\n\n\nIs the mean of DiffChol the same as the difference in means of After - Before? Should it be? Why or why not?\n\nchol %&gt;% \n  get_summary_stats(type = \"common\") %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      variable\n      n\n      min\n      max\n      median\n      iqr\n      mean\n      sd\n      se\n      ci\n    \n  \n  \n    Before\n24\n137\n250\n179\n44.50\n187.792\n33.160\n6.769\n14.002\n    After\n24\n125\n215\n165\n38.00\n168.250\n26.796\n5.470\n11.315\n    DiffChol\n24\n-49\n13\n-19\n20.25\n-19.542\n16.806\n3.430\n7.096"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#eda-differences-in-cholesterol-levels-after---before-diet-1",
    "href": "slides/Day10_part2_bsta511.html#eda-differences-in-cholesterol-levels-after---before-diet-1",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "EDA: Differences in cholesterol levels: After - Before diet",
    "text": "EDA: Differences in cholesterol levels: After - Before diet\nCompare and contrast the 3 distributions. Comment on shape, center, and spread.\n\n\nBefore & After\n\nggplot(chol, aes(x=Before)) +\n  geom_density()\n\n\n\nggplot(chol, aes(x=After)) +\n  geom_density()\n\n\n\n\n\nDiffChol\n\nggplot(chol, aes(x=DiffChol)) + \n  geom_density()\n\n\n\nggplot(chol, aes(x=DiffChol)) + \n  geom_boxplot()"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#steps-in-a-hypothesis-test-1",
    "href": "slides/Day10_part2_bsta511.html#steps-in-a-hypothesis-test-1",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Steps in a Hypothesis Test",
    "text": "Steps in a Hypothesis Test\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#step-2-null-alternative-hypotheses",
    "href": "slides/Day10_part2_bsta511.html#step-2-null-alternative-hypotheses",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Step 2: Null & Alternative Hypotheses",
    "text": "Step 2: Null & Alternative Hypotheses\n\nQuestion: Is there evidence to support that cholesterol levels changed after the vegetarian diet?\n\n\n\nNull and alternative hypotheses in words Include as much context as possible\n\n\n\\(H_0\\): The population mean difference in cholesterol levels after a vegetarian diet is fill in\n\\(H_A\\): The population mean difference in cholesterol levels after a vegetarian diet is fill in\n\n\nNull and alternative hypotheses in symbols\nfill in the missing parts of the hypotheses.\n\\[\\begin{align}\nH_0:& \\delta = \\\\\nH_A:& \\delta \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#step-3-test-statistic",
    "href": "slides/Day10_part2_bsta511.html#step-3-test-statistic",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Step 3: Test statistic",
    "text": "Step 3: Test statistic\n\nchol %&gt;% select(DiffChol) %&gt;% get_summary_stats(type = \"common\") %&gt;% gt()\n\n\n\n\n\n  \n    \n      variable\n      n\n      min\n      max\n      median\n      iqr\n      mean\n      sd\n      se\n      ci\n    \n  \n  \n    DiffChol\n24\n-49\n13\n-19\n20.25\n-19.542\n16.806\n3.43\n7.096\n  \n  \n  \n\n\n\n\n\n\n\\[\nt_{\\bar{x}_d} = \\frac{\\bar{x}_d - \\delta_0}{\\frac{s_d}{\\sqrt{n}}}\n\\]\n\nCalculate the test statistic.\nBased on the value of the test statistic, do you think we are going to reject or fail to reject \\(H_0\\)?\nWhat probability distribution does the test statistic have?\nAre the assumptions for a paired t-test satisfied so that we can use the probability distribution to calculate the \\(p\\)-value??"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#step-4-p-value",
    "href": "slides/Day10_part2_bsta511.html#step-4-p-value",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate the p-value and shade in the area representing the p-value:"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#step-5-conclusion-to-hypothesis-test",
    "href": "slides/Day10_part2_bsta511.html#step-5-conclusion-to-hypothesis-test",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\\[\\begin{align}\nH_0:& \\delta = 0 \\\\\nH_A:& \\delta \\neq 0 \\\\\n\\end{align}\\]\n\nRecall the \\(p\\)-value = \\(8.434775 \\cdot 10 ^{-6}\\)\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nStats class conclusion\n\nThere is sufficient evidence that the (population) mean difference in cholesterol levels after a vegetarian diet is different from 0 mg/dL ( \\(p\\)-value &lt; 0.001).\n\nMore realistic manuscript conclusion:\n\nAfter a vegetarian diet, cholesterol levels decreased by on average 19.54 mg/dL (SE = 3.43 mg/dL, 2-sided \\(p\\)-value &lt; 0.001)."
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#ci-for-the-mean-difference-in-cholesterol-levels",
    "href": "slides/Day10_part2_bsta511.html#ci-for-the-mean-difference-in-cholesterol-levels",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "95% CI for the mean difference in cholesterol levels",
    "text": "95% CI for the mean difference in cholesterol levels\n\nchol %&gt;% \n  select(DiffChol) %&gt;% \n  get_summary_stats(type = \"common\") %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      variable\n      n\n      min\n      max\n      median\n      iqr\n      mean\n      sd\n      se\n      ci\n    \n  \n  \n    DiffChol\n24\n-49\n13\n-19\n20.25\n-19.542\n16.806\n3.43\n7.096\n  \n  \n  \n\n\n\n\nCI for \\(\\mu_d\\) (or \\(\\delta\\)): How was \\(t^*\\) calculated?\n\n\n\\[\\begin{align}\n\\bar{x}_d &\\pm t^*\\cdot\\frac{s_d}{\\sqrt{n}}\\\\\n-19.542 &\\pm 2.069\\cdot\\frac{16.806}{\\sqrt{24}}\\\\\n-19.542 &\\pm 2.069\\cdot 3.43\\\\\n-19.542 &\\pm 7.096\\\\\n(-26.638&, -12.445)\n\\end{align}\\]\n\nConclusion:\nWe are 95% that the (population) mean difference in cholesterol levels after a vegetarian diet is between -26.638 mg/dL and -12.445 mg/dL.\n\nBased on the CI, is there evidence the diet made a difference in cholesterol levels? Why or why not?"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#r-option-1-run-a-1-sample-t.test-using-the-paired-differences",
    "href": "slides/Day10_part2_bsta511.html#r-option-1-run-a-1-sample-t.test-using-the-paired-differences",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "R option 1: Run a 1-sample t.test using the paired differences",
    "text": "R option 1: Run a 1-sample t.test using the paired differences\n\\(H_A: \\delta \\neq 0\\)\n\nt.test(x = chol$DiffChol, mu = 0)\n\n\n    One Sample t-test\n\ndata:  chol$DiffChol\nt = -5.6965, df = 23, p-value = 8.435e-06\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -26.63811 -12.44522\nsample estimates:\nmean of x \n-19.54167 \n\n\nRun the code without mu = 0. Do the results change? Why or why not?"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#r-option-2-run-a-2-sample-t.test-with-paired-true-option",
    "href": "slides/Day10_part2_bsta511.html#r-option-2-run-a-2-sample-t.test-with-paired-true-option",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "R option 2: Run a 2-sample t.test with paired = TRUE option",
    "text": "R option 2: Run a 2-sample t.test with paired = TRUE option\n\\(H_A: \\delta \\neq 0\\)\n\nFor a 2-sample t-test we specify both x= and y=\nNote: mu = 0 is the default value and doesn’t need to be specified\n\n\nt.test(x = chol$Before, y = chol$After, mu = 0, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  chol$Before and chol$After\nt = 5.6965, df = 23, p-value = 8.435e-06\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 12.44522 26.63811\nsample estimates:\nmean difference \n       19.54167 \n\n\nWhat is different in the output compared to option 1?"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#r-option-3-run-a-2-sample-t.test-with-paired-true-option-but-using-the-long-data-and-a-formula-12",
    "href": "slides/Day10_part2_bsta511.html#r-option-3-run-a-2-sample-t.test-with-paired-true-option-but-using-the-long-data-and-a-formula-12",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "R option 3: Run a 2-sample t.test with paired = TRUE option, but using the long data and a “formula” (1/2)",
    "text": "R option 3: Run a 2-sample t.test with paired = TRUE option, but using the long data and a “formula” (1/2)\n\n\n\nThe data have to be in a long format for option 3, where each person has 2 rows: one for Before and one for After.\n\nThe long dataset chol_long was created for the slide “EDA: Spaghetti plot of cholesterol levels before & after diet”.\nSee the code to create it there.\n\nWhat information is being stored in each of the columns?\n\n\n\n# first 16 rows of long data:\nhead(chol_long, 16)\n\n# A tibble: 16 × 3\n   ID    Time   Cholesterol\n   &lt;fct&gt; &lt;fct&gt;        &lt;dbl&gt;\n 1 1     Before         195\n 2 1     After          146\n 3 2     Before         145\n 4 2     After          155\n 5 3     Before         205\n 6 3     After          178\n 7 4     Before         159\n 8 4     After          146\n 9 5     Before         244\n10 5     After          208\n11 6     Before         166\n12 6     After          147\n13 7     Before         250\n14 7     After          202\n15 8     Before         236\n16 8     After          215"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#r-option-3-run-a-2-sample-t.test-with-paired-true-option-but-using-the-long-data-and-a-formula-22",
    "href": "slides/Day10_part2_bsta511.html#r-option-3-run-a-2-sample-t.test-with-paired-true-option-but-using-the-long-data-and-a-formula-22",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "R option 3: Run a 2-sample t.test with paired = TRUE option, but using the long data and a “formula” (2/2)",
    "text": "R option 3: Run a 2-sample t.test with paired = TRUE option, but using the long data and a “formula” (2/2)\n\n\n\n\nUse the usual t.test\nWhat’s different is that\n\ninstead of specifying the variables with x= and y=,\nwe give a formula of the form y ~ x using just the variable names,\nand then specify the name of the dataset using data =\n\nThis method is often used in practice, and more similar to the coding style of running a regression model (BSTA 512 & 513)\n\n\n\n\n# using long data \n# with columns Cholesterol & Time\nt.test(Cholesterol ~ Time, \n       paired = TRUE, \n       data = chol_long)\n\n\n    Paired t-test\n\ndata:  Cholesterol by Time\nt = 5.6965, df = 23, p-value = 8.435e-06\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 12.44522 26.63811\nsample estimates:\nmean difference \n       19.54167 \n\n\n\nWhat is different in the output compared to option 1?\nRerun the test using Time ~ Cholesterol (switch the variables). What do you get?"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#compare-the-3-options",
    "href": "slides/Day10_part2_bsta511.html#compare-the-3-options",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Compare the 3 options",
    "text": "Compare the 3 options\n\nHow is the code similar and different for the 3 options?\nGiven a dataset, how would you choose which of the 3 options to use?\n\n\n# option 1\nt.test(x = chol$DiffChol, mu = 0) %&gt;% tidy() %&gt;% gt() # tidy from broom package\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    -19.54167\n-5.696519\n8.434775e-06\n23\n-26.63811\n-12.44522\nOne Sample t-test\ntwo.sided\n  \n  \n  \n\n\n\n# option 2\nt.test(x = chol$Before, y = chol$After, mu = 0, paired = TRUE) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    19.54167\n5.696519\n8.434775e-06\n23\n12.44522\n26.63811\nPaired t-test\ntwo.sided\n  \n  \n  \n\n\n\n# option 3\nt.test(Cholesterol ~ Time, paired = TRUE, data = chol_long) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    19.54167\n5.696519\n8.434775e-06\n23\n12.44522\n26.63811\nPaired t-test\ntwo.sided"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#what-if-we-wanted-to-test-whether-the-diet-decreased-cholesterol-levels",
    "href": "slides/Day10_part2_bsta511.html#what-if-we-wanted-to-test-whether-the-diet-decreased-cholesterol-levels",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "What if we wanted to test whether the diet decreased cholesterol levels?",
    "text": "What if we wanted to test whether the diet decreased cholesterol levels?\n\n\nWhat changes in each of the steps?\n\nSet the level of significance \\(\\alpha\\)\nSpecify the hypotheses \\(H_0\\) and \\(H_A\\)\n\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#r-what-if-we-wanted-to-test-whether-the-diet-decreased-cholesterol-levels",
    "href": "slides/Day10_part2_bsta511.html#r-what-if-we-wanted-to-test-whether-the-diet-decreased-cholesterol-levels",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "R: What if we wanted to test whether the diet decreased cholesterol levels?",
    "text": "R: What if we wanted to test whether the diet decreased cholesterol levels?\n\nWhich of the 3 options to run a paired t-test in R is being used below?\nHow did the code change to account for testing a decrease in cholesterol levels?\nWhich values in the output changed compared to testing for a change in cholesterol levels? How did they change?\n\n\n# alternative = c(\"two.sided\", \"less\", \"greater\")\nt.test(x = chol$DiffChol, mu = 0, alternative = \"less\") %&gt;% \n  tidy() %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    -19.54167\n-5.696519\n4.217387e-06\n23\n-Inf\n-13.6623\nOne Sample t-test\nless"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#one-sided-confidence-intervals",
    "href": "slides/Day10_part2_bsta511.html#one-sided-confidence-intervals",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "One-sided confidence intervals",
    "text": "One-sided confidence intervals\n\n\nFormula for a 2-sided (1- \\(\\alpha\\) )% CI:\n\\[\\bar{x} \\pm t^*\\cdot\\frac{s}{\\sqrt{n}}\\]\n\n\\(t^*\\) = qt(1-alpha/2, df = n-1)\n\\(\\alpha\\) is split over both tails of the distribution\n\n\n\n\nA one-sided (1- \\(\\alpha\\) )% CI has all (1- \\(\\alpha\\) )% on just the left or the right tail of the distribution:\n\\[\\begin{align}\n(\\bar{x} - t^*\\cdot\\frac{s}{\\sqrt{n}},~\\infty) \\\\\n(\\infty,~\\bar{x} + t^*\\cdot\\frac{s}{\\sqrt{n}})\n\\end{align}\\]\n\n\\(t^*\\) = qt(1-alpha, df = n-1) for a\n1-sided lower (1- \\(\\alpha\\) )% CI\n\\(t^*\\) = qt(alpha, df = n-1) for a 1-sided upper (1- \\(\\alpha\\) )% CI\nA 1-sided CI gives estimates for a lower or upper bound of the population mean.\nSee Section 4.2.3 of the V&H book for more"
  },
  {
    "objectID": "slides/Day10_part2_bsta511.html#today-whats-next",
    "href": "slides/Day10_part2_bsta511.html#today-whats-next",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Today & what’s next?",
    "text": "Today & what’s next?\n\nCI’s and hypothesis tests for different scenarios:\n\\[\\text{point estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\\(\\frac{s_d}{\\sqrt{n}}\\)\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n???\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#where-are-we-continuous-outcome-zoomed-in",
    "href": "slides_md/Day10_part1_bsta511_md.html#where-are-we-continuous-outcome-zoomed-in",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Where are we? Continuous outcome zoomed in",
    "text": "Where are we? Continuous outcome zoomed in"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#goals-for-today-part-1",
    "href": "slides_md/Day10_part1_bsta511_md.html#goals-for-today-part-1",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Goals for today: Part 1",
    "text": "Goals for today: Part 1\n\n(4.3, 5.1) Hypothesis testing for mean from one sample\n\nIntroduce hypothesis testing using the case of analyzing a mean from one sample (group)\n\n\n\n\nSteps of a hypothesis test:\n\nlevel of significance\nnull ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\ntest statistic\np-value\nconclusion\n\n\n\n\nRun a hypothesis test in R\n\nLoad a dataset - need to specify location of dataset\nR projects\nRun a t-test in R\ntidy() the test output using broom package\n\n\n\n\n\n\n(4.3.3) Confidence intervals (CIs) vs. hypothesis tests"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#goals-for-today-part-2---class-discussion",
    "href": "slides_md/Day10_part1_bsta511_md.html#goals-for-today-part-2---class-discussion",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Goals for today: Part 2 - Class discussion",
    "text": "Goals for today: Part 2 - Class discussion\n\n(5.2) Inference for mean difference from dependent/paired 2 samples\n\nInference: CIs and hypothesis testing\nExploratory data analysis (EDA) to visualize data\nRun paired t-test in R\n\n\n\nOne-sided CIs\n\n\nClass discussion\n\nInference for the mean difference from dependent/paired data is a special case of the inference for the mean from just one sample, that was already covered.\nThus this part will be used for class discussion to practice CIs and hypothesis testing for one mean and apply it in this new setting.\nIn class I will briefly introduce this topic, explain how it is similar and different from what we already covered, and let you work through the slides and code."
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#moritzs-tip-of-the-day-use-r-projects-to-organize-analyses",
    "href": "slides_md/Day10_part1_bsta511_md.html#moritzs-tip-of-the-day-use-r-projects-to-organize-analyses",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "MoRitz’s tip of the day: use R projects to organize analyses",
    "text": "MoRitz’s tip of the day: use R projects to organize analyses\n\n\n\n\n\n\n\n\nMoRitz loves using R projects to\n\norganize analyses and\nmake it easier to load data files\nand also save output\n\nOther bonuses include\n\nmaking to it easier to collaborate with others,\nincluding yourself when accessing files from different computers.\n\n\nWe will discuss how to use projects later in today’s slides when loading a dataset.\nSee file Projects in RStudio for more information."
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#is-98.6f-really-the-mean-healthy-body-temperature",
    "href": "slides_md/Day10_part1_bsta511_md.html#is-98.6f-really-the-mean-healthy-body-temperature",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Is 98.6°F really the mean “healthy” body temperature?",
    "text": "Is 98.6°F really the mean “healthy” body temperature?\n\nWhere did the 98.6°F value come from?\n\nGerman physician Carl Reinhold August Wunderlich determined 98.6°F (or 37°C) based on temperatures from 25,000 patients in Leipzig in 1851.\n\n1992 JAMA article by Mackowiak, Wasserman, & Levine\n\nThey claim that 98.2°F (36.8°C) is a more accurate average body temp\nSample: n = 148 healthy men and women aged 18 - 40 years\n\nIn January 2020, a group from Stanford published Decreasing human body temperature in the United States since the Industrial Revolution in eLIFE.\n\n“determined that mean body temperature in men and women, after adjusting for age, height, weight and, in some models date and time of day, has decreased monotonically by 0.03°C (0.05°F) per birth decade”\nSeptember 2023 update: Defining Usual Oral Temperature Ranges in Outpatients Using an Unsupervised Learning Algorithm in JAMA Internal Medicine\n\nAverage is 36.64 °C (97.95 °F); “range of mean temperatures for the coolest to the warmest individuals was 36.24 °C to 36.89 °C” (97.23 to 98.40 °F); based 2008-2017 data\n“findings suggest that age, sex, height, weight, and time of day are factors that contribute to variations in individualized normal temperature ranges.”\n\n\nNYT article The Average Human Body Temperature Is Not 98.6 Degrees, Oct 12, 2023, by Dana G. Smith\n\nQuestion: based on the 1992 JAMA data, is there evidence to support that the population mean body temperature is different from 98.6°F?"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#question-based-on-the-1992-jama-data-is-there-evidence-to-support-that-the-population-mean-body-temperature-is-different-from-98.6f",
    "href": "slides_md/Day10_part1_bsta511_md.html#question-based-on-the-1992-jama-data-is-there-evidence-to-support-that-the-population-mean-body-temperature-is-different-from-98.6f",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Question: based on the 1992 JAMA data, is there evidence to support that the population mean body temperature is different from 98.6°F?",
    "text": "Question: based on the 1992 JAMA data, is there evidence to support that the population mean body temperature is different from 98.6°F?\nTwo approaches to answer this question:\n\nCreate a confidence interval (CI) for the population mean \\(\\mu\\) and determine whether 98.6°F is inside the CI or not.\n\nis 98.6°F a plausible value?\n\nRun a hypothesis test to see if there is evidence that the population mean \\(\\mu\\) is significantly different from 98.6°F or not.\n\nThis does not give us a range of plausible values for the population mean \\(\\mu\\).\nInstead, we calculate a test statistic and p-value\n\nto see how likely we are to observe the sample mean \\(\\bar{x}\\)\nor a more extreme sample mean\nassuming that the population mean \\(\\mu\\) is 98.6°F."
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#approach-1-create-a-95-c-i-for-the-population-mean-body-temperature",
    "href": "slides_md/Day10_part1_bsta511_md.html#approach-1-create-a-95-c-i-for-the-population-mean-body-temperature",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Approach 1: Create a 95% C I for the population mean body temperature",
    "text": "Approach 1: Create a 95% C I for the population mean body temperature\n\nUse data based on the results from the 1992 JAMA study\n\nThe original dataset used in the JAMA article is not available\nHowever, Allen Shoemaker from Calvin College created a dataset with the same summary statistics as in the JAMA article, which we will use:\n\n\n\\[\\bar{x} = 98.25,~s=0.733,~n=130\\] CI for \\(\\mu\\):\n\n\n\\[\\begin{align}\n\\bar{x} &\\pm t^*\\cdot\\frac{s}{\\sqrt{n}}\\\\\n98.25 &\\pm 1.979\\cdot\\frac{0.733}{\\sqrt{130}}\\\\\n98.25 &\\pm 0.127\\\\\n(98.123&, 98.377)\n\\end{align}\\]\n\nUsed \\(t^*\\) = qt(.975, df=129)\nConclusion:\nWe are 95% confident that the (population) mean body temperature is between 98.123°F and 98.377°F.\n\nHow does the CI compare to 98.6°F?"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#approach-2-hypothesis-test",
    "href": "slides_md/Day10_part1_bsta511_md.html#approach-2-hypothesis-test",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Approach 2: Hypothesis Test",
    "text": "Approach 2: Hypothesis Test\nFrom before:\n\nRun a hypothesis test to see if there is evidence that the population mean \\(\\mu\\) is significantly different from 98.6°F or not.\n\nThis does not give us a range of plausible values for the population mean \\(\\mu\\).\nInstead, we calculate a test statistic and p-value\n\nto see how likely we are to observe the sample mean \\(\\bar{x}\\)\nor a more extreme sample mean\nassuming that the population mean \\(\\mu\\) is 98.6°F.\n\n\n\nHow do we calculate a test statistic and p-value?"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#recall-the-sampling-distribution-of-the-mean",
    "href": "slides_md/Day10_part1_bsta511_md.html#recall-the-sampling-distribution-of-the-mean",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Recall the sampling distribution of the mean",
    "text": "Recall the sampling distribution of the mean\nFrom the Central Limit Theorem (CLT), we know that\n\nFor “large” sample sizes ( \\(n\\geq 30\\) ),\n\nthe sampling distribution of the sample mean\ncan be approximated by a normal distribution,with\n\nmean equal to the population mean value \\(\\mu\\), and\nstandard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\n\n\n\\[\\bar{X}\\sim N\\Big(\\mu_{\\bar{X}} = \\mu, \\sigma_{\\bar{X}}= \\frac{\\sigma}{\\sqrt{n}}\\Big)\\]\n\nFor small sample sizes, if the population is known to be normally distributed, then\n\nthe same result holds"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#case-1-suppose-we-know-the-population-sd-sigma",
    "href": "slides_md/Day10_part1_bsta511_md.html#case-1-suppose-we-know-the-population-sd-sigma",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Case 1: suppose we know the population sd \\(\\sigma\\)",
    "text": "Case 1: suppose we know the population sd \\(\\sigma\\)\n\nHow likely we are to observe the sample mean \\(\\bar{x}\\) ,\n\nor a more extreme sample mean,\nassuming that the population mean \\(\\mu\\) is 98.6°F?\n\nUse \\(\\bar{x} = 98.25\\), \\(\\sigma=0.733\\), and \\(n=130\\)"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#case-2-we-dont-know-the-population-sd-sigma",
    "href": "slides_md/Day10_part1_bsta511_md.html#case-2-we-dont-know-the-population-sd-sigma",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Case 2: we don’t know the population sd \\(\\sigma\\)",
    "text": "Case 2: we don’t know the population sd \\(\\sigma\\)\n\n\n\nThis is usually the case in real life\nWe estimate \\(\\sigma\\) with the sample standard deviation \\(s\\)\nFrom last time, we know that in this case we need to use the t-distribution with d.f. = n-1, instead of the normal distribution\nQuestion: How likely we are to observe the sample mean \\(\\bar{x}\\) or a more extreme sample mean, assuming that the population mean \\(\\mu\\) is 98.6°F?\nUse \\(\\bar{x} = 98.25\\), \\(s=0.733\\), and \\(n=130\\)"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#steps-in-a-hypothesis-test",
    "href": "slides_md/Day10_part1_bsta511_md.html#steps-in-a-hypothesis-test",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Steps in a Hypothesis Test",
    "text": "Steps in a Hypothesis Test\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#step-2-null-alternative-hypotheses-12",
    "href": "slides_md/Day10_part1_bsta511_md.html#step-2-null-alternative-hypotheses-12",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 2: Null & Alternative Hypotheses (1/2)",
    "text": "Step 2: Null & Alternative Hypotheses (1/2)\nIn statistics, a hypothesis is a statement about the value of an unknown population parameter.\nA hypothesis test consists of a test between two competing hypotheses:\n\na null hypothesis \\(H_0\\) (pronounced “H-naught”) vs. \nan alternative hypothesis \\(H_A\\) (also denoted \\(H_1\\))\n\nExample of hypotheses in words:\n\\[\\begin{aligned}\nH_0 &: \\text{The population mean body temperature is 98.6°F}\\\\\n\\text{vs. } H_A &: \\text{The population mean body temperature is not 98.6°F}\n\\end{aligned}\\]\n\n\\(H_0\\) is a claim that there is “no effect” or “no difference of interest.”\n\\(H_A\\) is the claim a researcher wants to establish or find evidence to support. It is viewed as a “challenger” hypothesis to the null hypothesis \\(H_0\\)"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#step-2-null-alternative-hypotheses-22",
    "href": "slides_md/Day10_part1_bsta511_md.html#step-2-null-alternative-hypotheses-22",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 2: Null & Alternative Hypotheses (2/2)",
    "text": "Step 2: Null & Alternative Hypotheses (2/2)\nNotation for hypotheses:\n\\[\\begin{aligned}\nH_0 &: \\mu = \\mu_0\\\\\n\\text{vs. } H_A&: \\mu \\neq, &lt;, \\textrm{or}, &gt; \\mu_0\n\\end{aligned}\\]\nWe call \\(\\mu_0\\) the null value\n\n\n\\(H_A: \\mu \\neq \\mu_0\\)\n\n\nnot choosing a priori whether we believe the population mean is greater or less than the null value \\(\\mu_0\\)\n\n\n\n\\(H_A: \\mu &lt; \\mu_0\\)\n\n\nbelieve the population mean is less than the null value \\(\\mu_0\\)\n\n\n\n\\(H_A: \\mu &gt; \\mu_0\\)\n\n\nbelieve the population mean is greater than the null value \\(\\mu_0\\)\n\n\n\n\n\n\\(H_A: \\mu \\neq \\mu_0\\) is the most common option, since it’s the most conservative\n\nExample:\n\\[\\begin{aligned}\nH_0 &: \\mu = 98.6\\\\\n\\text{vs. } H_A&: \\mu \\neq 98.6\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#step-3-test-statistic-its-distribution",
    "href": "slides_md/Day10_part1_bsta511_md.html#step-3-test-statistic-its-distribution",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 3: Test statistic (& its distribution)",
    "text": "Step 3: Test statistic (& its distribution)\n\n\nCase 1: know population sd \\(\\sigma\\)\n\\[\n\\text{test statistic} = z_{\\bar{x}} = \\frac{\\bar{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\n\\]\n\nStatistical theory tells us that \\(z_{\\bar{x}}\\) follows a Standard Normal distribution \\(N(0,1)\\)\n\n\nCase 2: don’t know population sd \\(\\sigma\\)\n\\[\n\\text{test statistic} = t_{\\bar{x}} = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\n\\]\n\nStatistical theory tells us that \\(t_{\\bar{x}}\\) follows a Student’s t distribution with degrees of freedom (df) = \\(n-1\\)\n\n\n\n\n\\(\\bar{x}\\) = sample mean, \\(\\mu_0\\) = hypothesized population mean from \\(H_0\\),\n\\(\\sigma\\) = population standard deviation, \\(s\\) = sample standard deviation,\n\\(n\\) = sample size\n\nAssumptions: same as CLT\n\nIndependent observations: the observations were collected independently.\nApproximately normal sample or big n: the distribution of the sample should be approximately normal, or the sample size should be at least 30."
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#step-3-test-statistic-calculation",
    "href": "slides_md/Day10_part1_bsta511_md.html#step-3-test-statistic-calculation",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 3: Test statistic calculation",
    "text": "Step 3: Test statistic calculation\nRecall that \\(\\bar{x} = 98.25\\), \\(s=0.733\\), and \\(n=130.\\)\nThe test statistic is:\n\\[t_{\\bar{x}} = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\n= \\frac{98.25 - 98.6}{\\frac{0.73}{\\sqrt{130}}}\n= -5.45\\]\n\nStatistical theory tells us that \\(t_{\\bar{x}}\\) follows a Student’s t-distribution with \\(d.f. = n-1 = 129\\).\n\n\n\n\n\n\n\n\n\nAssumptions met?"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#step-4-p-value",
    "href": "slides_md/Day10_part1_bsta511_md.html#step-4-p-value",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\n\n\nThe \\(p\\)-value is a quantification of “surprise”\n\nAssuming \\(H_0\\) is true, how surprised are we with the observed results?\nEx: assuming that the true mean body temperature is 98.6°F, how surprised are we to get a sample mean of 98.25°F (or more extreme)?\n\nIf the \\(p\\)-value is “small,” it means there’s a small probability that we would get the observed statistic (or more extreme) when \\(H_0\\) is true."
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#step-4-p-value-calculation",
    "href": "slides_md/Day10_part1_bsta511_md.html#step-4-p-value-calculation",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 4: p-value calculation",
    "text": "Step 4: p-value calculation\nCalculate the p-value using the Student’s t-distribution with \\(d.f. = n-1 = 129\\):\n\\[p-value=P(T \\leq -5.45) + P(T \\geq 5.45) = 2.410889 \\times 10^{-07}\\]\n\n# use pt() instead of pnorm()\n# need to specify df\n2*pt(-5.4548, df = 130-1, lower.tail = TRUE)\n\n[1] 2.410889e-07"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#step-4-p-value-estimation-using-t-table",
    "href": "slides_md/Day10_part1_bsta511_md.html#step-4-p-value-estimation-using-t-table",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 4: p-value estimation using \\(t\\)-table",
    "text": "Step 4: p-value estimation using \\(t\\)-table\n\n\\(t\\)-table only gives us bounds on the p-value\nRecall from using the \\(t\\)-table for CIs, that the table gives us the cutoff values for varying tail probabilities (1-tail & 2-tail)\nFind the row with the appropriate degrees of freedom\n\nUse next smallest df in table if actual df not shown\nI.e., for df = 129, use df = 100 in table\n\nFigure out where the test statistic’s absolute value is in relation to the values in the columns, i.e. between which columns is the test statistic?\nThe header rows for those columns gives the lower & upper bounds for the p-value\n\nChoosing one-tail vs. two-tail test, depends on the alternative hypothesis \\(H_A\\).\nFor a 2-sided test ( \\(H_A: \\mu \\neq \\mu_0\\) ), use two-tails\nFor a 1-sided test ( \\(H_A: \\mu &lt; \\textrm{or} &gt; \\mu_0\\) ), use one-tail"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#using-a-t-table-to-estimate-p-value",
    "href": "slides_md/Day10_part1_bsta511_md.html#using-a-t-table-to-estimate-p-value",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Using a \\(t\\)-table to estimate p-value",
    "text": "Using a \\(t\\)-table to estimate p-value"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#step-1-significance-level-alpha",
    "href": "slides_md/Day10_part1_bsta511_md.html#step-1-significance-level-alpha",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 1: Significance Level \\(\\alpha\\)",
    "text": "Step 1: Significance Level \\(\\alpha\\)\n\nBefore doing a hypothesis test, we set a cut-off for how small the \\(p\\)-value should be in order to reject \\(H_0\\).\nWe call this the significance level, denoted by the Greek symbol alpha ( \\(\\alpha\\) )\nTypical \\(\\alpha\\) values are\n\n0.05 - most common by far!!\n0.01 and 0.1\n\nDecision rule:\n\nWhen \\(p\\)-value &lt; \\(\\alpha\\), we “reject the null hypothesis \\(H_0\\).”\nWhen \\(p\\)-value \\(\\geq \\alpha\\), we “fail to reject the null hypothesis \\(H_0\\).”\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\n“Failing to reject” \\(H_0\\) is NOT the same as “accepting” \\(H_0\\)!\nBy failing to reject \\(H_0\\) we are just saying that we don’t have sufficient evidence to support the alternative \\(H_A\\).\nThis does not imply that \\(H_0\\) is true!!"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#step-5-conclusion-to-hypothesis-test",
    "href": "slides_md/Day10_part1_bsta511_md.html#step-5-conclusion-to-hypothesis-test",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\\[\\begin{aligned}\nH_0 &: \\mu = 98.6\\\\\n\\text{vs. } H_A&: \\mu \\neq 98.6\n\\end{aligned}\\]\n\nRecall the \\(p\\)-value = \\(2.410889 \\times 10^{-07}\\)\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nBasic: (“stats class” conclusion)\n\nThere is sufficient evidence that the (population) mean body temperature is discernibly different from 98.6°F ( \\(p\\)-value &lt; 0.001).\n\nBetter: (“manuscript style” conclusion)\n\nThe average body temperature in the sample was 98.25°F (95% CI 98.12, 98.38°F), which is discernibly different from 98.6°F ( \\(p\\)-value &lt; 0.001)."
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#confidence-intervals-vs.-hypothesis-testing",
    "href": "slides_md/Day10_part1_bsta511_md.html#confidence-intervals-vs.-hypothesis-testing",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Confidence Intervals vs. Hypothesis Testing",
    "text": "Confidence Intervals vs. Hypothesis Testing\n\nSee also V&H Section 4.3.3"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#working-directory",
    "href": "slides_md/Day10_part1_bsta511_md.html#working-directory",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Working directory",
    "text": "Working directory\n\nIn order to load a dataset from a file, you need to tell R where the dataset is located\nTo do this you also need to know the location from which R is working, i.e. your working directory\nYou can figure out your working directory by running the getwd() function.\n\n\ngetwd()\n\n[1] \"/Users/niederha/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/teaching/BSTA 511/F24/0_webpage/BSTA_511_F24\"\n\n\n\nAbove is the working directory of this slides file\n\nIn this case, this is NOT the location of the actual qmd file though!\n\nTo make it easier to juggle the working directory, the location of your qmd file, and the location of the data,\n\nI highly recommend using R Projects!"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#r-projects",
    "href": "slides_md/Day10_part1_bsta511_md.html#r-projects",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "R projects",
    "text": "R projects\n\nI highly, highly, HIGHLY recommend using R Projects to organize your analyses and make it easier to load data files and also save output.\nWhen you create an R Project on your computer, the Project is associated with the folder (directory) you created it in.\n\nThis folder becomes the “root” of your working directory, and RStudio’s point of reference from where to load files from and to.\n\nI create separate Projects for every analysis project and every class I teach.\nYou can run multiple sessions of RStudio by opening different Projects, and the environments (or working directory) of each are working independently of each other.\n\n\n\n\n\n\n\nNote\n\n\n\n\nAlthough we are using Quarto files,\n\nI will show how to set up and use a “regular” R Project\ninstead of “Quarto Project”\n\nQuarto Projects include extra features and thus complexity. Once you are used to how regular R Projects work, you can try out a Quarto Project."
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#how-to-create-an-r-project",
    "href": "slides_md/Day10_part1_bsta511_md.html#how-to-create-an-r-project",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "How to create an R Project",
    "text": "How to create an R Project\n\nDemonstration in class recording\nPosit’s (RStudio’s) directions for creating Projects\n\nhttps://support.rstudio.com/hc/en-us/articles/200526207-Using-RStudio-Projects\n\nSee file Projects in RStudio for more information on R Projects."
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#load-the-dataset",
    "href": "slides_md/Day10_part1_bsta511_md.html#load-the-dataset",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Load the dataset",
    "text": "Load the dataset\n\nThe data are in a csv file called BodyTemperatures.csv\nYou need to tell R where the dataset is located!\nI recommend saving all datasets in a folder called data.\n\nThe code I will be providing you will be set up this way.\n\nTo make it easier to specify where the dataset is located, I recommend using the here() function from the here package: here::here().\n\n\n# read_csv() is a function from the readr package that is a part of the tidyverse\nlibrary(here)   # first install this package\n\nhere() starts at /Users/niederha/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/teaching/BSTA 511/F24/0_webpage/BSTA_511_F24\n\nBodyTemps &lt;- read_csv(here::here(\"data\", \"BodyTemperatures.csv\"))\n\nRows: 130 Columns: 3\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): Temperature, Gender, HeartRate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#                     location: look in \"data\" folder\n#                               for the file \"BodyTemperatures.csv\"\n\nglimpse(BodyTemps)\n\nRows: 130\nColumns: 3\n$ Temperature &lt;dbl&gt; 96.3, 96.7, 96.9, 97.0, 97.1, 97.1, 97.1, 97.2, 97.3, 97.4…\n$ Gender      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ HeartRate   &lt;dbl&gt; 70, 71, 74, 80, 73, 75, 82, 64, 69, 70, 68, 72, 78, 70, 75…"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#herehere",
    "href": "slides_md/Day10_part1_bsta511_md.html#herehere",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "here::here()",
    "text": "here::here()\nGeneral use of here::here()\nhere::here(\"folder_name\", \"filename\")\n\n\nResources for here::here():\n\nhow to use the here package (Jenny Richmond)\nOde to the here package (Jenny Bryan)\n\nProject-oriented workflow (Jenny Bryan)\n\n\n\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#t.test-base-rs-function-for-testing-one-mean",
    "href": "slides_md/Day10_part1_bsta511_md.html#t.test-base-rs-function-for-testing-one-mean",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "t.test: base R’s function for testing one mean",
    "text": "t.test: base R’s function for testing one mean\n\nUse the body temperature example with \\(H_A: \\mu \\neq 98.6\\)\nWe called the dataset BodyTemps when we loaded it\n\n\nglimpse(BodyTemps)\n\nRows: 130\nColumns: 3\n$ Temperature &lt;dbl&gt; 96.3, 96.7, 96.9, 97.0, 97.1, 97.1, 97.1, 97.2, 97.3, 97.4…\n$ Gender      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ HeartRate   &lt;dbl&gt; 70, 71, 74, 80, 73, 75, 82, 64, 69, 70, 68, 72, 78, 70, 75…\n\n(temps_ttest &lt;- t.test(x = BodyTemps$Temperature,\n       # alternative = \"two.sided\",  # default\n       mu = 98.6))\n\n\n    One Sample t-test\n\ndata:  BodyTemps$Temperature\nt = -5.4548, df = 129, p-value = 2.411e-07\nalternative hypothesis: true mean is not equal to 98.6\n95 percent confidence interval:\n 98.12200 98.37646\nsample estimates:\nmean of x \n 98.24923 \n\n\nNote that the test output also gives the 95% CI using the t-distribution."
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#tidy-the-t.test-output",
    "href": "slides_md/Day10_part1_bsta511_md.html#tidy-the-t.test-output",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "tidy() the t.test output",
    "text": "tidy() the t.test output\n\nUse the tidy() function from the broom package for briefer output in table format that’s stored as a tibble\nCombined with the gt() function from the gt package, we get a nice table\n\n\ntidy(temps_ttest) %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    98.24923\n-5.454823\n2.410632e-07\n129\n98.122\n98.37646\nOne Sample t-test\ntwo.sided\n  \n  \n  \n\n\n\n\n\nSince the tidy() output is a tibble, we can easily pull() specific values from it:\n\n\n\nUsing base R’s $\n\ntidy(temps_ttest)$p.value  \n\n[1] 2.410632e-07\n\n\nAdvantage: quick and easy\n\nOr the tidyverse way: using pull() from dplyr package\n\ntidy(temps_ttest) %&gt;% pull(p.value)\n\n[1] 2.410632e-07\n\n\nAdvantage: can use together with piping (%&gt;%) other functions"
  },
  {
    "objectID": "slides_md/Day10_part1_bsta511_md.html#whats-next",
    "href": "slides_md/Day10_part1_bsta511_md.html#whats-next",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "What’s next?",
    "text": "What’s next?\nCI’s and hypothesis testing for different scenarios:\n\n\n\n\n\n\n\n\n\n\n\nDay\nSection\nPopulation parameter\nSymbol\nPoint estimate\nSymbol\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\n\n11\n5.3\nDiff in pop means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n\n\n12\n8.2\nDiff in pop prop’s\n\\(p_1-p_2\\)\nDiff in sample prop’s\n\\(\\widehat{p}_1-\\widehat{p}_2\\)"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#where-are-we-continuous-outcome-zoomed-in",
    "href": "slides_md/Day10_part2_bsta511_md.html#where-are-we-continuous-outcome-zoomed-in",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Where are we? Continuous outcome zoomed in",
    "text": "Where are we? Continuous outcome zoomed in"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#what-we-covered-in-day-10-part-1",
    "href": "slides_md/Day10_part2_bsta511_md.html#what-we-covered-in-day-10-part-1",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "What we covered in Day 10 Part 1",
    "text": "What we covered in Day 10 Part 1\n\n(4.3, 5.1) Hypothesis testing for mean from one sample\n\nIntroduce hypothesis testing using the case of analyzing a mean from one sample (group)\n\n\n\n\nSteps of a hypothesis test:\n\nlevel of significance\nnull ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\ntest statistic\np-value\nconclusion\n\n\n\n\nRun a hypothesis test in R\n\nLoad a dataset - need to specify location of dataset\nR projects\nRun a t-test in R\ntidy() the test output using broom package\n\n\n\n\n\n\n(4.3.3) Confidence intervals (CIs) vs. hypothesis tests"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#goals-for-today-part-2---class-discussion",
    "href": "slides_md/Day10_part2_bsta511_md.html#goals-for-today-part-2---class-discussion",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Goals for today: Part 2 - Class discussion",
    "text": "Goals for today: Part 2 - Class discussion\n\n(5.2) Inference for mean difference from dependent/paired 2 samples\n\nInference: CIs and hypothesis testing\nExploratory data analysis (EDA) to visualize data\nRun paired t-test in R\n\n\n\nOne-sided CIs\n\n\nClass discussion\n\nInference for the mean difference from dependent/paired data is a special case of the inference for the mean from just one sample, that was already covered.\nThus this part will be used for class discussion to practice CIs and hypothesis testing for one mean and apply it in this new setting.\nIn class I will briefly introduce this topic, explain how it is similar and different from what we already covered, and let you work through the slides and code."
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#cis-and-hypothesis-tests-for-different-scenarios",
    "href": "slides_md/Day10_part2_bsta511_md.html#cis-and-hypothesis-tests-for-different-scenarios",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "CI’s and hypothesis tests for different scenarios:",
    "text": "CI’s and hypothesis tests for different scenarios:\n\n\\[\\text{point~estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test~stat} = \\frac{\\text{point~estimate}-\\text{null~value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n???\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#steps-in-a-hypothesis-test",
    "href": "slides_md/Day10_part2_bsta511_md.html#steps-in-a-hypothesis-test",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Steps in a Hypothesis Test",
    "text": "Steps in a Hypothesis Test\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#examples-of-paired-designs-two-samples",
    "href": "slides_md/Day10_part2_bsta511_md.html#examples-of-paired-designs-two-samples",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Examples of paired designs (two samples)",
    "text": "Examples of paired designs (two samples)\n\nEnroll pairs of identical twins to study a disease\nEnroll father & son pairs to study cholesterol levels\nStudying pairs of eyes\nEnroll people and collect data before & after an intervention (longitudinal data)\nTextbook example: Compare maximal speed of competitive swimmers wearing a wetsuit vs. wearing a regular swimsuit\n\nWIll use these data on homework\n\n\nCome up with 2 more examples of paired study designs."
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#can-a-vegetarian-diet-change-cholesterol-levels",
    "href": "slides_md/Day10_part2_bsta511_md.html#can-a-vegetarian-diet-change-cholesterol-levels",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Can a vegetarian diet change cholesterol levels?",
    "text": "Can a vegetarian diet change cholesterol levels?\n\nScenario:\n\n24 non-vegetarian people were enrolled in a study\nThey were instructed to adopt a vegetarian diet\nCholesterol levels were measured before and after the diet\n\nQuestion: Is there evidence to support that cholesterol levels changed after the vegetarian diet?\nHow to answer the question?\n\nFirst, calculate changes (differences) in cholesterol levels\n\nWe usually do after - before if the data are longitudinal\n\n\n\n\n\nCalculate CI for the\nmean difference \\(\\delta\\):\n\\[\\bar{x}_d \\pm t^*\\cdot\\frac{s_d}{\\sqrt{n}}\\]\n\nRun a hypothesis test\nHypotheses\n\\[\\begin{align}\nH_0:& \\delta = \\delta_0 \\\\\nH_A:& \\delta \\neq \\delta_0 \\\\\n(or&~ &lt;, &gt;)\n\\end{align}\\]\n\n Test statistic\n\\[\nt_{\\bar{x}_d} = \\frac{\\bar{x}_d - \\delta_0}{\\frac{s_d}{\\sqrt{n}}}\n\\]"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#eda-explore-the-cholesterol-data",
    "href": "slides_md/Day10_part2_bsta511_md.html#eda-explore-the-cholesterol-data",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "EDA: Explore the cholesterol data",
    "text": "EDA: Explore the cholesterol data\n\nScenario:\n\n24 non-vegetarian people were enrolled in a study\nThey were instructed to adopt a vegetarian diet\nCholesterol levels were measured before and after the diet\n\n\n\nchol &lt;- read_csv(here::here(\"data\", \"chol213.csv\"))\n\nRows: 24 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): Before, After\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(chol)\n\nRows: 24\nColumns: 2\n$ Before &lt;dbl&gt; 195, 145, 205, 159, 244, 166, 250, 236, 192, 224, 238, 197, 169…\n$ After  &lt;dbl&gt; 146, 155, 178, 146, 208, 147, 202, 215, 184, 208, 206, 169, 182…\n\nchol %&gt;% \n  get_summary_stats(type = \"common\") %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      variable\n      n\n      min\n      max\n      median\n      iqr\n      mean\n      sd\n      se\n      ci\n    \n  \n  \n    Before\n24\n137\n250\n179\n44.5\n187.792\n33.160\n6.769\n14.002\n    After\n24\n125\n215\n165\n38.0\n168.250\n26.796\n5.470\n11.315\n  \n  \n  \n\n\n\n\nMake sure you are able to load the data on your computer!"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#eda-cholesterol-levels-before-and-after-vegetarian-diet",
    "href": "slides_md/Day10_part2_bsta511_md.html#eda-cholesterol-levels-before-and-after-vegetarian-diet",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "EDA: Cholesterol levels before and after vegetarian diet",
    "text": "EDA: Cholesterol levels before and after vegetarian diet\nDescribe the distributions of the before & after data.\n\n\n\nggplot(chol, aes(x=Before)) +\n  geom_density()\n\n\n\nggplot(chol, aes(x=Before)) +\n  geom_boxplot()\n\n\n\n\n\n\nggplot(chol, aes(x=After)) +\n  geom_density()\n\n\n\nggplot(chol, aes(x=After)) +\n  geom_boxplot()"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#eda-spaghetti-plot-of-cholesterol-levels-before-after-diet",
    "href": "slides_md/Day10_part2_bsta511_md.html#eda-spaghetti-plot-of-cholesterol-levels-before-after-diet",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "EDA: Spaghetti plot of cholesterol levels before & after diet",
    "text": "EDA: Spaghetti plot of cholesterol levels before & after diet\n\nVisualize the individual before vs. after diet changes in cholesterol levels\n\nWhat does this figure tell us?\n\n\n\n\n\n\nSee code file for how to wrangle the data and create the figure - you will not be expected to do this yourself."
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#eda-differences-in-cholesterol-levels-after---before-diet",
    "href": "slides_md/Day10_part2_bsta511_md.html#eda-differences-in-cholesterol-levels-after---before-diet",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "EDA: Differences in cholesterol levels: After - Before diet",
    "text": "EDA: Differences in cholesterol levels: After - Before diet\nWhat is this code doing?\n\nchol &lt;- chol %&gt;% \n  mutate(DiffChol = After-Before) \nhead(chol, 8)\n\n# A tibble: 8 × 3\n  Before After DiffChol\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1    195   146      -49\n2    145   155       10\n3    205   178      -27\n4    159   146      -13\n5    244   208      -36\n6    166   147      -19\n7    250   202      -48\n8    236   215      -21\n\n\nIs the mean of DiffChol the same as the difference in means of After - Before? Should it be? Why or why not?\n\nchol %&gt;% \n  get_summary_stats(type = \"common\") %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      variable\n      n\n      min\n      max\n      median\n      iqr\n      mean\n      sd\n      se\n      ci\n    \n  \n  \n    Before\n24\n137\n250\n179\n44.50\n187.792\n33.160\n6.769\n14.002\n    After\n24\n125\n215\n165\n38.00\n168.250\n26.796\n5.470\n11.315\n    DiffChol\n24\n-49\n13\n-19\n20.25\n-19.542\n16.806\n3.430\n7.096"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#eda-differences-in-cholesterol-levels-after---before-diet-1",
    "href": "slides_md/Day10_part2_bsta511_md.html#eda-differences-in-cholesterol-levels-after---before-diet-1",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "EDA: Differences in cholesterol levels: After - Before diet",
    "text": "EDA: Differences in cholesterol levels: After - Before diet\nCompare and contrast the 3 distributions. Comment on shape, center, and spread.\n\n\nBefore & After\n\nggplot(chol, aes(x=Before)) +\n  geom_density()\n\n\n\nggplot(chol, aes(x=After)) +\n  geom_density()\n\n\n\n\n\nDiffChol\n\nggplot(chol, aes(x=DiffChol)) + \n  geom_density()\n\n\n\nggplot(chol, aes(x=DiffChol)) + \n  geom_boxplot()"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#steps-in-a-hypothesis-test-1",
    "href": "slides_md/Day10_part2_bsta511_md.html#steps-in-a-hypothesis-test-1",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Steps in a Hypothesis Test",
    "text": "Steps in a Hypothesis Test\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#step-2-null-alternative-hypotheses",
    "href": "slides_md/Day10_part2_bsta511_md.html#step-2-null-alternative-hypotheses",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Step 2: Null & Alternative Hypotheses",
    "text": "Step 2: Null & Alternative Hypotheses\n\nQuestion: Is there evidence to support that cholesterol levels changed after the vegetarian diet?\n\n\n\nNull and alternative hypotheses in words Include as much context as possible\n\n\n\\(H_0\\): The population mean difference in cholesterol levels after a vegetarian diet is fill in\n\\(H_A\\): The population mean difference in cholesterol levels after a vegetarian diet is fill in\n\n\nNull and alternative hypotheses in symbols\nfill in the missing parts of the hypotheses.\n\\[\\begin{align}\nH_0:& \\delta = \\\\\nH_A:& \\delta \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#step-3-test-statistic",
    "href": "slides_md/Day10_part2_bsta511_md.html#step-3-test-statistic",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Step 3: Test statistic",
    "text": "Step 3: Test statistic\n\nchol %&gt;% select(DiffChol) %&gt;% get_summary_stats(type = \"common\") %&gt;% gt()\n\n\n\n\n\n  \n    \n      variable\n      n\n      min\n      max\n      median\n      iqr\n      mean\n      sd\n      se\n      ci\n    \n  \n  \n    DiffChol\n24\n-49\n13\n-19\n20.25\n-19.542\n16.806\n3.43\n7.096\n  \n  \n  \n\n\n\n\n\n\n\\[\nt_{\\bar{x}_d} = \\frac{\\bar{x}_d - \\delta_0}{\\frac{s_d}{\\sqrt{n}}}\n\\]\n\nCalculate the test statistic.\nBased on the value of the test statistic, do you think we are going to reject or fail to reject \\(H_0\\)?\nWhat probability distribution does the test statistic have?\nAre the assumptions for a paired t-test satisfied so that we can use the probability distribution to calculate the \\(p\\)-value??"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#step-4-p-value",
    "href": "slides_md/Day10_part2_bsta511_md.html#step-4-p-value",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate the p-value and shade in the area representing the p-value:"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#step-5-conclusion-to-hypothesis-test",
    "href": "slides_md/Day10_part2_bsta511_md.html#step-5-conclusion-to-hypothesis-test",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\\[\\begin{align}\nH_0:& \\delta = 0 \\\\\nH_A:& \\delta \\neq 0 \\\\\n\\end{align}\\]\n\nRecall the \\(p\\)-value = \\(8.434775 \\cdot 10 ^{-6}\\)\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nStats class conclusion\n\nThere is sufficient evidence that the (population) mean difference in cholesterol levels after a vegetarian diet is different from 0 mg/dL ( \\(p\\)-value &lt; 0.001).\n\nMore realistic manuscript conclusion:\n\nAfter a vegetarian diet, cholesterol levels decreased by on average 19.54 mg/dL (SE = 3.43 mg/dL, 2-sided \\(p\\)-value &lt; 0.001)."
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#ci-for-the-mean-difference-in-cholesterol-levels",
    "href": "slides_md/Day10_part2_bsta511_md.html#ci-for-the-mean-difference-in-cholesterol-levels",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "95% CI for the mean difference in cholesterol levels",
    "text": "95% CI for the mean difference in cholesterol levels\n\nchol %&gt;% \n  select(DiffChol) %&gt;% \n  get_summary_stats(type = \"common\") %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      variable\n      n\n      min\n      max\n      median\n      iqr\n      mean\n      sd\n      se\n      ci\n    \n  \n  \n    DiffChol\n24\n-49\n13\n-19\n20.25\n-19.542\n16.806\n3.43\n7.096\n  \n  \n  \n\n\n\n\nCI for \\(\\mu_d\\) (or \\(\\delta\\)): How was \\(t^*\\) calculated?\n\n\n\\[\\begin{align}\n\\bar{x}_d &\\pm t^*\\cdot\\frac{s_d}{\\sqrt{n}}\\\\\n-19.542 &\\pm 2.069\\cdot\\frac{16.806}{\\sqrt{24}}\\\\\n-19.542 &\\pm 2.069\\cdot 3.43\\\\\n-19.542 &\\pm 7.096\\\\\n(-26.638&, -12.445)\n\\end{align}\\]\n\nConclusion:\nWe are 95% that the (population) mean difference in cholesterol levels after a vegetarian diet is between -26.638 mg/dL and -12.445 mg/dL.\n\nBased on the CI, is there evidence the diet made a difference in cholesterol levels? Why or why not?"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#r-option-1-run-a-1-sample-t.test-using-the-paired-differences",
    "href": "slides_md/Day10_part2_bsta511_md.html#r-option-1-run-a-1-sample-t.test-using-the-paired-differences",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "R option 1: Run a 1-sample t.test using the paired differences",
    "text": "R option 1: Run a 1-sample t.test using the paired differences\n\\(H_A: \\delta \\neq 0\\)\n\nt.test(x = chol$DiffChol, mu = 0)\n\n\n    One Sample t-test\n\ndata:  chol$DiffChol\nt = -5.6965, df = 23, p-value = 8.435e-06\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -26.63811 -12.44522\nsample estimates:\nmean of x \n-19.54167 \n\n\nRun the code without mu = 0. Do the results change? Why or why not?"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#r-option-2-run-a-2-sample-t.test-with-paired-true-option",
    "href": "slides_md/Day10_part2_bsta511_md.html#r-option-2-run-a-2-sample-t.test-with-paired-true-option",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "R option 2: Run a 2-sample t.test with paired = TRUE option",
    "text": "R option 2: Run a 2-sample t.test with paired = TRUE option\n\\(H_A: \\delta \\neq 0\\)\n\nFor a 2-sample t-test we specify both x= and y=\nNote: mu = 0 is the default value and doesn’t need to be specified\n\n\nt.test(x = chol$Before, y = chol$After, mu = 0, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  chol$Before and chol$After\nt = 5.6965, df = 23, p-value = 8.435e-06\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 12.44522 26.63811\nsample estimates:\nmean difference \n       19.54167 \n\n\nWhat is different in the output compared to option 1?"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#r-option-3-run-a-2-sample-t.test-with-paired-true-option-but-using-the-long-data-and-a-formula-12",
    "href": "slides_md/Day10_part2_bsta511_md.html#r-option-3-run-a-2-sample-t.test-with-paired-true-option-but-using-the-long-data-and-a-formula-12",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "R option 3: Run a 2-sample t.test with paired = TRUE option, but using the long data and a “formula” (1/2)",
    "text": "R option 3: Run a 2-sample t.test with paired = TRUE option, but using the long data and a “formula” (1/2)\n\n\n\nThe data have to be in a long format for option 3, where each person has 2 rows: one for Before and one for After.\n\nThe long dataset chol_long was created for the slide “EDA: Spaghetti plot of cholesterol levels before & after diet”.\nSee the code to create it there.\n\nWhat information is being stored in each of the columns?\n\n\n\n# first 16 rows of long data:\nhead(chol_long, 16)\n\n# A tibble: 16 × 3\n   ID    Time   Cholesterol\n   &lt;fct&gt; &lt;fct&gt;        &lt;dbl&gt;\n 1 1     Before         195\n 2 1     After          146\n 3 2     Before         145\n 4 2     After          155\n 5 3     Before         205\n 6 3     After          178\n 7 4     Before         159\n 8 4     After          146\n 9 5     Before         244\n10 5     After          208\n11 6     Before         166\n12 6     After          147\n13 7     Before         250\n14 7     After          202\n15 8     Before         236\n16 8     After          215"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#r-option-3-run-a-2-sample-t.test-with-paired-true-option-but-using-the-long-data-and-a-formula-22",
    "href": "slides_md/Day10_part2_bsta511_md.html#r-option-3-run-a-2-sample-t.test-with-paired-true-option-but-using-the-long-data-and-a-formula-22",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "R option 3: Run a 2-sample t.test with paired = TRUE option, but using the long data and a “formula” (2/2)",
    "text": "R option 3: Run a 2-sample t.test with paired = TRUE option, but using the long data and a “formula” (2/2)\n\n\n\n\nUse the usual t.test\nWhat’s different is that\n\ninstead of specifying the variables with x= and y=,\nwe give a formula of the form y ~ x using just the variable names,\nand then specify the name of the dataset using data =\n\nThis method is often used in practice, and more similar to the coding style of running a regression model (BSTA 512 & 513)\n\n\n\n\n# using long data \n# with columns Cholesterol & Time\nt.test(Cholesterol ~ Time, \n       paired = TRUE, \n       data = chol_long)\n\n\n    Paired t-test\n\ndata:  Cholesterol by Time\nt = 5.6965, df = 23, p-value = 8.435e-06\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 12.44522 26.63811\nsample estimates:\nmean difference \n       19.54167 \n\n\n\nWhat is different in the output compared to option 1?\nRerun the test using Time ~ Cholesterol (switch the variables). What do you get?"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#compare-the-3-options",
    "href": "slides_md/Day10_part2_bsta511_md.html#compare-the-3-options",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Compare the 3 options",
    "text": "Compare the 3 options\n\nHow is the code similar and different for the 3 options?\nGiven a dataset, how would you choose which of the 3 options to use?\n\n\n# option 1\nt.test(x = chol$DiffChol, mu = 0) %&gt;% tidy() %&gt;% gt() # tidy from broom package\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    -19.54167\n-5.696519\n8.434775e-06\n23\n-26.63811\n-12.44522\nOne Sample t-test\ntwo.sided\n  \n  \n  \n\n\n\n# option 2\nt.test(x = chol$Before, y = chol$After, mu = 0, paired = TRUE) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    19.54167\n5.696519\n8.434775e-06\n23\n12.44522\n26.63811\nPaired t-test\ntwo.sided\n  \n  \n  \n\n\n\n# option 3\nt.test(Cholesterol ~ Time, paired = TRUE, data = chol_long) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    19.54167\n5.696519\n8.434775e-06\n23\n12.44522\n26.63811\nPaired t-test\ntwo.sided"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#what-if-we-wanted-to-test-whether-the-diet-decreased-cholesterol-levels",
    "href": "slides_md/Day10_part2_bsta511_md.html#what-if-we-wanted-to-test-whether-the-diet-decreased-cholesterol-levels",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "What if we wanted to test whether the diet decreased cholesterol levels?",
    "text": "What if we wanted to test whether the diet decreased cholesterol levels?\n\n\nWhat changes in each of the steps?\n\nSet the level of significance \\(\\alpha\\)\nSpecify the hypotheses \\(H_0\\) and \\(H_A\\)\n\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#r-what-if-we-wanted-to-test-whether-the-diet-decreased-cholesterol-levels",
    "href": "slides_md/Day10_part2_bsta511_md.html#r-what-if-we-wanted-to-test-whether-the-diet-decreased-cholesterol-levels",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "R: What if we wanted to test whether the diet decreased cholesterol levels?",
    "text": "R: What if we wanted to test whether the diet decreased cholesterol levels?\n\nWhich of the 3 options to run a paired t-test in R is being used below?\nHow did the code change to account for testing a decrease in cholesterol levels?\nWhich values in the output changed compared to testing for a change in cholesterol levels? How did they change?\n\n\n# alternative = c(\"two.sided\", \"less\", \"greater\")\nt.test(x = chol$DiffChol, mu = 0, alternative = \"less\") %&gt;% \n  tidy() %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    -19.54167\n-5.696519\n4.217387e-06\n23\n-Inf\n-13.6623\nOne Sample t-test\nless"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#one-sided-confidence-intervals",
    "href": "slides_md/Day10_part2_bsta511_md.html#one-sided-confidence-intervals",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "One-sided confidence intervals",
    "text": "One-sided confidence intervals\n\n\nFormula for a 2-sided (1- \\(\\alpha\\) )% CI:\n\\[\\bar{x} \\pm t^*\\cdot\\frac{s}{\\sqrt{n}}\\]\n\n\\(t^*\\) = qt(1-alpha/2, df = n-1)\n\\(\\alpha\\) is split over both tails of the distribution\n\n\n\n\nA one-sided (1- \\(\\alpha\\) )% CI has all (1- \\(\\alpha\\) )% on just the left or the right tail of the distribution:\n\\[\\begin{align}\n(\\bar{x} - t^*\\cdot\\frac{s}{\\sqrt{n}},~\\infty) \\\\\n(\\infty,~\\bar{x} + t^*\\cdot\\frac{s}{\\sqrt{n}})\n\\end{align}\\]\n\n\\(t^*\\) = qt(1-alpha, df = n-1) for a\n1-sided lower (1- \\(\\alpha\\) )% CI\n\\(t^*\\) = qt(alpha, df = n-1) for a 1-sided upper (1- \\(\\alpha\\) )% CI\nA 1-sided CI gives estimates for a lower or upper bound of the population mean.\nSee Section 4.2.3 of the V&H book for more"
  },
  {
    "objectID": "slides_md/Day10_part2_bsta511_md.html#today-whats-next",
    "href": "slides_md/Day10_part2_bsta511_md.html#today-whats-next",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Today & what’s next?",
    "text": "Today & what’s next?\n\nCI’s and hypothesis tests for different scenarios:\n\\[\\text{point~estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test~stat} = \\frac{\\text{point~estimate}-\\text{null~value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\\(\\frac{s_d}{\\sqrt{n}}\\)\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n???\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)"
  },
  {
    "objectID": "slides_code/Day10_part2_bsta511_code.html",
    "href": "slides_code/Day10_part2_bsta511_code.html",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) # NEW!!\n\nset.seed(456)\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day10_part2_bsta511_code.html#load-packages",
    "href": "slides_code/Day10_part2_bsta511_code.html#load-packages",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) # NEW!!\n\nset.seed(456)\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day10_part2_bsta511_code.html#what-we-covered-in-day-10-part-1",
    "href": "slides_code/Day10_part2_bsta511_code.html#what-we-covered-in-day-10-part-1",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "What we covered in Day 10 Part 1",
    "text": "What we covered in Day 10 Part 1\n\n(4.3, 5.1) Hypothesis testing for mean from one sample\n\nIntroduce hypothesis testing using the case of analyzing a mean from one sample (group)\n\n\n\n\nSteps of a hypothesis test:\n\nlevel of significance\nnull ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\ntest statistic\np-value\nconclusion\n\n\n\n\nRun a hypothesis test in R\n\nLoad a dataset - need to specify location of dataset\nR projects\nRun a t-test in R\ntidy() the test output using broom package\n\n\n\n\n\n\n(4.3.3) Confidence intervals (CIs) vs. hypothesis tests"
  },
  {
    "objectID": "slides_code/Day10_part2_bsta511_code.html#goals-for-today-part-2---class-discussion",
    "href": "slides_code/Day10_part2_bsta511_code.html#goals-for-today-part-2---class-discussion",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Goals for today: Part 2 - Class discussion",
    "text": "Goals for today: Part 2 - Class discussion\n\n(5.2) Inference for mean difference from dependent/paired 2 samples\n\nInference: CIs and hypothesis testing\nExploratory data analysis (EDA) to visualize data\nRun paired t-test in R\n\n\n\nOne-sided CIs\n\n\nClass discussion\n\nInference for the mean difference from dependent/paired data is a special case of the inference for the mean from just one sample, that was already covered.\nThus this part will be used for class discussion to practice CIs and hypothesis testing for one mean and apply it in this new setting.\nIn class I will briefly introduce this topic, explain how it is similar and different from what we already covered, and let you work through the slides and code."
  },
  {
    "objectID": "slides_code/Day10_part2_bsta511_code.html#cis-and-hypothesis-tests-for-different-scenarios",
    "href": "slides_code/Day10_part2_bsta511_code.html#cis-and-hypothesis-tests-for-different-scenarios",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "CI’s and hypothesis tests for different scenarios:",
    "text": "CI’s and hypothesis tests for different scenarios:\n\n\\[\\text{point estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n???\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)"
  },
  {
    "objectID": "slides_code/Day10_part2_bsta511_code.html#steps-in-a-hypothesis-test",
    "href": "slides_code/Day10_part2_bsta511_code.html#steps-in-a-hypothesis-test",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Steps in a Hypothesis Test",
    "text": "Steps in a Hypothesis Test\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "slides_code/Day10_part2_bsta511_code.html#eda-explore-the-cholesterol-data",
    "href": "slides_code/Day10_part2_bsta511_code.html#eda-explore-the-cholesterol-data",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "EDA: Explore the cholesterol data",
    "text": "EDA: Explore the cholesterol data\n\nScenario:\n\n24 non-vegetarian people were enrolled in a study\nThey were instructed to adopt a vegetarian diet\nCholesterol levels were measured before and after the diet\n\n\n\nchol &lt;- read_csv(here::here(\"data\", \"chol213.csv\"))\nglimpse(chol)\n\nRows: 24\nColumns: 2\n$ Before &lt;dbl&gt; 195, 145, 205, 159, 244, 166, 250, 236, 192, 224, 238, 197, 169…\n$ After  &lt;dbl&gt; 146, 155, 178, 146, 208, 147, 202, 215, 184, 208, 206, 169, 182…\n\nchol %&gt;% \n  get_summary_stats(type = \"common\") %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      variable\n      n\n      min\n      max\n      median\n      iqr\n      mean\n      sd\n      se\n      ci\n    \n  \n  \n    Before\n24\n137\n250\n179\n44.5\n187.792\n33.160\n6.769\n14.002\n    After\n24\n125\n215\n165\n38.0\n168.250\n26.796\n5.470\n11.315\n  \n  \n  \n\n\n\n\nMake sure you are able to load the data on your computer!\n\nEDA: Cholesterol levels before and after vegetarian diet\nDescribe the distributions of the before & after data.\n\nggplot(chol, aes(x=Before)) +\n  geom_density()\n\n\n\nggplot(chol, aes(x=Before)) +\n  geom_boxplot()\n\n\n\n\n\nggplot(chol, aes(x=After)) +\n  geom_density()\n\n\n\nggplot(chol, aes(x=After)) +\n  geom_boxplot()\n\n\n\n\n\n\nEDA: Spaghetti plot of cholesterol levels before & after diet\n\nVisualize the individual before vs. after diet changes in cholesterol levels\n\nWhat does this figure tell us?\n\nchol_long &lt;- chol %&gt;% \n  # need an ID column for the plot\n  # make it factor so that coloring is not on continuous scale\n  mutate(ID = factor(1:n())) %&gt;% \n  # make data long for plot: \n  pivot_longer(\n    cols = Before:After,\n    names_to = \"Time\",  # need a column for Before & After on x-axis\n    values_to = \"Cholesterol\") %&gt;% # need a column of all cholesterol values for y-axis\n  mutate(\n    # change Time a factor variable so that can reorder\n    # levels so that Before is before After\n    Time = factor(Time, levels = c(\"Before\", \"After\"))\n    )\n  \nggplot(chol_long, \n       aes(x=Time, y = Cholesterol, \n           # need to include group = ID \n           # to create a line for each ID\n           color = ID, group = ID)) + \n  geom_line(show.legend = FALSE)\n\n\n\n\n\nYou will not be expected to create this figure yourself.\n\n\n\nEDA: Differences in cholesterol levels: After - Before diet\nWhat is this code doing?\n\nchol &lt;- chol %&gt;% \n  mutate(DiffChol = After-Before) \nhead(chol, 8)\n\n# A tibble: 8 × 3\n  Before After DiffChol\n   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1    195   146      -49\n2    145   155       10\n3    205   178      -27\n4    159   146      -13\n5    244   208      -36\n6    166   147      -19\n7    250   202      -48\n8    236   215      -21\n\n\nIs the mean of DiffChol the same as the difference in means of After - Before? Should it be? Why or why not?\n\nchol %&gt;% \n  get_summary_stats(type = \"common\") %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      variable\n      n\n      min\n      max\n      median\n      iqr\n      mean\n      sd\n      se\n      ci\n    \n  \n  \n    Before\n24\n137\n250\n179\n44.50\n187.792\n33.160\n6.769\n14.002\n    After\n24\n125\n215\n165\n38.00\n168.250\n26.796\n5.470\n11.315\n    DiffChol\n24\n-49\n13\n-19\n20.25\n-19.542\n16.806\n3.430\n7.096\n  \n  \n  \n\n\n\n\n\n\nEDA: Differences in cholesterol levels: After - Before diet\nCompare and contrast the 3 distributions. Comment on shape, center, and spread.\nBefore & After\n\nggplot(chol, aes(x=Before)) +\n  geom_density()\n\n\n\nggplot(chol, aes(x=After)) +\n  geom_density()\n\n\n\n\nDiffChol\n\nggplot(chol, aes(x=DiffChol)) + \n  geom_density()\n\n\n\nggplot(chol, aes(x=DiffChol)) + \n  geom_boxplot()"
  },
  {
    "objectID": "slides_code/Day10_part2_bsta511_code.html#step-2-null-alternative-hypotheses",
    "href": "slides_code/Day10_part2_bsta511_code.html#step-2-null-alternative-hypotheses",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Step 2: Null & Alternative Hypotheses",
    "text": "Step 2: Null & Alternative Hypotheses\n\nQuestion: Is there evidence to support that cholesterol levels changed after the vegetarian diet?\n\nNull and alternative hypotheses in words Include as much context as possible\n\n\n\\(H_0\\): The population mean difference in cholesterol levels after a vegetarian diet is fill in\n\\(H_A\\): The population mean difference in cholesterol levels after a vegetarian diet is fill in\n\nNull and alternative hypotheses in symbols\nfill in the missing parts of the hypotheses.\n\\[~~~~H_0: \\delta =  \\\\\nH_A: \\delta   \\\\\\]"
  },
  {
    "objectID": "slides_code/Day10_part2_bsta511_code.html#step-3-test-statistic",
    "href": "slides_code/Day10_part2_bsta511_code.html#step-3-test-statistic",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Step 3: Test statistic",
    "text": "Step 3: Test statistic\n\nchol %&gt;% select(DiffChol) %&gt;% get_summary_stats(type = \"common\") %&gt;% gt()\n\n\n\n\n\n  \n    \n      variable\n      n\n      min\n      max\n      median\n      iqr\n      mean\n      sd\n      se\n      ci\n    \n  \n  \n    DiffChol\n24\n-49\n13\n-19\n20.25\n-19.542\n16.806\n3.43\n7.096\n  \n  \n  \n\n\n\n\n\\[\nt_{\\bar{x}_d} = \\frac{\\bar{x}_d - \\delta_0}{\\frac{s_d}{\\sqrt{n}}}\n\\]\n\nCalculate the test statistic.\nBased on the value of the test statistic, do you think we are going to reject or fail to reject \\(H_0\\)?\nWhat probability distribution does the test statistic have?\nAre the assumptions for a paired t-test satisfied so that we can use the probability distribution to calculate the \\(p\\)-value??\n\n\nn &lt;- 24\nalpha &lt;- 0.05\nmu &lt;- 0\n(p_area &lt;- 1-alpha/2)\n\n[1] 0.975\n\n(xbar &lt;- mean(chol$DiffChol))\n\n[1] -19.54167\n\n(sd &lt;- sd(chol$DiffChol))\n\n[1] 16.80574\n\n(se &lt;- sd/sqrt(n))\n\n[1] 3.430458\n\n(tstat &lt;- (xbar - mu)/se)\n\n[1] -5.696519"
  },
  {
    "objectID": "slides_code/Day10_part2_bsta511_code.html#step-4-p-value",
    "href": "slides_code/Day10_part2_bsta511_code.html#step-4-p-value",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\n# specify upper and lower bounds of shaded region below\nmu &lt;- 0\nstd &lt;- se\n\n# The following figure is only an approximation of the \n# sampling distribution since I used a normal instead\n# of t-distribution to make it.\n\nggplot(data.frame(x = c(mu-6*std, mu+6*std)), aes(x = x)) + \n  stat_function(fun = dnorm, \n                args = list(mean = mu, sd = std)) + \n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks=c(mu, mu - 3.4*(1:6), mu + 3.4*(1:6))) +\n  theme(axis.text.x=element_text(angle = -30, hjust = 0)) +\n  labs(y = \"\", \n       x = \"sample mean difference\",\n       title = \"Sampling distribution of mean difference\") +\n  geom_vline(xintercept = c(-xbar, xbar), \n             color = \"red\")\n\n\n\n\n\nggplot(data = data.frame(x = c(-6, 6)), aes(x)) + \n  stat_function(fun = dt, args = list(df = n-1)) + \n  ylab(\"\") + \n  xlab(\"t-dist with df = 23\") +\n  scale_y_continuous(breaks = NULL) + \n  scale_x_continuous(breaks=c(mu, mu - (1:5), mu + (1:5))) +\n  geom_vline(xintercept = c(-tstat,tstat), \n             color = \"red\")\n\n\n\n\nCalculate the p-value and shade in the area representing the p-value:\n\n(pv &lt;- 2*(pt(tstat, df=n-1)))\n\n[1] 8.434775e-06"
  },
  {
    "objectID": "slides_code/Day10_part2_bsta511_code.html#step-5-conclusion-to-hypothesis-test",
    "href": "slides_code/Day10_part2_bsta511_code.html#step-5-conclusion-to-hypothesis-test",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\\[\\begin{align}\nH_0:& \\delta = 0 \\\\\nH_A:& \\delta \\neq 0 \\\\\n\\end{align}\\]\n\nRecall the \\(p\\)-value = \\(8.434775 \\cdot 10 ^{-6}\\)\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nStats class conclusion\n\nThere is sufficient evidence that the (population) mean difference in cholesterol levels after a vegetarian diet is different from 0 mg/dL ( \\(p\\)-value &lt; 0.001).\n\nMore realistic manuscript conclusion:\n\nAfter a vegetarian diet, cholesterol levels decreased by on average 19.54 mg/dL (SE = 3.43 mg/dL, 2-sided \\(p\\)-value &lt; 0.001)."
  },
  {
    "objectID": "slides_code/Day10_part2_bsta511_code.html#r-option-1-run-a-1-sample-t.test-using-the-paired-differences",
    "href": "slides_code/Day10_part2_bsta511_code.html#r-option-1-run-a-1-sample-t.test-using-the-paired-differences",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "R option 1: Run a 1-sample t.test using the paired differences",
    "text": "R option 1: Run a 1-sample t.test using the paired differences\n\\(H_A: \\delta \\neq 0\\)\n\nt.test(x = chol$DiffChol, mu = 0)\n\n\n    One Sample t-test\n\ndata:  chol$DiffChol\nt = -5.6965, df = 23, p-value = 8.435e-06\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -26.63811 -12.44522\nsample estimates:\nmean of x \n-19.54167 \n\n\nRun the code without mu = 0. Do the results change? Why or why not?"
  },
  {
    "objectID": "slides_code/Day10_part2_bsta511_code.html#r-option-2-run-a-2-sample-t.test-with-paired-true-option",
    "href": "slides_code/Day10_part2_bsta511_code.html#r-option-2-run-a-2-sample-t.test-with-paired-true-option",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "R option 2: Run a 2-sample t.test with paired = TRUE option",
    "text": "R option 2: Run a 2-sample t.test with paired = TRUE option\n\\(H_A: \\delta \\neq 0\\)\n\nFor a 2-sample t-test we specify both x= and y=\nNote: mu = 0 is the default value and doesn’t need to be specified\n\n\nt.test(x = chol$Before, y = chol$After, mu = 0, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  chol$Before and chol$After\nt = 5.6965, df = 23, p-value = 8.435e-06\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 12.44522 26.63811\nsample estimates:\nmean difference \n       19.54167 \n\n\nWhat is different in the output compared to option 1?"
  },
  {
    "objectID": "slides_code/Day10_part2_bsta511_code.html#r-option-3-run-a-2-sample-t.test-with-paired-true-option-but-using-the-long-data-and-a-formula",
    "href": "slides_code/Day10_part2_bsta511_code.html#r-option-3-run-a-2-sample-t.test-with-paired-true-option-but-using-the-long-data-and-a-formula",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "R option 3: Run a 2-sample t.test with paired = TRUE option, but using the long data and a “formula”",
    "text": "R option 3: Run a 2-sample t.test with paired = TRUE option, but using the long data and a “formula”\n\nThe data have to be in a long format for option 3, where each person has 2 rows: one for Before and one for After.\n\nThe long dataset chol_long was created for the slide “EDA: Spaghetti plot of cholesterol levels before & after diet”.\nSee the code to create it there.\n\nWhat information is being stored in each of the columns?\n\n\n# first 16 rows of long data:\nhead(chol_long, 16)\n\n# A tibble: 16 × 3\n   ID    Time   Cholesterol\n   &lt;fct&gt; &lt;fct&gt;        &lt;dbl&gt;\n 1 1     Before         195\n 2 1     After          146\n 3 2     Before         145\n 4 2     After          155\n 5 3     Before         205\n 6 3     After          178\n 7 4     Before         159\n 8 4     After          146\n 9 5     Before         244\n10 5     After          208\n11 6     Before         166\n12 6     After          147\n13 7     Before         250\n14 7     After          202\n15 8     Before         236\n16 8     After          215\n\n\n\nUse the usual t.test\nWhat’s different is that\n\ninstead of specifying the variables with x= and y=,\nwe give a formula of the form y ~ x using just the variable names,\nand then specify the name of the dataset using data =\n\nThis method is often used in practice, and more similar to the coding style of running a regression model (BSTA 512 & 513)\n\n\n# using long data \n# with columns Cholesterol & Time\nt.test(Cholesterol ~ Time, \n       paired = TRUE, \n       data = chol_long)\n\n\n    Paired t-test\n\ndata:  Cholesterol by Time\nt = 5.6965, df = 23, p-value = 8.435e-06\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 12.44522 26.63811\nsample estimates:\nmean difference \n       19.54167 \n\n\n\nWhat is different in the output compared to option 1?\nRerun the test using Time ~ Cholesterol (switch the variables). What do you get?"
  },
  {
    "objectID": "slides_code/Day10_part2_bsta511_code.html#compare-the-3-options",
    "href": "slides_code/Day10_part2_bsta511_code.html#compare-the-3-options",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "Compare the 3 options",
    "text": "Compare the 3 options\n\nHow is the code similar and different for the 3 options?\nGiven a dataset, how would you choose which of the 3 options to use?\n\n\n# option 1\nt.test(x = chol$DiffChol, mu = 0) %&gt;% tidy() %&gt;% gt() # tidy from broom package\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    -19.54167\n-5.696519\n8.434775e-06\n23\n-26.63811\n-12.44522\nOne Sample t-test\ntwo.sided\n  \n  \n  \n\n\n\n# option 2\nt.test(x = chol$Before, y = chol$After, mu = 0, paired = TRUE) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    19.54167\n5.696519\n8.434775e-06\n23\n12.44522\n26.63811\nPaired t-test\ntwo.sided\n  \n  \n  \n\n\n\n# option 3\nt.test(Cholesterol ~ Time, paired = TRUE, data = chol_long) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    19.54167\n5.696519\n8.434775e-06\n23\n12.44522\n26.63811\nPaired t-test\ntwo.sided"
  },
  {
    "objectID": "slides_code/Day10_part2_bsta511_code.html#r-what-if-we-wanted-to-test-whether-the-diet-decreased-cholesterol-levels",
    "href": "slides_code/Day10_part2_bsta511_code.html#r-what-if-we-wanted-to-test-whether-the-diet-decreased-cholesterol-levels",
    "title": "Day 10 Part 2: Inference for mean difference from two-sample dependent/paired data (Section 5.2)",
    "section": "R: What if we wanted to test whether the diet decreased cholesterol levels?",
    "text": "R: What if we wanted to test whether the diet decreased cholesterol levels?\n\nWhich of the 3 options to run a paired t-test in R is being used below?\nHow did the code change to account for testing a decrease in cholesterol levels?\nWhich values in the output changed compared to testing for a change in cholesterol levels? How did they change?\n\n\n# alternative = c(\"two.sided\", \"less\", \"greater\")\nt.test(x = chol$DiffChol, mu = 0, alternative = \"less\") %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    -19.54167\n-5.696519\n4.217387e-06\n23\n-Inf\n-13.6623\nOne Sample t-test\nless"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html",
    "href": "slides_code/Day10_bsta511_code.html",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) # NEW!!\n\nset.seed(456)\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#load-packages",
    "href": "slides_code/Day10_bsta511_code.html#load-packages",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) # NEW!!\n\nset.seed(456)\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#part-1",
    "href": "slides_code/Day10_bsta511_code.html#part-1",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Part 1",
    "text": "Part 1\n\n(4.3, 5.1) Hypothesis testing for mean from one sample\n\nIntroduce hypothesis testing using the case of analyzing a mean from one sample (group)\n\n\n\n\nSteps of a hypothesis test:\n\nlevel of significance\nnull ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\ntest statistic\np-value\nconclusion\n\n\n\n\nRun a hypothesis test in R\n\nLoad a dataset - need to specify location of dataset\nR projects\nRun a t-test in R\ntidy() the test output using broom package\n\n\n\n\n\n\n(4.3.3) Confidence intervals (CIs) vs. hypothesis tests"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#part-2---class-discussion",
    "href": "slides_code/Day10_bsta511_code.html#part-2---class-discussion",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Part 2 - Class discussion",
    "text": "Part 2 - Class discussion\n\n(5.2) Inference for mean difference from dependent/paired 2 samples\n\nInference: CIs and hypothesis testing\nExploratory data analysis (EDA) to visualize data\nRun paired t-test in R\n\n\n\nOne-sided CIs\n\n\nClass discussion\n\nInference for the mean difference from dependent/paired data is a special case of the inference for the mean from just one sample, that was already covered.\nThus this part will be used for class discussion to practice CIs and hypothesis testing for one mean and apply it in this new setting.\nIn class I will briefly introduce this topic, explain how it is similar and different from what we already covered, and let you work through the slides and code."
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#moritzs-tip-of-the-day-use-r-projects-to-organize-analyses",
    "href": "slides_code/Day10_bsta511_code.html#moritzs-tip-of-the-day-use-r-projects-to-organize-analyses",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "MoRitz’s tip of the day: use R projects to organize analyses",
    "text": "MoRitz’s tip of the day: use R projects to organize analyses\nMoRitz loves using R projects to\n\norganize analyses and\nmake it easier to load data files\nand also save output\n\nOther bonuses include\n\nmaking to it easier to collaborate with others,\nincluding yourself when accessing files from different computers.\n\n\nWe will discuss how to use projects later in today’s slides when loading a dataset.\nSee file Projects in RStudio for more information."
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#question-based-on-the-1992-jama-data-is-there-evidence-to-support-that-the-population-mean-body-temperature-is-different-from-98.6f",
    "href": "slides_code/Day10_bsta511_code.html#question-based-on-the-1992-jama-data-is-there-evidence-to-support-that-the-population-mean-body-temperature-is-different-from-98.6f",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Question: based on the 1992 JAMA data, is there evidence to support that the population mean body temperature is different from 98.6°F?",
    "text": "Question: based on the 1992 JAMA data, is there evidence to support that the population mean body temperature is different from 98.6°F?\nTwo approaches to answer this question:\n\nCreate a confidence interval (CI) for the population mean \\(\\mu\\) and determine whether 98.6°F is inside the CI or not.\n\nis 98.6°F a plausible value?\n\nRun a hypothesis test to see if there is evidence that the population mean \\(\\mu\\) is significantly different from 98.6°F or not.\n\nThis does not give us a range of plausible values for the population mean \\(\\mu\\).\nInstead, we calculate a test statistic and p-value\n\nto see how likely we are to observe the sample mean \\(\\bar{x}\\)\nor a more extreme sample mean\nassuming that the population mean \\(\\mu\\) is 98.6°F.\n\n\n\n\nApproach 1: Create a 95% C I for the population mean body temperature\n\nUse data based on the results from the 1992 JAMA study\n\nThe original dataset used in the JAMA article is not available\nHowever, Allen Shoemaker from Calvin College created a dataset with the same summary statistics as in the JAMA article, which we will use:\n\n\n\\[\\bar{x} = 98.25,~s=0.733,~n=130\\] CI for \\(\\mu\\):\n\\[\\begin{align}\n\\bar{x} &\\pm t^*\\cdot\\frac{s}{\\sqrt{n}}\\\\\n98.25 &\\pm 1.979\\cdot\\frac{0.733}{\\sqrt{130}}\\\\\n98.25 &\\pm 0.127\\\\\n(98.123&, 98.377)\n\\end{align}\\]\nUsed \\(t^*\\) = qt(.975, df=129)\nConclusion:\nWe are 95% confident that the (population) mean body temperature is between 98.123°F and 98.377°F.\n\nHow does the CI compare to 98.6°F?\n\n\n\nApproach 2: Hypothesis Test\nFrom before:\n\nRun a hypothesis test to see if there is evidence that the population mean \\(\\mu\\) is significantly different from 98.6°F or not.\n\nThis does not give us a range of plausible values for the population mean \\(\\mu\\).\nInstead, we calculate a test statistic and p-value\n\nto see how likely we are to observe the sample mean \\(\\bar{x}\\)\nor a more extreme sample mean\nassuming that the population mean \\(\\mu\\) is 98.6°F.\n\n\n\nHow do we calculate a test statistic and p-value?\n\n\nRecall the sampling distribution of the mean\nFrom the Central Limit Theorem (CLT), we know that\n\nFor “large” sample sizes ( \\(n\\geq 30\\) ),\n\nthe sampling distribution of the sample mean\ncan be approximated by a normal distribution,with\n\nmean equal to the population mean value \\(\\mu\\), and\nstandard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\n\n\n\\[\\bar{X}\\sim N\\Big(\\mu_{\\bar{X}} = \\mu, \\sigma_{\\bar{X}}= \\frac{\\sigma}{\\sqrt{n}}\\Big)\\]\n\nFor small sample sizes, if the population is known to be normally distributed, then\n\nthe same result holds\n\n\n\n\nCase 1: suppose we know the population sd \\(\\sigma\\)\n\nHow likely we are to observe the sample mean \\(\\bar{x}\\) ,\n\nor a more extreme sample mean,\nassuming that the population mean \\(\\mu\\) is 98.6°F?\n\nUse \\(\\bar{x} = 98.25\\), \\(\\sigma=0.733\\), and \\(n=130\\)\n\n\n\n\n\n\n\n\nCase 2: we don’t know the population sd \\(\\sigma\\)\n\nThis is usually the case in real life\nWe estimate \\(\\sigma\\) with the sample standard deviation \\(s\\)\nFrom last time, we know that in this case we need to use the t-distribution with d.f. = n-1, instead of the normal distribution\nQuestion: How likely we are to observe the sample mean \\(\\bar{x}\\) or a more extreme sample mean, assuming that the population mean \\(\\mu\\) is 98.6°F?\nUse \\(\\bar{x} = 98.25\\), \\(s=0.733\\), and \\(n=130\\)"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#step-2-null-alternative-hypotheses",
    "href": "slides_code/Day10_bsta511_code.html#step-2-null-alternative-hypotheses",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Step 2: Null & Alternative Hypotheses",
    "text": "Step 2: Null & Alternative Hypotheses\nIn statistics, a hypothesis is a statement about the value of an unknown population parameter.\nA hypothesis test consists of a test between two competing hypotheses:\n\na null hypothesis \\(H_0\\) (pronounced “H-naught”) vs. \nan alternative hypothesis \\(H_A\\) (also denoted \\(H_1\\))\n\nExample of hypotheses in words:\n\\[\\begin{aligned}\nH_0 &: \\text{The population mean body temperature is 98.6°F}\\\\\n\\text{vs. } H_A &: \\text{The population mean body temperature is not 98.6°F}\n\\end{aligned}\\]\n\n\\(H_0\\) is a claim that there is “no effect” or “no difference of interest.”\n\\(H_A\\) is the claim a researcher wants to establish or find evidence to support. It is viewed as a “challenger” hypothesis to the null hypothesis \\(H_0\\)\n\n\nNotation for hypotheses\n\\[\\begin{aligned}\nH_0 &: \\mu = \\mu_0\\\\\n\\text{vs. } H_A&: \\mu \\neq, &lt;, \\textrm{or}, &gt; \\mu_0\n\\end{aligned}\\]\nWe call \\(\\mu_0\\) the null value\n\n\n\\(H_A: \\mu \\neq \\mu_0\\)\n\n\nnot choosing a priori whether we believe the population mean is greater or less than the null value \\(\\mu_0\\)\n\n\n\n\\(H_A: \\mu &lt; \\mu_0\\)\n\n\nbelieve the population mean is less than the null value \\(\\mu_0\\)\n\n\n\n\\(H_A: \\mu &gt; \\mu_0\\)\n\n\nbelieve the population mean is greater than the null value \\(\\mu_0\\)\n\n\n\n\n\n\\(H_A: \\mu \\neq \\mu_0\\) is the most common option, since it’s the most conservative\n\nExample:\n\\[\\begin{aligned}\nH_0 &: \\mu = 98.6\\\\\n\\text{vs. } H_A&: \\mu \\neq 98.6\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#step-3-test-statistic-its-distribution",
    "href": "slides_code/Day10_bsta511_code.html#step-3-test-statistic-its-distribution",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Step 3: Test statistic (& its distribution)",
    "text": "Step 3: Test statistic (& its distribution)\nCase 1: know population sd \\(\\sigma\\)\n\\[\n\\text{test statistic} = z_{\\bar{x}} = \\frac{\\bar{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\n\\]\n\nStatistical theory tells us that \\(z_{\\bar{x}}\\) follows a Standard Normal distribution \\(N(0,1)\\)\n\nCase 2: don’t know population sd \\(\\sigma\\)\n\\[\n\\text{test statistic} = t_{\\bar{x}} = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\n\\]\n\nStatistical theory tells us that \\(t_{\\bar{x}}\\) follows a Student’s t distribution with degrees of freedom (df) = \\(n-1\\)\n\n\\(\\bar{x}\\) = sample mean,\n\\(\\mu_0\\) = hypothesized population mean from \\(H_0\\),\n\\(\\sigma\\) = population standard deviation,\n\\(s\\) = sample standard deviation,\n\\(n\\) = sample size\nAssumptions: same as CLT\n\nIndependent observations\n\nThe observations were collected independently.\n\nApproximately normal sample or big n\n\nThe distribution of the sample should be approximately normal\nor the sample size should be at least 30."
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#step-3-test-statistic-calculation",
    "href": "slides_code/Day10_bsta511_code.html#step-3-test-statistic-calculation",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Step 3: Test statistic calculation",
    "text": "Step 3: Test statistic calculation\nRecall that \\(\\bar{x} = 98.25\\), \\(s=0.733\\), and \\(n=130.\\)\nThe test statistic is:\n\\[t_{\\bar{x}} = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\n= \\frac{98.25 - 98.6}{\\frac{0.73}{\\sqrt{130}}}\n= -5.45\\]\n\nStatistical theory tells us that \\(t_{\\bar{x}}\\) follows a Student’s t-distribution with \\(d.f. = n-1 = 129\\).\n\n\n\n\n\n\nAssumptions met?"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#step-4-p-value",
    "href": "slides_code/Day10_bsta511_code.html#step-4-p-value",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\nThe \\(p\\)-value is a quantification of “surprise”\n\nAssuming \\(H_0\\) is true, how surprised are we with the observed results?\nEx: assuming that the true mean body temperature is 98.6°F, how surprised are we to get a sample mean of 98.25°F (or more extreme)?\n\nIf the \\(p\\)-value is “small,” it means there’s a small probability that we would get the observed statistic (or more extreme) when \\(H_0\\) is true."
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#step-4-p-value-calculation",
    "href": "slides_code/Day10_bsta511_code.html#step-4-p-value-calculation",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Step 4: p-value calculation",
    "text": "Step 4: p-value calculation\nCalculate the p-value using the Student’s t-distribution with \\(d.f. = n-1 = 129\\):\n\\[p-value=P(T \\leq -5.45) + P(T \\geq 5.45) = 2.410889 \\times 10^{-07}\\]\n\n# use pt() instead of pnorm()\n# need to specify df\n2*pt(-5.4548, df = 130-1, lower.tail = TRUE)\n\n[1] 2.410889e-07"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#step-4-p-value-estimation-using-t-table",
    "href": "slides_code/Day10_bsta511_code.html#step-4-p-value-estimation-using-t-table",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Step 4: p-value estimation using \\(t\\)-table",
    "text": "Step 4: p-value estimation using \\(t\\)-table\n\n\\(t\\)-table only gives us bounds on the p-value\nRecall from using the \\(t\\)-table for CIs, that the table gives us the cutoff values for varying tail probabilities (1-tail & 2-tail)\nFind the row with the appropriate degrees of freedom\n\nUse next smallest df in table if actual df not shown\nI.e., for df = 129, use df = 100 in table\n\nFigure out where the test statistic’s absolute value is in relation to the values in the columns, i.e. between which columns is the test statistic?\nThe header rows for those columns gives the lower & upper bounds for the p-value\n\nChoosing one-tail vs. two-tail test, depends on the alternative hypothesis \\(H_A\\).\nFor a 2-sided test ( \\(H_A: \\mu \\neq \\mu_0\\) ), use two-tails\nFor a 1-sided test ( \\(H_A: \\mu &lt; \\textrm{or} &gt; \\mu_0\\) ), use one-tail"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#step-1-significance-level-alpha",
    "href": "slides_code/Day10_bsta511_code.html#step-1-significance-level-alpha",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Step 1: Significance Level \\(\\alpha\\)",
    "text": "Step 1: Significance Level \\(\\alpha\\)\n\nBefore doing a hypothesis test, we set a cut-off for how small the \\(p\\)-value should be in order to reject \\(H_0\\).\nWe call this the significance level, denoted by the Greek symbol alpha ( \\(\\alpha\\) )\nTypical \\(\\alpha\\) values are\n\n0.05 - most common by far!!\n0.01 and 0.1\n\nDecision rule:\n\nWhen \\(p\\)-value &lt; \\(\\alpha\\), we “reject the null hypothesis \\(H_0\\).”\nWhen \\(p\\)-value \\(\\geq \\alpha\\), we “fail to reject the null hypothesis \\(H_0\\).”\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\n“Failing to reject” \\(H_0\\) is NOT the same as “accepting” \\(H_0\\)!\nBy failing to reject \\(H_0\\) we are just saying that we don’t have sufficient evidence to support the alternative \\(H_A\\).\nThis does not imply that \\(H_0\\) is true!!"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#step-5-conclusion-to-hypothesis-test",
    "href": "slides_code/Day10_bsta511_code.html#step-5-conclusion-to-hypothesis-test",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\\[\\begin{aligned}\nH_0 &: \\mu = 98.6\\\\\n\\text{vs. } H_A&: \\mu \\neq 98.6\n\\end{aligned}\\]\n\nRecall the \\(p\\)-value = \\(2.410889 \\times 10^{-07}\\)\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nBasic: (“stats class” conclusion)\n\nThere is sufficient evidence that the (population) mean body temperature is discernibly different from 98.6°F ( \\(p\\)-value &lt; 0.001).\n\nBetter: (“manuscript style” conclusion)\n\nThe average body temperature in the sample was 98.25°F (95% CI 98.12, 98.38°F), which is discernibly different from 98.6°F ( \\(p\\)-value &lt; 0.001)."
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#confidence-intervals-vs.-hypothesis-testing",
    "href": "slides_code/Day10_bsta511_code.html#confidence-intervals-vs.-hypothesis-testing",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Confidence Intervals vs. Hypothesis Testing",
    "text": "Confidence Intervals vs. Hypothesis Testing\n\nSee also V&H Section 4.3.3"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#working-directory",
    "href": "slides_code/Day10_bsta511_code.html#working-directory",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Working directory",
    "text": "Working directory\n\nIn order to load a dataset from a file, you need to tell R where the dataset is located\nTo do this you also need to know the location from which R is working, i.e. your working directory\nYou can figure out your working directory by running the getwd() function.\n\n\ngetwd()\n\n[1] \"/Users/niederha/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/teaching/BSTA 511/F24/0_webpage/BSTA_511_F24\"\n\n\n\nAbove is the working directory of this slides file\n\nIn this case, this is NOT the location of the actual qmd file though!\n\nTo make it easier to juggle the working directory, the location of your qmd file, and the location of the data,\n\nI highly recommend using R Projects!"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#r-projects",
    "href": "slides_code/Day10_bsta511_code.html#r-projects",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "R projects",
    "text": "R projects\n\nI highly, highly, HIGHLY recommend using R Projects to organize your analyses and make it easier to load data files and also save output.\nWhen you create an R Project on your computer, the Project is associated with the folder (directory) you created it in.\n\nThis folder becomes the “root” of your working directory, and RStudio’s point of reference from where to load files from and to.\n\nI create separate Projects for every analysis project and every class I teach.\nYou can run multiple sessions of RStudio by opening different Projects, and the environments (or working directory) of each are working independently of each other.\n\n\n\n\n\n\n\nNote\n\n\n\n\nAlthough we are using Quarto files,\n\nI will show how to set up and use a “regular” R Project\ninstead of “Quarto Project”\n\nQuarto Projects include extra features and thus complexity. Once you are used to how regular R Projects work, you can try out a Quarto Project."
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#how-to-create-an-r-project",
    "href": "slides_code/Day10_bsta511_code.html#how-to-create-an-r-project",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "How to create an R Project",
    "text": "How to create an R Project\n\nDemonstration in class recording\nPosit’s (RStudio’s) directions for creating Projects\n\nhttps://support.rstudio.com/hc/en-us/articles/200526207-Using-RStudio-Projects\n\nSee file Projects in RStudio for more information on R Projects."
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#load-the-dataset",
    "href": "slides_code/Day10_bsta511_code.html#load-the-dataset",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Load the dataset",
    "text": "Load the dataset\n\nThe data are in a csv file called BodyTemperatures.csv\nYou need to tell R where the dataset is located!\nI recommend saving all datasets in a folder called data.\n\nThe code I will be providing you will be set up this way.\n\nTo make it easier to specify where the dataset is located, I recommend using the here() function from the here package: here::here().\n\n\n# read_csv() is a function from the readr package that is a part of the tidyverse\nlibrary(here)   # first install this package\n\nBodyTemps &lt;- read_csv(here::here(\"data\", \"BodyTemperatures.csv\"))\n#                     location: look in \"data\" folder\n#                               for the file \"BodyTemperatures.csv\"\n\nglimpse(BodyTemps)\n\nRows: 130\nColumns: 3\n$ Temperature &lt;dbl&gt; 96.3, 96.7, 96.9, 97.0, 97.1, 97.1, 97.1, 97.2, 97.3, 97.4…\n$ Gender      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ HeartRate   &lt;dbl&gt; 70, 71, 74, 80, 73, 75, 82, 64, 69, 70, 68, 72, 78, 70, 75…"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#herehere",
    "href": "slides_code/Day10_bsta511_code.html#herehere",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "here::here()",
    "text": "here::here()\nGeneral use of here::here()\nhere::here(\"folder_name\", \"filename\")\nResources for here::here():\n\nhow to use the here package (Jenny Richmond)\nOde to the here package (Jenny Bryan)\n\nProject-oriented workflow (Jenny Bryan)"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#t.test-base-rs-function-for-testing-one-mean",
    "href": "slides_code/Day10_bsta511_code.html#t.test-base-rs-function-for-testing-one-mean",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "t.test: base R’s function for testing one mean",
    "text": "t.test: base R’s function for testing one mean\n\nUse the body temperature example with \\(H_A: \\mu \\neq 98.6\\)\nWe called the dataset BodyTemps when we loaded it\n\n\nglimpse(BodyTemps)\n\nRows: 130\nColumns: 3\n$ Temperature &lt;dbl&gt; 96.3, 96.7, 96.9, 97.0, 97.1, 97.1, 97.1, 97.2, 97.3, 97.4…\n$ Gender      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ HeartRate   &lt;dbl&gt; 70, 71, 74, 80, 73, 75, 82, 64, 69, 70, 68, 72, 78, 70, 75…\n\n(temps_ttest &lt;- t.test(x = BodyTemps$Temperature,\n       # alternative = \"two.sided\",  # default\n       mu = 98.6))\n\n\n    One Sample t-test\n\ndata:  BodyTemps$Temperature\nt = -5.4548, df = 129, p-value = 2.411e-07\nalternative hypothesis: true mean is not equal to 98.6\n95 percent confidence interval:\n 98.12200 98.37646\nsample estimates:\nmean of x \n 98.24923 \n\n\nNote that the test output also gives the 95% CI using the t-distribution."
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#tidy-the-t.test-output",
    "href": "slides_code/Day10_bsta511_code.html#tidy-the-t.test-output",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "tidy() the t.test output",
    "text": "tidy() the t.test output\n\nUse the tidy() function from the broom package for briefer output in table format that’s stored as a tibble\nCombined with the gt() function from the gt package, we get a nice table\n\n\ntidy(temps_ttest) %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    98.24923\n-5.454823\n2.410632e-07\n129\n98.122\n98.37646\nOne Sample t-test\ntwo.sided\n  \n  \n  \n\n\n\n\n\nSince the tidy() output is a tibble, we can easily pull() specific values from it:\n\n\n\nUsing base R’s $\n\ntidy(temps_ttest)$p.value  \n\n[1] 2.410632e-07\n\n\nAdvantage: quick and easy\n\nOr the tidyverse way: using pull() from dplyr package\n\ntidy(temps_ttest) %&gt;% pull(p.value)\n\n[1] 2.410632e-07\n\n\nAdvantage: can use together with piping (%&gt;%) other functions"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#whats-next",
    "href": "slides_code/Day10_bsta511_code.html#whats-next",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "What’s next?",
    "text": "What’s next?\nCI’s and hypothesis testing for different scenarios:\n\n\n\n\n\n\n\n\n\n\n\nDay\nSection\nPopulation parameter\nSymbol\nPoint estimate\nSymbol\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\n\n11\n5.3\nDiff in pop means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n\n\n12\n8.2\nDiff in pop prop’s\n\\(p_1-p_2\\)\nDiff in sample prop’s\n\\(\\widehat{p}_1-\\widehat{p}_2\\)"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#what-we-covered-in-day-10-part-1",
    "href": "slides_code/Day10_bsta511_code.html#what-we-covered-in-day-10-part-1",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "What we covered in Day 10 Part 1",
    "text": "What we covered in Day 10 Part 1\n\n(4.3, 5.1) Hypothesis testing for mean from one sample\n\nIntroduce hypothesis testing using the case of analyzing a mean from one sample (group)\n\n\n\n\nSteps of a hypothesis test:\n\nlevel of significance\nnull ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\ntest statistic\np-value\nconclusion\n\n\n\n\nRun a hypothesis test in R\n\nLoad a dataset - need to specify location of dataset\nR projects\nRun a t-test in R\ntidy() the test output using broom package\n\n\n\n\n\n\n(4.3.3) Confidence intervals (CIs) vs. hypothesis tests"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#goals-for-today-part-2---class-discussion",
    "href": "slides_code/Day10_bsta511_code.html#goals-for-today-part-2---class-discussion",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Goals for today: Part 2 - Class discussion",
    "text": "Goals for today: Part 2 - Class discussion\n\n(5.2) Inference for mean difference from dependent/paired 2 samples\n\nInference: CIs and hypothesis testing\nExploratory data analysis (EDA) to visualize data\nRun paired t-test in R\n\n\n\nOne-sided CIs\n\n\nClass discussion\n\nInference for the mean difference from dependent/paired data is a special case of the inference for the mean from just one sample, that was already covered.\nThus this part will be used for class discussion to practice CIs and hypothesis testing for one mean and apply it in this new setting.\nIn class I will briefly introduce this topic, explain how it is similar and different from what we already covered, and let you work through the slides and code."
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#cis-and-hypothesis-tests-for-different-scenarios",
    "href": "slides_code/Day10_bsta511_code.html#cis-and-hypothesis-tests-for-different-scenarios",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "CI’s and hypothesis tests for different scenarios:",
    "text": "CI’s and hypothesis tests for different scenarios:\n\n\\[\\text{point~estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test~stat} = \\frac{\\text{point~estimate}-\\text{null~value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n???\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#steps-in-a-hypothesis-test-1",
    "href": "slides_code/Day10_bsta511_code.html#steps-in-a-hypothesis-test-1",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Steps in a Hypothesis Test",
    "text": "Steps in a Hypothesis Test\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#examples-of-paired-designs-two-samples",
    "href": "slides_code/Day10_bsta511_code.html#examples-of-paired-designs-two-samples",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Examples of paired designs (two samples)",
    "text": "Examples of paired designs (two samples)\n\nEnroll pairs of identical twins to study a disease\nEnroll father & son pairs to study cholesterol levels\nStudying pairs of eyes\nEnroll people and collect data before & after an intervention (longitudinal data)\nBook: Compare maximal speed of competitive swimmers wearing a wetsuit vs. wearing a regular swimsuit"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#eda-explore-the-cholesterol-data",
    "href": "slides_code/Day10_bsta511_code.html#eda-explore-the-cholesterol-data",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "EDA: Explore the cholesterol data",
    "text": "EDA: Explore the cholesterol data\n\nScenario:\n\n24 non-vegetarian people were enrolled in a study\nThey were instructed to adopt a vegetarian diet\nCholesterol levels were measured before and after the diet\n\n\n\nchol &lt;- read_csv(\"data/chol213.csv\")\nglimpse(chol)\n\nRows: 24\nColumns: 2\n$ Before &lt;dbl&gt; 195, 145, 205, 159, 244, 166, 250, 236, 192, 224, 238, 197, 169…\n$ After  &lt;dbl&gt; 146, 155, 178, 146, 208, 147, 202, 215, 184, 208, 206, 169, 182…\n\nchol %&gt;% get_summary_stats(type = \"common\") %&gt;% gt()\n\n\n\n\n\n  \n    \n      variable\n      n\n      min\n      max\n      median\n      iqr\n      mean\n      sd\n      se\n      ci\n    \n  \n  \n    Before\n24\n137\n250\n179\n44.5\n187.792\n33.160\n6.769\n14.002\n    After\n24\n125\n215\n165\n38.0\n168.250\n26.796\n5.470\n11.315\n  \n  \n  \n\n\n\n\n\nEDA: Cholesterol levels before and after vegetarian diet\n\nggplot(chol, aes(x=Before)) +\n  geom_density()\n\n\n\nggplot(chol, aes(x=Before)) +\n  geom_boxplot()\n\n\n\n\n\nggplot(chol, aes(x=After)) +\n  geom_density()\n\n\n\nggplot(chol, aes(x=After)) +\n  geom_boxplot()\n\n\n\n\n\n\nEDA: Spaghetti plot of cholesterol levels before & after diet\n\nVisualize the individual before vs. after diet changes in cholesterol levels\n\n\nchol_long &lt;- chol %&gt;% \n  # need an ID column for the plot\n  # make it factor so that coloring is not on continuous scale\n  mutate(ID = factor(1:n())) %&gt;% \n  # make data long for plot: \n  pivot_longer(\n    cols = Before:After,\n    names_to = \"Time\",  # need a column for Before & After on x-axis\n    values_to = \"Cholesterol\") %&gt;% # need a column of all cholesterol values for y-axis\n  mutate(\n    # change Time a factor variable so that can reorder\n    # levels so that Before is before After\n    Time = factor(Time, levels = c(\"Before\", \"After\"))\n    )\n  \nggplot(chol_long, \n       aes(x=Time, y = Cholesterol, \n           # need to include group = ID \n           # to create a line for each ID\n           color = ID, group = ID)) + \n  geom_line(show.legend = FALSE)\n\n\n\n\n\n\nEDA: Differences in cholesterol levels: After - Before diet\n\nchol &lt;- chol %&gt;% \n  mutate(DiffChol = After-Before) \nhead(chol, 12)\n\n# A tibble: 12 × 3\n   Before After DiffChol\n    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1    195   146      -49\n 2    145   155       10\n 3    205   178      -27\n 4    159   146      -13\n 5    244   208      -36\n 6    166   147      -19\n 7    250   202      -48\n 8    236   215      -21\n 9    192   184       -8\n10    224   208      -16\n11    238   206      -32\n12    197   169      -28\n\n\n\nchol %&gt;% \n  get_summary_stats(type = \"common\") %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      variable\n      n\n      min\n      max\n      median\n      iqr\n      mean\n      sd\n      se\n      ci\n    \n  \n  \n    Before\n24\n137\n250\n179\n44.50\n187.792\n33.160\n6.769\n14.002\n    After\n24\n125\n215\n165\n38.00\n168.250\n26.796\n5.470\n11.315\n    DiffChol\n24\n-49\n13\n-19\n20.25\n-19.542\n16.806\n3.430\n7.096\n  \n  \n  \n\n\n\n\n\n\nEDA: Differences in cholesterol levels: After - Before diet\n\nggplot(chol, aes(x=Before)) +\n  geom_density()\n\n\n\nggplot(chol, aes(x=After)) +\n  geom_density()\n\n\n\n\n\nggplot(chol, aes(x=DiffChol)) + \n  geom_density()\n\n\n\nggplot(chol, aes(x=DiffChol)) + \n  geom_boxplot()"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#step-2-null-alternative-hypotheses-1",
    "href": "slides_code/Day10_bsta511_code.html#step-2-null-alternative-hypotheses-1",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Step 2: Null & Alternative Hypotheses",
    "text": "Step 2: Null & Alternative Hypotheses\n\nQuestion: Is there evidence to support that cholesterol levels changed after the vegetarian diet?\n\nNull and alternative hypotheses in words Include as much context as possible\n\n\n\\(H_0\\): The population mean difference in cholesterol levels after a vegetarian diet is\n\\(H_A\\): The population mean difference in cholesterol levels after a vegetarian diet is\n\nNull and alternative hypotheses in symbols\n\\[~~~~H_0: \\delta =  \\\\\nH_A: \\delta   \\\\\\]"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#step-3-test-statistic",
    "href": "slides_code/Day10_bsta511_code.html#step-3-test-statistic",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Step 3: Test statistic",
    "text": "Step 3: Test statistic\n\nchol %&gt;% select(DiffChol) %&gt;% get_summary_stats(type = \"common\") %&gt;% gt()\n\n\n\n\n\n  \n    \n      variable\n      n\n      min\n      max\n      median\n      iqr\n      mean\n      sd\n      se\n      ci\n    \n  \n  \n    DiffChol\n24\n-49\n13\n-19\n20.25\n-19.542\n16.806\n3.43\n7.096\n  \n  \n  \n\n\n\n\n\\[\nt_{\\bar{x}_d} = \\frac{\\bar{x}_d - \\delta_0}{\\frac{s_d}{\\sqrt{n}}}\n\\]\n\nBased on the value of the test statistic, do you think we are going to reject or fail to reject \\(H_0\\)?\nWhat probability distribution does the test statistic have?\nAre the assumptions for a paired t-test satisfied so that we can use the probability distribution to calculate the \\(p\\)-value??\n\n\nn &lt;- 24\nalpha &lt;- 0.05\nmu &lt;- 0\n(p_area &lt;- 1-alpha/2)\n\n[1] 0.975\n\n(xbar &lt;- mean(chol$DiffChol))\n\n[1] -19.54167\n\n(sd &lt;- sd(chol$DiffChol))\n\n[1] 16.80574\n\n(se &lt;- sd/sqrt(n))\n\n[1] 3.430458\n\n(tstat &lt;- (xbar - mu)/se)\n\n[1] -5.696519"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#step-4-p-value-1",
    "href": "slides_code/Day10_bsta511_code.html#step-4-p-value-1",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\n# specify upper and lower bounds of shaded region below\nmu &lt;- 0\nstd &lt;- se\n\n# The following figure is only an approximation of the \n# sampling distribution since I used a normal instead\n# of t-distribution to make it.\n\nggplot(data.frame(x = c(mu-6*std, mu+6*std)), aes(x = x)) + \n  stat_function(fun = dnorm, \n                args = list(mean = mu, sd = std)) + \n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks=c(mu, mu - 3.4*(1:6), mu + 3.4*(1:6))) +\n  theme(axis.text.x=element_text(angle = -30, hjust = 0)) +\n  labs(y = \"\", \n       x = \"sample mean difference\",\n       title = \"Sampling distribution of mean difference\") +\n  geom_vline(xintercept = c(-xbar, xbar), \n             color = \"red\")\n\n\n\n\n\nggplot(data = data.frame(x = c(-6, 6)), aes(x)) + \n  stat_function(fun = dt, args = list(df = n-1)) + \n  ylab(\"\") + \n  xlab(\"t-dist with df = 23\") +\n  scale_y_continuous(breaks = NULL) + \n  scale_x_continuous(breaks=c(mu, mu - (1:5), mu + (1:5))) +\n  geom_vline(xintercept = c(-tstat,tstat), \n             color = \"red\")\n\n\n\n\nCalculate the p-value:\n\n(pv &lt;- 2*(pt(tstat, df=n-1)))\n\n[1] 8.434775e-06"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#step-5-conclusion-to-hypothesis-test-1",
    "href": "slides_code/Day10_bsta511_code.html#step-5-conclusion-to-hypothesis-test-1",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\\[~~~~H_0: \\delta = 0 \\\\\nH_A: \\delta \\neq 0  \\\\\\]\n\nRecall the \\(p\\)-value = \\(8.434775 \\cdot 10 ^{-6}\\)\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nStats class conclusion\n\nThere is sufficient evidence that the (population) mean difference in cholesterol levels after a vegetarian diet is different from 0 mg/dL ( \\(p\\)-value &lt; 0.001).\n\nMore realistic manuscript conclusion:\n\nAfter a vegetarian diet, cholesterol levels decreased by on average 19.54 mg/dL (SE = 3.43 mg/dL, 2-sided \\(p\\)-value &lt; 0.001)."
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#r-option-1-run-a-1-sample-t.test-using-the-paired-differences",
    "href": "slides_code/Day10_bsta511_code.html#r-option-1-run-a-1-sample-t.test-using-the-paired-differences",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "R option 1: Run a 1-sample t.test using the paired differences",
    "text": "R option 1: Run a 1-sample t.test using the paired differences\n\\(H_A: \\delta \\neq 0\\)\n\nt.test(x = chol$DiffChol, mu = 0)\n\n\n    One Sample t-test\n\ndata:  chol$DiffChol\nt = -5.6965, df = 23, p-value = 8.435e-06\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -26.63811 -12.44522\nsample estimates:\nmean of x \n-19.54167"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#r-option-2-run-a-2-sample-t.test-with-paired-true-option",
    "href": "slides_code/Day10_bsta511_code.html#r-option-2-run-a-2-sample-t.test-with-paired-true-option",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "R option 2: Run a 2-sample t.test with paired = TRUE option",
    "text": "R option 2: Run a 2-sample t.test with paired = TRUE option\n\\(H_A: \\delta \\neq 0\\)\n\nFor a 2-sample t-test we specify both x= and y=\nNote: mu = 0 is the default value and doesn’t need to be specified\n\n\nt.test(x = chol$Before, y = chol$After, mu = 0, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  chol$Before and chol$After\nt = 5.6965, df = 23, p-value = 8.435e-06\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 12.44522 26.63811\nsample estimates:\nmean difference \n       19.54167"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#r-option-3-run-a-2-sample-t.test-with-paired-true-option-but-using-the-long-data-and-a-formula",
    "href": "slides_code/Day10_bsta511_code.html#r-option-3-run-a-2-sample-t.test-with-paired-true-option-but-using-the-long-data-and-a-formula",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "R option 3: Run a 2-sample t.test with paired = TRUE option, but using the long data and a “formula”",
    "text": "R option 3: Run a 2-sample t.test with paired = TRUE option, but using the long data and a “formula”\n\nUse the usual t.test\nWhat’s different is that\n\ninstead of specifying the variables with x= and y=,\nwe give a formula of the form y ~ x using just the variable names,\nand then specify the name of the dataset using data =\n\nThis method is often used in practice, and more similar to the coding style of running a regression model (BSTA 512 & 513)\n\n\n# using long data with columns Cholesterol & Time:\nt.test(Cholesterol ~ Time, \n       paired = TRUE, \n       data = chol_long)\n\n\n    Paired t-test\n\ndata:  Cholesterol by Time\nt = 5.6965, df = 23, p-value = 8.435e-06\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 12.44522 26.63811\nsample estimates:\nmean difference \n       19.54167"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#tidy-the-t.test-output-compare-the-3-options",
    "href": "slides_code/Day10_bsta511_code.html#tidy-the-t.test-output-compare-the-3-options",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "tidy the t.test output & compare the 3 options",
    "text": "tidy the t.test output & compare the 3 options\n\n# option 1\nt.test(x = chol$DiffChol, mu = 0) %&gt;% tidy() %&gt;% gt() # tidy from broom package\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    -19.54167\n-5.696519\n8.434775e-06\n23\n-26.63811\n-12.44522\nOne Sample t-test\ntwo.sided\n  \n  \n  \n\n\n\n# option 2\nt.test(x = chol$Before, y = chol$After, mu = 0, paired = TRUE) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    19.54167\n5.696519\n8.434775e-06\n23\n12.44522\n26.63811\nPaired t-test\ntwo.sided\n  \n  \n  \n\n\n\n# option 3\nt.test(Cholesterol ~ Time, paired = TRUE, data = chol_long) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    19.54167\n5.696519\n8.434775e-06\n23\n12.44522\n26.63811\nPaired t-test\ntwo.sided"
  },
  {
    "objectID": "slides_code/Day10_bsta511_code.html#r-what-if-we-wanted-to-test-whether-the-diet-decreased-cholesterol-levels",
    "href": "slides_code/Day10_bsta511_code.html#r-what-if-we-wanted-to-test-whether-the-diet-decreased-cholesterol-levels",
    "title": "DRAFT Day 10: Hypothesis testing for one-sample mean (4.3, 5.1) & two-sample paired data mean (5.2)",
    "section": "R: What if we wanted to test whether the diet decreased cholesterol levels?",
    "text": "R: What if we wanted to test whether the diet decreased cholesterol levels?\n\n# alternative = c(\"two.sided\", \"less\", \"greater\")\nt.test(x = chol$DiffChol, mu = 0, alternative = \"less\") %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    -19.54167\n-5.696519\n4.217387e-06\n23\n-Inf\n-13.6623\nOne Sample t-test\nless"
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html",
    "href": "slides_code/Day10_part1_bsta511_code.html",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) # NEW!!\n\nset.seed(456)\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#load-packages",
    "href": "slides_code/Day10_part1_bsta511_code.html#load-packages",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) # NEW!!\n\nset.seed(456)\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#part-1",
    "href": "slides_code/Day10_part1_bsta511_code.html#part-1",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Part 1",
    "text": "Part 1\n\n(4.3, 5.1) Hypothesis testing for mean from one sample\n\nIntroduce hypothesis testing using the case of analyzing a mean from one sample (group)\n\n\n\n\nSteps of a hypothesis test:\n\nlevel of significance\nnull ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\ntest statistic\np-value\nconclusion\n\n\n\n\nRun a hypothesis test in R\n\nLoad a dataset - need to specify location of dataset\nR projects\nRun a t-test in R\ntidy() the test output using broom package\n\n\n\n\n\n\n(4.3.3) Confidence intervals (CIs) vs. hypothesis tests"
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#part-2---class-discussion",
    "href": "slides_code/Day10_part1_bsta511_code.html#part-2---class-discussion",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Part 2 - Class discussion",
    "text": "Part 2 - Class discussion\n\n(5.2) Inference for mean difference from dependent/paired 2 samples\n\nInference: CIs and hypothesis testing\nExploratory data analysis (EDA) to visualize data\nRun paired t-test in R\n\n\n\nOne-sided CIs\n\n\nClass discussion\n\nInference for the mean difference from dependent/paired data is a special case of the inference for the mean from just one sample, that was already covered.\nThus this part will be used for class discussion to practice CIs and hypothesis testing for one mean and apply it in this new setting.\nIn class I will briefly introduce this topic, explain how it is similar and different from what we already covered, and let you work through the slides and code."
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#moritzs-tip-of-the-day-use-r-projects-to-organize-analyses",
    "href": "slides_code/Day10_part1_bsta511_code.html#moritzs-tip-of-the-day-use-r-projects-to-organize-analyses",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "MoRitz’s tip of the day: use R projects to organize analyses",
    "text": "MoRitz’s tip of the day: use R projects to organize analyses\nMoRitz loves using R projects to\n\norganize analyses and\nmake it easier to load data files\nand also save output\n\nOther bonuses include\n\nmaking to it easier to collaborate with others,\nincluding yourself when accessing files from different computers.\n\n\nWe will discuss how to use projects later in today’s slides when loading a dataset.\nSee file Projects in RStudio for more information."
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#question-based-on-the-1992-jama-data-is-there-evidence-to-support-that-the-population-mean-body-temperature-is-different-from-98.6f",
    "href": "slides_code/Day10_part1_bsta511_code.html#question-based-on-the-1992-jama-data-is-there-evidence-to-support-that-the-population-mean-body-temperature-is-different-from-98.6f",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Question: based on the 1992 JAMA data, is there evidence to support that the population mean body temperature is different from 98.6°F?",
    "text": "Question: based on the 1992 JAMA data, is there evidence to support that the population mean body temperature is different from 98.6°F?\nTwo approaches to answer this question:\n\nCreate a confidence interval (CI) for the population mean \\(\\mu\\) and determine whether 98.6°F is inside the CI or not.\n\nis 98.6°F a plausible value?\n\nRun a hypothesis test to see if there is evidence that the population mean \\(\\mu\\) is significantly different from 98.6°F or not.\n\nThis does not give us a range of plausible values for the population mean \\(\\mu\\).\nInstead, we calculate a test statistic and p-value\n\nto see how likely we are to observe the sample mean \\(\\bar{x}\\)\nor a more extreme sample mean\nassuming that the population mean \\(\\mu\\) is 98.6°F.\n\n\n\n\nApproach 1: Create a 95% C I for the population mean body temperature\n\nUse data based on the results from the 1992 JAMA study\n\nThe original dataset used in the JAMA article is not available\nHowever, Allen Shoemaker from Calvin College created a dataset with the same summary statistics as in the JAMA article, which we will use:\n\n\n\\[\\bar{x} = 98.25,~s=0.733,~n=130\\] CI for \\(\\mu\\):\n\\[\\begin{align}\n\\bar{x} &\\pm t^*\\cdot\\frac{s}{\\sqrt{n}}\\\\\n98.25 &\\pm 1.979\\cdot\\frac{0.733}{\\sqrt{130}}\\\\\n98.25 &\\pm 0.127\\\\\n(98.123&, 98.377)\n\\end{align}\\]\nUsed \\(t^*\\) = qt(.975, df=129)\nConclusion:\nWe are 95% confident that the (population) mean body temperature is between 98.123°F and 98.377°F.\n\nHow does the CI compare to 98.6°F?\n\n\n\nApproach 2: Hypothesis Test\nFrom before:\n\nRun a hypothesis test to see if there is evidence that the population mean \\(\\mu\\) is significantly different from 98.6°F or not.\n\nThis does not give us a range of plausible values for the population mean \\(\\mu\\).\nInstead, we calculate a test statistic and p-value\n\nto see how likely we are to observe the sample mean \\(\\bar{x}\\)\nor a more extreme sample mean\nassuming that the population mean \\(\\mu\\) is 98.6°F.\n\n\n\nHow do we calculate a test statistic and p-value?\n\n\nRecall the sampling distribution of the mean\nFrom the Central Limit Theorem (CLT), we know that\n\nFor “large” sample sizes ( \\(n\\geq 30\\) ),\n\nthe sampling distribution of the sample mean\ncan be approximated by a normal distribution,with\n\nmean equal to the population mean value \\(\\mu\\), and\nstandard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\n\n\n\\[\\bar{X}\\sim N\\Big(\\mu_{\\bar{X}} = \\mu, \\sigma_{\\bar{X}}= \\frac{\\sigma}{\\sqrt{n}}\\Big)\\]\n\nFor small sample sizes, if the population is known to be normally distributed, then\n\nthe same result holds\n\n\n\n\nCase 1: suppose we know the population sd \\(\\sigma\\)\n\nHow likely we are to observe the sample mean \\(\\bar{x}\\) ,\n\nor a more extreme sample mean,\nassuming that the population mean \\(\\mu\\) is 98.6°F?\n\nUse \\(\\bar{x} = 98.25\\), \\(\\sigma=0.733\\), and \\(n=130\\)\n\n\n\n\n\n\n\n\nCase 2: we don’t know the population sd \\(\\sigma\\)\n\nThis is usually the case in real life\nWe estimate \\(\\sigma\\) with the sample standard deviation \\(s\\)\nFrom last time, we know that in this case we need to use the t-distribution with d.f. = n-1, instead of the normal distribution\nQuestion: How likely we are to observe the sample mean \\(\\bar{x}\\) or a more extreme sample mean, assuming that the population mean \\(\\mu\\) is 98.6°F?\nUse \\(\\bar{x} = 98.25\\), \\(s=0.733\\), and \\(n=130\\)"
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#step-2-null-alternative-hypotheses",
    "href": "slides_code/Day10_part1_bsta511_code.html#step-2-null-alternative-hypotheses",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 2: Null & Alternative Hypotheses",
    "text": "Step 2: Null & Alternative Hypotheses\nIn statistics, a hypothesis is a statement about the value of an unknown population parameter.\nA hypothesis test consists of a test between two competing hypotheses:\n\na null hypothesis \\(H_0\\) (pronounced “H-naught”) vs. \nan alternative hypothesis \\(H_A\\) (also denoted \\(H_1\\))\n\nExample of hypotheses in words:\n\\[\\begin{aligned}\nH_0 &: \\text{The population mean body temperature is 98.6°F}\\\\\n\\text{vs. } H_A &: \\text{The population mean body temperature is not 98.6°F}\n\\end{aligned}\\]\n\n\\(H_0\\) is a claim that there is “no effect” or “no difference of interest.”\n\\(H_A\\) is the claim a researcher wants to establish or find evidence to support. It is viewed as a “challenger” hypothesis to the null hypothesis \\(H_0\\)\n\n\nNotation for hypotheses\n\\[\\begin{aligned}\nH_0 &: \\mu = \\mu_0\\\\\n\\text{vs. } H_A&: \\mu \\neq, &lt;, \\textrm{or}, &gt; \\mu_0\n\\end{aligned}\\]\nWe call \\(\\mu_0\\) the null value\n\n\n\\(H_A: \\mu \\neq \\mu_0\\)\n\n\nnot choosing a priori whether we believe the population mean is greater or less than the null value \\(\\mu_0\\)\n\n\n\n\\(H_A: \\mu &lt; \\mu_0\\)\n\n\nbelieve the population mean is less than the null value \\(\\mu_0\\)\n\n\n\n\\(H_A: \\mu &gt; \\mu_0\\)\n\n\nbelieve the population mean is greater than the null value \\(\\mu_0\\)\n\n\n\n\n\n\\(H_A: \\mu \\neq \\mu_0\\) is the most common option, since it’s the most conservative\n\nExample:\n\\[\\begin{aligned}\nH_0 &: \\mu = 98.6\\\\\n\\text{vs. } H_A&: \\mu \\neq 98.6\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#step-3-test-statistic-its-distribution",
    "href": "slides_code/Day10_part1_bsta511_code.html#step-3-test-statistic-its-distribution",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 3: Test statistic (& its distribution)",
    "text": "Step 3: Test statistic (& its distribution)\nCase 1: know population sd \\(\\sigma\\)\n\\[\n\\text{test statistic} = z_{\\bar{x}} = \\frac{\\bar{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\n\\]\n\nStatistical theory tells us that \\(z_{\\bar{x}}\\) follows a Standard Normal distribution \\(N(0,1)\\)\n\nCase 2: don’t know population sd \\(\\sigma\\)\n\\[\n\\text{test statistic} = t_{\\bar{x}} = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\n\\]\n\nStatistical theory tells us that \\(t_{\\bar{x}}\\) follows a Student’s t distribution with degrees of freedom (df) = \\(n-1\\)\n\n\\(\\bar{x}\\) = sample mean,\n\\(\\mu_0\\) = hypothesized population mean from \\(H_0\\),\n\\(\\sigma\\) = population standard deviation,\n\\(s\\) = sample standard deviation,\n\\(n\\) = sample size\nAssumptions: same as CLT\n\nIndependent observations\n\nThe observations were collected independently.\n\nApproximately normal sample or big n\n\nThe distribution of the sample should be approximately normal\nor the sample size should be at least 30."
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#step-3-test-statistic-calculation",
    "href": "slides_code/Day10_part1_bsta511_code.html#step-3-test-statistic-calculation",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 3: Test statistic calculation",
    "text": "Step 3: Test statistic calculation\nRecall that \\(\\bar{x} = 98.25\\), \\(s=0.733\\), and \\(n=130.\\)\nThe test statistic is:\n\\[t_{\\bar{x}} = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}\n= \\frac{98.25 - 98.6}{\\frac{0.73}{\\sqrt{130}}}\n= -5.45\\]\n\nStatistical theory tells us that \\(t_{\\bar{x}}\\) follows a Student’s t-distribution with \\(d.f. = n-1 = 129\\).\n\n\n\n\n\n\nAssumptions met?"
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#step-4-p-value",
    "href": "slides_code/Day10_part1_bsta511_code.html#step-4-p-value",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\nThe \\(p\\)-value is a quantification of “surprise”\n\nAssuming \\(H_0\\) is true, how surprised are we with the observed results?\nEx: assuming that the true mean body temperature is 98.6°F, how surprised are we to get a sample mean of 98.25°F (or more extreme)?\n\nIf the \\(p\\)-value is “small,” it means there’s a small probability that we would get the observed statistic (or more extreme) when \\(H_0\\) is true."
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#step-4-p-value-calculation",
    "href": "slides_code/Day10_part1_bsta511_code.html#step-4-p-value-calculation",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 4: p-value calculation",
    "text": "Step 4: p-value calculation\nCalculate the p-value using the Student’s t-distribution with \\(d.f. = n-1 = 129\\):\n\\[p-value=P(T \\leq -5.45) + P(T \\geq 5.45) = 2.410889 \\times 10^{-07}\\]\n\n# use pt() instead of pnorm()\n# need to specify df\n2*pt(-5.4548, df = 130-1, lower.tail = TRUE)\n\n[1] 2.410889e-07"
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#step-4-p-value-estimation-using-t-table",
    "href": "slides_code/Day10_part1_bsta511_code.html#step-4-p-value-estimation-using-t-table",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 4: p-value estimation using \\(t\\)-table",
    "text": "Step 4: p-value estimation using \\(t\\)-table\n\n\\(t\\)-table only gives us bounds on the p-value\nRecall from using the \\(t\\)-table for CIs, that the table gives us the cutoff values for varying tail probabilities (1-tail & 2-tail)\nFind the row with the appropriate degrees of freedom\n\nUse next smallest df in table if actual df not shown\nI.e., for df = 129, use df = 100 in table\n\nFigure out where the test statistic’s absolute value is in relation to the values in the columns, i.e. between which columns is the test statistic?\nThe header rows for those columns gives the lower & upper bounds for the p-value\n\nChoosing one-tail vs. two-tail test, depends on the alternative hypothesis \\(H_A\\).\nFor a 2-sided test ( \\(H_A: \\mu \\neq \\mu_0\\) ), use two-tails\nFor a 1-sided test ( \\(H_A: \\mu &lt; \\textrm{or} &gt; \\mu_0\\) ), use one-tail"
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#step-1-significance-level-alpha",
    "href": "slides_code/Day10_part1_bsta511_code.html#step-1-significance-level-alpha",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 1: Significance Level \\(\\alpha\\)",
    "text": "Step 1: Significance Level \\(\\alpha\\)\n\nBefore doing a hypothesis test, we set a cut-off for how small the \\(p\\)-value should be in order to reject \\(H_0\\).\nWe call this the significance level, denoted by the Greek symbol alpha ( \\(\\alpha\\) )\nTypical \\(\\alpha\\) values are\n\n0.05 - most common by far!!\n0.01 and 0.1\n\nDecision rule:\n\nWhen \\(p\\)-value &lt; \\(\\alpha\\), we “reject the null hypothesis \\(H_0\\).”\nWhen \\(p\\)-value \\(\\geq \\alpha\\), we “fail to reject the null hypothesis \\(H_0\\).”\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\n“Failing to reject” \\(H_0\\) is NOT the same as “accepting” \\(H_0\\)!\nBy failing to reject \\(H_0\\) we are just saying that we don’t have sufficient evidence to support the alternative \\(H_A\\).\nThis does not imply that \\(H_0\\) is true!!"
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#step-5-conclusion-to-hypothesis-test",
    "href": "slides_code/Day10_part1_bsta511_code.html#step-5-conclusion-to-hypothesis-test",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\\[\\begin{aligned}\nH_0 &: \\mu = 98.6\\\\\n\\text{vs. } H_A&: \\mu \\neq 98.6\n\\end{aligned}\\]\n\nRecall the \\(p\\)-value = \\(2.410889 \\times 10^{-07}\\)\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nBasic: (“stats class” conclusion)\n\nThere is sufficient evidence that the (population) mean body temperature is discernibly different from 98.6°F ( \\(p\\)-value &lt; 0.001).\n\nBetter: (“manuscript style” conclusion)\n\nThe average body temperature in the sample was 98.25°F (95% CI 98.12, 98.38°F), which is discernibly different from 98.6°F ( \\(p\\)-value &lt; 0.001)."
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#confidence-intervals-vs.-hypothesis-testing",
    "href": "slides_code/Day10_part1_bsta511_code.html#confidence-intervals-vs.-hypothesis-testing",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Confidence Intervals vs. Hypothesis Testing",
    "text": "Confidence Intervals vs. Hypothesis Testing\n\nSee also V&H Section 4.3.3"
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#working-directory",
    "href": "slides_code/Day10_part1_bsta511_code.html#working-directory",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Working directory",
    "text": "Working directory\n\nIn order to load a dataset from a file, you need to tell R where the dataset is located\nTo do this you also need to know the location from which R is working, i.e. your working directory\nYou can figure out your working directory by running the getwd() function.\n\n\ngetwd()\n\n[1] \"/Users/niederha/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/teaching/BSTA 511/F24/0_webpage/BSTA_511_F24\"\n\n\n\nAbove is the working directory of this slides file\n\nIn this case, this is NOT the location of the actual qmd file though!\n\nTo make it easier to juggle the working directory, the location of your qmd file, and the location of the data,\n\nI highly recommend using R Projects!"
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#r-projects",
    "href": "slides_code/Day10_part1_bsta511_code.html#r-projects",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "R projects",
    "text": "R projects\n\nI highly, highly, HIGHLY recommend using R Projects to organize your analyses and make it easier to load data files and also save output.\nWhen you create an R Project on your computer, the Project is associated with the folder (directory) you created it in.\n\nThis folder becomes the “root” of your working directory, and RStudio’s point of reference from where to load files from and to.\n\nI create separate Projects for every analysis project and every class I teach.\nYou can run multiple sessions of RStudio by opening different Projects, and the environments (or working directory) of each are working independently of each other.\n\n\n\n\n\n\n\nNote\n\n\n\n\nAlthough we are using Quarto files,\n\nI will show how to set up and use a “regular” R Project\ninstead of “Quarto Project”\n\nQuarto Projects include extra features and thus complexity. Once you are used to how regular R Projects work, you can try out a Quarto Project."
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#how-to-create-an-r-project",
    "href": "slides_code/Day10_part1_bsta511_code.html#how-to-create-an-r-project",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "How to create an R Project",
    "text": "How to create an R Project\n\nDemonstration in class recording\nPosit’s (RStudio’s) directions for creating Projects\n\nhttps://support.rstudio.com/hc/en-us/articles/200526207-Using-RStudio-Projects\n\nSee file Projects in RStudio for more information on R Projects."
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#load-the-dataset",
    "href": "slides_code/Day10_part1_bsta511_code.html#load-the-dataset",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "Load the dataset",
    "text": "Load the dataset\n\nThe data are in a csv file called BodyTemperatures.csv\nYou need to tell R where the dataset is located!\nI recommend saving all datasets in a folder called data.\n\nThe code I will be providing you will be set up this way.\n\nTo make it easier to specify where the dataset is located, I recommend using the here() function from the here package: here::here().\n\n\n# read_csv() is a function from the readr package that is a part of the tidyverse\nlibrary(here)   # first install this package\n\nBodyTemps &lt;- read_csv(here::here(\"data\", \"BodyTemperatures.csv\"))\n#                     location: look in \"data\" folder\n#                               for the file \"BodyTemperatures.csv\"\n\nglimpse(BodyTemps)\n\nRows: 130\nColumns: 3\n$ Temperature &lt;dbl&gt; 96.3, 96.7, 96.9, 97.0, 97.1, 97.1, 97.1, 97.2, 97.3, 97.4…\n$ Gender      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ HeartRate   &lt;dbl&gt; 70, 71, 74, 80, 73, 75, 82, 64, 69, 70, 68, 72, 78, 70, 75…"
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#herehere",
    "href": "slides_code/Day10_part1_bsta511_code.html#herehere",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "here::here()",
    "text": "here::here()\nGeneral use of here::here()\nhere::here(\"folder_name\", \"filename\")\nResources for here::here():\n\nhow to use the here package (Jenny Richmond)\nOde to the here package (Jenny Bryan)\n\nProject-oriented workflow (Jenny Bryan)"
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#t.test-base-rs-function-for-testing-one-mean",
    "href": "slides_code/Day10_part1_bsta511_code.html#t.test-base-rs-function-for-testing-one-mean",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "t.test: base R’s function for testing one mean",
    "text": "t.test: base R’s function for testing one mean\n\nUse the body temperature example with \\(H_A: \\mu \\neq 98.6\\)\nWe called the dataset BodyTemps when we loaded it\n\n\nglimpse(BodyTemps)\n\nRows: 130\nColumns: 3\n$ Temperature &lt;dbl&gt; 96.3, 96.7, 96.9, 97.0, 97.1, 97.1, 97.1, 97.2, 97.3, 97.4…\n$ Gender      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ HeartRate   &lt;dbl&gt; 70, 71, 74, 80, 73, 75, 82, 64, 69, 70, 68, 72, 78, 70, 75…\n\n(temps_ttest &lt;- t.test(x = BodyTemps$Temperature,\n       # alternative = \"two.sided\",  # default\n       mu = 98.6))\n\n\n    One Sample t-test\n\ndata:  BodyTemps$Temperature\nt = -5.4548, df = 129, p-value = 2.411e-07\nalternative hypothesis: true mean is not equal to 98.6\n95 percent confidence interval:\n 98.12200 98.37646\nsample estimates:\nmean of x \n 98.24923 \n\n\nNote that the test output also gives the 95% CI using the t-distribution."
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#tidy-the-t.test-output",
    "href": "slides_code/Day10_part1_bsta511_code.html#tidy-the-t.test-output",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "tidy() the t.test output",
    "text": "tidy() the t.test output\n\nUse the tidy() function from the broom package for briefer output in table format that’s stored as a tibble\nCombined with the gt() function from the gt package, we get a nice table\n\n\ntidy(temps_ttest) %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    98.24923\n-5.454823\n2.410632e-07\n129\n98.122\n98.37646\nOne Sample t-test\ntwo.sided\n  \n  \n  \n\n\n\n\n\nSince the tidy() output is a tibble, we can easily pull() specific values from it:\n\n\n\nUsing base R’s $\n\ntidy(temps_ttest)$p.value  \n\n[1] 2.410632e-07\n\n\nAdvantage: quick and easy\n\nOr the tidyverse way: using pull() from dplyr package\n\ntidy(temps_ttest) %&gt;% pull(p.value)\n\n[1] 2.410632e-07\n\n\nAdvantage: can use together with piping (%&gt;%) other functions"
  },
  {
    "objectID": "slides_code/Day10_part1_bsta511_code.html#whats-next",
    "href": "slides_code/Day10_part1_bsta511_code.html#whats-next",
    "title": "Day 10 Part 1: Hypothesis testing for mean from one-sample (Sections 4.3, 5.1)",
    "section": "What’s next?",
    "text": "What’s next?\nCI’s and hypothesis testing for different scenarios:\n\n\n\n\n\n\n\n\n\n\n\nDay\nSection\nPopulation parameter\nSymbol\nPoint estimate\nSymbol\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\n\n11\n5.3\nDiff in pop means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n\n\n12\n8.2\nDiff in pop prop’s\n\\(p_1-p_2\\)\nDiff in sample prop’s\n\\(\\widehat{p}_1-\\widehat{p}_2\\)"
  },
  {
    "objectID": "weeks/week_06.html#datasets",
    "href": "weeks/week_06.html#datasets",
    "title": "Week 6",
    "section": "Datasets",
    "text": "Datasets\nAll datasets can be found on GitHub\n\nBodyTemperatures.csv\nchol213.csv"
  },
  {
    "objectID": "weeks/week_07.html",
    "href": "weeks/week_07.html",
    "title": "Week 7",
    "section": "",
    "text": "Inference for difference in means from two independent sample (2-sample t-test)\nPower and sample size calculations\n\n\n\n\n\nInference for proportions\n\nCIs vs. hypothesis tests\nOne proportion from one sample\nDifference in proportions from two independent samples"
  },
  {
    "objectID": "weeks/week_07.html#overview-of-week-7",
    "href": "weeks/week_07.html#overview-of-week-7",
    "title": "Week 7",
    "section": "",
    "text": "Inference for difference in means from two independent sample (2-sample t-test)\nPower and sample size calculations\n\n\n\n\n\nInference for proportions\n\nCIs vs. hypothesis tests\nOne proportion from one sample\nDifference in proportions from two independent samples"
  },
  {
    "objectID": "weeks/week_07.html#slides-recordings",
    "href": "weeks/week_07.html#slides-recordings",
    "title": "Week 7",
    "section": "Slides & Recordings",
    "text": "Slides & Recordings\n\nPre-recorded lessons are on Echo Cloud (aka echo 360 or echo video).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nTopic\nSlides: html\nSlides: pdf\nSlides: web-page\nSlides with notes\nRecording Link\nDuration\nCode: qmd\nCode: html\n\n\n\n\n11\n2-sample means test: 5.3.2\nDay 11: slides 1-17\n\n\n\nDay 11 Part 1\n36 min\n\n\n\n\n\n2-sample means CI; R; pooled SD: 5.3.1, 5.3.5\nDay 11: slides 18-26\nsame\n\nsame\nDay 11 Part 2\n26 min\nsame\nsame\n\n\n\nPower & sample size: 1 or 2 means: 5.4 & extra\nDay 11: slides 27-45\nsame\n\nsame\nDay 11 Part 3\n57 min\nsame\nsame\n\n\n12\n1 & 2 proportions test & CI: 8.1.1, 8.2\nDay 12: slides 1-23\n *\n\n *\nDay 12 Part 1\n48 min\n\n\n\n\n\nR: 1 & 2 proportions test & CI\nDay 12: slides 24-34\nsame\n\nsame\nDay 12 Part 2\n26 min\nsame\n\n\n\n\nPower for 1- and 2- proportions\nDay 12: slides 35-43\nsame\n\nsame\nDay 12 Part 3\n25 min\nsame\n\n\n\n\n*Note: page numbers in pdf files do not always align with those in the original html slides"
  },
  {
    "objectID": "weeks/week_07.html#datasets",
    "href": "weeks/week_07.html#datasets",
    "title": "Week 7",
    "section": "Datasets",
    "text": "Datasets\nAll datasets can be found on GitHub\n\nDay 11: CaffeineTaps.csv"
  },
  {
    "objectID": "weeks/week_07.html#class-discussion",
    "href": "weeks/week_07.html#class-discussion",
    "title": "Week 7",
    "section": "Class discussion",
    "text": "Class discussion\nDuring class you will be working in groups discussing the following:\n\nDay 11\n\nBook 5.26 a-b\nR1: DDS expenditures by ethnicity (parts a-l)\nPSS1: Book 4.22 - Power & sample size T/F questions\nPSS2: Power & sample size calculations\n\n\n\nDay 12\n\nBook\n\n8.2 (warm-up),\n8.8 (see instructions in HW),\n8.24 (see instructions in HW)"
  },
  {
    "objectID": "weeks/week_07.html#homework",
    "href": "weeks/week_07.html#homework",
    "title": "Week 7",
    "section": "Homework",
    "text": "Homework\n\nHW 5 covers Days 11 -12 and is due on Sat, 11/16\nCheck out this spreadsheet for typos in book."
  },
  {
    "objectID": "slides/Day11_bsta511.html#moritzs-tip-of-the-day",
    "href": "slides/Day11_bsta511.html#moritzs-tip-of-the-day",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "MoRitz’s tip of the day",
    "text": "MoRitz’s tip of the day\nAdd tabbed sections to your html file using tabset.\n\nFirst tabSecond tabRead up on tabsets\n\n\n\nYou can make subsections appear as different tabs in your html file.\nThis is the first tab.\nIt was created by adding ::: panel-tabset right above the subsection ### First tab (see the code file).\nLook up to the right of where it says “First tab,” and you will see a second tab with the creative name “Second tab.”\nIf you are viewing the html output of this file, you can click on the different tabs to see what’s in them.\nTo stop new tabs from being created, close off the original ::: panel-tabset command with ::: at the end.\n\nIn the source code file, you will see the ::: at the end of the ### Read up on tabsets tab.\n\n\n\n\n\nWelcome to the second tab!\n\n\n\n\n\n\n\n\n\nYou can read up more about creating tabs at\n\nhttps://quarto.org/docs/interactive/layout.html#tabset-panel\n\n\n\nIf you are reading the source code file, the next line contains :::, which closes the tabsets."
  },
  {
    "objectID": "slides/Day11_bsta511.html#where-are-we",
    "href": "slides/Day11_bsta511.html#where-are-we",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "slides/Day11_bsta511.html#where-are-we-continuous-outcome-zoomed-in",
    "href": "slides/Day11_bsta511.html#where-are-we-continuous-outcome-zoomed-in",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Where are we? Continuous outcome zoomed in",
    "text": "Where are we? Continuous outcome zoomed in"
  },
  {
    "objectID": "slides/Day11_bsta511.html#where-are-we-1",
    "href": "slides/Day11_bsta511.html#where-are-we-1",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Where are we?",
    "text": "Where are we?\n\nCI’s and hypothesis tests for different scenarios:\n\\[\\text{point estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\\(\\frac{s_d}{\\sqrt{n}}\\)\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n???\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)"
  },
  {
    "objectID": "slides/Day11_bsta511.html#goals-for-today",
    "href": "slides/Day11_bsta511.html#goals-for-today",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Goals for today",
    "text": "Goals for today\n2-sample t-test (Section 5.3)\n\nStatistical inference for difference in means from 2 independent samples\n\nWhat are \\(H_0\\) and \\(H_a\\)?\nWhat is the SE for \\(\\bar{x}_1 - \\bar{x}_2\\)?\nHypothesis test\nConfidence Interval\nRun test in R - using long vs. wide data\nSatterthwaite’s df\nPooled SD\n\n\nPower and sample size (4.3.4, 5.4, plus notes)\n\nCritical values & rejection region\nType I & II errors\nPower\nHow to calculate sample size needed for a study?"
  },
  {
    "objectID": "slides/Day11_bsta511.html#examples-of-designs-with-two-independent-samples",
    "href": "slides/Day11_bsta511.html#examples-of-designs-with-two-independent-samples",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Examples of designs with two independent samples",
    "text": "Examples of designs with two independent samples\n\nAny study where participants are randomized to a control and treatment group\nStudy where create two groups based on whether they were exposed or not to some condition (can be observational)\nBook: “Does treatment using embryonic stem cells (ESCs) help improve heart function following a heart attack?”\nBook: “Is there evidence that newborns from mothers who smoke have a different average birth weight than newborns from mothers who do not smoke?”\nThe key is that the data from the two groups are independent of each other."
  },
  {
    "objectID": "slides/Day11_bsta511.html#steps-in-a-hypothesis-test",
    "href": "slides/Day11_bsta511.html#steps-in-a-hypothesis-test",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Steps in a Hypothesis Test",
    "text": "Steps in a Hypothesis Test\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "slides/Day11_bsta511.html#does-caffeine-increase-finger-tapsmin-on-average",
    "href": "slides/Day11_bsta511.html#does-caffeine-increase-finger-tapsmin-on-average",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Does caffeine increase finger taps/min (on average)?",
    "text": "Does caffeine increase finger taps/min (on average)?\nStudy Design:\n\n\n20 male college students students were trained to tap their fingers at a rapid rate.\nEach then drank 2 cups of coffee (double-blind)\n\nControl group: decaf\nCaffeine group: ~ 200 mg caffeine\n\nAfter 2 hours, students were tested.\nTaps/minute recorded\n\n\n\nHand, David J.; Daly, Fergus; McConway, K.; Lunn, D. and Ostrowski, E. (1993). A handbook of small data sets. London, U.K.: Chapman and Hall.\n\n\n\nLoad the data from the csv file CaffeineTaps.csv\nThe code below is for when the data file is in a folder called data that is in your R project folder (your working directory)\n\n\n\nCaffTaps &lt;- read_csv(here::here(\"data\", \"CaffeineTaps.csv\"))\n\nglimpse(CaffTaps)\n\nRows: 20\nColumns: 2\n$ Taps  &lt;dbl&gt; 246, 248, 250, 252, 248, 250, 246, 248, 245, 250, 242, 245, 244,…\n$ Group &lt;chr&gt; \"Caffeine\", \"Caffeine\", \"Caffeine\", \"Caffeine\", \"Caffeine\", \"Caf…"
  },
  {
    "objectID": "slides/Day11_bsta511.html#eda-explore-the-finger-taps-data",
    "href": "slides/Day11_bsta511.html#eda-explore-the-finger-taps-data",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "EDA: Explore the finger taps data",
    "text": "EDA: Explore the finger taps data\n\n\nDotplot of taps/minute stratified by group\n\nggplot(CaffTaps, aes(x=Taps)) +\n  geom_dotplot() +\n  facet_wrap(vars(Group), ncol=1)\n\n\n\n\n\nSummary statistics stratified by group\n\n# get_summary_stats() from rstatix package\nsumstats &lt;- CaffTaps %&gt;% \n  group_by(Group) %&gt;% \n  get_summary_stats(type = \"mean_sd\") \nsumstats %&gt;% gt()\n\n\n\n\n\n  \n    \n      Group\n      variable\n      n\n      mean\n      sd\n    \n  \n  \n    Caffeine\nTaps\n10\n248.3\n2.214\n    NoCaffeine\nTaps\n10\n244.8\n2.394\n  \n  \n  \n\n\n\ndiff(sumstats$mean)\n\n[1] -3.5"
  },
  {
    "objectID": "slides/Day11_bsta511.html#step-2-null-alternative-hypotheses",
    "href": "slides/Day11_bsta511.html#step-2-null-alternative-hypotheses",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step 2: Null & Alternative Hypotheses",
    "text": "Step 2: Null & Alternative Hypotheses\n\nQuestion: Is there evidence to support that drinking caffeine increases the number of finger taps/min?\n\n\n\nNull and alternative hypotheses in words\nInclude as much context as possible\n\n\n\\(H_0\\): The population difference in mean finger taps/min between the caffeine and control groups is …\n\\(H_A\\): The population difference in mean finger taps/min between the caffeine and control groups is …\n\n\nNull and alternative hypotheses in symbols\n\\[\\begin{align}\nH_0:& \\mu_{caff} - \\mu_{ctrl} = \\\\\nH_A:& \\mu_{caff} - \\mu_{ctrl} \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "slides/Day11_bsta511.html#step-3-test-statistic-part-1",
    "href": "slides/Day11_bsta511.html#step-3-test-statistic-part-1",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step 3: Test statistic (part 1)",
    "text": "Step 3: Test statistic (part 1)\nRecall that in general the test statistic has the form:\n\\[\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\] Thus, for a two sample independent means test, we have:\n\\[\\text{test statistic} = \\frac{\\bar{x}_1 - \\bar{x}_2 - 0}{SE_{\\bar{x}_1 - \\bar{x}_2}}\\]\n\nWhat is the formula for \\(SE_{\\bar{x}_1 - \\bar{x}_2}\\)?\nWhat is the probability distribution of the test statistic?\nWhat assumptions need to be satisfied?"
  },
  {
    "objectID": "slides/Day11_bsta511.html#what-distribution-does-barx_1---barx_2-have",
    "href": "slides/Day11_bsta511.html#what-distribution-does-barx_1---barx_2-have",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "What distribution does \\(\\bar{X}_1 - \\bar{X}_2\\) have?",
    "text": "What distribution does \\(\\bar{X}_1 - \\bar{X}_2\\) have?\n\n\nLet \\(\\bar{X}_1\\) and \\(\\bar{X}_2\\) be the means of random samples from two independent groups, with parameters shown in table:\n\n\n\n\n\nGroup 1\nGroup 2\n\n\n\n\nsample size\n\\(n_1\\)\n\\(n_2\\)\n\n\npop mean\n\\(\\mu_1\\)\n\\(\\mu_2\\)\n\n\npop sd\n\\(\\sigma_1\\)\n\\(\\sigma_2\\)\n\n\n\n\n\n\nSome theoretical statistics:\n\nIf \\(\\bar{X}_1\\) and \\(\\bar{X}_2\\) are independent normal r.v.’s, then \\(\\bar{X}_1 - \\bar{X}_2\\) is also normal\nWhat is the mean of \\(\\bar{X}_1 - \\bar{X}_2\\)?\n\n\\[E[\\bar{X}_1 - \\bar{X}_2] = E[\\bar{X}_1] - E[\\bar{X}_2] = \\mu_1-\\mu_2\\]\n\nWhat is the standard deviation of \\(\\bar{X}_1 - \\bar{X}_2\\)?\n\n\\[\\begin{align}\nVar(\\bar{X}_1 - \\bar{X}_2) &= Var(\\bar{X}_1) + Var(\\bar{X}_2) = \\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2} \\\\\nSD(\\bar{X}_1 - \\bar{X}_2) &= \\sqrt{\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}}\n\\end{align}\\]"
  },
  {
    "objectID": "slides/Day11_bsta511.html#step-3-test-statistic-part-2",
    "href": "slides/Day11_bsta511.html#step-3-test-statistic-part-2",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step 3: Test statistic (part 2)",
    "text": "Step 3: Test statistic (part 2)\n\n\n\n\\[\nt_{\\bar{x}_1 - \\bar{x}_2} = \\frac{\\bar{x}_1 - \\bar{x}_2 - 0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\]\n\n\n\\(\\bar{x}_1, \\bar{x}_2\\) are the sample means\n\\(\\mu_0=0\\) is the mean value specified in \\(H_0\\)\n\\(s_1, s_2\\) are the sample SD’s\n\\(n_1, n_2\\) are the sample sizes\n\n\n\n\nStatistical theory tells us that \\(t_{\\bar{x}_1 - \\bar{x}_2}\\) follows a student’s t-distribution with\n\n\\(df \\approx\\) smaller of \\(n_1-1\\) and \\(n_2-1\\)\nthis is a conservative estimate (smaller than actual \\(df\\) )\n\n\nAssumptions:\n\nIndependent observations & samples\n\nThe observations were collected independently.\nIn particular, the observations from the two groups were not paired in any meaningful way.\n\nApproximately normal samples or big n’s\n\nThe distributions of the samples should be approximately normal\nor both their sample sizes should be at least 30."
  },
  {
    "objectID": "slides/Day11_bsta511.html#step-3-test-statistic-part-3",
    "href": "slides/Day11_bsta511.html#step-3-test-statistic-part-3",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step 3: Test statistic (part 3)",
    "text": "Step 3: Test statistic (part 3)\n\n\n\n\n\n\n\n\n  \n    \n      Group\n      variable\n      n\n      mean\n      sd\n    \n  \n  \n    Caffeine\nTaps\n10\n248.3\n2.214\n    NoCaffeine\nTaps\n10\n244.8\n2.394\n  \n  \n  \n\n\n\n\n\\[\n\\text{test statistic} = t_{\\bar{x}_1 - \\bar{x}_2} = \\frac{\\bar{x}_1 - \\bar{x}_2 - 0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\]\n\n\n\n\n\n\n\nBased on the value of the test statistic, do you think we are going to reject or fail to reject \\(H_0\\)?"
  },
  {
    "objectID": "slides/Day11_bsta511.html#step-3b-assumptions-satisfied",
    "href": "slides/Day11_bsta511.html#step-3b-assumptions-satisfied",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step “3b”: Assumptions satisfied?",
    "text": "Step “3b”: Assumptions satisfied?\n\n\nAssumptions:\n\nIndependent observations & samples\n\nThe observations were collected independently.\nIn particular, the observations from the two groups were not paired in any meaningful way.\n\nApproximately normal samples or big n’s\n\nThe distributions of the samples should be approximately normal\nor both their sample sizes should be at least 30."
  },
  {
    "objectID": "slides/Day11_bsta511.html#step-4-p-value",
    "href": "slides/Day11_bsta511.html#step-4-p-value",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate the p-value:"
  },
  {
    "objectID": "slides/Day11_bsta511.html#step-5-conclusion-to-hypothesis-test",
    "href": "slides/Day11_bsta511.html#step-5-conclusion-to-hypothesis-test",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\\[\\begin{align}\nH_0:& \\mu_{caff} - \\mu_{ctrl} = 0\\\\\nH_A:& \\mu_{caff} - \\mu_{ctrl} &gt; 0\\\\\n\\end{align}\\]\n\nRecall the \\(p\\)-value = 0.00397\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nStats class conclusion\n\nThere is sufficient evidence that the (population) difference in mean finger taps/min with vs. without caffeine is greater than 0 ( \\(p\\)-value = 0.004).\n\nMore realistic manuscript conclusion:\n\nThe mean finger taps/min were 244.8 (SD = 2.4) and 248.3 (SD = 2.2) for the control and caffeine groups, and the increase of 3.5 taps/min was statistically discrenible ( \\(p\\)-value = 0.004)."
  },
  {
    "objectID": "slides/Day11_bsta511.html#ci-for-the-mean-difference-in-cholesterol-levels",
    "href": "slides/Day11_bsta511.html#ci-for-the-mean-difference-in-cholesterol-levels",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "95% CI for the mean difference in cholesterol levels",
    "text": "95% CI for the mean difference in cholesterol levels\n\n\n\n\n\n\n  \n    \n      Group\n      variable\n      n\n      mean\n      sd\n    \n  \n  \n    Caffeine\nTaps\n10\n248.3\n2.214\n    NoCaffeine\nTaps\n10\n244.8\n2.394\n  \n  \n  \n\n\n\n\n\n\nCI for \\(\\mu_{caff} - \\mu_{ctrl}\\):\n\\[\\bar{x}_{caff} - \\bar{x}_{ctrl} \\pm t^* \\cdot \\sqrt{\\frac{s_{caff}^2}{n_{caff}}+\\frac{s_{ctrl}^2}{n_{ctrl}}}\\]\n\n\n\n\n  \n\nInterpretation:\nWe are 95% confident that the (population) difference in mean finger taps/min between the caffeine and control groups is between 1.167 mg/dL and 5.833 mg/dL.\n\nBased on the CI, is there evidence that drinking caffeine made a difference in finger taps/min? Why or why not?"
  },
  {
    "objectID": "slides/Day11_bsta511.html#r-2-sample-t-test-with-long-data",
    "href": "slides/Day11_bsta511.html#r-2-sample-t-test-with-long-data",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "R: 2-sample t-test (with long data)",
    "text": "R: 2-sample t-test (with long data)\n\nThe CaffTaps data are in a long format, meaning that\n\nall of the outcome values are in one column and\nanother column indicates which group the values are from\n\nThis is a common format for data from multiple samples, especially if the sample sizes are different.\n\n\n\n(Taps_2ttest &lt;- t.test(formula = Taps ~ Group, \n                       alternative = \"greater\", \n                       data = CaffTaps))\n\n\n    Welch Two Sample t-test\n\ndata:  Taps by Group\nt = 3.3942, df = 17.89, p-value = 0.001628\nalternative hypothesis: true difference in means between group Caffeine and group NoCaffeine is greater than 0\n95 percent confidence interval:\n 1.711272      Inf\nsample estimates:\n  mean in group Caffeine mean in group NoCaffeine \n                   248.3                    244.8"
  },
  {
    "objectID": "slides/Day11_bsta511.html#tidy-the-t.test-output",
    "href": "slides/Day11_bsta511.html#tidy-the-t.test-output",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "tidy the t.test output",
    "text": "tidy the t.test output\n\n# use tidy command from broom package for briefer output that's a tibble\ntidy(Taps_2ttest) %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    3.5\n248.3\n244.8\n3.394168\n0.001627703\n17.89012\n1.711272\nInf\nWelch Two Sample t-test\ngreater\n  \n  \n  \n\n\n\n\n\nPull the p-value:\n\n\ntidy(Taps_2ttest)$p.value  # we can pull specific values from the tidy output\n\n[1] 0.001627703"
  },
  {
    "objectID": "slides/Day11_bsta511.html#r-2-sample-t-test-with-wide-data",
    "href": "slides/Day11_bsta511.html#r-2-sample-t-test-with-wide-data",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "R: 2-sample t-test (with wide data)",
    "text": "R: 2-sample t-test (with wide data)\n\n\n# make CaffTaps data wide: pivot_wider needs an ID column so that it \n# knows how to \"match\" values from the Caffeine and NoCaffeine groups\nCaffTaps_wide &lt;- CaffTaps %&gt;% \n  mutate(id = rep(1:10, 2)) %&gt;% #  \"fake\" IDs for pivot_wider step\n  pivot_wider(names_from = \"Group\",\n              values_from = \"Taps\")\n\nglimpse(CaffTaps_wide)\n\nRows: 10\nColumns: 3\n$ id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n$ Caffeine   &lt;dbl&gt; 246, 248, 250, 252, 248, 250, 246, 248, 245, 250\n$ NoCaffeine &lt;dbl&gt; 242, 245, 244, 248, 247, 248, 242, 244, 246, 242\n\nt.test(x = CaffTaps_wide$Caffeine, y = CaffTaps_wide$NoCaffeine, alternative = \"greater\") %&gt;% \n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    3.5\n248.3\n244.8\n3.394168\n0.001627703\n17.89012\n1.711272\nInf\nWelch Two Sample t-test\ngreater"
  },
  {
    "objectID": "slides/Day11_bsta511.html#why-are-the-dfs-in-the-r-output-different",
    "href": "slides/Day11_bsta511.html#why-are-the-dfs-in-the-r-output-different",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Why are the df’s in the R output different?",
    "text": "Why are the df’s in the R output different?\nFrom many slides ago:\n\nStatistical theory tells us that \\(t_{\\bar{x}_1 - \\bar{x}_2}\\) follows a student’s t-distribution with\n\n\\(df \\approx\\) smaller of \\(n_1-1\\) and \\(n_2-1\\)\nthis is a conservative estimate (smaller than actual \\(df\\) )\n\n\nThe actual degrees of freedom are calculated using Satterthwaite’s method:\n\\[\\nu = \\frac{[ (s_1^2/n_1) + (s_2^2/n_2) ]^2}\n{(s_1^2/n_1)^2/(n_1 - 1) + (s_2^2/n_2)^2/(n_2-1) }\n= \\frac{ [ SE_1^2 + SE_2^2 ]^2}{ SE_1^4/df_1 + SE_2^4/df_2 }\\]\n\nVerify the p-value in the R output using \\(\\nu\\) = 17.89012:\n\npt(3.3942, df = 17.89012, lower.tail = FALSE)\n\n[1] 0.001627588"
  },
  {
    "objectID": "slides/Day11_bsta511.html#pooled-standard-deviation-estimate",
    "href": "slides/Day11_bsta511.html#pooled-standard-deviation-estimate",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Pooled standard deviation estimate",
    "text": "Pooled standard deviation estimate\n\nSometimes we have reasons to believe that the population SD’s from the two groups are equal, such as when randomizing participants to two groups\n\n\n\n\nIn this case we can use a pooled SD:\n\n\\[s_{pooled}^2 = \\frac{s_1^2 (n_1-1) + s_2^2 (n_2-1)}{n_1 + n_2 - 2}\\]\n\n\n\\(n_1\\), \\(n_2\\) are the sample sizes, and\n\\(s_1\\), \\(s_2\\) are the sample standard deviations\nof the two groups\n\n\n\n\nWe use the pooled SD instead of \\(s_1^2\\) and \\(s_2^2\\) when calculating the standard error\n\n\\[SE = \\sqrt{\\frac{s_{pooled}^2}{n_1} + \\frac{s_{pooled}^2}{n_2}}= s_{pooled}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\]\n\n\nTest statistic with pooled SD:\n\\[t_{\\bar{x}_1 - \\bar{x}_2} = \\frac{\\bar{x}_1 - \\bar{x}_2 -0}{s_{pooled}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]\n\nCI with pooled SD:\n\\[(\\bar{x}_1 - \\bar{x}_2) \\pm t^{\\star} \\cdot s_{pooled} \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\]\n\n\n\nThe \\(t\\) distribution degrees of freedom are now:\n\n\\[df = (n_1 - 1) + (n_2 - 1) = n_1 + n_2 - 2.\\]"
  },
  {
    "objectID": "slides/Day11_bsta511.html#r-2-sample-t-test-with-pooled-sd",
    "href": "slides/Day11_bsta511.html#r-2-sample-t-test-with-pooled-sd",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "R: 2-sample t-test with pooled SD",
    "text": "R: 2-sample t-test with pooled SD\n\n# t-test with pooled SD\nt.test(formula = Taps ~ Group, \n       alternative = \"greater\", \n       var.equal = TRUE,  # pooled SD \n       data = CaffTaps) %&gt;% \n  tidy() %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      estimate\n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    3.5\n248.3\n244.8\n3.394168\n0.001616497\n18\n1.711867\nInf\nTwo Sample t-test\ngreater\n  \n  \n  \n\n\n\n# t-test without pooled SD\nt.test(formula = Taps ~ Group, \n       alternative = \"greater\", \n       var.equal = FALSE,  # default, NOT pooled SD \n       data = CaffTaps) %&gt;% \n  tidy() %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      estimate\n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    3.5\n248.3\n244.8\n3.394168\n0.001627703\n17.89012\n1.711272\nInf\nWelch Two Sample t-test\ngreater\n  \n  \n  \n\n\n\n\nSimilar output in this case - why??"
  },
  {
    "objectID": "slides/Day11_bsta511.html#whats-next",
    "href": "slides/Day11_bsta511.html#whats-next",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "What’s next?",
    "text": "What’s next?\n\nCI’s and hypothesis tests for different scenarios:\n\\[\\text{point estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\\(\\frac{s_d}{\\sqrt{n}}\\)\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\\(\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\) or pooled\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n???\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)\n???"
  },
  {
    "objectID": "slides/Day11_bsta511.html#critical-values",
    "href": "slides/Day11_bsta511.html#critical-values",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Critical values",
    "text": "Critical values\n\n\nCritical values are the cutoff values that determine whether a test statistic is statistically significant or not.\nIf a test statistic is greater in absolute value than the critical value, we reject \\(H_0\\)\n\n\n\n\n\n\n\n\n\n\n\n\nCritical values are determined by\n\nthe significance level \\(\\alpha\\),\nwhether a test is 1- or 2-sided, &\nthe probability distribution being used to calculate the p-value (such as normal or t-distribution).\n\nThe critical values in the figure should look very familiar!\n\nWhere have we used these before?\n\n\n\n\n\n\nHow can we calculate the critical values using R?"
  },
  {
    "objectID": "slides/Day11_bsta511.html#rejection-region",
    "href": "slides/Day11_bsta511.html#rejection-region",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Rejection region",
    "text": "Rejection region\n\nIf the absolute value of the test statistic is greater than the critical value, we reject \\(H_0\\)\n\nIn this case the test statistic is in the rejection region.\nOtherwise it’s in the nonrejection region.\n\n\n\n\n\n\n\nStats & Geospatial Analysis\n\n\n\n\nWhat do rejection regions look like for 1-sided tests?"
  },
  {
    "objectID": "slides/Day11_bsta511.html#justice-system-analogy",
    "href": "slides/Day11_bsta511.html#justice-system-analogy",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Justice system analogy",
    "text": "Justice system analogy\n\n\nType I and Type II Errors - Making Mistakes in the Justice System"
  },
  {
    "objectID": "slides/Day11_bsta511.html#type-i-ii-errors",
    "href": "slides/Day11_bsta511.html#type-i-ii-errors",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Type I & II Errors",
    "text": "Type I & II Errors\n\n\n\n\n\n\n\n\n\n\\(\\alpha\\) = probability of making a Type I error\n\nThis is the significance level (usually 0.05)\nSet before study starts\n\n\\(\\beta\\) = probability of making a Type II error\nIdeally we want\n\nsmall Type I & II errors and\nbig power\n\n\n\n\n\n\n\n\n\n\nApplet for visualizing Type I & II errors and power: https://rpsychologist.com/d3/NHST/"
  },
  {
    "objectID": "slides/Day11_bsta511.html#relationship-between-type-i-ii-errors",
    "href": "slides/Day11_bsta511.html#relationship-between-type-i-ii-errors",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Relationship between Type I & II errors",
    "text": "Relationship between Type I & II errors\n\nType I vs. Type II error\n\nDecreasing P(Type I error) leads to\n\nincreasing P(Type II error)\n\nWe typically keep P(Type I error) = \\(\\alpha\\) set to 0.05\n\n\nFrom the applet at https://rpsychologist.com/d3/NHST/"
  },
  {
    "objectID": "slides/Day11_bsta511.html#relationship-between-type-ii-errors-and-power",
    "href": "slides/Day11_bsta511.html#relationship-between-type-ii-errors-and-power",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Relationship between Type II errors and power",
    "text": "Relationship between Type II errors and power\n\nPower = P(correctly rejecting the null hypothesis)\n\n\n\n\n\nPower is also called the\n\ntrue positive rate,\nprobability of detection, or\nthe sensitivity of a test\n\n\n\n\n\n\n\n\n\n\n\nPower vs. Type II error\n\nPower = 1 - P(Type II error) = 1 - \\(\\beta\\)\nThus as \\(\\beta\\) = P(Type II error) decreases, the power increases\nP(Type II error) decreases as the mean of the alternative population shifts further away from the mean of the null population (effect size gets bigger).\nTypically want at least 80% power; 90% power is good"
  },
  {
    "objectID": "slides/Day11_bsta511.html#example-calculating-power",
    "href": "slides/Day11_bsta511.html#example-calculating-power",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Example calculating power",
    "text": "Example calculating power\n\n\nSuppose the mean of the null population is 0 ( \\(H_0: \\mu=0\\) ) with standard error 1\nFind the power of a 2-sided test if the actual \\(\\mu=3\\), assuming the SE doesn’t change.\n\n\n\n\n\n\n\n\n\n\nPower = \\(P(\\)Reject \\(H_0\\) when alternative pop is \\(N(3,1))\\)\nWhen \\(\\alpha\\) = 0.05, we reject \\(H_0\\) when the test statistic z is at least 1.96\nThus for \\(X\\sim N(3,1)\\) we need to calculate \\(P(X \\le -1.96) + P(X \\ge 1.96)\\):\n\n\n\n\n# left tail + right tail:\npnorm(-1.96, mean=3, sd=1, lower.tail=TRUE) + pnorm(1.96, mean=3, sd=1, lower.tail=FALSE)\n\n[1] 0.8508304\n\n\n\n\nThe left tail probability pnorm(-1.96, mean=3, sd=1, lower.tail=TRUE) is essentially 0 in this case.\n\n\n\nNote that this power calculation specified the value of the SE instead of the standard deviation and sample size \\(n\\) individually."
  },
  {
    "objectID": "slides/Day11_bsta511.html#sample-size-calculation-for-testing-one-mean",
    "href": "slides/Day11_bsta511.html#sample-size-calculation-for-testing-one-mean",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Sample size calculation for testing one mean",
    "text": "Sample size calculation for testing one mean\n\n\nRecall in our body temperature example that \\(\\mu_0=98.6\\) °F and \\(\\bar{x}= 98.25\\) °F.\n\nThe p-value from the hypothesis test was highly significant (very small).\nWhat would the sample size \\(n\\) need to be for 80% power?\n\nCalculate \\(n\\),\n\ngiven \\(\\alpha\\), power ( \\(1-\\beta\\) ), “true” alternative mean \\(\\mu\\), and null \\(\\mu_0\\),\nassuming the test statistic is normal (instead of t-distribution):\n\n\n\n\n\\[n=\\left(s\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{\\mu-\\mu_0}\\right)^2\\]\n\n\nmu &lt;- 98.25\nmu0 &lt;- 98.6\nsd &lt;- 0.73\nalpha &lt;- 0.05\nbeta &lt;- 0.20\nn &lt;- (sd*(qnorm(1-alpha/2) + qnorm(1-beta)) / (mu-mu0))^2\nn\n\n[1] 34.14423\n\nceiling(n)  # always round UP to the next highest integer \n\n[1] 35\n\n\n\n\nWe would only need a sample size of 35 for 80% power!\nHowever, this is an under-estimate since we used the normal instead of t-distribution.\n\n\nSee http://powerandsamplesize.com/Calculators/Test-1-Mean/1-Sample-Equality."
  },
  {
    "objectID": "slides/Day11_bsta511.html#power-calculation-for-testing-one-mean",
    "href": "slides/Day11_bsta511.html#power-calculation-for-testing-one-mean",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Power calculation for testing one mean",
    "text": "Power calculation for testing one mean\n\nConversely, we can calculate how much power we had in our body temperature one-sample test, given the sample size of 130.\n\nCalculate power,\n\ngiven \\(\\alpha\\), \\(n\\), “true” alternative mean \\(\\mu\\), and null \\(\\mu_0\\),\nassuming the test statistic is normal (instead of t-distribution)\n\n\n\\[1-\\beta=\n        \\Phi\\left(z-z_{1-\\alpha/2}\\right)+\\Phi\\left(-z-z_{1-\\alpha/2}\\right)\n        \\quad ,\\quad \\text{where } z=\\frac{\\mu-\\mu_0}{s/\\sqrt{n}}\\]\n\\(\\Phi\\) is the probability for a standard normal distribution\n\nmu &lt;- 98.25; mu0 &lt;- 98.6; sd &lt;- 0.73; alpha &lt;- 0.05; n &lt;- 130\n(z &lt;- (mu-mu0) / (sd/sqrt(n)) )\n\n[1] -5.466595\n\n(Power &lt;- pnorm(z-qnorm(1-alpha/2)) + pnorm(-z-qnorm(1-alpha/2)))\n\n[1] 0.9997731\n\n\nIf the population mean is 98.2 instead of 98.6, we have a 99.98% chance of correctly rejecting \\(H_0\\) when the sample size is 130."
  },
  {
    "objectID": "slides/Day11_bsta511.html#r-package-pwr-for-power-analyses",
    "href": "slides/Day11_bsta511.html#r-package-pwr-for-power-analyses",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "R package pwr for power analyses",
    "text": "R package pwr for power analyses\n\n\nUse pwr.t.test for both one- and two-sample t-tests.\n\nSpecify all parameters except for the one being solved for.\n\npwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL,\ntype = c(\"two.sample\", \"one.sample\", \"paired\"),\nalternative = c(\"two.sided\", \"less\", \"greater\"))\nd is Cohen’s d effect size: small = 0.2, medium = 0.5, large = 0.8\n\n\nOne-sample test (or paired t-test):\n\n\n\n\\[d = \\frac{\\mu-\\mu_0}{s}\\]\n\n\n\n\nTwo-sample test (independent):\n\n\n\n\\[d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{pooled}}\\]\n\n\n\n\n\n\\(\\bar{x}_1 - \\bar{x}_2\\) is the difference in means between the two groups that one would want to be able to detect as being significant,\n\\(s_{pooled}\\) is the pooled SD between the two groups - often assume have same sd in each group\nR package pwr for basic statistical tests\n\nhttps://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html"
  },
  {
    "objectID": "slides/Day11_bsta511.html#pwr-sample-size-for-one-mean-test",
    "href": "slides/Day11_bsta511.html#pwr-sample-size-for-one-mean-test",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "pwr: sample size for one mean test",
    "text": "pwr: sample size for one mean test\n\npwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL,\ntype = c(\"two.sample\", \"one.sample\", \"paired\"), alternative = c(\"two.sided\", \"less\", \"greater\"))\n\n\nd is Cohen’s d effect size: \\(d = \\frac{\\mu-\\mu_0}{s}\\)\n\nSpecify all parameters except for the sample size:\n\n\n\nlibrary(pwr)\nt.n &lt;- pwr.t.test(\n  d = (98.6-98.25)/0.73, \n  sig.level = 0.05, \n  power = 0.80, \n  type = \"one.sample\")\n\nt.n\n\n\n     One-sample t test power calculation \n\n              n = 36.11196\n              d = 0.4794521\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n\nplot(t.n)"
  },
  {
    "objectID": "slides/Day11_bsta511.html#pwr-power-for-one-mean-test",
    "href": "slides/Day11_bsta511.html#pwr-power-for-one-mean-test",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "pwr: power for one mean test",
    "text": "pwr: power for one mean test\n\npwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL,\ntype = c(\"two.sample\", \"one.sample\", \"paired\"), alternative = c(\"two.sided\", \"less\", \"greater\"))\n\n\nd is Cohen’s d effect size: \\(d = \\frac{\\mu-\\mu_0}{s}\\)\n\nSpecify all parameters except for the power:\n\n\n\nt.power &lt;- pwr.t.test(\n  d = (98.6-98.25)/0.73, \n  sig.level = 0.05, \n  # power = 0.80, \n  n = 130,\n  type = \"one.sample\")\n\nt.power\n\n\n     One-sample t test power calculation \n\n              n = 130\n              d = 0.4794521\n      sig.level = 0.05\n          power = 0.9997354\n    alternative = two.sided\n\n\n\n\nplot(t.power)"
  },
  {
    "objectID": "slides/Day11_bsta511.html#pwr-two-sample-t-test-sample-size",
    "href": "slides/Day11_bsta511.html#pwr-two-sample-t-test-sample-size",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "pwr: Two-sample t-test: sample size",
    "text": "pwr: Two-sample t-test: sample size\n\npwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL,\ntype = c(\"two.sample\", \"one.sample\", \"paired\"), alternative = c(\"two.sided\", \"less\", \"greater\"))\n\n\n\nd is Cohen’s d effect size: \\(d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{pooled}}\\)\n\nExample: Suppose the data collected for the caffeine taps study were pilot day for a larger study. Investigators want to know what sample size they would need to detect a 2 point difference between the two groups. Assume the SD in both groups is 2.3.\nSpecify all parameters except for the sample size:\n\n\n\n\n\nt2.n &lt;- pwr.t.test(\n  d = 2/2.3, \n  sig.level = 0.05, \n  power = 0.80, \n  type = \"two.sample\") \n\nt2.n\n\n\n     Two-sample t test power calculation \n\n              n = 21.76365\n              d = 0.8695652\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nplot(t2.n)"
  },
  {
    "objectID": "slides/Day11_bsta511.html#pwr-two-sample-t-test-power",
    "href": "slides/Day11_bsta511.html#pwr-two-sample-t-test-power",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "pwr: Two-sample t-test: power",
    "text": "pwr: Two-sample t-test: power\n\npwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL,\ntype = c(\"two.sample\", \"one.sample\", \"paired\"), alternative = c(\"two.sided\", \"less\", \"greater\"))\n\n\n\nd is Cohen’s d effect size: \\(d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{pooled}}\\)\n\nExample: Suppose the data collected for the caffeine taps study were pilot day for a larger study. Investigators want to know what sample size they would need to detect a 2 point difference between the two groups. Assume the SD in both groups is 2.3.\nSpecify all parameters except for the power:\n\n\n\n\n\nt2.power &lt;- pwr.t.test(\n  d = 2/2.3, \n  sig.level = 0.05, \n  # power = 0.80, \n  n = 22,\n  type = \"two.sample\") \n\nt2.power\n\n\n     Two-sample t test power calculation \n\n              n = 22\n              d = 0.8695652\n      sig.level = 0.05\n          power = 0.8044288\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nplot(t2.power)"
  },
  {
    "objectID": "slides/Day11_bsta511.html#what-information-do-we-need-for-a-power-or-sample-size-calculation",
    "href": "slides/Day11_bsta511.html#what-information-do-we-need-for-a-power-or-sample-size-calculation",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "What information do we need for a power (or sample size) calculation?",
    "text": "What information do we need for a power (or sample size) calculation?\n\n\nThere are 4 pieces of information:\n\nLevel of significance \\(\\alpha\\)\n\nUsually fixed to 0.05\n\nPower\n\nIdeally at least 0.80\n\nSample size\nEffect size (expected change)\n\nGiven any 3 pieces of information, we can solve for the 4th.\n\n\npwr.t.test(\n  d = (98.6-98.25)/0.73,\n  sig.level = 0.05, \n  # power = 0.80, \n  n=130,\n  type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 130\n              d = 0.4794521\n      sig.level = 0.05\n          power = 0.9997354\n    alternative = two.sided"
  },
  {
    "objectID": "slides/Day11_bsta511.html#more-software-for-power-and-sample-size-calculations-pass",
    "href": "slides/Day11_bsta511.html#more-software-for-power-and-sample-size-calculations-pass",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "More software for power and sample size calculations: PASS",
    "text": "More software for power and sample size calculations: PASS\n\nPASS is a very powerful (& expensive) software that does power and sample size calculations for many advanced statistical modeling techniques.\n\nEven if you don’t have access to PASS, their documentation is very good and free online.\nDocumentation includes formulas and references.\nPASS documentation for powering means\n\nOne mean, paired means, two independent means\n\n\nOne-sample t-test documentation: https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/PASS/One-Sample_T-Tests.pdf"
  },
  {
    "objectID": "slides/Day11_bsta511.html#octri-berd-power-sample-size-presentations",
    "href": "slides/Day11_bsta511.html#octri-berd-power-sample-size-presentations",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "OCTRI-BERD power & sample size presentations",
    "text": "OCTRI-BERD power & sample size presentations\n\n\nPower and Sample Size 101\n\nPresented by Meike Niederhausen; April 13, 2023\nSlides: http://bit.ly/PSS101-BERD-April2023\nRecording\n\nPower and Sample Size for Clinical Trials: An Introduction\n\nPresented by Yiyi Chen; Feb 18, 2021\nSlides: http://bit.ly/PSS-ClinicalTrials\nRecording\n\nPlanning a Study with Power and Sample Size Considerations in Mind\n\nPresented by David Yanez; May 29, 2019\nSlides\nRecording\n\nPower and Sample Size Simulations in R\n\nPresented by Robin Baudier; Sept 21, 2023\nSlides\nRecording"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html",
    "href": "slides_code/Day11_bsta511_code.html",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) # new-ish\nlibrary(here) # new-ish\nlibrary(pwr) # NEW!!\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#load-packages",
    "href": "slides_code/Day11_bsta511_code.html#load-packages",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) # new-ish\nlibrary(here) # new-ish\nlibrary(pwr) # NEW!!\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#moritzs-tip-of-the-day",
    "href": "slides_code/Day11_bsta511_code.html#moritzs-tip-of-the-day",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "MoRitz’s tip of the day",
    "text": "MoRitz’s tip of the day\nAdd tabbed sections to your html file using tabset.\n\nFirst tabSecond tabRead up on tabsets\n\n\n\nYou can make subsections appear as different tabs in your html file.\nThis is the first tab.\nIt was created by adding ::: panel-tabset right above the subsection ### First tab (see the code file).\nLook up to the right of where it says “First tab,” and you will see a second tab with the creative name “Second tab.”\nIf you are viewing the html output of this file, you can click on the different tabs to see what’s in them.\nTo stop new tabs from being created, close off the original ::: panel-tabset command with ::: at the end.\n\nIn the code file, you will see the ::: at the end of the ### Read up on tabsets tab.\n\n\n\n\n\nWelcome to the second tab!\n\n\n\n\n\n\n\n\n\nYou can read up more about creating tabs at\n\nhttps://quarto.org/docs/interactive/layout.html#tabset-panel\n\n\n\nIf you are reading the source code file, the next line contains :::, which closes the tabsets."
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#goals-for-today-section-5.3",
    "href": "slides_code/Day11_bsta511_code.html#goals-for-today-section-5.3",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Goals for today (Section 5.3)",
    "text": "Goals for today (Section 5.3)\n\nStatistical inference for difference in means from 2 independent samples\n\nWhat are \\(H_0\\) and \\(H_a\\)?\nWhat is the SE for \\(\\bar{x}_1 - \\bar{x}_2\\)?\nHypothesis test\nConfidence Interval\nRun test in R - using long vs. wide data\nSatterthwaite’s df\nPooled SD"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#examples-of-designs-with-two-independent-samples",
    "href": "slides_code/Day11_bsta511_code.html#examples-of-designs-with-two-independent-samples",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Examples of designs with two independent samples",
    "text": "Examples of designs with two independent samples\n\nAny study where participants are randomized to a control and treatment group\nStudy where create two groups based on whether they were exposed or not to some condition (can be observational)\nBook: “Does treatment using embryonic stem cells (ESCs) help improve heart function following a heart attack?”\nBook: “Is there evidence that newborns from mothers who smoke have a different average birth weight than newborns from mothers who do not smoke?”\nThe key is that the data from the two groups are independent of each other."
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#steps-in-a-hypothesis-test",
    "href": "slides_code/Day11_bsta511_code.html#steps-in-a-hypothesis-test",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Steps in a Hypothesis Test",
    "text": "Steps in a Hypothesis Test\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#eda-explore-the-finger-taps-data",
    "href": "slides_code/Day11_bsta511_code.html#eda-explore-the-finger-taps-data",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "EDA: Explore the finger taps data",
    "text": "EDA: Explore the finger taps data\nDotplot of taps/minute stratified by group\n\nggplot(CaffTaps, aes(x=Taps)) +\n  geom_dotplot() +\n  facet_wrap(vars(Group), ncol=1)\n\n\n\n\nSummary statistics stratified by group\n\n# get_summary_stats() from rstatix package\nsumstats &lt;- CaffTaps %&gt;% \n  group_by(Group) %&gt;% \n  get_summary_stats(type = \"mean_sd\") \nsumstats %&gt;% gt()\n\n\n\n\n\n  \n    \n      Group\n      variable\n      n\n      mean\n      sd\n    \n  \n  \n    Caffeine\nTaps\n10\n248.3\n2.214\n    NoCaffeine\nTaps\n10\n244.8\n2.394\n  \n  \n  \n\n\n\ndiff(sumstats$mean)\n\n[1] -3.5"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#what-distribution-does-barx_1---barx_2-have",
    "href": "slides_code/Day11_bsta511_code.html#what-distribution-does-barx_1---barx_2-have",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "What distribution does \\(\\bar{X}_1 - \\bar{X}_2\\) have?",
    "text": "What distribution does \\(\\bar{X}_1 - \\bar{X}_2\\) have?\nLet \\(\\bar{X}_1\\) and \\(\\bar{X}_2\\) be the means of random samples from two independent groups, with parameters:\n\n\n\n\nGroup 1\nGroup 2\n\n\n\n\nsample size\n\\(n_1\\)\n\\(n_2\\)\n\n\npop mean\n\\(\\mu_1\\)\n\\(\\mu_2\\)\n\n\npop sd\n\\(\\sigma_1\\)\n\\(\\sigma_2\\)\n\n\n\nSome theoretical statistics:\n\nIf \\(\\bar{X}_1\\) and \\(\\bar{X}_2\\) are independent normal r.v.’s, then \\(\\bar{X}_1 - \\bar{X}_2\\) is also normal\nWhat is the mean of \\(\\bar{X}_1 - \\bar{X}_2\\)?\n\n\\[E[\\bar{X}_1 - \\bar{X}_2] = E[\\bar{X}_1] - E[\\bar{X}_2] = \\mu_1-\\mu_2\\]\n\nWhat is the standard deviation of \\(\\bar{X}_1 - \\bar{X}_2\\)?\n\n\\[Var(\\bar{X}_1 - \\bar{X}_2) = Var(\\bar{X}_1) + Var(\\bar{X}_2) = \\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2} \\\\\nSD(\\bar{X}_1 - \\bar{X}_2) = \\sqrt{\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}}\\]"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#step-3-test-statistic-part-2",
    "href": "slides_code/Day11_bsta511_code.html#step-3-test-statistic-part-2",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step 3: Test statistic (part 2)",
    "text": "Step 3: Test statistic (part 2)\n\\[\nt_{\\bar{x}_1 - \\bar{x}_2} = \\frac{\\bar{x}_1 - \\bar{x}_2 - 0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\]\n\n\\(\\bar{x}_1, \\bar{x}_2\\) are the sample means\n\\(\\mu_0=0\\) is the mean value specified in \\(H_0\\)\n\\(s_1, s_2\\) are the sample SD’s\n\\(n_1, n_2\\) are the sample sizes\nStatistical theory tells us that \\(t_{\\bar{x}_1 - \\bar{x}_2}\\) follows a student’s t-distribution with\n\n\\(df \\approx\\) smaller of \\(n_1-1\\) and \\(n_2-1\\)\nthis is a conservative estimate (smaller than actual \\(df\\) )\n\n\nAssumptions:\n\nIndependent observations & samples\n\nThe observations were collected independently.\nIn particular, the observations from the two groups were not paired in any meaningful way.\n\nApproximately normal samples or big n’s\n\nThe distributions of the samples should be approximately normal\nor both their sample sizes should be at least 30."
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#step-3-test-statistic-part-3",
    "href": "slides_code/Day11_bsta511_code.html#step-3-test-statistic-part-3",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step 3: Test statistic (part 3)",
    "text": "Step 3: Test statistic (part 3)\n\n\n\n\n\n\n  \n    \n      Group\n      variable\n      n\n      mean\n      sd\n    \n  \n  \n    Caffeine\nTaps\n10\n248.3\n2.214\n    NoCaffeine\nTaps\n10\n244.8\n2.394\n  \n  \n  \n\n\n\n\n\\[\n\\text{test statistic} = t_{\\bar{x}_1 - \\bar{x}_2} = \\frac{\\bar{x}_1 - \\bar{x}_2 - 0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\]\n\nBased on the value of the test statistic, do you think we are going to reject or fail to reject \\(H_0\\)?"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#step-3b-assumptions-satisfied",
    "href": "slides_code/Day11_bsta511_code.html#step-3b-assumptions-satisfied",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step “3b”: Assumptions satisfied?",
    "text": "Step “3b”: Assumptions satisfied?\nAssumptions:\n\nIndependent observations & samples\n\nThe observations were collected independently.\nIn particular, the observations from the two groups were not paired in any meaningful way.\n\nApproximately normal samples or big n’s\n\nThe distributions of the samples should be approximately normal\nor both their sample sizes should be at least 30.\n\n\n\nggplot(CaffTaps, aes(x=Taps)) +\n  geom_dotplot() +\n  facet_wrap(vars(Group), ncol=1)\n\n\n\n\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\n# specify upper and lower bounds of shaded region below\nmu &lt;- 0\nstd &lt;- se\n\n# The following figure is only an approximation of the \n# sampling distribution since I used a normal instead\n# of t-distribution to make it.\n\nggplot(data.frame(x = c(mu-5*std, mu+5*std)), aes(x = x)) + \n  stat_function(fun = dnorm, \n                args = list(mean = mu, sd = std)) + \n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks=c(mu, mu - 1*(1:5), mu + 1*(1:5))) +\n  theme(axis.text.x=element_text(angle = -30, hjust = 0)) +\n  labs(y = \"\", \n       x = \"difference in means\",\n       title = \"Sampling distribution of difference in means\") +\n  geom_vline(xintercept = c(diff_x), \n             color = \"red\")\n\n\n\n\n\nggplot(data = data.frame(x = c(-5, 5)), aes(x)) + \n  stat_function(fun = dt, args = list(df = min(n1 -1, n2-1))) + \n  ylab(\"\") + \n  xlab(\"t-dist with df = 9\") +\n  scale_y_continuous(breaks = NULL) + \n  scale_x_continuous(breaks=c(mu, mu - (1:5), mu + (1:5))) +\n  geom_vline(xintercept = c(tstat), \n             color = \"red\")\n\n\n\n\nCalculate the p-value:"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#r-2-sample-t-test-with-long-data",
    "href": "slides_code/Day11_bsta511_code.html#r-2-sample-t-test-with-long-data",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "R: 2-sample t-test (with long data)",
    "text": "R: 2-sample t-test (with long data)\n\nThe CaffTaps data are in a long format, meaning that\n\nall of the outcome values are in one column and\nanother column indicates which group the values are from\n\nThis is a common format for data from multiple samples, especially if the sample sizes are different.\n\n\n(Taps_2ttest &lt;- t.test(formula = Taps ~ Group, alternative = \"greater\", data = CaffTaps))\n\n\n    Welch Two Sample t-test\n\ndata:  Taps by Group\nt = 3.3942, df = 17.89, p-value = 0.001628\nalternative hypothesis: true difference in means between group Caffeine and group NoCaffeine is greater than 0\n95 percent confidence interval:\n 1.711272      Inf\nsample estimates:\n  mean in group Caffeine mean in group NoCaffeine \n                   248.3                    244.8 \n\n\n\ntidy the t.test output\n\n# use tidy command from broom package for briefer output that's a tibble\ntidy(Taps_2ttest) %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    3.5\n248.3\n244.8\n3.394168\n0.001627703\n17.89012\n1.711272\nInf\nWelch Two Sample t-test\ngreater\n  \n  \n  \n\n\n\n\n\nPull the p-value:\n\n\ntidy(Taps_2ttest)$p.value  # we can pull specific values from the tidy output\n\n[1] 0.001627703"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#r-2-sample-t-test-with-wide-data",
    "href": "slides_code/Day11_bsta511_code.html#r-2-sample-t-test-with-wide-data",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "R: 2-sample t-test (with wide data)",
    "text": "R: 2-sample t-test (with wide data)\n\n# make CaffTaps data wide: pivot_wider needs an ID column so that it \n# knows how to \"match\" values from the Caffeine and NoCaffeine groups\nCaffTaps_wide &lt;- CaffTaps %&gt;% \n  mutate(id = rep(1:10, 2)) %&gt;% #  \"fake\" IDs for pivot_wider step\n  pivot_wider(names_from = \"Group\",\n              values_from = \"Taps\")\nglimpse(CaffTaps_wide)\n\nRows: 10\nColumns: 3\n$ id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n$ Caffeine   &lt;dbl&gt; 246, 248, 250, 252, 248, 250, 246, 248, 245, 250\n$ NoCaffeine &lt;dbl&gt; 242, 245, 244, 248, 247, 248, 242, 244, 246, 242\n\nt.test(x = CaffTaps_wide$Caffeine, y = CaffTaps_wide$NoCaffeine, alternative = \"greater\") %&gt;% \n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    3.5\n248.3\n244.8\n3.394168\n0.001627703\n17.89012\n1.711272\nInf\nWelch Two Sample t-test\ngreater"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#why-are-the-dfs-in-the-r-output-different",
    "href": "slides_code/Day11_bsta511_code.html#why-are-the-dfs-in-the-r-output-different",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Why are the df’s in the R output different?",
    "text": "Why are the df’s in the R output different?\nFrom many slides ago:\n\nStatistical theory tells us that \\(t_{\\bar{x}_1 - \\bar{x}_2}\\) follows a student’s t-distribution with\n\n\\(df \\approx\\) smaller of \\(n_1-1\\) and \\(n_2-1\\)\nthis is a conservative estimate (smaller than actual \\(df\\) )\n\n\nThe actual degrees of freedom are calculated using Satterthwaite’s method:\n\\[\\nu = \\frac{[ (s_1^2/n_1) + (s_2^2/n_2) ]^2}\n{(s_1^2/n_1)^2/(n_1 - 1) + (s_2^2/n_2)^2/(n_2-1) }\n= \\frac{ [ SE_1^2 + SE_2^2 ]^2}{ SE_1^4/df_1 + SE_2^4/df_2 }\\]\n\nVerify the p-value in the R output using \\(\\nu\\) = 17.89012:\n\npt(3.3942, df = 17.89012, lower.tail = FALSE)\n\n[1] 0.001627588"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#r-2-sample-t-test-with-pooled-sd",
    "href": "slides_code/Day11_bsta511_code.html#r-2-sample-t-test-with-pooled-sd",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "R: 2-sample t-test with pooled SD",
    "text": "R: 2-sample t-test with pooled SD\n\n# t-test with pooled SD\nt.test(formula = Taps ~ Group, alternative = \"greater\", \n       var.equal = TRUE,  # pooled SD \n       data = CaffTaps) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    3.5\n248.3\n244.8\n3.394168\n0.001616497\n18\n1.711867\nInf\nTwo Sample t-test\ngreater\n  \n  \n  \n\n\n\n# t-test without pooled SD\nt.test(formula = Taps ~ Group, alternative = \"greater\", \n       var.equal = FALSE,  # default, NOT pooled SD \n       data = CaffTaps) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    3.5\n248.3\n244.8\n3.394168\n0.001627703\n17.89012\n1.711272\nInf\nWelch Two Sample t-test\ngreater\n  \n  \n  \n\n\n\n\nSimilar output in this case - why??"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#rejection-region",
    "href": "slides_code/Day11_bsta511_code.html#rejection-region",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Rejection region",
    "text": "Rejection region\n\nIf the absolute value of the test statistic is greater than the critical value, we reject \\(H_0\\)\n\nIn this case the test statistic is in the rejection region.\nOtherwise it’s in the nonrejection region.\n\nWhat do rejection regions look like for 2-sided vs. 1-sided tests?"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#hypothesis-testing-errors",
    "href": "slides_code/Day11_bsta511_code.html#hypothesis-testing-errors",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Hypothesis Testing “Errors”",
    "text": "Hypothesis Testing “Errors”\n\nJustice system analogy\nType I and Type II Errors - Making Mistakes in the Justice System\nhttp://www.intuitor.com/statistics/T1T2Errors.html"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#type-i-ii-errors",
    "href": "slides_code/Day11_bsta511_code.html#type-i-ii-errors",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Type I & II Errors",
    "text": "Type I & II Errors\n\n\\(\\alpha\\) = probability of making a Type I error\n\nThis is the significance level (usually 0.05)\nSet before study starts\n\n\\(\\beta\\) = probability of making a Type II error\nIdeally we want\n\nsmall Type I & II errors and\nbig power\n\n\n\n\n\n\n\nApplet for visualizing Type I & II errors and power: https://rpsychologist.com/d3/NHST/"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#relationship-between-type-i-ii-errors",
    "href": "slides_code/Day11_bsta511_code.html#relationship-between-type-i-ii-errors",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Relationship between Type I & II errors",
    "text": "Relationship between Type I & II errors\n\nType I vs. Type II error\n\nDecreasing P(Type I error) leads to\n\nincreasing P(Type II error)\n\nWe typically keep P(Type I error) = \\(\\alpha\\) set to 0.05\n\n\nTry out the applet at https://rpsychologist.com/d3/NHST/"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#relationship-between-type-ii-errors-and-power",
    "href": "slides_code/Day11_bsta511_code.html#relationship-between-type-ii-errors-and-power",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Relationship between Type II errors and power",
    "text": "Relationship between Type II errors and power\n\nPower = P(correctly rejecting the null hypothesis)\n\n\n\nAlso called the\n\ntrue positive rate,\nprobability of detection, or\nthe sensitivity of a test\n\nPower vs. Type II error\n\nPower = 1 - P(Type II error) = 1 - \\(\\beta\\)\nThus as \\(\\beta\\) = P(Type II error) decreases, the power increases\nP(Type II error) decreases as the mean of the alternative population gets further away from the mean of the null population (effect size gets bigger).\nTypically want at least 80% power; 90% power is good"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#power-calculation-for-testing-one-mean",
    "href": "slides_code/Day11_bsta511_code.html#power-calculation-for-testing-one-mean",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Power calculation for testing one mean",
    "text": "Power calculation for testing one mean\nConversely, we can calculate how much power we had in our example given the sample size of 130.\n\nCalculate power,\n\ngiven \\(\\alpha\\), \\(n\\), “true” alternative mean \\(\\mu\\), and null \\(\\mu_0\\),\nassuming the test statistic is normal (instead of t-distribution)\n\n\n\\[1-\\beta=\n        \\Phi\\left(z-z_{1-\\alpha/2}\\right)+\\Phi\\left(-z-z_{1-\\alpha/2}\\right)\n        \\quad ,\\quad z=\\frac{\\mu-\\mu_0}{s/\\sqrt{n}}\\]\n\nmu &lt;- 98.25; mu0 &lt;- 98.6; sd &lt;- 0.73; alpha &lt;- 0.05; n &lt;- 130\n(z &lt;- (mu-mu0) / (sd/sqrt(n)) )\n\n[1] -5.466595\n\n(Power &lt;- pnorm(z-qnorm(1-alpha/2)) + pnorm(-z-qnorm(1-alpha/2)))\n\n[1] 0.9997731\n\n\nIf the population mean is 98.2 instead of 98.6, we have a 99.98% chance of correctly rejecting \\(H_0\\) when the sample size is 130.\nWe would say this was over powered."
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#r-package-pwr-for-power-analyses",
    "href": "slides_code/Day11_bsta511_code.html#r-package-pwr-for-power-analyses",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "R package pwr for power analyses",
    "text": "R package pwr for power analyses\n\nUse pwr.t.test for both one- and two-sample t-tests.\n\nSpecify all parameters except for the one being solved for.\n\npwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL,\ntype = c(\"two.sample\", \"one.sample\", \"paired\"),\nalternative = c(\"two.sided\", \"less\", \"greater\"))\nd is Cohen’s d effect size: small = 0.2, medium = 0.5, large = 0.8\nOne-sample test (or paired t-test):\n\\[d = \\frac{\\mu-\\mu_0}{s}\\]\nTwo-sample test (independent):\n\\[d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{pooled}}\\]\n\n\\(\\bar{x}_1 - \\bar{x}_2\\) is the difference in means between the two groups that one would want to be able to detect as being significant,\n\\(s_{pooled}\\) is the pooled SD between the two groups - often assume have same sd in each group\nR package pwr for basic statistical tests\n\nhttps://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#pwr-sample-size-for-one-mean-test",
    "href": "slides_code/Day11_bsta511_code.html#pwr-sample-size-for-one-mean-test",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "pwr: sample size for one mean test",
    "text": "pwr: sample size for one mean test\npwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL,\ntype = c(\"two.sample\", \"one.sample\", \"paired\"), alternative = c(\"two.sided\", \"less\", \"greater\"))\n\nd is Cohen’s d effect size: \\(d = \\frac{\\mu-\\mu_0}{s}\\)\n\nSpecify all parameters except for the sample size:\n\nlibrary(pwr)\nt.n &lt;- pwr.t.test(\n  d = (98.6-98.25)/0.73, \n  sig.level = 0.05, \n  power = 0.80, \n  type = \"one.sample\")\n\nt.n\n\n\n     One-sample t test power calculation \n\n              n = 36.11196\n              d = 0.4794521\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nplot(t.n)"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#pwr-power-for-one-mean-test",
    "href": "slides_code/Day11_bsta511_code.html#pwr-power-for-one-mean-test",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "pwr: power for one mean test",
    "text": "pwr: power for one mean test\npwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL,\ntype = c(\"two.sample\", \"one.sample\", \"paired\"), alternative = c(\"two.sided\", \"less\", \"greater\"))\n\nd is Cohen’s d effect size: \\(d = \\frac{\\mu-\\mu_0}{s}\\)\n\nSpecify all parameters except for the power:\n\nt.power &lt;- pwr.t.test(\n  d = (98.6-98.25)/0.73, \n  sig.level = 0.05, \n  # power = 0.80, \n  n = 130,\n  type = \"one.sample\")\n\nt.power\n\n\n     One-sample t test power calculation \n\n              n = 130\n              d = 0.4794521\n      sig.level = 0.05\n          power = 0.9997354\n    alternative = two.sided\n\nplot(t.power)"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#pwr-two-sample-t-test-sample-size",
    "href": "slides_code/Day11_bsta511_code.html#pwr-two-sample-t-test-sample-size",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "pwr: Two-sample t-test: sample size",
    "text": "pwr: Two-sample t-test: sample size\npwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL,\ntype = c(\"two.sample\", \"one.sample\", \"paired\"), alternative = c(\"two.sided\", \"less\", \"greater\"))\n\nd is Cohen’s d effect size: \\(d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{pooled}}\\)\n\nExample: Suppose the data collected for the caffeine taps study were pilot day for a larger study. Investigators want to know what sample size they would need to detect a 2 point difference between the two groups. Assume the SD in both groups is 2.3.\nSpecify all parameters except for the sample size:\n\nt2.n &lt;- pwr.t.test(\n  d = 2/2.3, \n  sig.level = 0.05, \n  power = 0.80, \n  type = \"two.sample\") \n\nt2.n\n\n\n     Two-sample t test power calculation \n\n              n = 21.76365\n              d = 0.8695652\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\nplot(t2.n)"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#pwr-two-sample-t-test-power",
    "href": "slides_code/Day11_bsta511_code.html#pwr-two-sample-t-test-power",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "pwr: Two-sample t-test: power",
    "text": "pwr: Two-sample t-test: power\npwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL,\ntype = c(\"two.sample\", \"one.sample\", \"paired\"), alternative = c(\"two.sided\", \"less\", \"greater\"))\n\nd is Cohen’s d effect size: \\(d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{pooled}}\\)\n\nExample: Suppose the data collected for the caffeine taps study were pilot day for a larger study. Investigators want to know what sample size they would need to detect a 2 point difference between the two groups. Assume the SD in both groups is 2.3.\nSpecify all parameters except for the power:\n\nt2.power &lt;- pwr.t.test(\n  d = 2/2.3, \n  sig.level = 0.05, \n  # power = 0.80, \n  n = 22,\n  type = \"two.sample\") \n\nt2.power\n\n\n     Two-sample t test power calculation \n\n              n = 22\n              d = 0.8695652\n      sig.level = 0.05\n          power = 0.8044288\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\nplot(t2.power)"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#what-information-do-we-need-for-a-power-or-sample-size-calculation",
    "href": "slides_code/Day11_bsta511_code.html#what-information-do-we-need-for-a-power-or-sample-size-calculation",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "What information do we need for a power (or sample size) calculation?",
    "text": "What information do we need for a power (or sample size) calculation?\nThere are 4 pieces of information:\n\nLevel of significance \\(\\alpha\\)\n\nUsually fixed to 0.05\n\nPower\n\nIdeally at least 0.80\n\nSample size\nEffect size (expected change)\n\nGiven any 3 pieces of information, we can solve for the 4th.\n\npwr.t.test(\n  d = (98.6-98.25)/0.73,\n  sig.level = 0.05, \n  # power = 0.80, \n  n=130,\n  type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 130\n              d = 0.4794521\n      sig.level = 0.05\n          power = 0.9997354\n    alternative = two.sided"
  },
  {
    "objectID": "slides_code/Day11_bsta511_code.html#more-software-for-power-and-sample-size-calculations-pass",
    "href": "slides_code/Day11_bsta511_code.html#more-software-for-power-and-sample-size-calculations-pass",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "More software for power and sample size calculations: PASS",
    "text": "More software for power and sample size calculations: PASS\n\nPASS is a very powerful (& expensive) software that does power and sample size calculations for many advanced statistical modeling techniques.\n\nEven if you don’t have access to PASS, their documentation is very good and free online.\nDocumentation includes formulas and references.\nPASS documentation for powering means\n\nOne mean, paired means, two independent means\n\n\nOne-sample t-test documentation: https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/PASS/One-Sample_T-Tests.pdf"
  },
  {
    "objectID": "slides/Day12_bsta511.html",
    "href": "slides/Day12_bsta511.html",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "",
    "text": "With code folding we can hide or show the code in the html output by clicking on the Code buttons in the html file.\nNote the &lt;/&gt; Code button on the top right of the html output.\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee more information at https://quarto.org/docs/output-formats/html-code.html#folding-code"
  },
  {
    "objectID": "slides/Day12_bsta511.html#moritzs-tip-of-the-day-code-folding",
    "href": "slides/Day12_bsta511.html#moritzs-tip-of-the-day-code-folding",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "MoRitz’s tip of the day: code folding",
    "text": "MoRitz’s tip of the day: code folding\n\n\n\n\nWith code folding we can hide or show the code in the html output by clicking on the Code buttons in the html file.\nNote the &lt;/&gt; Code button on the top right of the html output.\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee more information at https://quarto.org/docs/output-formats/html-code.html#folding-code"
  },
  {
    "objectID": "slides/Day12_bsta511.html#where-are-we",
    "href": "slides/Day12_bsta511.html#where-are-we",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Where are we?",
    "text": "Where are we?\n\nCI’s and hypothesis tests for different scenarios:\n\\[\\text{point estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\\(\\frac{s_d}{\\sqrt{n}}\\)\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\\(\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\) or pooled\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n???\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)\n???"
  },
  {
    "objectID": "slides/Day12_bsta511.html#goals-for-today-sections-8.1-8.2",
    "href": "slides/Day12_bsta511.html#goals-for-today-sections-8.1-8.2",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Goals for today (Sections 8.1-8.2)",
    "text": "Goals for today (Sections 8.1-8.2)\n\nStatistical inference for a single proportion or the difference of two (independent) proportions\n\nSampling distribution for a proportion or difference in proportions\nWhat are \\(H_0\\) and \\(H_a\\)?\nWhat are the SE’s for \\(\\hat{p}\\) and \\(\\hat{p}_1-\\hat{p}_2\\)?\nHypothesis test\nConfidence Interval\nHow are the SE’s different for a hypothesis test & CI?\nHow to run proportions tests in R\nPower & sample size for proportions tests (extra material)"
  },
  {
    "objectID": "slides/Day12_bsta511.html#motivating-example",
    "href": "slides/Day12_bsta511.html#motivating-example",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Motivating example",
    "text": "Motivating example\n\nOne proportion\n\nA 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year.\n\nWhat is the CI for the proportion?\nThe study also reported that 36% of noncollege young males had participated in sports betting. Is the proportion for male college students different from 0.36?\n\n\nTwo proportions\n\nThere were 214 men in the sample of noncollege young males (36% participated in sports betting in the previous year).\nCompare the difference in proportions between the college and noncollege young males.\n\nCI & Hypothesis test\n\n\n\n\nBarnes GM, Welte JW, Hoffman JH, Tidwell MC. Comparisons of gambling and alcohol use among college students and noncollege young people in the United States. J Am Coll Health. 2010 Mar-Apr;58(5):443-52. doi: 10.1080/07448480903540499. PMID: 20304756; PMCID: PMC4104810."
  },
  {
    "objectID": "slides/Day12_bsta511.html#steps-in-a-hypothesis-test",
    "href": "slides/Day12_bsta511.html#steps-in-a-hypothesis-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Steps in a Hypothesis Test",
    "text": "Steps in a Hypothesis Test\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "slides/Day12_bsta511.html#step-2-null-alternative-hypotheses",
    "href": "slides/Day12_bsta511.html#step-2-null-alternative-hypotheses",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 2: Null & Alternative Hypotheses",
    "text": "Step 2: Null & Alternative Hypotheses\n\nNull and alternative hypotheses in words and in symbols.\n\n\nOne sample test\n\n\\(H_0\\): The population proportion of young male college students that participated in sports betting in the previous year is 0.36.\n\\(H_A\\): The population proportion of young male college students that participated in sports betting in the previous year is not 0.36.\n\n\\[\\begin{align}\nH_0:& p = 0.36\\\\\nH_A:& p \\neq 0.36\\\\\n\\end{align}\\]\n\n\n\nTwo samples test\n\n\\(H_0\\): The difference in population proportions of young male college and noncollege students that participated in sports betting in the previous year is 0.\n\\(H_A\\): The difference in population proportions of young male college and noncollege students that participated in sports betting in the previous year is not 0.\n\n\\[\\begin{align}\nH_0:& p_{coll} - p_{noncoll} = 0\\\\\nH_A:& p_{coll} - p_{noncoll} \\neq 0\\\\\n\\end{align}\\]"
  },
  {
    "objectID": "slides/Day12_bsta511.html#sampling-distribution-of-hatp",
    "href": "slides/Day12_bsta511.html#sampling-distribution-of-hatp",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Sampling distribution of \\(\\hat{p}\\)",
    "text": "Sampling distribution of \\(\\hat{p}\\)\n\n\\(\\hat{p}=\\frac{X}{n}\\) where \\(X\\) is the number of “successes” and \\(n\\) is the sample size.\n\\(X \\sim Bin(n,p)\\), where \\(p\\) is the population proportion.\nFor \\(n\\) “big enough”, the normal distribution can be used to approximate a binomial distribution:\n\n\\[Bin(n,p) \\rightarrow N\\Big(\\mu = np, \\sigma = \\sqrt{np(1-p)} \\Big)\\]\n\nSince \\(\\hat{p}=\\frac{X}{n}\\) is a linear transformation of \\(X\\), we have for large n:\n\n\\[\\hat{p} \\sim N\\Big(\\mu_{\\hat{p}} = p, \\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}} \\Big)\\]\n\nHow we apply this result to CI’s and test statistics is different!!!"
  },
  {
    "objectID": "slides/Day12_bsta511.html#step-3-test-statistic",
    "href": "slides/Day12_bsta511.html#step-3-test-statistic",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 3: Test statistic",
    "text": "Step 3: Test statistic\n\nSampling distribution of \\(\\hat{p}\\) if we assume \\(H_0: p=p_0\\) is true:\n\\[\\hat{p} \\sim N\\Big(\\mu_{\\hat{p}} = p, \\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}} \\Big)\n\\sim N\\Big(\n\\mu_{\\hat{p}}=p_0, \\sigma_{\\hat{p}}=\\sqrt{\\frac{p_0\\cdot(1-p_0)}{n}}\n\\Big)\\]\nTest statistic for a one sample proportion test:\n\\[\n\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\n= z_{\\hat{p}} = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0\\cdot(1-p_0)}{n}}}\n\\]\n\n\n\n\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year.\n What is the test statistic when testing \\(H_0: p=0.36\\) vs.  \\(H_A: p \\neq 0.36\\)?\n\n\\[\\begin{align}\nz_{\\hat{p}} &= \\frac{94/269 - 0.36}{\\sqrt{\\frac{0.36\\cdot(1-0.36)}{269}}} \\\\\n& -0.3607455\n\\end{align}\\]"
  },
  {
    "objectID": "slides/Day12_bsta511.html#step-3b-conditions-satisfied",
    "href": "slides/Day12_bsta511.html#step-3b-conditions-satisfied",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step “3b”: Conditions satisfied?",
    "text": "Step “3b”: Conditions satisfied?\nConditions:\n\nIndependent observations?\n\nThe observations were collected independently.\n\nThe number of expected successes and expected failures is at least 10.\n\n\\(n_1 p_0 \\ge 10, \\ \\ n_1(1-p_0)\\ge 10\\)\n\n\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year.\n Testing \\(H_0: p=0.36\\) vs. \\(H_A: p \\neq 0.36\\).\n Are the conditions satisfied?"
  },
  {
    "objectID": "slides/Day12_bsta511.html#step-4-p-value",
    "href": "slides/Day12_bsta511.html#step-4-p-value",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate the p-value:\n\\[\\begin{align}\n2 &\\cdot P(\\hat{p}&lt;0.35) \\\\\n&= 2 \\cdot P\\Big(Z_{\\hat{p}} &lt; \\frac{94/269 - 0.36}{\\sqrt{\\frac{0.36\\cdot(1-0.36)}{269}}}\\Big)\\\\\n&=2 \\cdot P(Z_{\\hat{p}} &lt; -0.3607455)\\\\\n&= 0.7182897\n\\end{align}\\]\n\n2*pnorm(-0.3607455)\n\n[1] 0.7182897"
  },
  {
    "objectID": "slides/Day12_bsta511.html#step-5-conclusion-to-hypothesis-test",
    "href": "slides/Day12_bsta511.html#step-5-conclusion-to-hypothesis-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\\[\\begin{align}\nH_0:& p = 0.36\\\\\nH_A:& p \\neq 0.36\\\\\n\\end{align}\\]\n\nRecall the \\(p\\)-value = 0.7182897\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nStats class conclusion\n\nThere is insufficient evidence that the (population) proportion of young male college students that participated in sports betting in the previous year is different than 0.36 ( \\(p\\)-value = 0.72).\n\nMore realistic manuscript conclusion:\n\nIn a sample of 269 male college students, 35% had participated in sports betting in the previous year, which is not different from 36% ( \\(p\\)-value = 0.72)."
  },
  {
    "objectID": "slides/Day12_bsta511.html#ci-for-population-proportion",
    "href": "slides/Day12_bsta511.html#ci-for-population-proportion",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "95% CI for population proportion",
    "text": "95% CI for population proportion\n\n\n\nWhat to use for SE in CI formula?\n\n\\[\\hat{p} \\pm z^* \\cdot SE_{\\hat{p}}\\]\n\n\n\n\nSampling distribution of \\(\\hat{p}\\):\n\n\\[\\hat{p} \\sim N\\Big(\\mu_{\\hat{p}} = p, \\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}} \\Big)\\]\n\n\n\n\nProblem: We don’t know what \\(p\\) is - it’s what we’re estimating with the CI.\nSolution: approximate \\(p\\) with \\(\\hat{p}\\):\n\n\\[SE_{\\hat{p}} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\n\n\n\n\n\n\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year.\nFind the 95% CI for the population proportion.\n\n\\[\\begin{align}\n94/269 &\\pm 1.96 \\cdot SE_{\\hat{p}}\\\\\nSE_{\\hat{p}} &= \\sqrt{\\frac{(94/269)(1-94/269)}{269}}\\\\\n(0.293 &, 0.407)\n\\end{align}\\]\n\n\nInterpretation:\nWe are 95% confident that the (population) proportion of young male college students that participated in sports betting in the previous year is in (0.29, 0.41)."
  },
  {
    "objectID": "slides/Day12_bsta511.html#conditions-for-one-proportion-test-vs.-ci",
    "href": "slides/Day12_bsta511.html#conditions-for-one-proportion-test-vs.-ci",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Conditions for one proportion: test vs. CI",
    "text": "Conditions for one proportion: test vs. CI\n\n\n\nHypothesis test conditions\n\nIndependent observations\n\nThe observations were collected independently.\n\n\n\n\nThe number of expected successes and expected failures is at least 10.\n\n\\[n_1 p_0 \\ge 10, \\ \\ n_1(1-p_0)\\ge 10\\]\n\n\n\nConfidence interval conditions\n\nIndependent observations\n\nThe observations were collected independently.\n\n\n\n\nThe number of successes and failures is at least 10:\n\n\\[n_1\\hat{p}_1 \\ge 10, \\ \\ n_1(1-\\hat{p}_1)\\ge 10\\]"
  },
  {
    "objectID": "slides/Day12_bsta511.html#sampling-distribution-of-hatp_1-hatp_2",
    "href": "slides/Day12_bsta511.html#sampling-distribution-of-hatp_1-hatp_2",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Sampling distribution of \\(\\hat{p}_1-\\hat{p}_2\\)",
    "text": "Sampling distribution of \\(\\hat{p}_1-\\hat{p}_2\\)\n\n\n\\(\\hat{p}_1=\\frac{X_1}{n_1}\\) and \\(\\hat{p}_2=\\frac{X_2}{n_2}\\),\n\n\\(X_1\\) & \\(X_2\\) are the number of “successes”\n\\(n_1\\) & \\(n_2\\) are the sample sizes of the 1st & 2nd samples\n\n\n\n\nEach \\(\\hat{p}\\) can be approximated by a normal distribution, for “big enough” \\(n\\)\nSince the difference of independent normal random variables is also normal, it follows that for “big enough” \\(n_1\\) and \\(n_2\\)\n\n\\[\\hat{p}_1 - \\hat{p}_2 \\sim N \\Big(\\mu_{\\hat{p}_1 - \\hat{p}_2} = p_1 - p_2, ~~\n\\sigma_{\\hat{p}_1 - \\hat{p}_2} =\n\\sqrt{\n\\frac{p_1\\cdot(1-p_1)}{n_1} + \\frac{p_2\\cdot(1-p_2)}{n_2}}\n\\Big)\\]\nwhere \\(p_1\\) & \\(p_2\\) are the population proportions, respectively.\n\nHow we apply this result to CI’s and test statistics is different!!!"
  },
  {
    "objectID": "slides/Day12_bsta511.html#step-3-test-statistic-12",
    "href": "slides/Day12_bsta511.html#step-3-test-statistic-12",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 3: Test statistic (1/2)",
    "text": "Step 3: Test statistic (1/2)\n\nSampling distribution of \\(\\hat{p}_1 - \\hat{p}_2\\): \\[\\hat{p}_1 - \\hat{p}_2 \\sim N \\Big(\\mu_{\\hat{p}_1 - \\hat{p}_2} = p_1 - p_2, ~~\n\\sigma_{\\hat{p}_1 - \\hat{p}_2} =\n\\sqrt{\n\\frac{p_1\\cdot(1-p_1)}{n_1} + \\frac{p_2\\cdot(1-p_2)}{n_2}}\n\\Big)\\]\nSince we assume \\(H_0: p_1 - p_2 = 0\\) is true, we “pool” the proportions of the two samples to calculate the SE:\n\\[\\text{pooled proportion} = \\hat{p}_{pool} = \\dfrac{\\text{total number of successes} }{ \\text{total number of cases}} = \\frac{x_1+x_2}{n_1+n_2}\\]\nTest statistic:\n\\[\n\\text{test statistic} = z_{\\hat{p}_1 - \\hat{p}_2} = \\frac{\\hat{p}_1 - \\hat{p}_2 - 0}{\\sqrt{\\frac{\\hat{p}_{pool}\\cdot(1-\\hat{p}_{pool})}{n_1} + \\frac{\\hat{p}_{pool}\\cdot(1-\\hat{p}_{pool})}{n_2}}}\n\\]"
  },
  {
    "objectID": "slides/Day12_bsta511.html#step-3-test-statistic-22",
    "href": "slides/Day12_bsta511.html#step-3-test-statistic-22",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 3: Test statistic (2/2)",
    "text": "Step 3: Test statistic (2/2)\n\n\\[\n\\text{test statistic} = z_{\\hat{p}_1 - \\hat{p}_2} = \\frac{\\hat{p}_1 - \\hat{p}_2 - 0}{\\sqrt{\\frac{\\hat{p}_{pool}\\cdot(1-\\hat{p}_{pool})}{n_1} + \\frac{\\hat{p}_{pool}\\cdot(1-\\hat{p}_{pool})}{n_2}}}\n\\]\n\\[\\text{pooled proportion} = \\hat{p}_{pool} = \\dfrac{\\text{total number of successes} }{ \\text{total number of cases}} = \\frac{x_1+x_2}{n_1+n_2}\\]\n\n\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year, and out of 214 noncollege young males 36% had.\nWhat is the test statistic when testing \\(H_0: p_{coll} - p_{noncoll} = 0\\) vs.  \\(H_A: p_{coll} - p_{noncoll} \\neq 0\\)?\n\\[\\begin{align}\nz_{\\hat{p}_1 - \\hat{p}_2} &= \\frac{94/269 - 77/214-0}{\\sqrt{0.354\\cdot(1-0.354)(\\frac{1}{269}+\\frac{1}{214})}}\\\\\n&=-0.2367497\n\\end{align}\\]"
  },
  {
    "objectID": "slides/Day12_bsta511.html#step-3b-conditions-satisfied-1",
    "href": "slides/Day12_bsta511.html#step-3b-conditions-satisfied-1",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step “3b”: Conditions satisfied?",
    "text": "Step “3b”: Conditions satisfied?\n\nConditions:\n\nIndependent observations & samples\n\nThe observations were collected independently.\nIn particular, observations from the two groups weren’t paired in any meaningful way.\n\nThe number of expected successes and expected failures is at least 10 for each group - using the pooled proportion:\n\n\\(n_1\\hat{p}_{pool} \\ge 10, \\ \\ n_1(1-\\hat{p}_{pool}) \\ge 10\\)\n\\(n_2\\hat{p}_{pool} \\ge 10, \\ \\ n_2(1-\\hat{p}_{pool}) \\ge 10\\)\n\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year, and out of 214 noncollege young males 36% had.\nTesting \\(H_0: p_{coll} - p_{noncoll} = 0\\) vs.  \\(H_A: p_{coll} - p_{noncoll} \\neq 0\\)? .\n Are the conditions satisfied?"
  },
  {
    "objectID": "slides/Day12_bsta511.html#step-4-p-value-1",
    "href": "slides/Day12_bsta511.html#step-4-p-value-1",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\n\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate the p-value:\n\\[\\begin{align}\n2 &\\cdot P(\\hat{p}_1 - \\hat{p}_2&lt;0.35-0.36) \\\\\n= 2 &\\cdot P\\Big(Z_{\\hat{p}_1 - \\hat{p}_2} &lt; \\\\\n&\\frac{94/269 - 77/214-0}{\\sqrt{0.354\\cdot(1-0.354)(\\frac{1}{269}+\\frac{1}{214})}}\\Big)\\\\\n=2 &\\cdot P(Z_{\\hat{p}} &lt; -0.2367497) \\\\\n= & 0.812851\n\\end{align}\\]\n\n\n\n\n2*pnorm(-0.2367497)\n\n[1] 0.812851"
  },
  {
    "objectID": "slides/Day12_bsta511.html#step-5-conclusion-to-hypothesis-test-1",
    "href": "slides/Day12_bsta511.html#step-5-conclusion-to-hypothesis-test-1",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\n\\[\\begin{align}\nH_0:& p_{coll} - p_{noncoll} = 0\\\\\nH_A:& p_{coll} - p_{noncoll} \\neq 0\\\\\n\\end{align}\\]\n\nRecall the \\(p\\)-value = 0.812851\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nStats class conclusion\n\nThere is insufficient evidence that the difference in (population) proportions of young male college and noncollege students that participated in sports betting in the previous year are different ( \\(p\\)-value = 0.81).\n\nMore realistic manuscript conclusion:\n\n35% of young male college students (n=269) and 36% of noncollege young males (n=214) participated in sports betting in the previous year ( \\(p\\)-value = 0.81)."
  },
  {
    "objectID": "slides/Day12_bsta511.html#ci-for-population-difference-in-proportions",
    "href": "slides/Day12_bsta511.html#ci-for-population-difference-in-proportions",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "95% CI for population difference in proportions",
    "text": "95% CI for population difference in proportions\n\n\n\nWhat to use for SE in CI formula?\n\n\\[\\hat{p}_1 - \\hat{p}_2 \\pm z^* \\cdot SE_{\\hat{p}_1 - \\hat{p}_2}\\]\n\n\n\n\nSE in sampling distribution of \\(\\hat{p}_1 - \\hat{p}_2\\)\n\n\\[\\sigma_{\\hat{p}_1 - \\hat{p}_2} =\n\\sqrt{\n\\frac{p_1\\cdot(1-p_1)}{n_1} + \\frac{p_2\\cdot(1-p_2)}{n_2}} \\]\n\n\n\n\nProblem: We don’t know what \\(p\\) is - it’s what we’re estimating with the CI.\nSolution: approximate \\(p_1\\), \\(p_2\\) with \\(\\hat{p}_1\\), \\(\\hat{p}_2\\):\n\n\\[SE_{\\hat{p}_1 - \\hat{p}_2} = \\sqrt{\n\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}}\\]\n\n\n\n\n\n\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year, and out of 214 noncollege young males 36% had. Find the 95% CI for the difference in population proportions.\n\n\n\n\\[\\frac{94}{269} - \\frac{77}{214} \\pm 1.96 \\cdot SE_{\\hat{p}_1 - \\hat{p}_2}\\]\n\\[\\begin{align}\n& SE_{\\hat{p}_1 - \\hat{p}_2}=\\\\\n& \\sqrt{\n\\frac{94/269 \\cdot (1-94/269)}{269} +\n\\frac{77/214 \\cdot (1-77/214)}{214}}\n\\end{align}\\]\n\n\n\n\nInterpretation:\nWe are 95% confident that the difference in (population) proportions of young male college and noncollege students that participated in sports betting in the previous year is in (-0.127, 0.106)."
  },
  {
    "objectID": "slides/Day12_bsta511.html#conditions-for-difference-in-proportions-test-vs.-ci",
    "href": "slides/Day12_bsta511.html#conditions-for-difference-in-proportions-test-vs.-ci",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Conditions for difference in proportions: test vs. CI",
    "text": "Conditions for difference in proportions: test vs. CI\n\n\n\nHypothesis test conditions\n\nIndependent observations & samples\n\nThe observations were collected independently.\nIn particular, observations from the two groups weren’t paired in any meaningful way.\n\n\n\n\nThe number of expected successes and expected failures is at least 10 for each group - using the pooled proportion:\n\n\\(n_1\\hat{p}_{pool} \\ge 10, \\ \\ n_1(1-\\hat{p}_{pool}) \\ge 10\\)\n\\(n_2\\hat{p}_{pool} \\ge 10, \\ \\ n_2(1-\\hat{p}_{pool}) \\ge 10\\)\n\n\n\n\n\nConfidence interval conditions\n\nIndependent observations & samples\n\nThe observations were collected independently.\nIn particular, observations from the two groups weren’t paired in any meaningful way.\n\n\n\n\nThe number of successes and failures is at least 10 for each group.\n\n\\(n_1\\hat{p}_1 \\ge 10, \\ \\ n_1(1-\\hat{p}_1) \\ge 10\\)\n\\(n_2\\hat{p}_2 \\ge 10, \\ \\ n_2(1-\\hat{p}_2) \\ge 10\\)"
  },
  {
    "objectID": "slides/Day12_bsta511.html#summary-stats-input-for-1-sample-proportion-test",
    "href": "slides/Day12_bsta511.html#summary-stats-input-for-1-sample-proportion-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Summary stats input for 1-sample proportion test",
    "text": "Summary stats input for 1-sample proportion test\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year.\nTest \\(H_0: p=0.36\\) vs. \\(H_A: p \\neq 0.36\\)?\n\n.35*269 # number of \"successes\"; round this value\n\n[1] 94.15\n\nprop.test(x = 94, n = 269,              # x = # successes & n = sample size\n          p = 0.36,                     # null value p0\n          alternative = \"two.sided\",    # 2-sided alternative\n          correct = FALSE)              # no continuity correction\n\n\n    1-sample proportions test without continuity correction\n\ndata:  94 out of 269, null probability 0.36\nX-squared = 0.13014, df = 1, p-value = 0.7183\nalternative hypothesis: true p is not equal to 0.36\n95 percent confidence interval:\n 0.2949476 0.4081767\nsample estimates:\n        p \n0.3494424 \n\n\nCan tidy() test output:\n\nprop.test(x = 94, n = 269, p = 0.36, alternative = \"two.sided\", correct = FALSE) %&gt;% \n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.1301373\n0.7182897\n1\n0.2949476\n0.4081767\n1-sample proportions test without continuity correction\ntwo.sided"
  },
  {
    "objectID": "slides/Day12_bsta511.html#dataset-input-for-1-sample-proportion-test-12",
    "href": "slides/Day12_bsta511.html#dataset-input-for-1-sample-proportion-test-12",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Dataset input for 1-sample proportion test (1/2)",
    "text": "Dataset input for 1-sample proportion test (1/2)\n\n\nSince we don’t have a dataset, we first need to create a dataset based on the results:\n\n“out of 269 male college students, 35% had participated in sports betting in the previous year”\n\n\n.35*269 # number of \"successes\"; round this value\n\n[1] 94.15\n\nSportsBet1 &lt;- tibble(\n  Coll = c(rep(\"Bet\", 94),\n           rep(\"NotBet\",269-94))\n  )\n\n\n\nglimpse(SportsBet1)\n\nRows: 269\nColumns: 1\n$ Coll &lt;chr&gt; \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"B…\n\nSportsBet1 %&gt;% tabyl(Coll)\n\n   Coll   n   percent\n    Bet  94 0.3494424\n NotBet 175 0.6505576\n\n\n\nR code for proportions test requires input as a base R table:\n\ntable(SportsBet1$Coll)\n\n\n   Bet NotBet \n    94    175"
  },
  {
    "objectID": "slides/Day12_bsta511.html#dataset-input-for-1-sample-proportion-test-22",
    "href": "slides/Day12_bsta511.html#dataset-input-for-1-sample-proportion-test-22",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Dataset input for 1-sample proportion test (2/2)",
    "text": "Dataset input for 1-sample proportion test (2/2)\n\n\nWhen using a dataset, prop.test requires the input x to be a table\nNote that we do not also specify n since the table already includes all needed information.\n\n\nprop.test(x = table(SportsBet1$Coll),   # table() of data\n          p = 0.36,                     # null value p0\n          alternative = \"two.sided\",    # 2-sided alternative\n          correct = FALSE)              # no continuity correction\n\n\n    1-sample proportions test without continuity correction\n\ndata:  table(SportsBet1$Coll), null probability 0.36\nX-squared = 0.13014, df = 1, p-value = 0.7183\nalternative hypothesis: true p is not equal to 0.36\n95 percent confidence interval:\n 0.2949476 0.4081767\nsample estimates:\n        p \n0.3494424 \n\n\nCompare output with summary stats method:\n\nprop.test(x = 94, n = 269,              # x = # successes & n = sample size\n          p = 0.36,                     # null value p0\n          alternative = \"two.sided\",    # 2-sided alternative\n          correct = FALSE)  %&gt;%         # no continuity correction\n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.1301373\n0.7182897\n1\n0.2949476\n0.4081767\n1-sample proportions test without continuity correction\ntwo.sided"
  },
  {
    "objectID": "slides/Day12_bsta511.html#continuity-correction-1-prop-z-test-with-vs.-without-cc",
    "href": "slides/Day12_bsta511.html#continuity-correction-1-prop-z-test-with-vs.-without-cc",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Continuity correction: 1-prop z-test with vs. without CC",
    "text": "Continuity correction: 1-prop z-test with vs. without CC\n\n\nRecall that when we approximated the\nbinomial distribution with a normal distribution to calculate a probability,\nthat we included a continuity correction (CC)\nto account for approximating a discrete distribution with a continuous distribution.\n\n\nprop.test(x = 94, n = 269, p = 0.36, alternative = \"two.sided\", \n          correct = FALSE) %&gt;% \n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.1301373\n0.7182897\n1\n0.2949476\n0.4081767\n1-sample proportions test without continuity correction\ntwo.sided\n  \n  \n  \n\n\n\n\n\n\nprop.test(x = 94, n = 269, p = 0.36, alternative = \"two.sided\", \n          correct = TRUE) %&gt;% \n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.08834805\n0.7662879\n1\n0.2931841\n0.4100774\n1-sample proportions test with continuity correction\ntwo.sided\n  \n  \n  \n\n\n\n\nDifferences are small when sample sizes are large."
  },
  {
    "objectID": "slides/Day12_bsta511.html#summary-stats-input-for-2-samples-proportion-test",
    "href": "slides/Day12_bsta511.html#summary-stats-input-for-2-samples-proportion-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Summary stats input for 2-samples proportion test",
    "text": "Summary stats input for 2-samples proportion test\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year, and out of 214 noncollege young males 36% had. Test \\(H_0: p_{coll} - p_{noncoll} = 0\\) vs. \\(H_A: p_{coll} - p_{noncoll} \\neq 0\\).\n\n\n\n# round the number of successes:\n.35*269 # number of \"successes\" in college students\n\n[1] 94.15\n\n.36*214 # number of \"successes\" in noncollege students\n\n[1] 77.04\n\nNmbrBet &lt;- c(94, 77)                    # vector for # of successes in each group\nTotalNmbr &lt;- c(269, 214)                # vector for sample size in each group\n\nprop.test(x = NmbrBet,                  # x is # of successes in each group \n          n = TotalNmbr,                # n is sample size in each group\n          alternative = \"two.sided\",    # 2-sided alternative\n          correct = FALSE)              # no continuity correction\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  NmbrBet out of TotalNmbr\nX-squared = 0.05605, df = 1, p-value = 0.8129\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.09628540  0.07554399\nsample estimates:\n   prop 1    prop 2 \n0.3494424 0.3598131"
  },
  {
    "objectID": "slides/Day12_bsta511.html#dataset-input-for-2-samples-proportion-test-12",
    "href": "slides/Day12_bsta511.html#dataset-input-for-2-samples-proportion-test-12",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Dataset input for 2-samples proportion test (1/2)",
    "text": "Dataset input for 2-samples proportion test (1/2)\n\n\n\nSince we don’t have a dataset, we first need to create a dataset based on the results:\n\n“out of 269 male college students, 35% had participated in sports betting in the previous year, and out of 214 noncollege young males 36% had”\n\n\n\n# round the number of successes:\n.35*269 # college students\n\n[1] 94.15\n\n.36*214 # noncollege students\n\n[1] 77.04\n\nSportsBet2 &lt;- tibble(\n  Group = c(rep(\"College\", 269), \n         rep(\"NonCollege\", 214)),\n  Bet = c(rep(\"yes\", 94), \n          rep(\"no\", 269-94),\n          rep(\"yes\", 77), \n          rep(\"no\", 214-77))\n)\n\n\n\nglimpse(SportsBet2)\n\nRows: 483\nColumns: 2\n$ Group &lt;chr&gt; \"College\", \"College\", \"College\", \"College\", \"College\", \"College\"…\n$ Bet   &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"…\n\nSportsBet2 %&gt;% tabyl(Group, Bet)\n\n      Group  no yes\n    College 175  94\n NonCollege 137  77\n\n\n\nR code for proportions test requires input as a base R table:\n\ntable(SportsBet2$Group, \n      SportsBet2$Bet)\n\n            \n              no yes\n  College    175  94\n  NonCollege 137  77"
  },
  {
    "objectID": "slides/Day12_bsta511.html#dataset-input-for-2-samples-proportion-test-22",
    "href": "slides/Day12_bsta511.html#dataset-input-for-2-samples-proportion-test-22",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Dataset input for 2-samples proportion test (2/2)",
    "text": "Dataset input for 2-samples proportion test (2/2)\n\n\nWhen using a dataset, prop.test requires the input x to be a table\nNote that we do not also specify n since the table already includes all needed information.\n\n\nprop.test(x = table(SportsBet2$Group, SportsBet2$Bet),\n       alternative = \"two.sided\",\n       correct = FALSE)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  table(SportsBet2$Group, SportsBet2$Bet)\nX-squared = 0.05605, df = 1, p-value = 0.8129\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.07554399  0.09628540\nsample estimates:\n   prop 1    prop 2 \n0.6505576 0.6401869 \n\n\nCompare output with summary stats method:\n\nprop.test(x = NmbrBet,                  # x is # of successes in each group \n          n = TotalNmbr,                # n is sample size in each group\n          alternative = \"two.sided\",    # 2-sided alternative\n          correct = FALSE) %&gt;%          # no continuity correction\n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.3598131\n0.05605044\n0.8128509\n1\n-0.0962854\n0.07554399\n2-sample test for equality of proportions without continuity correction\ntwo.sided"
  },
  {
    "objectID": "slides/Day12_bsta511.html#continuity-correction-2-prop-z-test-with-vs.-without-cc",
    "href": "slides/Day12_bsta511.html#continuity-correction-2-prop-z-test-with-vs.-without-cc",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Continuity correction: 2-prop z-test with vs. without CC",
    "text": "Continuity correction: 2-prop z-test with vs. without CC\n\n\nRecall that when we approximated the\nbinomial distribution with a normal distribution to calculate a probability,\nthat we included a continuity correction (CC)\nto account for approximating a discrete distribution with a continuous distribution.\n\n\nprop.test(x = NmbrBet, n = TotalNmbr, alternative = \"two.sided\", \n          correct = FALSE) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.3598131\n0.05605044\n0.8128509\n1\n-0.0962854\n0.07554399\n2-sample test for equality of proportions without continuity correction\ntwo.sided\n  \n  \n  \n\n\n\n\n\n\nprop.test(x = NmbrBet, n = TotalNmbr, alternative = \"two.sided\", \n          correct = TRUE) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.3598131\n0.01987511\n0.8878864\n1\n-0.1004806\n0.07973918\n2-sample test for equality of proportions with continuity correction\ntwo.sided\n  \n  \n  \n\n\n\n\nDifferences are small when sample sizes are large."
  },
  {
    "objectID": "slides/Day12_bsta511.html#sample-size-calculation-for-testing-one-proportion",
    "href": "slides/Day12_bsta511.html#sample-size-calculation-for-testing-one-proportion",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Sample size calculation for testing one proportion",
    "text": "Sample size calculation for testing one proportion\n\n\nRecall in our sports betting example that the null \\(p_0=0.36\\) and the observed proportion was \\(\\hat{p}=0.35\\).\n\nThe p-value from the hypothesis test was not significant.\nHow big would the sample size \\(n\\) need to be in order for the p-value to be significant?\n\nCalculate \\(n\\)\n\ngiven \\(\\alpha\\), power ( \\(1-\\beta\\) ), “true” alternative proportion \\(p\\), and null \\(p_0\\):\n\n\n\n\n\\[n=p(1-p)\\left(\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{p-p_0}\\right)^2\\]\n\n\np &lt;- 0.35\np0 &lt;- 0.36\nalpha &lt;- 0.05\nbeta &lt;- 0.20  #power=1-beta; want &gt;=80% power\nn &lt;- p*(1-p)*((qnorm(1-alpha/2) + qnorm(1-beta)) /\n                (p-p0))^2\nn\n\n[1] 17856.2\n\nceiling(n) \n\n[1] 17857\n\n\n\n\nWe would need a sample size of at least 17,857!"
  },
  {
    "objectID": "slides/Day12_bsta511.html#power-calculation-for-testing-one-proportion",
    "href": "slides/Day12_bsta511.html#power-calculation-for-testing-one-proportion",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Power calculation for testing one proportion",
    "text": "Power calculation for testing one proportion\n\nConversely, we can calculate how much power we had in our example given the sample size of 269.\n\nCalculate power,\n\ngiven \\(\\alpha\\), \\(n\\), “true” alternative proportion \\(p\\), and null \\(p_0\\)\n\n\n\\[1-\\beta=\n            \\Phi\\left(z-z_{1-\\alpha/2}\\right)+\\Phi\\left(-z-z_{1-\\alpha/2}\\right)\n            \\quad ,\\quad \\text{where } z=\\frac{p-p_0}{\\sqrt{\\frac{p(1-p)}{n}}}\\]\n\\(\\Phi\\) is the probability for a standard normal distribution\n\np &lt;- 0.35; p0 &lt;- 0.36; alpha &lt;- 0.05; n &lt;- 269\n(z &lt;- (p-p0)/sqrt(p*(1-p)/n))\n\n[1] -0.343863\n\n(Power &lt;- pnorm(z - qnorm(1-alpha/2)) +  pnorm(-z - qnorm(1-alpha/2)))\n\n[1] 0.06365242\n\n\nIf the population proportion is 0.35 instead of 0.36, we only have a 6.4% chance of correctly rejecting \\(H_0\\) when the sample size is 269."
  },
  {
    "objectID": "slides/Day12_bsta511.html#r-package-pwr-for-power-analyses",
    "href": "slides/Day12_bsta511.html#r-package-pwr-for-power-analyses",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "R package pwr for power analyses",
    "text": "R package pwr for power analyses\n\n\nSpecify all parameters except for the one being solved for.\nOne proportion\n\npwr.p.test(h = NULL, n = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\",\"less\",\"greater\"))\n\nTwo proportions (same sample sizes)\n\npwr.2p.test(h = NULL, n = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\",\"less\",\"greater\"))\n\nTwo proportions (different sample sizes)\n\npwr.2p2n.test(h = NULL, n1 = NULL, n2 = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\", \"less\",\"greater\"))\n\n\n\n\\(h\\) is the effect size, and calculated using an arcsine transformation:\n\\[h = \\text{ES.h(p1, p2)} = 2\\arcsin(\\sqrt{p_1})-2\\arcsin(\\sqrt{p_2})\\]\n\n\nSee PASS documentation for\n\ntesting 1 proportion using effect size vs. other ways of powering a test of 1 proportion\ntesting 2 proportions using effect size vs. other ways of powering a test of 2 proportions."
  },
  {
    "objectID": "slides/Day12_bsta511.html#pwr-sample-size-for-one-proportion-test",
    "href": "slides/Day12_bsta511.html#pwr-sample-size-for-one-proportion-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "pwr: sample size for one proportion test",
    "text": "pwr: sample size for one proportion test\n\npwr.p.test(h = NULL, n = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\",\"less\",\"greater\"))\n\n\n\n\\(h\\) is the effect size: h = ES.h(p1, p2)\n\np1 and p2 are the two proportions being tested\none of them is the null proportion \\(p_0\\), and the other is the alternative proportion\n\n\nSpecify all parameters except for the sample size:\n\n\n\n\n\nlibrary(pwr)\n\np.n &lt;- pwr.p.test(\n  h = ES.h(p1 = 0.36, p2 = 0.35),\n  sig.level = 0.05, \n  power = 0.80, \n  alternative = \"two.sided\")\np.n\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.02089854\n              n = 17971.09\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n\n\nplot(p.n)"
  },
  {
    "objectID": "slides/Day12_bsta511.html#pwr-power-for-one-proportion-test",
    "href": "slides/Day12_bsta511.html#pwr-power-for-one-proportion-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "pwr: power for one proportion test",
    "text": "pwr: power for one proportion test\n\npwr.p.test(h = NULL, n = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\",\"less\",\"greater\"))\n\n\n\n\\(h\\) is the effect size: h = ES.h(p1, p2)\n\np1 and p2 are the two proportions being tested\none of them is the null proportion \\(p_0\\), and the other is the alternative proportion\n\n\nSpecify all parameters except for the power:\n\n\n\n\n\nlibrary(pwr)\n\np.power &lt;- pwr.p.test(\n  h = ES.h(p1 = 0.36, p2 = 0.35),\n  sig.level = 0.05, \n  # power = 0.80, \n  n = 269,\n  alternative = \"two.sided\")\np.power\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.02089854\n              n = 269\n      sig.level = 0.05\n          power = 0.06356445\n    alternative = two.sided\n\n\n\n\n\nplot(p.power)"
  },
  {
    "objectID": "slides/Day12_bsta511.html#pwr-sample-size-for-two-proportions-test",
    "href": "slides/Day12_bsta511.html#pwr-sample-size-for-two-proportions-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "pwr: sample size for two proportions test",
    "text": "pwr: sample size for two proportions test\n\n\nTwo proportions (same sample sizes)\n\npwr.2p.test(h = NULL, n = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\",\"less\",\"greater\"))\n\n\n\n\\(h\\) is the effect size: h = ES.h(p1, p2); p1 and p2 are the two proportions being tested\n\nSpecify all parameters except for the sample size:\n\n\n\n\n\np2.n &lt;- pwr.2p.test(\n  h = ES.h(p1 = 0.36, p2 = 0.35),\n  sig.level = 0.05, \n  power = 0.80, \n  alternative = \"two.sided\")\np2.n\n\n\n     Difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.02089854\n              n = 35942.19\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: same sample sizes\n\n\n\n\n\nNote: \\(n\\) in output is the number per sample!\n\n\nplot(p2.n)"
  },
  {
    "objectID": "slides/Day12_bsta511.html#pwr-power-for-two-proportions-test",
    "href": "slides/Day12_bsta511.html#pwr-power-for-two-proportions-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "pwr: power for two proportions test",
    "text": "pwr: power for two proportions test\n\n\nTwo proportions (different sample sizes)\n\npwr.2p2n.test(h = NULL, n1 = NULL, n2 = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\", \"less\",\"greater\"))\n\n\n\n\\(h\\) is the effect size: h = ES.h(p1, p2); p1 and p2 are the two proportions being tested\n\nSpecify all parameters except for the power:\n\n\n\n\n\np2.n2 &lt;- pwr.2p2n.test(\n  h = ES.h(p1 = 0.36, p2 = 0.35),\n  n1 = 214,\n  n2 = 269,\n  sig.level = 0.05, \n  # power = 0.80, \n  alternative = \"two.sided\")\np2.n2\n\n\n     difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.02089854\n             n1 = 214\n             n2 = 269\n      sig.level = 0.05\n          power = 0.05598413\n    alternative = two.sided\n\nNOTE: different sample sizes\n\n\n\n\n\nNote: \\(n\\) in output is the number per sample!\n\n\nplot(p2.n2)"
  },
  {
    "objectID": "slides/Day12_bsta511.html#where-are-we-1",
    "href": "slides/Day12_bsta511.html#where-are-we-1",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Where are we?",
    "text": "Where are we?\n\nCI’s and hypothesis tests for different scenarios:\n\\[\\text{point estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\\(\\frac{s_d}{\\sqrt{n}}\\)\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\\(\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\) or pooled\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n\\(\\sqrt{\\frac{p(1-p)}{n}}\\)\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)\n\\(\\sqrt{\\frac{p_1\\cdot(1-p_1)}{n_1} + \\frac{p_2\\cdot(1-p_2)}{n_2}}\\)"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html",
    "href": "slides_md/Day11_bsta511_md.html",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "",
    "text": "Add tabbed sections to your html file using tabset.\n\nFirst tabSecond tabRead up on tabsets\n\n\n\nYou can make subsections appear as different tabs in your html file.\nThis is the first tab.\nIt was created by adding ::: panel-tabset right above the subsection ### First tab (see the code file).\nLook up to the right of where it says “First tab,” and you will see a second tab with the creative name “Second tab.”\nIf you are viewing the html output of this file, you can click on the different tabs to see what’s in them.\nTo stop new tabs from being created, close off the original ::: panel-tabset command with ::: at the end.\n\nIn the source code file, you will see the ::: at the end of the ### Read up on tabsets tab.\n\n\n\n\n\nWelcome to the second tab!\n\n\n\n\n\n\n\n\n\nYou can read up more about creating tabs at\n\nhttps://quarto.org/docs/interactive/layout.html#tabset-panel\n\n\n\nIf you are reading the source code file, the next line contains :::, which closes the tabsets."
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#moritzs-tip-of-the-day",
    "href": "slides_md/Day11_bsta511_md.html#moritzs-tip-of-the-day",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "",
    "text": "Add tabbed sections to your html file using tabset.\n\nFirst tabSecond tabRead up on tabsets\n\n\n\nYou can make subsections appear as different tabs in your html file.\nThis is the first tab.\nIt was created by adding ::: panel-tabset right above the subsection ### First tab (see the code file).\nLook up to the right of where it says “First tab,” and you will see a second tab with the creative name “Second tab.”\nIf you are viewing the html output of this file, you can click on the different tabs to see what’s in them.\nTo stop new tabs from being created, close off the original ::: panel-tabset command with ::: at the end.\n\nIn the source code file, you will see the ::: at the end of the ### Read up on tabsets tab.\n\n\n\n\n\nWelcome to the second tab!\n\n\n\n\n\n\n\n\n\nYou can read up more about creating tabs at\n\nhttps://quarto.org/docs/interactive/layout.html#tabset-panel\n\n\n\nIf you are reading the source code file, the next line contains :::, which closes the tabsets."
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#where-are-we",
    "href": "slides_md/Day11_bsta511_md.html#where-are-we",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#where-are-we-continuous-outcome-zoomed-in",
    "href": "slides_md/Day11_bsta511_md.html#where-are-we-continuous-outcome-zoomed-in",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Where are we? Continuous outcome zoomed in",
    "text": "Where are we? Continuous outcome zoomed in"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#where-are-we-1",
    "href": "slides_md/Day11_bsta511_md.html#where-are-we-1",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Where are we?",
    "text": "Where are we?\n\nCI’s and hypothesis tests for different scenarios:\n\\[\\text{point estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\\(\\frac{s_d}{\\sqrt{n}}\\)\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n???\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#goals-for-today",
    "href": "slides_md/Day11_bsta511_md.html#goals-for-today",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Goals for today",
    "text": "Goals for today\n\n2-sample t-test (Section 5.3)\n\nStatistical inference for difference in means from 2 independent samples\n\nWhat are \\(H_0\\) and \\(H_a\\)?\nWhat is the SE for \\(\\bar{x}_1 - \\bar{x}_2\\)?\nHypothesis test\nConfidence Interval\nRun test in R - using long vs. wide data\nSatterthwaite’s df\nPooled SD\n\n\n\n\nPower and sample size (4.3.4, 5.4, plus notes)\n\nCritical values & rejection region\nType I & II errors\nPower\nHow to calculate sample size needed for a study?"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#examples-of-designs-with-two-independent-samples",
    "href": "slides_md/Day11_bsta511_md.html#examples-of-designs-with-two-independent-samples",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Examples of designs with two independent samples",
    "text": "Examples of designs with two independent samples\n\nAny study where participants are randomized to a control and treatment group\nStudy where create two groups based on whether they were exposed or not to some condition (can be observational)\nBook: “Does treatment using embryonic stem cells (ESCs) help improve heart function following a heart attack?”\nBook: “Is there evidence that newborns from mothers who smoke have a different average birth weight than newborns from mothers who do not smoke?”\nThe key is that the data from the two groups are independent of each other."
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#steps-in-a-hypothesis-test",
    "href": "slides_md/Day11_bsta511_md.html#steps-in-a-hypothesis-test",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Steps in a Hypothesis Test",
    "text": "Steps in a Hypothesis Test\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#does-caffeine-increase-finger-tapsmin-on-average",
    "href": "slides_md/Day11_bsta511_md.html#does-caffeine-increase-finger-tapsmin-on-average",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Does caffeine increase finger taps/min (on average)?",
    "text": "Does caffeine increase finger taps/min (on average)?\nStudy Design:\n\n\n20 male college students students were trained to tap their fingers at a rapid rate.\nEach then drank 2 cups of coffee (double-blind)\n\nControl group: decaf\nCaffeine group: ~ 200 mg caffeine\n\nAfter 2 hours, students were tested.\nTaps/minute recorded\n\n\n\nHand, David J.; Daly, Fergus; McConway, K.; Lunn, D. and Ostrowski, E. (1993). A handbook of small data sets. London, U.K.: Chapman and Hall.\n\n\n\nLoad the data from the csv file CaffeineTaps.csv\nThe code below is for when the data file is in a folder called data that is in your R project folder (your working directory)\n\n\n\nCaffTaps &lt;- read_csv(here::here(\"data\", \"CaffeineTaps.csv\"))\n\nRows: 20 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Group\ndbl (1): Taps\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(CaffTaps)\n\nRows: 20\nColumns: 2\n$ Taps  &lt;dbl&gt; 246, 248, 250, 252, 248, 250, 246, 248, 245, 250, 242, 245, 244,…\n$ Group &lt;chr&gt; \"Caffeine\", \"Caffeine\", \"Caffeine\", \"Caffeine\", \"Caffeine\", \"Caf…"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#eda-explore-the-finger-taps-data",
    "href": "slides_md/Day11_bsta511_md.html#eda-explore-the-finger-taps-data",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "EDA: Explore the finger taps data",
    "text": "EDA: Explore the finger taps data\n\n\nDotplot of taps/minute stratified by group\n\nggplot(CaffTaps, aes(x=Taps)) +\n  geom_dotplot() +\n  facet_wrap(vars(Group), ncol=1)\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\nSummary statistics stratified by group\n\n# get_summary_stats() from rstatix package\nsumstats &lt;- CaffTaps %&gt;% \n  group_by(Group) %&gt;% \n  get_summary_stats(type = \"mean_sd\") \nsumstats %&gt;% gt()\n\n\n\n\n\n  \n    \n      Group\n      variable\n      n\n      mean\n      sd\n    \n  \n  \n    Caffeine\nTaps\n10\n248.3\n2.214\n    NoCaffeine\nTaps\n10\n244.8\n2.394\n  \n  \n  \n\n\n\ndiff(sumstats$mean)\n\n[1] -3.5"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#step-2-null-alternative-hypotheses",
    "href": "slides_md/Day11_bsta511_md.html#step-2-null-alternative-hypotheses",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step 2: Null & Alternative Hypotheses",
    "text": "Step 2: Null & Alternative Hypotheses\n\nQuestion: Is there evidence to support that drinking caffeine increases the number of finger taps/min?\n\n\n\nNull and alternative hypotheses in words\nInclude as much context as possible\n\n\n\\(H_0\\): The population difference in mean finger taps/min between the caffeine and control groups is …\n\\(H_A\\): The population difference in mean finger taps/min between the caffeine and control groups is …\n\n\nNull and alternative hypotheses in symbols\n\\[\\begin{align}\nH_0:& \\mu_{caff} - \\mu_{ctrl} = \\\\\nH_A:& \\mu_{caff} - \\mu_{ctrl} \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#step-3-test-statistic-part-1",
    "href": "slides_md/Day11_bsta511_md.html#step-3-test-statistic-part-1",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step 3: Test statistic (part 1)",
    "text": "Step 3: Test statistic (part 1)\nRecall that in general the test statistic has the form:\n\\[\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\] Thus, for a two sample independent means test, we have:\n\\[\\text{test statistic} = \\frac{\\bar{x}_1 - \\bar{x}_2 - 0}{SE_{\\bar{x}_1 - \\bar{x}_2}}\\]\n\nWhat is the formula for \\(SE_{\\bar{x}_1 - \\bar{x}_2}\\)?\nWhat is the probability distribution of the test statistic?\nWhat assumptions need to be satisfied?"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#what-distribution-does-barx_1---barx_2-have",
    "href": "slides_md/Day11_bsta511_md.html#what-distribution-does-barx_1---barx_2-have",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "What distribution does \\(\\bar{X}_1 - \\bar{X}_2\\) have?",
    "text": "What distribution does \\(\\bar{X}_1 - \\bar{X}_2\\) have?\n\n\nLet \\(\\bar{X}_1\\) and \\(\\bar{X}_2\\) be the means of random samples from two independent groups, with parameters shown in table:\n\n\n\n\n\nGroup 1\nGroup 2\n\n\n\n\nsample size\n\\(n_1\\)\n\\(n_2\\)\n\n\npop mean\n\\(\\mu_1\\)\n\\(\\mu_2\\)\n\n\npop sd\n\\(\\sigma_1\\)\n\\(\\sigma_2\\)\n\n\n\n\n\n\nSome theoretical statistics:\n\nIf \\(\\bar{X}_1\\) and \\(\\bar{X}_2\\) are independent normal r.v.’s, then \\(\\bar{X}_1 - \\bar{X}_2\\) is also normal\nWhat is the mean of \\(\\bar{X}_1 - \\bar{X}_2\\)?\n\n\\[E[\\bar{X}_1 - \\bar{X}_2] = E[\\bar{X}_1] - E[\\bar{X}_2] = \\mu_1-\\mu_2\\]\n\nWhat is the standard deviation of \\(\\bar{X}_1 - \\bar{X}_2\\)?\n\n\\[\\begin{align}\nVar(\\bar{X}_1 - \\bar{X}_2) &= Var(\\bar{X}_1) + Var(\\bar{X}_2) = \\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2} \\\\\nSD(\\bar{X}_1 - \\bar{X}_2) &= \\sqrt{\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}}\n\\end{align}\\]"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#step-3-test-statistic-part-2",
    "href": "slides_md/Day11_bsta511_md.html#step-3-test-statistic-part-2",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step 3: Test statistic (part 2)",
    "text": "Step 3: Test statistic (part 2)\n\n\n\n\\[\nt_{\\bar{x}_1 - \\bar{x}_2} = \\frac{\\bar{x}_1 - \\bar{x}_2 - 0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\]\n\n\n\\(\\bar{x}_1, \\bar{x}_2\\) are the sample means\n\\(\\mu_0=0\\) is the mean value specified in \\(H_0\\)\n\\(s_1, s_2\\) are the sample SD’s\n\\(n_1, n_2\\) are the sample sizes\n\n\n\n\nStatistical theory tells us that \\(t_{\\bar{x}_1 - \\bar{x}_2}\\) follows a student’s t-distribution with\n\n\\(df \\approx\\) smaller of \\(n_1-1\\) and \\(n_2-1\\)\nthis is a conservative estimate (smaller than actual \\(df\\) )\n\n\nAssumptions:\n\nIndependent observations & samples\n\nThe observations were collected independently.\nIn particular, the observations from the two groups were not paired in any meaningful way.\n\nApproximately normal samples or big n’s\n\nThe distributions of the samples should be approximately normal\nor both their sample sizes should be at least 30."
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#step-3-test-statistic-part-3",
    "href": "slides_md/Day11_bsta511_md.html#step-3-test-statistic-part-3",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step 3: Test statistic (part 3)",
    "text": "Step 3: Test statistic (part 3)\n\n\n\n\n\n\n\n\n  \n    \n      Group\n      variable\n      n\n      mean\n      sd\n    \n  \n  \n    Caffeine\nTaps\n10\n248.3\n2.214\n    NoCaffeine\nTaps\n10\n244.8\n2.394\n  \n  \n  \n\n\n\n\n\\[\n\\text{test statistic} = t_{\\bar{x}_1 - \\bar{x}_2} = \\frac{\\bar{x}_1 - \\bar{x}_2 - 0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\]\n\n\n\n\n\n\n\nBased on the value of the test statistic, do you think we are going to reject or fail to reject \\(H_0\\)?"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#step-3b-assumptions-satisfied",
    "href": "slides_md/Day11_bsta511_md.html#step-3b-assumptions-satisfied",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step “3b”: Assumptions satisfied?",
    "text": "Step “3b”: Assumptions satisfied?\n\n\nAssumptions:\n\nIndependent observations & samples\n\nThe observations were collected independently.\nIn particular, the observations from the two groups were not paired in any meaningful way.\n\nApproximately normal samples or big n’s\n\nThe distributions of the samples should be approximately normal\nor both their sample sizes should be at least 30.\n\n\n\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`."
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#step-4-p-value",
    "href": "slides_md/Day11_bsta511_md.html#step-4-p-value",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate the p-value:"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#step-5-conclusion-to-hypothesis-test",
    "href": "slides_md/Day11_bsta511_md.html#step-5-conclusion-to-hypothesis-test",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\\[\\begin{align}\nH_0:& \\mu_{caff} - \\mu_{ctrl} = 0\\\\\nH_A:& \\mu_{caff} - \\mu_{ctrl} &gt; 0\\\\\n\\end{align}\\]\n\nRecall the \\(p\\)-value = 0.00397\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nStats class conclusion\n\nThere is sufficient evidence that the (population) difference in mean finger taps/min with vs. without caffeine is greater than 0 ( \\(p\\)-value = 0.004).\n\nMore realistic manuscript conclusion:\n\nThe mean finger taps/min were 244.8 (SD = 2.4) and 248.3 (SD = 2.2) for the control and caffeine groups, and the increase of 3.5 taps/min was statistically discrenible ( \\(p\\)-value = 0.004)."
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#ci-for-the-mean-difference-in-cholesterol-levels",
    "href": "slides_md/Day11_bsta511_md.html#ci-for-the-mean-difference-in-cholesterol-levels",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "95% CI for the mean difference in cholesterol levels",
    "text": "95% CI for the mean difference in cholesterol levels\n\n\n\n\n\n\n  \n    \n      Group\n      variable\n      n\n      mean\n      sd\n    \n  \n  \n    Caffeine\nTaps\n10\n248.3\n2.214\n    NoCaffeine\nTaps\n10\n244.8\n2.394\n  \n  \n  \n\n\n\n\n\n\nCI for \\(\\mu_{caff} - \\mu_{ctrl}\\):\n\\[\\bar{x}_{caff} - \\bar{x}_{ctrl} \\pm t^* \\cdot \\sqrt{\\frac{s_{caff}^2}{n_{caff}}+\\frac{s_{ctrl}^2}{n_{ctrl}}}\\]\n\n\n\n\n  \n\nInterpretation:\nWe are 95% confident that the (population) difference in mean finger taps/min between the caffeine and control groups is between 1.167 mg/dL and 5.833 mg/dL.\n\nBased on the CI, is there evidence that drinking caffeine made a difference in finger taps/min? Why or why not?"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#r-2-sample-t-test-with-long-data",
    "href": "slides_md/Day11_bsta511_md.html#r-2-sample-t-test-with-long-data",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "R: 2-sample t-test (with long data)",
    "text": "R: 2-sample t-test (with long data)\n\nThe CaffTaps data are in a long format, meaning that\n\nall of the outcome values are in one column and\nanother column indicates which group the values are from\n\nThis is a common format for data from multiple samples, especially if the sample sizes are different.\n\n\n\n(Taps_2ttest &lt;- t.test(formula = Taps ~ Group, \n                       alternative = \"greater\", \n                       data = CaffTaps))\n\n\n    Welch Two Sample t-test\n\ndata:  Taps by Group\nt = 3.3942, df = 17.89, p-value = 0.001628\nalternative hypothesis: true difference in means between group Caffeine and group NoCaffeine is greater than 0\n95 percent confidence interval:\n 1.711272      Inf\nsample estimates:\n  mean in group Caffeine mean in group NoCaffeine \n                   248.3                    244.8"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#tidy-the-t.test-output",
    "href": "slides_md/Day11_bsta511_md.html#tidy-the-t.test-output",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "tidy the t.test output",
    "text": "tidy the t.test output\n\n# use tidy command from broom package for briefer output that's a tibble\ntidy(Taps_2ttest) %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    3.5\n248.3\n244.8\n3.394168\n0.001627703\n17.89012\n1.711272\nInf\nWelch Two Sample t-test\ngreater\n  \n  \n  \n\n\n\n\n\nPull the p-value:\n\n\ntidy(Taps_2ttest)$p.value  # we can pull specific values from the tidy output\n\n[1] 0.001627703"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#r-2-sample-t-test-with-wide-data",
    "href": "slides_md/Day11_bsta511_md.html#r-2-sample-t-test-with-wide-data",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "R: 2-sample t-test (with wide data)",
    "text": "R: 2-sample t-test (with wide data)\n\n\n# make CaffTaps data wide: pivot_wider needs an ID column so that it \n# knows how to \"match\" values from the Caffeine and NoCaffeine groups\nCaffTaps_wide &lt;- CaffTaps %&gt;% \n  mutate(id = rep(1:10, 2)) %&gt;% #  \"fake\" IDs for pivot_wider step\n  pivot_wider(names_from = \"Group\",\n              values_from = \"Taps\")\n\nglimpse(CaffTaps_wide)\n\nRows: 10\nColumns: 3\n$ id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n$ Caffeine   &lt;dbl&gt; 246, 248, 250, 252, 248, 250, 246, 248, 245, 250\n$ NoCaffeine &lt;dbl&gt; 242, 245, 244, 248, 247, 248, 242, 244, 246, 242\n\nt.test(x = CaffTaps_wide$Caffeine, y = CaffTaps_wide$NoCaffeine, alternative = \"greater\") %&gt;% \n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    3.5\n248.3\n244.8\n3.394168\n0.001627703\n17.89012\n1.711272\nInf\nWelch Two Sample t-test\ngreater"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#why-are-the-dfs-in-the-r-output-different",
    "href": "slides_md/Day11_bsta511_md.html#why-are-the-dfs-in-the-r-output-different",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Why are the df’s in the R output different?",
    "text": "Why are the df’s in the R output different?\nFrom many slides ago:\n\nStatistical theory tells us that \\(t_{\\bar{x}_1 - \\bar{x}_2}\\) follows a student’s t-distribution with\n\n\\(df \\approx\\) smaller of \\(n_1-1\\) and \\(n_2-1\\)\nthis is a conservative estimate (smaller than actual \\(df\\) )\n\n\nThe actual degrees of freedom are calculated using Satterthwaite’s method:\n\\[\\nu = \\frac{[ (s_1^2/n_1) + (s_2^2/n_2) ]^2}\n{(s_1^2/n_1)^2/(n_1 - 1) + (s_2^2/n_2)^2/(n_2-1) }\n= \\frac{ [ SE_1^2 + SE_2^2 ]^2}{ SE_1^4/df_1 + SE_2^4/df_2 }\\]\n\nVerify the p-value in the R output using \\(\\nu\\) = 17.89012:\n\npt(3.3942, df = 17.89012, lower.tail = FALSE)\n\n[1] 0.001627588"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#pooled-standard-deviation-estimate",
    "href": "slides_md/Day11_bsta511_md.html#pooled-standard-deviation-estimate",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Pooled standard deviation estimate",
    "text": "Pooled standard deviation estimate\n\nSometimes we have reasons to believe that the population SD’s from the two groups are equal, such as when randomizing participants to two groups\n\n\n\n\nIn this case we can use a pooled SD:\n\n\\[s_{pooled}^2 = \\frac{s_1^2 (n_1-1) + s_2^2 (n_2-1)}{n_1 + n_2 - 2}\\]\n\n\n\\(n_1\\), \\(n_2\\) are the sample sizes, and\n\\(s_1\\), \\(s_2\\) are the sample standard deviations\nof the two groups\n\n\n\n\nWe use the pooled SD instead of \\(s_1^2\\) and \\(s_2^2\\) when calculating the standard error\n\n\\[SE = \\sqrt{\\frac{s_{pooled}^2}{n_1} + \\frac{s_{pooled}^2}{n_2}}= s_{pooled}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\]\n\n\nTest statistic with pooled SD:\n\\[t_{\\bar{x}_1 - \\bar{x}_2} = \\frac{\\bar{x}_1 - \\bar{x}_2 -0}{s_{pooled}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]\n\nCI with pooled SD:\n\\[(\\bar{x}_1 - \\bar{x}_2) \\pm t^{\\star} \\cdot s_{pooled} \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\]\n\n\n\nThe \\(t\\) distribution degrees of freedom are now:\n\n\\[df = (n_1 - 1) + (n_2 - 1) = n_1 + n_2 - 2.\\]"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#r-2-sample-t-test-with-pooled-sd",
    "href": "slides_md/Day11_bsta511_md.html#r-2-sample-t-test-with-pooled-sd",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "R: 2-sample t-test with pooled SD",
    "text": "R: 2-sample t-test with pooled SD\n\n# t-test with pooled SD\nt.test(formula = Taps ~ Group, \n       alternative = \"greater\", \n       var.equal = TRUE,  # pooled SD \n       data = CaffTaps) %&gt;% \n  tidy() %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      estimate\n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    3.5\n248.3\n244.8\n3.394168\n0.001616497\n18\n1.711867\nInf\nTwo Sample t-test\ngreater\n  \n  \n  \n\n\n\n# t-test without pooled SD\nt.test(formula = Taps ~ Group, \n       alternative = \"greater\", \n       var.equal = FALSE,  # default, NOT pooled SD \n       data = CaffTaps) %&gt;% \n  tidy() %&gt;% \n  gt()\n\n\n\n\n\n  \n    \n      estimate\n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    3.5\n248.3\n244.8\n3.394168\n0.001627703\n17.89012\n1.711272\nInf\nWelch Two Sample t-test\ngreater\n  \n  \n  \n\n\n\n\nSimilar output in this case - why??"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#whats-next",
    "href": "slides_md/Day11_bsta511_md.html#whats-next",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "What’s next?",
    "text": "What’s next?\n\nCI’s and hypothesis tests for different scenarios:\n\\[\\text{point estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\\(\\frac{s_d}{\\sqrt{n}}\\)\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\\(\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\) or pooled\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n???\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)\n???"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#critical-values",
    "href": "slides_md/Day11_bsta511_md.html#critical-values",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Critical values",
    "text": "Critical values\n\n\nCritical values are the cutoff values that determine whether a test statistic is statistically significant or not.\nIf a test statistic is greater in absolute value than the critical value, we reject \\(H_0\\)\n\n\n\n\n\n\n\n\n\n\n\n\nCritical values are determined by\n\nthe significance level \\(\\alpha\\),\nwhether a test is 1- or 2-sided, &\nthe probability distribution being used to calculate the p-value (such as normal or t-distribution).\n\nThe critical values in the figure should look very familiar!\n\nWhere have we used these before?\n\n\n\n\n\n\nHow can we calculate the critical values using R?"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#rejection-region",
    "href": "slides_md/Day11_bsta511_md.html#rejection-region",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Rejection region",
    "text": "Rejection region\n\nIf the absolute value of the test statistic is greater than the critical value, we reject \\(H_0\\)\n\nIn this case the test statistic is in the rejection region.\nOtherwise it’s in the nonrejection region.\n\n\n\n\n\n\n\nStats & Geospatial Analysis\n\n\n\n\nWhat do rejection regions look like for 1-sided tests?"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#justice-system-analogy",
    "href": "slides_md/Day11_bsta511_md.html#justice-system-analogy",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Justice system analogy",
    "text": "Justice system analogy\n\n\n\n\nType I and Type II Errors - Making Mistakes in the Justice System"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#type-i-ii-errors",
    "href": "slides_md/Day11_bsta511_md.html#type-i-ii-errors",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Type I & II Errors",
    "text": "Type I & II Errors\n\n\n\n\n\n\n\n\n\n\\(\\alpha\\) = probability of making a Type I error\n\nThis is the significance level (usually 0.05)\nSet before study starts\n\n\\(\\beta\\) = probability of making a Type II error\nIdeally we want\n\nsmall Type I & II errors and\nbig power\n\n\n\n\n\n\n\n\n\n\nApplet for visualizing Type I & II errors and power: https://rpsychologist.com/d3/NHST/"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#relationship-between-type-i-ii-errors",
    "href": "slides_md/Day11_bsta511_md.html#relationship-between-type-i-ii-errors",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Relationship between Type I & II errors",
    "text": "Relationship between Type I & II errors\n\nType I vs. Type II error\n\nDecreasing P(Type I error) leads to\n\nincreasing P(Type II error)\n\nWe typically keep P(Type I error) = \\(\\alpha\\) set to 0.05\n\n\nFrom the applet at https://rpsychologist.com/d3/NHST/"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#relationship-between-type-ii-errors-and-power",
    "href": "slides_md/Day11_bsta511_md.html#relationship-between-type-ii-errors-and-power",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Relationship between Type II errors and power",
    "text": "Relationship between Type II errors and power\n\nPower = P(correctly rejecting the null hypothesis)\n\n\n\n\n\nPower is also called the\n\ntrue positive rate,\nprobability of detection, or\nthe sensitivity of a test\n\n\n\n\n\n\n\n\n\n\n\nPower vs. Type II error\n\nPower = 1 - P(Type II error) = 1 - \\(\\beta\\)\nThus as \\(\\beta\\) = P(Type II error) decreases, the power increases\nP(Type II error) decreases as the mean of the alternative population shifts further away from the mean of the null population (effect size gets bigger).\nTypically want at least 80% power; 90% power is good"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#example-calculating-power",
    "href": "slides_md/Day11_bsta511_md.html#example-calculating-power",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Example calculating power",
    "text": "Example calculating power\n\n\nSuppose the mean of the null population is 0 ( \\(H_0: \\mu=0\\) ) with standard error 1\nFind the power of a 2-sided test if the actual \\(\\mu=3\\), assuming the SE doesn’t change.\n\n\n\n\n\n\n\n\n\n\nPower = \\(P(\\)Reject \\(H_0\\) when alternative pop is \\(N(3,1))\\)\nWhen \\(\\alpha\\) = 0.05, we reject \\(H_0\\) when the test statistic z is at least 1.96\nThus for \\(X\\sim N(3,1)\\) we need to calculate \\(P(X \\le -1.96) + P(X \\ge 1.96)\\):\n\n\n\n\n# left tail + right tail:\npnorm(-1.96, mean=3, sd=1, lower.tail=TRUE) + pnorm(1.96, mean=3, sd=1, lower.tail=FALSE)\n\n[1] 0.8508304\n\n\n\n\nThe left tail probability pnorm(-1.96, mean=3, sd=1, lower.tail=TRUE) is essentially 0 in this case.\n\n\n\nNote that this power calculation specified the value of the SE instead of the standard deviation and sample size \\(n\\) individually."
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#sample-size-calculation-for-testing-one-mean",
    "href": "slides_md/Day11_bsta511_md.html#sample-size-calculation-for-testing-one-mean",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Sample size calculation for testing one mean",
    "text": "Sample size calculation for testing one mean\n\n\nRecall in our body temperature example that \\(\\mu_0=98.6\\) °F and \\(\\bar{x}= 98.25\\) °F.\n\nThe p-value from the hypothesis test was highly significant (very small).\nWhat would the sample size \\(n\\) need to be for 80% power?\n\nCalculate \\(n\\),\n\ngiven \\(\\alpha\\), power ( \\(1-\\beta\\) ), “true” alternative mean \\(\\mu\\), and null \\(\\mu_0\\),\nassuming the test statistic is normal (instead of t-distribution):\n\n\n\n\n\\[n=\\left(s\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{\\mu-\\mu_0}\\right)^2\\]\n\n\nmu &lt;- 98.25\nmu0 &lt;- 98.6\nsd &lt;- 0.73\nalpha &lt;- 0.05\nbeta &lt;- 0.20\nn &lt;- (sd*(qnorm(1-alpha/2) + qnorm(1-beta)) / (mu-mu0))^2\nn\n\n[1] 34.14423\n\nceiling(n)  # always round UP to the next highest integer \n\n[1] 35\n\n\n\n\nWe would only need a sample size of 35 for 80% power!\nHowever, this is an under-estimate since we used the normal instead of t-distribution.\n\n\nSee http://powerandsamplesize.com/Calculators/Test-1-Mean/1-Sample-Equality."
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#power-calculation-for-testing-one-mean",
    "href": "slides_md/Day11_bsta511_md.html#power-calculation-for-testing-one-mean",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "Power calculation for testing one mean",
    "text": "Power calculation for testing one mean\n\nConversely, we can calculate how much power we had in our body temperature one-sample test, given the sample size of 130.\n\nCalculate power,\n\ngiven \\(\\alpha\\), \\(n\\), “true” alternative mean \\(\\mu\\), and null \\(\\mu_0\\),\nassuming the test statistic is normal (instead of t-distribution)\n\n\n\\[1-\\beta=\n        \\Phi\\left(z-z_{1-\\alpha/2}\\right)+\\Phi\\left(-z-z_{1-\\alpha/2}\\right)\n        \\quad ,\\quad \\text{where } z=\\frac{\\mu-\\mu_0}{s/\\sqrt{n}}\\]\n\\(\\Phi\\) is the probability for a standard normal distribution\n\nmu &lt;- 98.25; mu0 &lt;- 98.6; sd &lt;- 0.73; alpha &lt;- 0.05; n &lt;- 130\n(z &lt;- (mu-mu0) / (sd/sqrt(n)) )\n\n[1] -5.466595\n\nPower &lt;- pnorm(z-qnorm(1-alpha/2)) + pnorm(-z-qnorm(1-alpha/2))\nPower\n\n[1] 0.9997731\n\n\nIf the population mean is 98.2 instead of 98.6, we have a 99.98% chance of correctly rejecting \\(H_0\\) when the sample size is 130."
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#r-package-pwr-for-power-analyses",
    "href": "slides_md/Day11_bsta511_md.html#r-package-pwr-for-power-analyses",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "R package pwr for power analyses",
    "text": "R package pwr for power analyses\n\n\nUse pwr.t.test for both one- and two-sample t-tests.\n\nSpecify all parameters except for the one being solved for.\n\npwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL,\ntype = c(\"two.sample\", \"one.sample\", \"paired\"),\nalternative = c(\"two.sided\", \"less\", \"greater\"))\nd is Cohen’s d effect size: small = 0.2, medium = 0.5, large = 0.8\n\n\nOne-sample test (or paired t-test):\n\n\n\n\\[d = \\frac{\\mu-\\mu_0}{s}\\]\n\n\n\n\nTwo-sample test (independent):\n\n\n\n\\[d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{pooled}}\\]\n\n\n\n\n\n\\(\\bar{x}_1 - \\bar{x}_2\\) is the difference in means between the two groups that one would want to be able to detect as being significant,\n\\(s_{pooled}\\) is the pooled SD between the two groups - often assume have same sd in each group\nR package pwr for basic statistical tests\n\nhttps://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#pwr-sample-size-for-one-mean-test",
    "href": "slides_md/Day11_bsta511_md.html#pwr-sample-size-for-one-mean-test",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "pwr: sample size for one mean test",
    "text": "pwr: sample size for one mean test\n\npwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL,\ntype = c(\"two.sample\", \"one.sample\", \"paired\"), alternative = c(\"two.sided\", \"less\", \"greater\"))\n\n\nd is Cohen’s d effect size: \\(d = \\frac{\\mu-\\mu_0}{s}\\)\n\nSpecify all parameters except for the sample size:\n\n\n\nlibrary(pwr)\nt.n &lt;- pwr.t.test(\n  d = (98.6-98.25)/0.73, \n  sig.level = 0.05, \n  power = 0.80, \n  type = \"one.sample\")\n\nt.n\n\n\n     One-sample t test power calculation \n\n              n = 36.11196\n              d = 0.4794521\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n\nplot(t.n)"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#pwr-power-for-one-mean-test",
    "href": "slides_md/Day11_bsta511_md.html#pwr-power-for-one-mean-test",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "pwr: power for one mean test",
    "text": "pwr: power for one mean test\n\npwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL,\ntype = c(\"two.sample\", \"one.sample\", \"paired\"), alternative = c(\"two.sided\", \"less\", \"greater\"))\n\n\nd is Cohen’s d effect size: \\(d = \\frac{\\mu-\\mu_0}{s}\\)\n\nSpecify all parameters except for the power:\n\n\n\nt.power &lt;- pwr.t.test(\n  d = (98.6-98.25)/0.73, \n  sig.level = 0.05, \n  # power = 0.80, \n  n = 130,\n  type = \"one.sample\")\n\nt.power\n\n\n     One-sample t test power calculation \n\n              n = 130\n              d = 0.4794521\n      sig.level = 0.05\n          power = 0.9997354\n    alternative = two.sided\n\n\n\n\nplot(t.power)"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#pwr-two-sample-t-test-sample-size",
    "href": "slides_md/Day11_bsta511_md.html#pwr-two-sample-t-test-sample-size",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "pwr: Two-sample t-test: sample size",
    "text": "pwr: Two-sample t-test: sample size\n\npwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL,\ntype = c(\"two.sample\", \"one.sample\", \"paired\"), alternative = c(\"two.sided\", \"less\", \"greater\"))\n\n\n\nd is Cohen’s d effect size: \\(d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{pooled}}\\)\n\nExample: Suppose the data collected for the caffeine taps study were pilot day for a larger study. Investigators want to know what sample size they would need to detect a 2 point difference between the two groups. Assume the SD in both groups is 2.3.\nSpecify all parameters except for the sample size:\n\n\n\n\n\nt2.n &lt;- pwr.t.test(\n  d = 2/2.3, \n  sig.level = 0.05, \n  power = 0.80, \n  type = \"two.sample\") \n\nt2.n\n\n\n     Two-sample t test power calculation \n\n              n = 21.76365\n              d = 0.8695652\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nplot(t2.n)"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#pwr-two-sample-t-test-power",
    "href": "slides_md/Day11_bsta511_md.html#pwr-two-sample-t-test-power",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "pwr: Two-sample t-test: power",
    "text": "pwr: Two-sample t-test: power\n\npwr.t.test(n = NULL, d = NULL, sig.level = 0.05, power = NULL,\ntype = c(\"two.sample\", \"one.sample\", \"paired\"), alternative = c(\"two.sided\", \"less\", \"greater\"))\n\n\n\nd is Cohen’s d effect size: \\(d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{pooled}}\\)\n\nExample: Suppose the data collected for the caffeine taps study were pilot day for a larger study. Investigators want to know what sample size they would need to detect a 2 point difference between the two groups. Assume the SD in both groups is 2.3.\nSpecify all parameters except for the power:\n\n\n\n\n\nt2.power &lt;- pwr.t.test(\n  d = 2/2.3, \n  sig.level = 0.05, \n  # power = 0.80, \n  n = 22,\n  type = \"two.sample\") \n\nt2.power\n\n\n     Two-sample t test power calculation \n\n              n = 22\n              d = 0.8695652\n      sig.level = 0.05\n          power = 0.8044288\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n\nplot(t2.power)"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#what-information-do-we-need-for-a-power-or-sample-size-calculation",
    "href": "slides_md/Day11_bsta511_md.html#what-information-do-we-need-for-a-power-or-sample-size-calculation",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "What information do we need for a power (or sample size) calculation?",
    "text": "What information do we need for a power (or sample size) calculation?\n\n\nThere are 4 pieces of information:\n\nLevel of significance \\(\\alpha\\)\n\nUsually fixed to 0.05\n\nPower\n\nIdeally at least 0.80\n\nSample size\nEffect size (expected change)\n\nGiven any 3 pieces of information, we can solve for the 4th.\n\n\npwr.t.test(\n  d = (98.6-98.25)/0.73,\n  sig.level = 0.05, \n  # power = 0.80, \n  n=130,\n  type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 130\n              d = 0.4794521\n      sig.level = 0.05\n          power = 0.9997354\n    alternative = two.sided"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#more-software-for-power-and-sample-size-calculations-pass",
    "href": "slides_md/Day11_bsta511_md.html#more-software-for-power-and-sample-size-calculations-pass",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "More software for power and sample size calculations: PASS",
    "text": "More software for power and sample size calculations: PASS\n\nPASS is a very powerful (& expensive) software that does power and sample size calculations for many advanced statistical modeling techniques.\n\nEven if you don’t have access to PASS, their documentation is very good and free online.\nDocumentation includes formulas and references.\nPASS documentation for powering means\n\nOne mean, paired means, two independent means\n\n\nOne-sample t-test documentation: https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/PASS/One-Sample_T-Tests.pdf"
  },
  {
    "objectID": "slides_md/Day11_bsta511_md.html#octri-berd-power-sample-size-presentations",
    "href": "slides_md/Day11_bsta511_md.html#octri-berd-power-sample-size-presentations",
    "title": "Day 11: Inference for difference in means from two independent samples and Power (Sections 5.3, 5.4)",
    "section": "OCTRI-BERD power & sample size presentations",
    "text": "OCTRI-BERD power & sample size presentations\n\n\nPower and Sample Size 101\n\nPresented by Meike Niederhausen; April 13, 2023\nSlides: http://bit.ly/PSS101-BERD-April2023\nRecording\n\nPower and Sample Size for Clinical Trials: An Introduction\n\nPresented by Yiyi Chen; Feb 18, 2021\nSlides: http://bit.ly/PSS-ClinicalTrials\nRecording\n\nPlanning a Study with Power and Sample Size Considerations in Mind\n\nPresented by David Yanez; May 29, 2019\nSlides\nRecording\n\nPower and Sample Size Simulations in R\n\nPresented by Robin Baudier; Sept 21, 2023\nSlides\nRecording"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html",
    "href": "slides_md/Day12_bsta511_md.html",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "",
    "text": "With code folding we can hide or show the code in the html output by clicking on the Code buttons in the html file.\nNote the &lt;/&gt; Code button on the top right of the html output.\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee more information at https://quarto.org/docs/output-formats/html-code.html#folding-code"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#moritzs-tip-of-the-day-code-folding",
    "href": "slides_md/Day12_bsta511_md.html#moritzs-tip-of-the-day-code-folding",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "",
    "text": "With code folding we can hide or show the code in the html output by clicking on the Code buttons in the html file.\nNote the &lt;/&gt; Code button on the top right of the html output.\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee more information at https://quarto.org/docs/output-formats/html-code.html#folding-code"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#where-are-we",
    "href": "slides_md/Day12_bsta511_md.html#where-are-we",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Where are we?",
    "text": "Where are we?\n\nCI’s and hypothesis tests for different scenarios:\n\\[\\text{point estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\\(\\frac{s_d}{\\sqrt{n}}\\)\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\\(\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\) or pooled\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n???\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)\n???"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#goals-for-today-sections-8.1-8.2",
    "href": "slides_md/Day12_bsta511_md.html#goals-for-today-sections-8.1-8.2",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Goals for today (Sections 8.1-8.2)",
    "text": "Goals for today (Sections 8.1-8.2)\n\nStatistical inference for a single proportion or the difference of two (independent) proportions\n\nSampling distribution for a proportion or difference in proportions\nWhat are \\(H_0\\) and \\(H_a\\)?\nWhat are the SE’s for \\(\\hat{p}\\) and \\(\\hat{p}_1-\\hat{p}_2\\)?\nHypothesis test\nConfidence Interval\nHow are the SE’s different for a hypothesis test & CI?\nHow to run proportions tests in R\nPower & sample size for proportions tests (extra material)"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#motivating-example",
    "href": "slides_md/Day12_bsta511_md.html#motivating-example",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Motivating example",
    "text": "Motivating example\n\nOne proportion\n\nA 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year.\n\nWhat is the CI for the proportion?\nThe study also reported that 36% of noncollege young males had participated in sports betting. Is the proportion for male college students different from 0.36?\n\n\nTwo proportions\n\nThere were 214 men in the sample of noncollege young males (36% participated in sports betting in the previous year).\nCompare the difference in proportions between the college and noncollege young males.\n\nCI & Hypothesis test\n\n\n\n\nBarnes GM, Welte JW, Hoffman JH, Tidwell MC. Comparisons of gambling and alcohol use among college students and noncollege young people in the United States. J Am Coll Health. 2010 Mar-Apr;58(5):443-52. doi: 10.1080/07448480903540499. PMID: 20304756; PMCID: PMC4104810."
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#steps-in-a-hypothesis-test",
    "href": "slides_md/Day12_bsta511_md.html#steps-in-a-hypothesis-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Steps in a Hypothesis Test",
    "text": "Steps in a Hypothesis Test\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#step-2-null-alternative-hypotheses",
    "href": "slides_md/Day12_bsta511_md.html#step-2-null-alternative-hypotheses",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 2: Null & Alternative Hypotheses",
    "text": "Step 2: Null & Alternative Hypotheses\n\nNull and alternative hypotheses in words and in symbols.\n\n\nOne sample test\n\n\\(H_0\\): The population proportion of young male college students that participated in sports betting in the previous year is 0.36.\n\\(H_A\\): The population proportion of young male college students that participated in sports betting in the previous year is not 0.36.\n\n\\[\\begin{align}\nH_0:& p = 0.36\\\\\nH_A:& p \\neq 0.36\\\\\n\\end{align}\\]\n\n\n\nTwo samples test\n\n\\(H_0\\): The difference in population proportions of young male college and noncollege students that participated in sports betting in the previous year is 0.\n\\(H_A\\): The difference in population proportions of young male college and noncollege students that participated in sports betting in the previous year is not 0.\n\n\\[\\begin{align}\nH_0:& p_{coll} - p_{noncoll} = 0\\\\\nH_A:& p_{coll} - p_{noncoll} \\neq 0\\\\\n\\end{align}\\]"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#sampling-distribution-of-hatp",
    "href": "slides_md/Day12_bsta511_md.html#sampling-distribution-of-hatp",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Sampling distribution of \\(\\hat{p}\\)",
    "text": "Sampling distribution of \\(\\hat{p}\\)\n\n\\(\\hat{p}=\\frac{X}{n}\\) where \\(X\\) is the number of “successes” and \\(n\\) is the sample size.\n\\(X \\sim Bin(n,p)\\), where \\(p\\) is the population proportion.\nFor \\(n\\) “big enough”, the normal distribution can be used to approximate a binomial distribution:\n\n\\[Bin(n,p) \\rightarrow N\\Big(\\mu = np, \\sigma = \\sqrt{np(1-p)} \\Big)\\]\n\nSince \\(\\hat{p}=\\frac{X}{n}\\) is a linear transformation of \\(X\\), we have for large n:\n\n\\[\\hat{p} \\sim N\\Big(\\mu_{\\hat{p}} = p, \\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}} \\Big)\\]\n\nHow we apply this result to CI’s and test statistics is different!!!"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#step-3-test-statistic",
    "href": "slides_md/Day12_bsta511_md.html#step-3-test-statistic",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 3: Test statistic",
    "text": "Step 3: Test statistic\n\nSampling distribution of \\(\\hat{p}\\) if we assume \\(H_0: p=p_0\\) is true:\n\\[\\hat{p} \\sim N\\Big(\\mu_{\\hat{p}} = p, \\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}} \\Big)\n\\sim N\\Big(\n\\mu_{\\hat{p}}=p_0, \\sigma_{\\hat{p}}=\\sqrt{\\frac{p_0\\cdot(1-p_0)}{n}}\n\\Big)\\]\nTest statistic for a one sample proportion test:\n\\[\n\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\n= z_{\\hat{p}} = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0\\cdot(1-p_0)}{n}}}\n\\]\n\n\n\n\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year.\n What is the test statistic when testing \\(H_0: p=0.36\\) vs.  \\(H_A: p \\neq 0.36\\)?\n\n\\[\\begin{align}\nz_{\\hat{p}} &= \\frac{94/269 - 0.36}{\\sqrt{\\frac{0.36\\cdot(1-0.36)}{269}}} \\\\\n& -0.3607455\n\\end{align}\\]"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#step-3b-conditions-satisfied",
    "href": "slides_md/Day12_bsta511_md.html#step-3b-conditions-satisfied",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step “3b”: Conditions satisfied?",
    "text": "Step “3b”: Conditions satisfied?\nConditions:\n\nIndependent observations?\n\nThe observations were collected independently.\n\nThe number of expected successes and expected failures is at least 10.\n\n\\(n_1 p_0 \\ge 10, \\ \\ n_1(1-p_0)\\ge 10\\)\n\n\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year.\n Testing \\(H_0: p=0.36\\) vs. \\(H_A: p \\neq 0.36\\).\n Are the conditions satisfied?"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#step-4-p-value",
    "href": "slides_md/Day12_bsta511_md.html#step-4-p-value",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate the p-value:\n\\[\\begin{align}\n2 &\\cdot P(\\hat{p}&lt;0.35) \\\\\n&= 2 \\cdot P\\Big(Z_{\\hat{p}} &lt; \\frac{94/269 - 0.36}{\\sqrt{\\frac{0.36\\cdot(1-0.36)}{269}}}\\Big)\\\\\n&=2 \\cdot P(Z_{\\hat{p}} &lt; -0.3607455)\\\\\n&= 0.7182897\n\\end{align}\\]\n\n2*pnorm(-0.3607455)\n\n[1] 0.7182897"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#step-5-conclusion-to-hypothesis-test",
    "href": "slides_md/Day12_bsta511_md.html#step-5-conclusion-to-hypothesis-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\\[\\begin{align}\nH_0:& p = 0.36\\\\\nH_A:& p \\neq 0.36\\\\\n\\end{align}\\]\n\nRecall the \\(p\\)-value = 0.7182897\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nStats class conclusion\n\nThere is insufficient evidence that the (population) proportion of young male college students that participated in sports betting in the previous year is different than 0.36 ( \\(p\\)-value = 0.72).\n\nMore realistic manuscript conclusion:\n\nIn a sample of 269 male college students, 35% had participated in sports betting in the previous year, which is not different from 36% ( \\(p\\)-value = 0.72)."
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#ci-for-population-proportion",
    "href": "slides_md/Day12_bsta511_md.html#ci-for-population-proportion",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "95% CI for population proportion",
    "text": "95% CI for population proportion\n\n\n\nWhat to use for SE in CI formula?\n\n\\[\\hat{p} \\pm z^* \\cdot SE_{\\hat{p}}\\]\n\n\n\n\nSampling distribution of \\(\\hat{p}\\):\n\n\\[\\hat{p} \\sim N\\Big(\\mu_{\\hat{p}} = p, \\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}} \\Big)\\]\n\n\n\n\nProblem: We don’t know what \\(p\\) is - it’s what we’re estimating with the CI.\nSolution: approximate \\(p\\) with \\(\\hat{p}\\):\n\n\\[SE_{\\hat{p}} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\n\n\n\n\n\n\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year.\nFind the 95% CI for the population proportion.\n\n\\[\\begin{align}\n94/269 &\\pm 1.96 \\cdot SE_{\\hat{p}}\\\\\nSE_{\\hat{p}} &= \\sqrt{\\frac{(94/269)(1-94/269)}{269}}\\\\\n(0.293 &, 0.407)\n\\end{align}\\]\n\n\nInterpretation:\nWe are 95% confident that the (population) proportion of young male college students that participated in sports betting in the previous year is in (0.29, 0.41)."
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#conditions-for-one-proportion-test-vs.-ci",
    "href": "slides_md/Day12_bsta511_md.html#conditions-for-one-proportion-test-vs.-ci",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Conditions for one proportion: test vs. CI",
    "text": "Conditions for one proportion: test vs. CI\n\n\n\nHypothesis test conditions\n\nIndependent observations\n\nThe observations were collected independently.\n\n\n\n\nThe number of expected successes and expected failures is at least 10.\n\n\\[n_1 p_0 \\ge 10, \\ \\ n_1(1-p_0)\\ge 10\\]\n\n\n\nConfidence interval conditions\n\nIndependent observations\n\nThe observations were collected independently.\n\n\n\n\nThe number of successes and failures is at least 10:\n\n\\[n_1\\hat{p}_1 \\ge 10, \\ \\ n_1(1-\\hat{p}_1)\\ge 10\\]"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#sampling-distribution-of-hatp_1-hatp_2",
    "href": "slides_md/Day12_bsta511_md.html#sampling-distribution-of-hatp_1-hatp_2",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Sampling distribution of \\(\\hat{p}_1-\\hat{p}_2\\)",
    "text": "Sampling distribution of \\(\\hat{p}_1-\\hat{p}_2\\)\n\n\n\\(\\hat{p}_1=\\frac{X_1}{n_1}\\) and \\(\\hat{p}_2=\\frac{X_2}{n_2}\\),\n\n\\(X_1\\) & \\(X_2\\) are the number of “successes”\n\\(n_1\\) & \\(n_2\\) are the sample sizes of the 1st & 2nd samples\n\n\n\n\nEach \\(\\hat{p}\\) can be approximated by a normal distribution, for “big enough” \\(n\\)\nSince the difference of independent normal random variables is also normal, it follows that for “big enough” \\(n_1\\) and \\(n_2\\)\n\n\\[\\hat{p}_1 - \\hat{p}_2 \\sim N \\Big(\\mu_{\\hat{p}_1 - \\hat{p}_2} = p_1 - p_2, ~~\n\\sigma_{\\hat{p}_1 - \\hat{p}_2} =\n\\sqrt{\n\\frac{p_1\\cdot(1-p_1)}{n_1} + \\frac{p_2\\cdot(1-p_2)}{n_2}}\n\\Big)\\]\nwhere \\(p_1\\) & \\(p_2\\) are the population proportions, respectively.\n\nHow we apply this result to CI’s and test statistics is different!!!"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#step-3-test-statistic-12",
    "href": "slides_md/Day12_bsta511_md.html#step-3-test-statistic-12",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 3: Test statistic (1/2)",
    "text": "Step 3: Test statistic (1/2)\n\nSampling distribution of \\(\\hat{p}_1 - \\hat{p}_2\\): \\[\\hat{p}_1 - \\hat{p}_2 \\sim N \\Big(\\mu_{\\hat{p}_1 - \\hat{p}_2} = p_1 - p_2, ~~\n\\sigma_{\\hat{p}_1 - \\hat{p}_2} =\n\\sqrt{\n\\frac{p_1\\cdot(1-p_1)}{n_1} + \\frac{p_2\\cdot(1-p_2)}{n_2}}\n\\Big)\\]\nSince we assume \\(H_0: p_1 - p_2 = 0\\) is true, we “pool” the proportions of the two samples to calculate the SE:\n\\[\\text{pooled proportion} = \\hat{p}_{pool} = \\dfrac{\\text{total number of successes} }{ \\text{total number of cases}} = \\frac{x_1+x_2}{n_1+n_2}\\]\nTest statistic:\n\\[\n\\text{test statistic} = z_{\\hat{p}_1 - \\hat{p}_2} = \\frac{\\hat{p}_1 - \\hat{p}_2 - 0}{\\sqrt{\\frac{\\hat{p}_{pool}\\cdot(1-\\hat{p}_{pool})}{n_1} + \\frac{\\hat{p}_{pool}\\cdot(1-\\hat{p}_{pool})}{n_2}}}\n\\]"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#step-3-test-statistic-22",
    "href": "slides_md/Day12_bsta511_md.html#step-3-test-statistic-22",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 3: Test statistic (2/2)",
    "text": "Step 3: Test statistic (2/2)\n\n\\[\n\\text{test statistic} = z_{\\hat{p}_1 - \\hat{p}_2} = \\frac{\\hat{p}_1 - \\hat{p}_2 - 0}{\\sqrt{\\frac{\\hat{p}_{pool}\\cdot(1-\\hat{p}_{pool})}{n_1} + \\frac{\\hat{p}_{pool}\\cdot(1-\\hat{p}_{pool})}{n_2}}}\n\\]\n\\[\\text{pooled proportion} = \\hat{p}_{pool} = \\dfrac{\\text{total number of successes} }{ \\text{total number of cases}} = \\frac{x_1+x_2}{n_1+n_2}\\]\n\n\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year, and out of 214 noncollege young males 36% had.\nWhat is the test statistic when testing \\(H_0: p_{coll} - p_{noncoll} = 0\\) vs.  \\(H_A: p_{coll} - p_{noncoll} \\neq 0\\)?\n\\[\\begin{align}\nz_{\\hat{p}_1 - \\hat{p}_2} &= \\frac{94/269 - 77/214-0}{\\sqrt{0.354\\cdot(1-0.354)(\\frac{1}{269}+\\frac{1}{214})}}\\\\\n&=-0.2367497\n\\end{align}\\]"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#step-3b-conditions-satisfied-1",
    "href": "slides_md/Day12_bsta511_md.html#step-3b-conditions-satisfied-1",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step “3b”: Conditions satisfied?",
    "text": "Step “3b”: Conditions satisfied?\n\nConditions:\n\nIndependent observations & samples\n\nThe observations were collected independently.\nIn particular, observations from the two groups weren’t paired in any meaningful way.\n\nThe number of expected successes and expected failures is at least 10 for each group - using the pooled proportion:\n\n\\(n_1\\hat{p}_{pool} \\ge 10, \\ \\ n_1(1-\\hat{p}_{pool}) \\ge 10\\)\n\\(n_2\\hat{p}_{pool} \\ge 10, \\ \\ n_2(1-\\hat{p}_{pool}) \\ge 10\\)\n\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year, and out of 214 noncollege young males 36% had.\nTesting \\(H_0: p_{coll} - p_{noncoll} = 0\\) vs.  \\(H_A: p_{coll} - p_{noncoll} \\neq 0\\)? .\n Are the conditions satisfied?"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#step-4-p-value-1",
    "href": "slides_md/Day12_bsta511_md.html#step-4-p-value-1",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\n\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate the p-value:\n\\[\\begin{align}\n2 &\\cdot P(\\hat{p}_1 - \\hat{p}_2&lt;0.35-0.36) \\\\\n= 2 &\\cdot P\\Big(Z_{\\hat{p}_1 - \\hat{p}_2} &lt; \\\\\n&\\frac{94/269 - 77/214-0}{\\sqrt{0.354\\cdot(1-0.354)(\\frac{1}{269}+\\frac{1}{214})}}\\Big)\\\\\n=2 &\\cdot P(Z_{\\hat{p}} &lt; -0.2367497) \\\\\n= & 0.812851\n\\end{align}\\]\n\n\n\n\n2*pnorm(-0.2367497)\n\n[1] 0.812851"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#step-5-conclusion-to-hypothesis-test-1",
    "href": "slides_md/Day12_bsta511_md.html#step-5-conclusion-to-hypothesis-test-1",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\n\\[\\begin{align}\nH_0:& p_{coll} - p_{noncoll} = 0\\\\\nH_A:& p_{coll} - p_{noncoll} \\neq 0\\\\\n\\end{align}\\]\n\nRecall the \\(p\\)-value = 0.812851\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nStats class conclusion\n\nThere is insufficient evidence that the difference in (population) proportions of young male college and noncollege students that participated in sports betting in the previous year are different ( \\(p\\)-value = 0.81).\n\nMore realistic manuscript conclusion:\n\n35% of young male college students (n=269) and 36% of noncollege young males (n=214) participated in sports betting in the previous year ( \\(p\\)-value = 0.81)."
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#ci-for-population-difference-in-proportions",
    "href": "slides_md/Day12_bsta511_md.html#ci-for-population-difference-in-proportions",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "95% CI for population difference in proportions",
    "text": "95% CI for population difference in proportions\n\n\n\nWhat to use for SE in CI formula?\n\n\\[\\hat{p}_1 - \\hat{p}_2 \\pm z^* \\cdot SE_{\\hat{p}_1 - \\hat{p}_2}\\]\n\n\n\n\nSE in sampling distribution of \\(\\hat{p}_1 - \\hat{p}_2\\)\n\n\\[\\sigma_{\\hat{p}_1 - \\hat{p}_2} =\n\\sqrt{\n\\frac{p_1\\cdot(1-p_1)}{n_1} + \\frac{p_2\\cdot(1-p_2)}{n_2}} \\]\n\n\n\n\nProblem: We don’t know what \\(p\\) is - it’s what we’re estimating with the CI.\nSolution: approximate \\(p_1\\), \\(p_2\\) with \\(\\hat{p}_1\\), \\(\\hat{p}_2\\):\n\n\\[SE_{\\hat{p}_1 - \\hat{p}_2} = \\sqrt{\n\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}}\\]\n\n\n\n\n\n\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year, and out of 214 noncollege young males 36% had. Find the 95% CI for the difference in population proportions.\n\n\n\n\\[\\frac{94}{269} - \\frac{77}{214} \\pm 1.96 \\cdot SE_{\\hat{p}_1 - \\hat{p}_2}\\]\n\\[\\begin{align}\n& SE_{\\hat{p}_1 - \\hat{p}_2}=\\\\\n& \\sqrt{\n\\frac{94/269 \\cdot (1-94/269)}{269} +\n\\frac{77/214 \\cdot (1-77/214)}{214}}\n\\end{align}\\]\n\n\n\n\nInterpretation:\nWe are 95% confident that the difference in (population) proportions of young male college and noncollege students that participated in sports betting in the previous year is in (-0.127, 0.106)."
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#conditions-for-difference-in-proportions-test-vs.-ci",
    "href": "slides_md/Day12_bsta511_md.html#conditions-for-difference-in-proportions-test-vs.-ci",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Conditions for difference in proportions: test vs. CI",
    "text": "Conditions for difference in proportions: test vs. CI\n\n\n\nHypothesis test conditions\n\nIndependent observations & samples\n\nThe observations were collected independently.\nIn particular, observations from the two groups weren’t paired in any meaningful way.\n\n\n\n\nThe number of expected successes and expected failures is at least 10 for each group - using the pooled proportion:\n\n\\(n_1\\hat{p}_{pool} \\ge 10, \\ \\ n_1(1-\\hat{p}_{pool}) \\ge 10\\)\n\\(n_2\\hat{p}_{pool} \\ge 10, \\ \\ n_2(1-\\hat{p}_{pool}) \\ge 10\\)\n\n\n\n\n\nConfidence interval conditions\n\nIndependent observations & samples\n\nThe observations were collected independently.\nIn particular, observations from the two groups weren’t paired in any meaningful way.\n\n\n\n\nThe number of successes and failures is at least 10 for each group.\n\n\\(n_1\\hat{p}_1 \\ge 10, \\ \\ n_1(1-\\hat{p}_1) \\ge 10\\)\n\\(n_2\\hat{p}_2 \\ge 10, \\ \\ n_2(1-\\hat{p}_2) \\ge 10\\)"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#summary-stats-input-for-1-sample-proportion-test",
    "href": "slides_md/Day12_bsta511_md.html#summary-stats-input-for-1-sample-proportion-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Summary stats input for 1-sample proportion test",
    "text": "Summary stats input for 1-sample proportion test\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year.\nTest \\(H_0: p=0.36\\) vs. \\(H_A: p \\neq 0.36\\)?\n\n.35*269 # number of \"successes\"; round this value\n\n[1] 94.15\n\nprop.test(x = 94, n = 269,              # x = # successes & n = sample size\n          p = 0.36,                     # null value p0\n          alternative = \"two.sided\",    # 2-sided alternative\n          correct = FALSE)              # no continuity correction\n\n\n    1-sample proportions test without continuity correction\n\ndata:  94 out of 269, null probability 0.36\nX-squared = 0.13014, df = 1, p-value = 0.7183\nalternative hypothesis: true p is not equal to 0.36\n95 percent confidence interval:\n 0.2949476 0.4081767\nsample estimates:\n        p \n0.3494424 \n\n\nCan tidy() test output:\n\nprop.test(x = 94, n = 269, p = 0.36, alternative = \"two.sided\", correct = FALSE) %&gt;% \n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.1301373\n0.7182897\n1\n0.2949476\n0.4081767\n1-sample proportions test without continuity correction\ntwo.sided"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#dataset-input-for-1-sample-proportion-test-12",
    "href": "slides_md/Day12_bsta511_md.html#dataset-input-for-1-sample-proportion-test-12",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Dataset input for 1-sample proportion test (1/2)",
    "text": "Dataset input for 1-sample proportion test (1/2)\n\n\nSince we don’t have a dataset, we first need to create a dataset based on the results:\n\n“out of 269 male college students, 35% had participated in sports betting in the previous year”\n\n\n.35*269 # number of \"successes\"; round this value\n\n[1] 94.15\n\nSportsBet1 &lt;- tibble(\n  Coll = c(rep(\"Bet\", 94),\n           rep(\"NotBet\",269-94))\n  )\n\n\n\nglimpse(SportsBet1)\n\nRows: 269\nColumns: 1\n$ Coll &lt;chr&gt; \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"B…\n\nSportsBet1 %&gt;% tabyl(Coll)\n\n   Coll   n   percent\n    Bet  94 0.3494424\n NotBet 175 0.6505576\n\n\n\nR code for proportions test requires input as a base R table:\n\ntable(SportsBet1$Coll)\n\n\n   Bet NotBet \n    94    175"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#dataset-input-for-1-sample-proportion-test-22",
    "href": "slides_md/Day12_bsta511_md.html#dataset-input-for-1-sample-proportion-test-22",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Dataset input for 1-sample proportion test (2/2)",
    "text": "Dataset input for 1-sample proportion test (2/2)\n\n\nWhen using a dataset, prop.test requires the input x to be a table\nNote that we do not also specify n since the table already includes all needed information.\n\n\nprop.test(x = table(SportsBet1$Coll),   # table() of data\n          p = 0.36,                     # null value p0\n          alternative = \"two.sided\",    # 2-sided alternative\n          correct = FALSE)              # no continuity correction\n\n\n    1-sample proportions test without continuity correction\n\ndata:  table(SportsBet1$Coll), null probability 0.36\nX-squared = 0.13014, df = 1, p-value = 0.7183\nalternative hypothesis: true p is not equal to 0.36\n95 percent confidence interval:\n 0.2949476 0.4081767\nsample estimates:\n        p \n0.3494424 \n\n\nCompare output with summary stats method:\n\nprop.test(x = 94, n = 269,              # x = # successes & n = sample size\n          p = 0.36,                     # null value p0\n          alternative = \"two.sided\",    # 2-sided alternative\n          correct = FALSE)  %&gt;%         # no continuity correction\n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.1301373\n0.7182897\n1\n0.2949476\n0.4081767\n1-sample proportions test without continuity correction\ntwo.sided"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#continuity-correction-1-prop-z-test-with-vs.-without-cc",
    "href": "slides_md/Day12_bsta511_md.html#continuity-correction-1-prop-z-test-with-vs.-without-cc",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Continuity correction: 1-prop z-test with vs. without CC",
    "text": "Continuity correction: 1-prop z-test with vs. without CC\n\n\nRecall that when we approximated the\nbinomial distribution with a normal distribution to calculate a probability,\nthat we included a continuity correction (CC)\nto account for approximating a discrete distribution with a continuous distribution.\n\n\nprop.test(x = 94, n = 269, p = 0.36, alternative = \"two.sided\", \n          correct = FALSE) %&gt;% \n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.1301373\n0.7182897\n1\n0.2949476\n0.4081767\n1-sample proportions test without continuity correction\ntwo.sided\n  \n  \n  \n\n\n\n\n\n\nprop.test(x = 94, n = 269, p = 0.36, alternative = \"two.sided\", \n          correct = TRUE) %&gt;% \n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.08834805\n0.7662879\n1\n0.2931841\n0.4100774\n1-sample proportions test with continuity correction\ntwo.sided\n  \n  \n  \n\n\n\n\nDifferences are small when sample sizes are large."
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#summary-stats-input-for-2-samples-proportion-test",
    "href": "slides_md/Day12_bsta511_md.html#summary-stats-input-for-2-samples-proportion-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Summary stats input for 2-samples proportion test",
    "text": "Summary stats input for 2-samples proportion test\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year, and out of 214 noncollege young males 36% had. Test \\(H_0: p_{coll} - p_{noncoll} = 0\\) vs. \\(H_A: p_{coll} - p_{noncoll} \\neq 0\\).\n\n\n\n# round the number of successes:\n.35*269 # number of \"successes\" in college students\n\n[1] 94.15\n\n.36*214 # number of \"successes\" in noncollege students\n\n[1] 77.04\n\nNmbrBet &lt;- c(94, 77)                    # vector for # of successes in each group\nTotalNmbr &lt;- c(269, 214)                # vector for sample size in each group\n\nprop.test(x = NmbrBet,                  # x is # of successes in each group \n          n = TotalNmbr,                # n is sample size in each group\n          alternative = \"two.sided\",    # 2-sided alternative\n          correct = FALSE)              # no continuity correction\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  NmbrBet out of TotalNmbr\nX-squared = 0.05605, df = 1, p-value = 0.8129\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.09628540  0.07554399\nsample estimates:\n   prop 1    prop 2 \n0.3494424 0.3598131"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#dataset-input-for-2-samples-proportion-test-12",
    "href": "slides_md/Day12_bsta511_md.html#dataset-input-for-2-samples-proportion-test-12",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Dataset input for 2-samples proportion test (1/2)",
    "text": "Dataset input for 2-samples proportion test (1/2)\n\n\n\nSince we don’t have a dataset, we first need to create a dataset based on the results:\n\n“out of 269 male college students, 35% had participated in sports betting in the previous year, and out of 214 noncollege young males 36% had”\n\n\n\n# round the number of successes:\n.35*269 # college students\n\n[1] 94.15\n\n.36*214 # noncollege students\n\n[1] 77.04\n\nSportsBet2 &lt;- tibble(\n  Group = c(rep(\"College\", 269), \n         rep(\"NonCollege\", 214)),\n  Bet = c(rep(\"yes\", 94), \n          rep(\"no\", 269-94),\n          rep(\"yes\", 77), \n          rep(\"no\", 214-77))\n)\n\n\n\nglimpse(SportsBet2)\n\nRows: 483\nColumns: 2\n$ Group &lt;chr&gt; \"College\", \"College\", \"College\", \"College\", \"College\", \"College\"…\n$ Bet   &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"…\n\nSportsBet2 %&gt;% tabyl(Group, Bet)\n\n      Group  no yes\n    College 175  94\n NonCollege 137  77\n\n\n\nR code for proportions test requires input as a base R table:\n\ntable(SportsBet2$Group, \n      SportsBet2$Bet)\n\n            \n              no yes\n  College    175  94\n  NonCollege 137  77"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#dataset-input-for-2-samples-proportion-test-22",
    "href": "slides_md/Day12_bsta511_md.html#dataset-input-for-2-samples-proportion-test-22",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Dataset input for 2-samples proportion test (2/2)",
    "text": "Dataset input for 2-samples proportion test (2/2)\n\n\nWhen using a dataset, prop.test requires the input x to be a table\nNote that we do not also specify n since the table already includes all needed information.\n\n\nprop.test(x = table(SportsBet2$Group, SportsBet2$Bet),\n       alternative = \"two.sided\",\n       correct = FALSE)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  table(SportsBet2$Group, SportsBet2$Bet)\nX-squared = 0.05605, df = 1, p-value = 0.8129\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.07554399  0.09628540\nsample estimates:\n   prop 1    prop 2 \n0.6505576 0.6401869 \n\n\nCompare output with summary stats method:\n\nprop.test(x = NmbrBet,                  # x is # of successes in each group \n          n = TotalNmbr,                # n is sample size in each group\n          alternative = \"two.sided\",    # 2-sided alternative\n          correct = FALSE) %&gt;%          # no continuity correction\n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.3598131\n0.05605044\n0.8128509\n1\n-0.0962854\n0.07554399\n2-sample test for equality of proportions without continuity correction\ntwo.sided"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#continuity-correction-2-prop-z-test-with-vs.-without-cc",
    "href": "slides_md/Day12_bsta511_md.html#continuity-correction-2-prop-z-test-with-vs.-without-cc",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Continuity correction: 2-prop z-test with vs. without CC",
    "text": "Continuity correction: 2-prop z-test with vs. without CC\n\n\nRecall that when we approximated the\nbinomial distribution with a normal distribution to calculate a probability,\nthat we included a continuity correction (CC)\nto account for approximating a discrete distribution with a continuous distribution.\n\n\nprop.test(x = NmbrBet, n = TotalNmbr, alternative = \"two.sided\", \n          correct = FALSE) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.3598131\n0.05605044\n0.8128509\n1\n-0.0962854\n0.07554399\n2-sample test for equality of proportions without continuity correction\ntwo.sided\n  \n  \n  \n\n\n\n\n\n\nprop.test(x = NmbrBet, n = TotalNmbr, alternative = \"two.sided\", \n          correct = TRUE) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.3598131\n0.01987511\n0.8878864\n1\n-0.1004806\n0.07973918\n2-sample test for equality of proportions with continuity correction\ntwo.sided\n  \n  \n  \n\n\n\n\nDifferences are small when sample sizes are large."
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#sample-size-calculation-for-testing-one-proportion",
    "href": "slides_md/Day12_bsta511_md.html#sample-size-calculation-for-testing-one-proportion",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Sample size calculation for testing one proportion",
    "text": "Sample size calculation for testing one proportion\n\n\nRecall in our sports betting example that the null \\(p_0=0.36\\) and the observed proportion was \\(\\hat{p}=0.35\\).\n\nThe p-value from the hypothesis test was not significant.\nHow big would the sample size \\(n\\) need to be in order for the p-value to be significant?\n\nCalculate \\(n\\)\n\ngiven \\(\\alpha\\), power ( \\(1-\\beta\\) ), “true” alternative proportion \\(p\\), and null \\(p_0\\):\n\n\n\n\n\\[n=p(1-p)\\left(\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{p-p_0}\\right)^2\\]\n\n\np &lt;- 0.35\np0 &lt;- 0.36\nalpha &lt;- 0.05\nbeta &lt;- 0.20  #power=1-beta; want &gt;=80% power\nn &lt;- p*(1-p)*((qnorm(1-alpha/2) + qnorm(1-beta)) /\n                (p-p0))^2\nn\n\n[1] 17856.2\n\nceiling(n) \n\n[1] 17857\n\n\n\n\nWe would need a sample size of at least 17,857!"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#power-calculation-for-testing-one-proportion",
    "href": "slides_md/Day12_bsta511_md.html#power-calculation-for-testing-one-proportion",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Power calculation for testing one proportion",
    "text": "Power calculation for testing one proportion\n\nConversely, we can calculate how much power we had in our example given the sample size of 269.\n\nCalculate power,\n\ngiven \\(\\alpha\\), \\(n\\), “true” alternative proportion \\(p\\), and null \\(p_0\\)\n\n\n\\[1-\\beta=\n            \\Phi\\left(z-z_{1-\\alpha/2}\\right)+\\Phi\\left(-z-z_{1-\\alpha/2}\\right)\n            \\quad ,\\quad \\text{where } z=\\frac{p-p_0}{\\sqrt{\\frac{p(1-p)}{n}}}\\]\n\\(\\Phi\\) is the probability for a standard normal distribution\n\np &lt;- 0.35; p0 &lt;- 0.36; alpha &lt;- 0.05; n &lt;- 269\n(z &lt;- (p-p0)/sqrt(p*(1-p)/n))\n\n[1] -0.343863\n\n(Power &lt;- pnorm(z - qnorm(1-alpha/2)) +  pnorm(-z - qnorm(1-alpha/2)))\n\n[1] 0.06365242\n\n\nIf the population proportion is 0.35 instead of 0.36, we only have a 6.4% chance of correctly rejecting \\(H_0\\) when the sample size is 269."
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#r-package-pwr-for-power-analyses",
    "href": "slides_md/Day12_bsta511_md.html#r-package-pwr-for-power-analyses",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "R package pwr for power analyses",
    "text": "R package pwr for power analyses\n\n\nSpecify all parameters except for the one being solved for.\nOne proportion\n\npwr.p.test(h = NULL, n = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\",\"less\",\"greater\"))\n\nTwo proportions (same sample sizes)\n\npwr.2p.test(h = NULL, n = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\",\"less\",\"greater\"))\n\nTwo proportions (different sample sizes)\n\npwr.2p2n.test(h = NULL, n1 = NULL, n2 = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\", \"less\",\"greater\"))\n\n\n\n\\(h\\) is the effect size, and calculated using an arcsine transformation:\n\\[h = \\text{ES.h(p1, p2)} = 2\\arcsin(\\sqrt{p_1})-2\\arcsin(\\sqrt{p_2})\\]\n\n\nSee PASS documentation for\n\ntesting 1 proportion using effect size vs. other ways of powering a test of 1 proportion\ntesting 2 proportions using effect size vs. other ways of powering a test of 2 proportions."
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#pwr-sample-size-for-one-proportion-test",
    "href": "slides_md/Day12_bsta511_md.html#pwr-sample-size-for-one-proportion-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "pwr: sample size for one proportion test",
    "text": "pwr: sample size for one proportion test\n\npwr.p.test(h = NULL, n = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\",\"less\",\"greater\"))\n\n\n\n\\(h\\) is the effect size: h = ES.h(p1, p2)\n\np1 and p2 are the two proportions being tested\none of them is the null proportion \\(p_0\\), and the other is the alternative proportion\n\n\nSpecify all parameters except for the sample size:\n\n\n\n\n\nlibrary(pwr)\n\np.n &lt;- pwr.p.test(\n  h = ES.h(p1 = 0.36, p2 = 0.35),\n  sig.level = 0.05, \n  power = 0.80, \n  alternative = \"two.sided\")\np.n\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.02089854\n              n = 17971.09\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n\n\nplot(p.n)"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#pwr-power-for-one-proportion-test",
    "href": "slides_md/Day12_bsta511_md.html#pwr-power-for-one-proportion-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "pwr: power for one proportion test",
    "text": "pwr: power for one proportion test\n\npwr.p.test(h = NULL, n = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\",\"less\",\"greater\"))\n\n\n\n\\(h\\) is the effect size: h = ES.h(p1, p2)\n\np1 and p2 are the two proportions being tested\none of them is the null proportion \\(p_0\\), and the other is the alternative proportion\n\n\nSpecify all parameters except for the power:\n\n\n\n\n\nlibrary(pwr)\n\np.power &lt;- pwr.p.test(\n  h = ES.h(p1 = 0.36, p2 = 0.35),\n  sig.level = 0.05, \n  # power = 0.80, \n  n = 269,\n  alternative = \"two.sided\")\np.power\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.02089854\n              n = 269\n      sig.level = 0.05\n          power = 0.06356445\n    alternative = two.sided\n\n\n\n\n\nplot(p.power)"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#pwr-sample-size-for-two-proportions-test",
    "href": "slides_md/Day12_bsta511_md.html#pwr-sample-size-for-two-proportions-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "pwr: sample size for two proportions test",
    "text": "pwr: sample size for two proportions test\n\n\nTwo proportions (same sample sizes)\n\npwr.2p.test(h = NULL, n = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\",\"less\",\"greater\"))\n\n\n\n\\(h\\) is the effect size: h = ES.h(p1, p2); p1 and p2 are the two proportions being tested\n\nSpecify all parameters except for the sample size:\n\n\n\n\n\np2.n &lt;- pwr.2p.test(\n  h = ES.h(p1 = 0.36, p2 = 0.35),\n  sig.level = 0.05, \n  power = 0.80, \n  alternative = \"two.sided\")\np2.n\n\n\n     Difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.02089854\n              n = 35942.19\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: same sample sizes\n\n\n\n\n\nNote: \\(n\\) in output is the number per sample!\n\n\nplot(p2.n)"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#pwr-power-for-two-proportions-test",
    "href": "slides_md/Day12_bsta511_md.html#pwr-power-for-two-proportions-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "pwr: power for two proportions test",
    "text": "pwr: power for two proportions test\n\n\nTwo proportions (different sample sizes)\n\npwr.2p2n.test(h = NULL, n1 = NULL, n2 = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\", \"less\",\"greater\"))\n\n\n\n\\(h\\) is the effect size: h = ES.h(p1, p2); p1 and p2 are the two proportions being tested\n\nSpecify all parameters except for the power:\n\n\n\n\n\np2.n2 &lt;- pwr.2p2n.test(\n  h = ES.h(p1 = 0.36, p2 = 0.35),\n  n1 = 214,\n  n2 = 269,\n  sig.level = 0.05, \n  # power = 0.80, \n  alternative = \"two.sided\")\np2.n2\n\n\n     difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.02089854\n             n1 = 214\n             n2 = 269\n      sig.level = 0.05\n          power = 0.05598413\n    alternative = two.sided\n\nNOTE: different sample sizes\n\n\n\n\n\nNote: \\(n\\) in output is the number per sample!\n\n\nplot(p2.n2)"
  },
  {
    "objectID": "slides_md/Day12_bsta511_md.html#where-are-we-1",
    "href": "slides_md/Day12_bsta511_md.html#where-are-we-1",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Where are we?",
    "text": "Where are we?\n\nCI’s and hypothesis tests for different scenarios:\n\\[\\text{point estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\\(\\frac{s_d}{\\sqrt{n}}\\)\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\\(\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\) or pooled\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n\\(\\sqrt{\\frac{p(1-p)}{n}}\\)\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)\n\\(\\sqrt{\\frac{p_1\\cdot(1-p_1)}{n_1} + \\frac{p_2\\cdot(1-p_2)}{n_2}}\\)"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html",
    "href": "slides_code/Day12_bsta511_code.html",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n\nCode\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) \nlibrary(here) \nlibrary(pwr) # new-ish\n\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#load-packages",
    "href": "slides_code/Day12_bsta511_code.html#load-packages",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n\nCode\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) \nlibrary(here) \nlibrary(pwr) # new-ish\n\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#moritzs-tip-of-the-day-code-folding",
    "href": "slides_code/Day12_bsta511_code.html#moritzs-tip-of-the-day-code-folding",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "MoRitz’s tip of the day: code folding",
    "text": "MoRitz’s tip of the day: code folding\n\nWith code folding we can hide or show the code in the html output by clicking on the Code buttons in the html file.\nNote the &lt;/&gt; Code button on the top right of the html output.\nSee the new options in the yaml above (in the .qmd file).\n\n\ncode-fold: show code-tools: true source: repo\n\nSee more information at https://quarto.org/docs/output-formats/html-code.html#folding-code"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#where-are-we",
    "href": "slides_code/Day12_bsta511_code.html#where-are-we",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Where are we?",
    "text": "Where are we?\nCI’s and hypothesis tests for different scenarios:\n\\[\\text{point estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\\(\\frac{s_d}{\\sqrt{n}}\\)\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\\(\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\) or pooled\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n???\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)\n???"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#goals-for-today-sections-8.1-8.2",
    "href": "slides_code/Day12_bsta511_code.html#goals-for-today-sections-8.1-8.2",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Goals for today (Sections 8.1-8.2)",
    "text": "Goals for today (Sections 8.1-8.2)\n\nStatistical inference for a single proportion or the difference of two (independent) proportions\n\nSampling distribution for a proportion or difference in proportions\nWhat are \\(H_0\\) and \\(H_a\\)?\nWhat are the SE’s for \\(\\hat{p}\\) and \\(\\hat{p}_1-\\hat{p}_2\\)?\nHypothesis test\nConfidence Interval\nHow are the SE’s different for a hypothesis test & CI?\nHow to run proportions tests in R\nPower & sample size for proportions tests (extra material)"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#steps-in-a-hypothesis-test",
    "href": "slides_code/Day12_bsta511_code.html#steps-in-a-hypothesis-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Steps in a Hypothesis Test",
    "text": "Steps in a Hypothesis Test\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic.\nCalculate the p-value based on the observed test statistic and its sampling distribution\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#sampling-distribution-of-hatp",
    "href": "slides_code/Day12_bsta511_code.html#sampling-distribution-of-hatp",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Sampling distribution of \\(\\hat{p}\\)",
    "text": "Sampling distribution of \\(\\hat{p}\\)\n\n\\(\\hat{p}=\\frac{X}{n}\\) where \\(X\\) is the number of “successes” and \\(n\\) is the sample size.\n\\(X \\sim Bin(n,p)\\), where \\(p\\) is the population proportion.\nFor \\(n\\) “big enough”, the normal distribution can be used to approximate a binomial distribution:\n\n\\[Bin(n,p) \\rightarrow N\\Big(\\mu = np, \\sigma = \\sqrt{np(1-p)} \\Big)\\]\n\nSince \\(\\hat{p}=\\frac{X}{n}\\) is a linear transformation of \\(X\\), we have for large n:\n\n\\[\\hat{p} \\sim N\\Big(\\mu_{\\hat{p}} = p, \\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}} \\Big)\\]\n\nHow we apply this result to CI’s and test statistics is different!!!"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#step-3-test-statistic",
    "href": "slides_code/Day12_bsta511_code.html#step-3-test-statistic",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 3: Test statistic",
    "text": "Step 3: Test statistic\nSampling distribution of \\(\\hat{p}\\) if we assume \\(H_0: p=p_0\\) is true:\n\\[\\hat{p} \\sim N\\Big(\\mu_{\\hat{p}} = p, \\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}} \\Big)\n\\sim N\\Big(\n\\mu_{\\hat{p}}=p_0, \\sigma_{\\hat{p}}=\\sqrt{\\frac{p_0\\cdot(1-p_0)}{n}}\n\\Big)\\]\nTest statistic for a one sample proportion test:\n\\[\n\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\n= z_{\\hat{p}} = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0\\cdot(1-p_0)}{n}}}\n\\]\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year.\n What is the test statistic when testing \\(H_0: p=0.36\\) vs.  \\(H_A: p \\neq 0.36\\)?\n\n\nCode\np0 &lt;- 0.36\nn &lt;- 269\nn*.35\n\n\n[1] 94.15\n\n\nCode\n(ph &lt;- 94/n)\n\n\n[1] 0.3494424\n\n\nCode\n(SEp &lt;- sqrt(p0*(1-p0)/n))\n\n\n[1] 0.02926612\n\n\nCode\n(zp &lt;- (ph-p0)/SEp)\n\n\n[1] -0.3607455\n\n\n\\[\\begin{align}\nz_{\\hat{p}} &= \\frac{94/269 - 0.36}{\\sqrt{\\frac{0.36\\cdot(1-0.36)}{269}}} \\\\\n& -0.3607455\n\\end{align}\\]"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#step-3b-conditions-satisfied",
    "href": "slides_code/Day12_bsta511_code.html#step-3b-conditions-satisfied",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step “3b”: Conditions satisfied?",
    "text": "Step “3b”: Conditions satisfied?\nConditions:\n\nIndependent observations\n\nThe observations were collected independently.\n\nThe number of expected successes and expected failures is at least 10.\n\n\\(n_1 p_0 \\ge 10, \\ \\ n_1(1-p_0)\\ge 10\\)\n\n\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year.\n Testing \\(H_0: p=0.36\\) vs. \\(H_A: p \\neq 0.36\\).\n Are the conditions satisfied?"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#step-4-p-value",
    "href": "slides_code/Day12_bsta511_code.html#step-4-p-value",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\n\n\n\n\n\n\n\n\n\nCalculate the p-value:\n\\[\\begin{align}\n2 &\\cdot P(\\hat{p}&lt;0.35) \\\\\n&= 2 \\cdot P\\Big(Z_{\\hat{p}} &lt; \\frac{94/269 - 0.36}{\\sqrt{\\frac{0.36\\cdot(1-0.36)}{269}}}\\Big)\\\\\n&=2 \\cdot P(Z_{\\hat{p}} &lt; -0.3607455)\\\\\n&= 0.7182897\n\\end{align}\\]\n\n\nCode\n2*pnorm(-0.3607455)\n\n\n[1] 0.7182897"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#step-5-conclusion-to-hypothesis-test",
    "href": "slides_code/Day12_bsta511_code.html#step-5-conclusion-to-hypothesis-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\\[\\begin{align}\nH_0:& p = 0.36\\\\\nH_A:& p \\neq 0.36\\\\\n\\end{align}\\]\n\nRecall the \\(p\\)-value = 0.7182897\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nStats class conclusion\n\nThere is insufficient evidence that the (population) proportion of young male college students that participated in sports betting in the previous year is different than 0.36 ( \\(p\\)-value = 0.72).\n\nMore realistic manuscript conclusion:\n\nIn a sample of 269 male college students, 35% had participated in sports betting in the previous year, which is not different from 36% ( \\(p\\)-value = 0.72)."
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#ci-for-population-proportion",
    "href": "slides_code/Day12_bsta511_code.html#ci-for-population-proportion",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "95% CI for population proportion",
    "text": "95% CI for population proportion\nWhat to use for SE in CI formula?\n\\[\\hat{p} \\pm z^* \\cdot SE_{\\hat{p}}\\]\nSampling distribution of \\(\\hat{p}\\):\n\\[\\hat{p} \\sim N\\Big(\\mu_{\\hat{p}} = p, \\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}} \\Big)\\]\nProblem: We don’t know what \\(p\\) is - it’s what we’re estimating with the CI.\nSolution: approximate \\(p\\) with \\(\\hat{p}\\):\n\\[SE_{\\hat{p}} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year.\nFind the 95% CI for the population proportion.\n\\[\\begin{align}\n94/269 &\\pm 1.96 \\cdot SE_{\\hat{p}}\\\\\nSE_{\\hat{p}} &= \\sqrt{\\frac{(94/269)(1-94/269)}{269}}\n\\end{align}\\]\nInterpretation:\nWe are 95% confident that the (population) proportion of young male college students that participated in sports betting in the previous year is in (0.29, 0.41)."
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#conditions-for-one-proportion-test-vs.-ci",
    "href": "slides_code/Day12_bsta511_code.html#conditions-for-one-proportion-test-vs.-ci",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Conditions for one proportion: test vs. CI",
    "text": "Conditions for one proportion: test vs. CI\nHypothesis test conditions\n\nIndependent observations\n\nThe observations were collected independently.\n\nThe number of expected successes and expected failures is at least 10.\n\n\\[n_1 p_0 \\ge 10, \\ \\ n_1(1-p_0)\\ge 10\\]\nConfidence interval conditions\n\nIndependent observations\n\nThe observations were collected independently.\n\nThe number of successes and failures is at least 10:\n\n\\[n_1\\hat{p}_1 \\ge 10, \\ \\ n_1(1-\\hat{p}_1)\\ge 10\\]"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#sampling-distribution-of-hatp_1-hatp_2",
    "href": "slides_code/Day12_bsta511_code.html#sampling-distribution-of-hatp_1-hatp_2",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Sampling distribution of \\(\\hat{p}_1-\\hat{p}_2\\)",
    "text": "Sampling distribution of \\(\\hat{p}_1-\\hat{p}_2\\)\n\n\\(\\hat{p}_1=\\frac{X_1}{n_1}\\) and \\(\\hat{p}_2=\\frac{X_2}{n_2}\\),\n\n\\(X_1\\) & \\(X_2\\) are the number of “successes”\n\\(n_1\\) & \\(n_2\\) are the sample sizes of the 1st & 2nd samples\n\n\n\n\nEach \\(\\hat{p}\\) can be approximated by a normal distribution, for “big enough” \\(n\\)\nSince the difference of independent normal random variables is also normal, it follows that for “big enough” \\(n_1\\) and \\(n_2\\)\n\n\\[\\hat{p}_1 - \\hat{p}_2 \\sim N \\Big(\\mu_{\\hat{p}_1 - \\hat{p}_2} = p_1 - p_2, ~~\n\\sigma_{\\hat{p}_1 - \\hat{p}_2} =\n\\sqrt{\n\\frac{p_1\\cdot(1-p_1)}{n_1} + \\frac{p_2\\cdot(1-p_2)}{n_2}}\n\\Big)\\]\nwhere \\(p_1\\) & \\(p_2\\) are the population proportions, respectively.\n\nHow we apply this result to CI’s and test statistics is different!!!"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#step-3-test-statistic-12",
    "href": "slides_code/Day12_bsta511_code.html#step-3-test-statistic-12",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 3: Test statistic (1/2)",
    "text": "Step 3: Test statistic (1/2)\nSampling distribution of \\(\\hat{p}_1 - \\hat{p}_2\\): \\[\\hat{p}_1 - \\hat{p}_2 \\sim N \\Big(\\mu_{\\hat{p}_1 - \\hat{p}_2} = p_1 - p_2, ~~\n\\sigma_{\\hat{p}_1 - \\hat{p}_2} =\n\\sqrt{\n\\frac{p_1\\cdot(1-p_1)}{n_1} + \\frac{p_2\\cdot(1-p_2)}{n_2}}\n\\Big)\\]\nSince we assume \\(H_0: p_1 - p_2 = 0\\) is true, we “pool” the proportions of the two samples to calculate the SE:\n\\[\\text{pooled proportion} = \\hat{p}_{pool} = \\dfrac{\\text{total number of successes} }{ \\text{total number of cases}} = \\frac{x_1+x_2}{n_1+n_2}\\]\nTest statistic:\n\\[\n\\text{test statistic} = z_{\\hat{p}_1 - \\hat{p}_2} = \\frac{\\hat{p}_1 - \\hat{p}_2 - 0}{\\sqrt{\\frac{\\hat{p}_{pool}\\cdot(1-\\hat{p}_{pool})}{n_1} + \\frac{\\hat{p}_{pool}\\cdot(1-\\hat{p}_{pool})}{n_2}}}\n\\]"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#step-3-test-statistic-22",
    "href": "slides_code/Day12_bsta511_code.html#step-3-test-statistic-22",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 3: Test statistic (2/2)",
    "text": "Step 3: Test statistic (2/2)\n\\[\n\\text{test statistic} = z_{\\hat{p}_1 - \\hat{p}_2} = \\frac{\\hat{p}_1 - \\hat{p}_2 - 0}{\\sqrt{\\frac{\\hat{p}_{pool}\\cdot(1-\\hat{p}_{pool})}{n_1} + \\frac{\\hat{p}_{pool}\\cdot(1-\\hat{p}_{pool})}{n_2}}}\n\\]\n\\[\\text{pooled proportion} = \\hat{p}_{pool} = \\dfrac{\\text{total number of successes} }{ \\text{total number of cases}} = \\frac{x_1+x_2}{n_1+n_2}\\]\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year, and out of 214 noncollege young males 36% had.\nWhat is the test statistic when testing \\(H_0: p_{coll} - p_{noncoll} = 0\\) vs.  \\(H_A: p_{coll} - p_{noncoll} \\neq 0\\)?\n\\[\\begin{align}\nz_{\\hat{p}_1 - \\hat{p}_2} &= \\frac{94/269 - 77/214-0}{\\sqrt{0.354\\cdot(1-0.354)(\\frac{1}{269}+\\frac{1}{214})}}\\\\\n&=-0.2367497\n\\end{align}\\]"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#step-3b-conditions-satisfied-1",
    "href": "slides_code/Day12_bsta511_code.html#step-3b-conditions-satisfied-1",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step “3b”: Conditions satisfied?",
    "text": "Step “3b”: Conditions satisfied?\nConditions:\n\nIndependent observations & samples\n\nThe observations were collected independently.\nIn particular, observations from the two groups weren’t paired in any meaningful way.\n\nThe number of expected successes and expected failures is at least 10 for each group - using the pooled proportion:\n\n\\(n_1\\hat{p}_{pool} \\ge 10, \\ \\ n_1(1-\\hat{p}_{pool}) \\ge 10\\)\n\\(n_2\\hat{p}_{pool} \\ge 10, \\ \\ n_2(1-\\hat{p}_{pool}) \\ge 10\\)\n\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year, and out of 214 noncollege young males 36% had.\nTesting \\(H_0: p_{coll} - p_{noncoll} = 0\\) vs.  \\(H_A: p_{coll} - p_{noncoll} \\neq 0\\)? .\nAre the conditions satisfied?"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#step-4-p-value-1",
    "href": "slides_code/Day12_bsta511_code.html#step-4-p-value-1",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 4: p-value",
    "text": "Step 4: p-value\nThe p-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\n\n\n\n\n\n\n\n\n\n\nCalculate the p-value:\n\\[\\begin{align}\n2 &\\cdot P(\\hat{p}_1 - \\hat{p}_2&lt;0.35-0.36) \\\\\n&= 2 \\cdot P\\Big(Z_{\\hat{p}_1 - \\hat{p}_2} &lt; \\\\\n&\\frac{94/269 - 77/214-0}{\\sqrt{0.354\\cdot(1-0.354)(\\frac{1}{269}+\\frac{1}{214})}}\\Big)\\\\\n&=2 \\cdot P(Z_{\\hat{p}} &lt; -0.2367497)\n\\end{align}\\]\n\n\nCode\n2*pnorm(-0.2367497)\n\n\n[1] 0.812851"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#step-5-conclusion-to-hypothesis-test-1",
    "href": "slides_code/Day12_bsta511_code.html#step-5-conclusion-to-hypothesis-test-1",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Step 5: Conclusion to hypothesis test",
    "text": "Step 5: Conclusion to hypothesis test\n\\[\\begin{align}\nH_0:& p_{coll} - p_{noncoll} = 0\\\\\nH_A:& p_{coll} - p_{noncoll} \\neq 0\\\\\n\\end{align}\\]\n\nRecall the \\(p\\)-value = 0.812851\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nStats class conclusion\n\nThere is insufficient evidence that the difference in (population) proportions of young male college and noncollege students that participated in sports betting in the previous year are different ( \\(p\\)-value = 0.81).\n\nMore realistic manuscript conclusion:\n\n35% of young male college students (n=269) and 36% of noncollege young males (n=214) participated in sports betting in the previous year ( \\(p\\)-value = 0.81)."
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#ci-for-population-difference-in-proportions",
    "href": "slides_code/Day12_bsta511_code.html#ci-for-population-difference-in-proportions",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "95% CI for population difference in proportions",
    "text": "95% CI for population difference in proportions\nWhat to use for SE in CI formula?\n\\[\\hat{p}_1 - \\hat{p}_2 \\pm z^* \\cdot SE_{\\hat{p}_1 - \\hat{p}_2}\\]\nSE in sampling distribution of \\(\\hat{p}_1 - \\hat{p}_2\\)\n\\[\\sigma_{\\hat{p}_1 - \\hat{p}_2} =\n\\sqrt{\n\\frac{p_1\\cdot(1-p_1)}{n_1} + \\frac{p_2\\cdot(1-p_2)}{n_2}} \\]\nProblem: We don’t know what \\(p\\) is - it’s what we’re estimating with the CI.\nSolution: approximate \\(p_1\\), \\(p_2\\) with \\(\\hat{p}_1\\), \\(\\hat{p}_2\\):\n\\[SE_{\\hat{p}_1 - \\hat{p}_2} = \\sqrt{\n\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}}\\]\n\nExample: A 2010 study found that out of 269 male college students, 35% had participated in sports betting in the previous year, and out of 214 noncollege young males 36% had. Find the 95% CI for the difference in population proportions.\n\\[\\frac{94}{269} - \\frac{77}{214} \\pm 1.96 \\cdot SE_{\\hat{p}_1 - \\hat{p}_2}\\]\n\\[\\begin{align}\n& SE_{\\hat{p}_1 - \\hat{p}_2}=\\\\\n& \\sqrt{\n\\frac{94/269 \\cdot (1-94/269)}{269} +\n\\frac{77/214 \\cdot (1-77/214)}{214}}\n\\end{align}\\]\nInterpretation:\nWe are 95% confident that the difference in (population) proportions of young male college and noncollege students that participated in sports betting in the previous year is in (-0.127, 0.106)."
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#conditions-for-difference-in-proportions-test-vs.-ci",
    "href": "slides_code/Day12_bsta511_code.html#conditions-for-difference-in-proportions-test-vs.-ci",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Conditions for difference in proportions: test vs. CI",
    "text": "Conditions for difference in proportions: test vs. CI\nHypothesis test conditions\n\nIndependent observations & samples\n\nThe observations were collected independently.\nIn particular, observations from the two groups weren’t paired in any meaningful way.\n\nThe number of expected successes and expected failures is at least 10 for each group - using the pooled proportion:\n\n\\(n_1\\hat{p}_{pool} \\ge 10, \\ \\ n_1(1-\\hat{p}_{pool}) \\ge 10\\)\n\\(n_2\\hat{p}_{pool} \\ge 10, \\ \\ n_2(1-\\hat{p}_{pool}) \\ge 10\\)\n\n\nConfidence interval conditions\n\nIndependent observations & samples\n\nThe observations were collected independently.\nIn particular, observations from the two groups weren’t paired in any meaningful way.\n\nThe number of successes and failures is at least 10 for each group.\n\n\\(n_1\\hat{p}_1 \\ge 10, \\ \\ n_1(1-\\hat{p}_1) \\ge 10\\)\n\\(n_2\\hat{p}_2 \\ge 10, \\ \\ n_2(1-\\hat{p}_2) \\ge 10\\)"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#r-1-sample-proportion-test-13",
    "href": "slides_code/Day12_bsta511_code.html#r-1-sample-proportion-test-13",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "R: 1-sample proportion test (1/3)",
    "text": "R: 1-sample proportion test (1/3)\nCreate a dataset based on the results:\n\n\nCode\n.35*269 # number of \"successes\"\n\n\n[1] 94.15\n\n\nCode\n# round this value\n\nSportsBet1 &lt;- tibble(\n  Coll = c(rep(\"Bet\", 94), \n           rep(\"NotBet\",269-94))\n  )\nglimpse(SportsBet1)\n\n\nRows: 269\nColumns: 1\n$ Coll &lt;chr&gt; \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"Bet\", \"B…\n\n\n\n\nCode\nSportsBet1 %&gt;% tabyl(Coll)\n\n\n   Coll   n   percent\n    Bet  94 0.3494424\n NotBet 175 0.6505576\n\n\nR code for proportions test requires input as a base R table:\n\n\nCode\ntable(SportsBet1$Coll)\n\n\n\n   Bet NotBet \n    94    175"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#r-1-sample-proportion-test-23",
    "href": "slides_code/Day12_bsta511_code.html#r-1-sample-proportion-test-23",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "R: 1-sample proportion test (2/3)",
    "text": "R: 1-sample proportion test (2/3)\nprop.test requires the input x to be a table\n\n\nCode\nprop.test(x = table(SportsBet1$Coll),\n       alternative = \"two.sided\",\n       p = 0.36,\n       correct = FALSE)\n\n\n\n    1-sample proportions test without continuity correction\n\ndata:  table(SportsBet1$Coll), null probability 0.36\nX-squared = 0.13014, df = 1, p-value = 0.7183\nalternative hypothesis: true p is not equal to 0.36\n95 percent confidence interval:\n 0.2949476 0.4081767\nsample estimates:\n        p \n0.3494424"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#r-1-sample-proportion-test-with-vs.-without-cc-33",
    "href": "slides_code/Day12_bsta511_code.html#r-1-sample-proportion-test-with-vs.-without-cc-33",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "R: 1-sample proportion test: with vs. without CC (3/3)",
    "text": "R: 1-sample proportion test: with vs. without CC (3/3)\nApply a continuity correction (CC) to the p-value calculation.\n\n\nCode\nprop.test(x = table(SportsBet1$Coll), alternative = \"two.sided\",\n       p = 0.36, correct = FALSE) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.1301373\n0.7182897\n1\n0.2949476\n0.4081767\n1-sample proportions test without continuity correction\ntwo.sided\n  \n  \n  \n\n\n\n\nCode\nprop.test(x = table(SportsBet1$Coll), alternative = \"two.sided\",\n       p = 0.36, correct = TRUE) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n\n  \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.3494424\n0.08834805\n0.7662879\n1\n0.2931841\n0.4100774\n1-sample proportions test with continuity correction\ntwo.sided\n  \n  \n  \n\n\n\n\nDifferences are small when sample sizes are large."
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#r-2-samples-proportions-test-23",
    "href": "slides_code/Day12_bsta511_code.html#r-2-samples-proportions-test-23",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "R: 2-samples proportions test (2/3)",
    "text": "R: 2-samples proportions test (2/3)\nprop.test requires the input x to be a table\n\n\nCode\nprop.test(x = table(SportsBet2$Group, SportsBet2$Bet),\n       alternative = \"two.sided\",\n       correct = FALSE)\n\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  table(SportsBet2$Group, SportsBet2$Bet)\nX-squared = 0.05605, df = 1, p-value = 0.8129\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.07554399  0.09628540\nsample estimates:\n   prop 1    prop 2 \n0.6505576 0.6401869"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#r-2-samples-proportions-test-with-vs.-without-cc-33",
    "href": "slides_code/Day12_bsta511_code.html#r-2-samples-proportions-test-with-vs.-without-cc-33",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "R: 2-samples proportions test: with vs. without CC (3/3)",
    "text": "R: 2-samples proportions test: with vs. without CC (3/3)\nApply a continuity correction (CC) to the p-value calculation.\n\n\nCode\nprop.test(x = table(SportsBet2$Group, SportsBet2$Bet), alternative = \"two.sided\", \n          correct = FALSE) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n\n  \n    \n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.6505576\n0.6401869\n0.05605044\n0.8128509\n1\n-0.07554399\n0.0962854\n2-sample test for equality of proportions without continuity correction\ntwo.sided\n  \n  \n  \n\n\n\n\nCode\nprop.test(x = table(SportsBet2$Group, SportsBet2$Bet), alternative = \"two.sided\", \n          correct = TRUE) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n\n  \n    \n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.6505576\n0.6401869\n0.01987511\n0.8878864\n1\n-0.07973918\n0.1004806\n2-sample test for equality of proportions with continuity correction\ntwo.sided\n  \n  \n  \n\n\n\n\nDifferences are small when sample sizes are large."
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#sample-size-calculation-for-testing-one-proportion",
    "href": "slides_code/Day12_bsta511_code.html#sample-size-calculation-for-testing-one-proportion",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Sample size calculation for testing one proportion",
    "text": "Sample size calculation for testing one proportion\n\nRecall in our sports betting example that the null \\(p_0=0.36\\) and the observed proportion was \\(\\hat{p}=0.35\\).\n\nThe p-value from the hypothesis test was not significant.\nHow big would the sample size \\(n\\) need to be in order for the p-value to be significant?\n\nCalculate \\(n\\)\n\ngiven \\(\\alpha\\), power ( \\(1-\\beta\\) ), “true” alternative proportion \\(p\\), and null \\(p_0\\):\n\n\n\\[n=p(1-p)\\left(\\frac{z_{1-\\alpha/2}+z_{1-\\beta}}{p-p_0}\\right)^2\\]\n\n\nCode\np &lt;- 0.35\np0 &lt;- 0.36\nalpha &lt;- 0.05\nbeta &lt;- 0.20  #power=1-beta; want &gt;=80% power\nn &lt;- p*(1-p)*((qnorm(1-alpha/2) + qnorm(1-beta)) /\n                (p-p0))^2\nn\n\n\n[1] 17856.2\n\n\nCode\nceiling(n) \n\n\n[1] 17857\n\n\nWe would need a sample size of at least 17,857!"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#power-calculation-for-testing-one-proportion",
    "href": "slides_code/Day12_bsta511_code.html#power-calculation-for-testing-one-proportion",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Power calculation for testing one proportion",
    "text": "Power calculation for testing one proportion\nConversely, we can calculate how much power we had in our example given the sample size of 269.\n\nCalculate power,\n\ngiven \\(\\alpha\\), \\(n\\), “true” alternative proportion \\(p\\), and null \\(p_0\\)\n\n\n\\[1-\\beta=\n            \\Phi\\left(z-z_{1-\\alpha/2}\\right)+\\Phi\\left(-z-z_{1-\\alpha/2}\\right)\n            \\quad ,\\quad \\text{where } z=\\frac{p-p_0}{\\sqrt{\\frac{p(1-p)}{n}}}\\]\n\\(\\Phi\\) is the probability for a standard normal distribution\n\n\nCode\np &lt;- 0.35; p0 &lt;- 0.36; alpha &lt;- 0.05; n &lt;- 269\n(z &lt;- (p-p0)/sqrt(p*(1-p)/n))\n\n\n[1] -0.343863\n\n\nCode\n(Power &lt;- pnorm(z - qnorm(1-alpha/2)) +  pnorm(-z - qnorm(1-alpha/2)))\n\n\n[1] 0.06365242\n\n\nIf the population proportion is 0.35 instead of 0.36, we only have a 6.4% chance of correctly rejecting \\(H_0\\) when the sample size is 269."
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#r-package-pwr-for-power-analyses",
    "href": "slides_code/Day12_bsta511_code.html#r-package-pwr-for-power-analyses",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "R package pwr for power analyses",
    "text": "R package pwr for power analyses\n\nSpecify all parameters except for the one being solved for.\nOne proportion\n\npwr.p.test(h = NULL, n = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\",\"less\",\"greater\"))\n\nTwo proportions (same sample sizes)\n\npwr.2p.test(h = NULL, n = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\",\"less\",\"greater\"))\n\nTwo proportions (different sample sizes)\n\npwr.2p2n.test(h = NULL, n1 = NULL, n2 = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\", \"less\",\"greater\"))\n\n\\(h\\) is the effect size, and calculated using an arcsine transformation:\n\\[h = \\text{ES.h(p1, p2)} = 2\\arcsin(\\sqrt{p_1})-2\\arcsin(\\sqrt{p_2})\\]\nSee PASS documentation for\n\ntesting 1 proportion using effect size vs. other ways of powering a test of 1 proportion\ntesting 2 proportions using effect size vs. other ways of powering a test of 2 proportions."
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#pwr-sample-size-for-one-proportion-test",
    "href": "slides_code/Day12_bsta511_code.html#pwr-sample-size-for-one-proportion-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "pwr: sample size for one proportion test",
    "text": "pwr: sample size for one proportion test\npwr.p.test(h = NULL, n = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\",\"less\",\"greater\"))\n\n\\(h\\) is the effect size: h = ES.h(p1, p2)\n\np1 and p2 are the two proportions being tested\none of them is the null proportion \\(p_0\\), and the other is the alternative proportion\n\n\nSpecify all parameters except for the sample size:\n\n\nCode\nlibrary(pwr)\n\np.n &lt;- pwr.p.test(\n  h = ES.h(p1 = 0.36, p2 = 0.35),\n  sig.level = 0.05, \n  power = 0.80, \n  alternative = \"two.sided\")\np.n\n\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.02089854\n              n = 17971.09\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\n\n\n\nCode\nplot(p.n)"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#pwr-power-for-one-proportion-test",
    "href": "slides_code/Day12_bsta511_code.html#pwr-power-for-one-proportion-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "pwr: power for one proportion test",
    "text": "pwr: power for one proportion test\npwr.p.test(h = NULL, n = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\",\"less\",\"greater\"))\n\n\\(h\\) is the effect size: h = ES.h(p1, p2)\n\np1 and p2 are the two proportions being tested\none of them is the null proportion \\(p_0\\), and the other is the alternative proportion\n\n\nSpecify all parameters except for the power:\n\n\nCode\nlibrary(pwr)\n\np.power &lt;- pwr.p.test(\n  h = ES.h(p1 = 0.36, p2 = 0.35),\n  sig.level = 0.05, \n  # power = 0.80, \n  n = 269,\n  alternative = \"two.sided\")\np.power\n\n\n\n     proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.02089854\n              n = 269\n      sig.level = 0.05\n          power = 0.06356445\n    alternative = two.sided\n\n\n\n\nCode\nplot(p.power)"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#pwr-sample-size-for-two-proportions-test",
    "href": "slides_code/Day12_bsta511_code.html#pwr-sample-size-for-two-proportions-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "pwr: sample size for two proportions test",
    "text": "pwr: sample size for two proportions test\n\nTwo proportions (same sample sizes)\n\npwr.2p.test(h = NULL, n = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\",\"less\",\"greater\"))\n\n\\(h\\) is the effect size: h = ES.h(p1, p2); p1 and p2 are the two proportions being tested\n\nSpecify all parameters except for the sample size:\n\n\nCode\np2.n &lt;- pwr.2p.test(\n  h = ES.h(p1 = 0.36, p2 = 0.35),\n  sig.level = 0.05, \n  power = 0.80, \n  alternative = \"two.sided\")\np2.n\n\n\n\n     Difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.02089854\n              n = 35942.19\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: same sample sizes\n\n\nNote: \\(n\\) in output is the number per sample!\n\n\nCode\nplot(p2.n)"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#pwr-power-for-two-proportions-test",
    "href": "slides_code/Day12_bsta511_code.html#pwr-power-for-two-proportions-test",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "pwr: power for two proportions test",
    "text": "pwr: power for two proportions test\n\nTwo proportions (different sample sizes)\n\npwr.2p2n.test(h = NULL, n1 = NULL, n2 = NULL, sig.level = 0.05, power = NULL,       alternative = c(\"two.sided\", \"less\",\"greater\"))\n\n\\(h\\) is the effect size: h = ES.h(p1, p2); p1 and p2 are the two proportions being tested\n\nSpecify all parameters except for the power:\n\n\nCode\np2.n2 &lt;- pwr.2p2n.test(\n  h = ES.h(p1 = 0.36, p2 = 0.35),\n  n1 = 214,\n  n2 = 269,\n  sig.level = 0.05, \n  # power = 0.80, \n  alternative = \"two.sided\")\np2.n2\n\n\n\n     difference of proportion power calculation for binomial distribution (arcsine transformation) \n\n              h = 0.02089854\n             n1 = 214\n             n2 = 269\n      sig.level = 0.05\n          power = 0.05598413\n    alternative = two.sided\n\nNOTE: different sample sizes\n\n\nNote: \\(n\\) in output is the number per sample!\n\n\nCode\nplot(p2.n2)"
  },
  {
    "objectID": "slides_code/Day12_bsta511_code.html#where-are-we-1",
    "href": "slides_code/Day12_bsta511_code.html#where-are-we-1",
    "title": "Day 12: Inference for a single proportion or difference of two (independent) proportions (Sections 8.1-8.2)",
    "section": "Where are we?",
    "text": "Where are we?\nCI’s and hypothesis tests for different scenarios:\n\\[\\text{point estimate} \\pm z^*(or~t^*)\\cdot SE,~~\\text{test stat} = \\frac{\\text{point estimate}-\\text{null value}}{SE}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nBook\nPopulation  parameter\nSymbol\nPoint estimate\nSymbol\nSE\n\n\n\n\n10\n5.1\nPop mean\n\\(\\mu\\)\nSample mean\n\\(\\bar{x}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n\n\n10\n5.2\nPop mean of paired diff\n\\(\\mu_d\\) or \\(\\delta\\)\nSample mean of paired diff\n\\(\\bar{x}_{d}\\)\n\\(\\frac{s_d}{\\sqrt{n}}\\)\n\n\n11\n5.3\nDiff in pop  means\n\\(\\mu_1-\\mu_2\\)\nDiff in sample  means\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\\(\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\) or pooled\n\n\n12\n8.1\nPop proportion\n\\(p\\)\nSample prop\n\\(\\widehat{p}\\)\n\\(\\sqrt{\\frac{p(1-p)}{n}}\\)\n\n\n12\n8.2\nDiff in pop  proportions\n\\(p_1-p_2\\)\nDiff in sample  proportions\n\\(\\widehat{p}_1-\\widehat{p}_2\\)\n\\(\\sqrt{\\frac{p_1\\cdot(1-p_1)}{n_1} + \\frac{p_2\\cdot(1-p_2)}{n_2}}\\)"
  },
  {
    "objectID": "slides/Day13_bsta511.html#moritzs-tip-of-the-day",
    "href": "slides/Day13_bsta511.html#moritzs-tip-of-the-day",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "MoRitz’s tip of the day",
    "text": "MoRitz’s tip of the day\nAdd text to a plot using annotate():\n\nggplot(NULL, aes(c(0,4))) +  # no dataset, create axes for x from 0 to 4\n  geom_area(stat = \"function\", fun = dchisq, args = list(df=2), \n            fill = \"blue\", xlim = c(0, 1.0414)) +\n  geom_area(stat = \"function\", fun = dchisq, args = list(df=2),\n            fill = \"violet\", xlim = c(1.0414, 4)) +\n  geom_vline(xintercept = 1.0414) +  # vertical line at x = 1.0414\n  annotate(\"text\", x = 1.1, y = .4, # add text at specified (x,y) coordinate\n           label = \"chi-squared = 1.0414\", hjust=0, size=6) + \n  annotate(\"text\", x = 1.3, y = .1, \n           label = \"p-value = 0.59\", hjust=0, size=6)"
  },
  {
    "objectID": "slides/Day13_bsta511.html#where-are-we",
    "href": "slides/Day13_bsta511.html#where-are-we",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "slides/Day13_bsta511.html#where-are-we-categorical-outcome-zoomed-in",
    "href": "slides/Day13_bsta511.html#where-are-we-categorical-outcome-zoomed-in",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Where are we? Categorical outcome zoomed in",
    "text": "Where are we? Categorical outcome zoomed in"
  },
  {
    "objectID": "slides/Day13_bsta511.html#goals-for-today-sections-8.3-8.4",
    "href": "slides/Day13_bsta511.html#goals-for-today-sections-8.3-8.4",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Goals for today (Sections 8.3-8.4)",
    "text": "Goals for today (Sections 8.3-8.4)\n\nStatistical inference for categorical data when either are\n\ncomparing more than two groups,\nor have categorical outcomes that have more than 2 levels,\nor both\n\nChi-squared tests of association (independence)\n\nHypotheses\ntest statistic\nChi-squared distribution\np-value\ntechnical conditions (assumptions)\nconclusion\nR: chisq.test()\n\nFisher’s Exact Test\nChi-squared test vs. testing difference in proportions\n\nTest of Homogeneity"
  },
  {
    "objectID": "slides/Day13_bsta511.html#is-there-an-association-between-depression-and-being-physically-active",
    "href": "slides/Day13_bsta511.html#is-there-an-association-between-depression-and-being-physically-active",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Is there an association between depression and being physically active?",
    "text": "Is there an association between depression and being physically active?\n\nData sampled from the NHANES R package:\n\nAmerican National Health and Nutrition Examination Surveys\nCollected 2009-2012 by US National Center for Health Statistics (NCHS)\nNHANES dataset: 10,000 rows, resampled from NHANESraw to undo oversampling effects\n\nTreat it as a simple random sample from the US population (for pedagogical purposes)\n\n\nDepressed\n\nSelf-reported number of days where participant felt down, depressed or hopeless.\nOne of None, Several, or Most (more than half the days).\nReported for participants aged 18 years or older.\n\nPhysActive\n\nParticipant does moderate or vigorous-intensity sports, fitness or recreational activities (Yes or No).\nReported for participants 12 years or older."
  },
  {
    "objectID": "slides/Day13_bsta511.html#hypotheses-for-a-chi-squared-test-of-association-independence",
    "href": "slides/Day13_bsta511.html#hypotheses-for-a-chi-squared-test-of-association-independence",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Hypotheses for a Chi-squared test of association (independence)",
    "text": "Hypotheses for a Chi-squared test of association (independence)\n\n\nGeneric wording:\nTest of “association” wording\n\n\\(H_0\\): There is no association between the two variables\n\\(H_A\\): There is an association between the two variables\n\nTest of “independence” wording\n\n\\(H_0\\): The variables are independent\n\\(H_A\\): The variables are not independent\n\n\n\n\nFor our example:\nTest of “association” wording\n\n\\(H_0\\): There is no association between depression and physical activity\n\\(H_A\\): There is an association between depression and physical activity\n\nTest of “independence” wording\n\n\\(H_0\\): The variables depression and physical activity are independent\n\\(H_A\\): The variables depression and physical activity are not independent\n\n\n\n\n\n\nNo symbols\n\n\nFor chi-squared test hypotheses we do not have versions using “symbols” like we do with tests of means or proportions."
  },
  {
    "objectID": "slides/Day13_bsta511.html#data-from-nhanes",
    "href": "slides/Day13_bsta511.html#data-from-nhanes",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Data from NHANES",
    "text": "Data from NHANES\n\nResults below are from\n\na random sample of 400 adults (≥ 18 yrs old)\nwith data for both the depression Depressed and physically active (PhysActive) variables.\n\n\n\n\n\n\n\n\nWhat does it mean for the variables to be independent?"
  },
  {
    "objectID": "slides/Day13_bsta511.html#h_0-variables-are-independent",
    "href": "slides/Day13_bsta511.html#h_0-variables-are-independent",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "\\(H_0\\): Variables are Independent",
    "text": "\\(H_0\\): Variables are Independent\n\n\n\nRecall from Chapter 2, that events \\(A\\) and \\(B\\) are independent if and only if\n\n\\[P(A~and~B)=P(A)P(B)\\]\n\nIf depression and being physically active are independent variables, then theoretically this condition needs to hold for every combination of levels, i.e.\n\n\n\\[\\begin{align}\nP(None~and~Yes) &= P(None)P(Yes)\\\\\nP(None~and~No) &= P(None)P(No)\\\\\nP(Several~and~Yes) &= P(Several)P(Yes)\\\\\nP(Several~and~No) &= P(Several)P(No)\\\\\nP(Most~and~Yes) &= P(Most)P(Yes)\\\\\nP(Most~and~No) &= P(Most)P(No)\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\nP(None~and~Yes) &= \\frac{314}{400}\\cdot\\frac{226}{400}\\\\\n& ...\\\\\nP(Most~and~No) &= \\frac{28}{400}\\cdot\\frac{174}{400}\n\\end{align}\\]\n\nWith these probabilities, for each cell of the table we calculate the expected counts for each cell under the \\(H_0\\) hypothesis that the variables are independent"
  },
  {
    "objectID": "slides/Day13_bsta511.html#expected-counts-if-variables-are-independent",
    "href": "slides/Day13_bsta511.html#expected-counts-if-variables-are-independent",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Expected counts (if variables are independent)",
    "text": "Expected counts (if variables are independent)\n\n\n\nThe expected counts (if \\(H_0\\) is true & the variables are independent) for each cell are\n\n\\(np\\) = total table size \\(\\cdot\\) probability of cell\n\n\nExpected count of Yes & None:\n\\[\\begin{align}\n400 \\cdot & P(None~and~Yes)\\\\\n&= 400 \\cdot P(None)P(Yes)\\\\\n&= 400 \\cdot\\frac{314}{400}\\cdot\\frac{226}{400}\\\\\n&= \\frac{314\\cdot 226}{400} \\\\\n&=  177.41\\\\\n&= \\frac{\\text{column total}\\cdot \\text{row total}}{\\text{table total}}\n\\end{align}\\]\n\n\n\n\n\n\n\nIf depression and being physically active are independent variables\n\n(as assumed by \\(H_0\\)),\n\nthen the observed counts should be close to the expected counts for each cell of the table"
  },
  {
    "objectID": "slides/Day13_bsta511.html#observed-vs.-expected-counts",
    "href": "slides/Day13_bsta511.html#observed-vs.-expected-counts",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Observed vs. Expected counts",
    "text": "Observed vs. Expected counts\n\n\n\nThe observed counts are the counts in the 2-way table summarizing the data\n\n\n\n\n\n\n\nExpected count for cell \\(i,j\\) :\n\n\nThe expected counts are the counts the we would expect to see in the 2-way table if there was no association between depression and being physically activity\n\n\n\n\n\n\n\n\n\\[\\textrm{Expected Count}_{\\textrm{row } i,\\textrm{ col }j}=\\frac{(\\textrm{row}~i~ \\textrm{total})\\cdot(\\textrm{column}~j~ \\textrm{total})}{\\textrm{table total}}\\]"
  },
  {
    "objectID": "slides/Day13_bsta511.html#the-chi2-test-statistic",
    "href": "slides/Day13_bsta511.html#the-chi2-test-statistic",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "The \\(\\chi^2\\) test statistic",
    "text": "The \\(\\chi^2\\) test statistic\n\n\nTest statistic for a test of association (independence):\n\\[\\chi^2 = \\sum_{\\textrm{all cells}} \\frac{(\\textrm{observed} - \\text{expected})^2}{\\text{expected}}\\]\n\nWhen the variables are independent, the observed and expected counts should be close to each other\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\n\\chi^2 &=  \\sum\\frac{(O-E)^2}{E} \\\\\n&= \\frac{(199-177.41)^2}{177.41} + \\frac{(26-32.77)^2}{32.77} + \\ldots + \\frac{(27-12.18)^2}{12.18} \\\\\n&=  41.2\n\\end{align}\\]\n\nIs this value big? Big enough to reject \\(H_0\\)?"
  },
  {
    "objectID": "slides/Day13_bsta511.html#the-chi2-distribution-calculating-the-p-value",
    "href": "slides/Day13_bsta511.html#the-chi2-distribution-calculating-the-p-value",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "The \\(\\chi^2\\) distribution & calculating the p-value",
    "text": "The \\(\\chi^2\\) distribution & calculating the p-value\n\n\nThe \\(\\chi^2\\) distribution shape depends on its degrees of freedom\n\nIt’s skewed right for smaller df,\n\ngets more symmetric for larger df\n\ndf = (# rows-1) x (# columns-1)\n\n\n\n\n\n\n\n\nThe p-value is always the area to the right of the test statistic for a \\(\\chi^2\\) test.\nWe can use the pchisq function in R to calculate the probability of being at least as big as the \\(\\chi^2\\) test statistic:\n\n\npv &lt;- pchisq(41.2, df = 2, \n       lower.tail = FALSE)\npv\n\n[1] 1.131185e-09\n\n\nWhat’s the conclusion to the \\(\\chi^2\\) test?"
  },
  {
    "objectID": "slides/Day13_bsta511.html#conclusion",
    "href": "slides/Day13_bsta511.html#conclusion",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Conclusion",
    "text": "Conclusion\nRecall the hypotheses to our \\(\\chi^2\\) test:\n\n\\(H_0\\): There is no association between depression and being physically activity\n\\(H_A\\): There is an association between depression and being physically activity\n\n\n\n\n\n\n\n\n\nConclusion:\nBased a random sample of 400 US adults from 2009-2012, there is sufficient evidence that there is an association between depression and being physically activity (p-value &lt; 0.001).\n\n\n\n\n\n\n\n\nWarning\n\n\nIf we fail to reject, we DO NOT have evidence of no association."
  },
  {
    "objectID": "slides/Day13_bsta511.html#technical-conditions",
    "href": "slides/Day13_bsta511.html#technical-conditions",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Technical conditions",
    "text": "Technical conditions\n\nIndependence\n\nEach case (person) that contributes a count to the table must be independent of all the other cases in the table\n\nIn particular, observational units cannot be represented in more than one cell.\nFor example, someone cannot choose both “Several” and “Most” for depression status. They have to choose exactly one option for each variable.\n\n\n\n\n\n\n\nSample size\n\nIn order for the distribution of the test statistic to be appropriately modeled by a chi-squared distribution we need\n2 \\(\\times\\) 2 table:\n\nexpected counts are at least 10 for each cell\n\nlarger tables:\n\nno more than 1/5 of the expected counts are less than 5, and\nall expected counts are greater than 1"
  },
  {
    "objectID": "slides/Day13_bsta511.html#depression-vs.-physical-activity-dataset",
    "href": "slides/Day13_bsta511.html#depression-vs.-physical-activity-dataset",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Depression vs. physical activity dataset",
    "text": "Depression vs. physical activity dataset\nCreate dataset based on results table:\n\n\n\nDepPA &lt;- tibble(\n  Depression = c(rep(\"None\", 314), \n         rep(\"Several\", 58),\n         rep(\"Most\", 28)),\n  PA = c(rep(\"Yes\", 199),  # None\n          rep(\"No\", 115),\n          rep(\"Yes\", 26), # Several\n          rep(\"No\", 32),\n          rep(\"Yes\", 1), # Most\n          rep(\"No\", 27))\n)\n\n\n\n\n\n\n\n\n\nSummary table of data:\n\n\n\nDepPA %&gt;% \n  tabyl(Depression, PA)\n\n Depression  No Yes\n       Most  27   1\n       None 115 199\n    Several  32  26\n\n\n\n\n# base R:\ntable(DepPA)\n\n          PA\nDepression  No Yes\n   Most     27   1\n   None    115 199\n   Several  32  26"
  },
  {
    "objectID": "slides/Day13_bsta511.html#chi2-test-in-r-using-dataset",
    "href": "slides/Day13_bsta511.html#chi2-test-in-r-using-dataset",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "\\(\\chi^2\\) test in R using dataset",
    "text": "\\(\\chi^2\\) test in R using dataset\n\n\nIf only have 2 columns in the dataset:\n\n(ChisqTest_DepPA &lt;- \n   chisq.test(table(DepPA)))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(DepPA)\nX-squared = 41.171, df = 2, p-value = 1.148e-09\n\n\nIf have &gt;2 columns in the dataset, we need to specify which columns to table:\n\n(ChisqTest_DepPA &lt;- \n   chisq.test(table(\n     DepPA$Depression, DepPA$PA)))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(DepPA$Depression, DepPA$PA)\nX-squared = 41.171, df = 2, p-value = 1.148e-09\n\n\n\nThe tidyverse way (fewer parentheses)\n\ntable(DepPA$Depression, DepPA$PA) %&gt;% \n  chisq.test() \n\n\n    Pearson's Chi-squared test\n\ndata:  .\nX-squared = 41.171, df = 2, p-value = 1.148e-09\n\n\ntidy() the output (from broom package):\n\ntable(DepPA$Depression, DepPA$PA) %&gt;% \n  chisq.test() %&gt;% \n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      statistic\n      p.value\n      parameter\n      method\n    \n  \n  \n    41.17067\n1.147897e-09\n2\nPearson's Chi-squared test\n  \n  \n  \n\n\n\n\nPull p-value\n\ntable(DepPA$Depression, DepPA$PA) %&gt;% \n  chisq.test() %&gt;% \n  tidy() %&gt;% pull(p.value)\n\n[1] 1.147897e-09"
  },
  {
    "objectID": "slides/Day13_bsta511.html#observed-expected-counts-in-r",
    "href": "slides/Day13_bsta511.html#observed-expected-counts-in-r",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Observed & expected counts in R",
    "text": "Observed & expected counts in R\n\n\nYou can see what the observed and expected counts are from the saved chi-squared test results:\n\nChisqTest_DepPA$observed\n\n         \n           No Yes\n  Most     27   1\n  None    115 199\n  Several  32  26\n\nChisqTest_DepPA$expected\n\n         \n              No    Yes\n  Most     12.18  15.82\n  None    136.59 177.41\n  Several  25.23  32.77\n\n\n\n\n\n\n\n\n\nWhy is it important to look at the expected counts?\nWhat are we looking for in the expected counts?"
  },
  {
    "objectID": "slides/Day13_bsta511.html#chi2-test-in-r-with-2-way-table",
    "href": "slides/Day13_bsta511.html#chi2-test-in-r-with-2-way-table",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "\\(\\chi^2\\) test in R with 2-way table",
    "text": "\\(\\chi^2\\) test in R with 2-way table\nCreate a base R table of the results:\n\n(DepPA_table &lt;- matrix(c(199, 26, 1, 115, 32, 27), nrow = 2, ncol = 3, byrow = T))\n\n     [,1] [,2] [,3]\n[1,]  199   26    1\n[2,]  115   32   27\n\ndimnames(DepPA_table) &lt;- list(\"PA\" = c(\"Yes\", \"No\"),   # row names\n                              \"Depression\" = c(\"None\", \"Several\", \"Most\"))  # column names\nDepPA_table\n\n     Depression\nPA    None Several Most\n  Yes  199      26    1\n  No   115      32   27\n\n\nRun \\(\\chi^2\\) test with 2-way table:\n\nchisq.test(DepPA_table) \n\n\n    Pearson's Chi-squared test\n\ndata:  DepPA_table\nX-squared = 41.171, df = 2, p-value = 1.148e-09\n\nchisq.test(DepPA_table)$expected\n\n     Depression\nPA      None Several  Most\n  Yes 177.41   32.77 15.82\n  No  136.59   25.23 12.18"
  },
  {
    "objectID": "slides/Day13_bsta511.html#yates-continuity-correction",
    "href": "slides/Day13_bsta511.html#yates-continuity-correction",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "(Yates’) Continuity correction",
    "text": "(Yates’) Continuity correction\n\nFor a 2x2 contingency table,\n\nthe \\(\\chi^2\\) test has the option of including a continuity correction\njust like with the proportions test\n\nThe default includes a continuity correction\nThere is no CC for bigger tables\n\n\n(DepPA_table2x2 &lt;- matrix(c(199, 27, 115, 59), nrow = 2, ncol = 2, byrow = T))\n\n     [,1] [,2]\n[1,]  199   27\n[2,]  115   59\n\ndimnames(DepPA_table2x2) &lt;- list(\"PA\" = c(\"Yes\", \"No\"),   # row names\n                              \"Depression\" = c(\"None\", \"Several/Most\"))  # column names\nDepPA_table2x2\n\n     Depression\nPA    None Several/Most\n  Yes  199           27\n  No   115           59\n\n\n\n\nOutput without a CC\n\nchisq.test(DepPA_table2x2, correct = FALSE) \n\n\n    Pearson's Chi-squared test\n\ndata:  DepPA_table2x2\nX-squared = 28.093, df = 1, p-value = 1.156e-07\n\n\n\nCompare to output with CC:\n\nchisq.test(DepPA_table2x2) \n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  DepPA_table2x2\nX-squared = 26.807, df = 1, p-value = 2.248e-07"
  },
  {
    "objectID": "slides/Day13_bsta511.html#example-with-smaller-sample-size",
    "href": "slides/Day13_bsta511.html#example-with-smaller-sample-size",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Example with smaller sample size",
    "text": "Example with smaller sample size\n\nSuppose that instead of taking a random sample of 400 adults (from the NHANES data), a study takes a random sample of 100 such that\n\n50 people that are physically active and\n50 people that are not physically active\n\n\n\n(DepPA100_table &lt;- matrix(c(43, 5, 2, 40, 4, 6), nrow = 2, ncol = 3, byrow = T))\n\n     [,1] [,2] [,3]\n[1,]   43    5    2\n[2,]   40    4    6\n\ndimnames(DepPA100_table) &lt;- list(\"PA\" = c(\"Yes\", \"No\"),   # row names\n                              \"Depression\" = c(\"None\", \"Several\", \"Most\"))  # column names\n\nDepPA100_table\n\n     Depression\nPA    None Several Most\n  Yes   43       5    2\n  No    40       4    6"
  },
  {
    "objectID": "slides/Day13_bsta511.html#chi-squared-test-warning",
    "href": "slides/Day13_bsta511.html#chi-squared-test-warning",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Chi-squared test warning",
    "text": "Chi-squared test warning\n\nchisq.test(DepPA100_table) \n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  DepPA100_table\nX-squared = 2.2195, df = 2, p-value = 0.3296\n\nchisq.test(DepPA100_table)$expected\n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\n\n     Depression\nPA    None Several Most\n  Yes 41.5     4.5    4\n  No  41.5     4.5    4\n\n\n\nRecall the sample size condition\n\nIn order for the test statistic to be modeled by a chi-squared distribution we need\n2 \\(\\times\\) 2 table: expected counts are at least 10 for each cell\nlarger tables:\n\nno more than 1/5 of the expected counts are less than 5, and\nall expected counts are greater than 1"
  },
  {
    "objectID": "slides/Day13_bsta511.html#fishers-exact-test",
    "href": "slides/Day13_bsta511.html#fishers-exact-test",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Fisher’s Exact Test",
    "text": "Fisher’s Exact Test\n\nCalled an exact test since it\n\ncalculates an exact probability for the p-value\n\ninstead of using an asymptotic approximation, such as the normal, t, or chi-squared distributions\n\nFor 2x2 tables the p-value is calculated using the hypergeometric probability distribution (see book for details)\n\n\n\nfisher.test(DepPA100_table)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  DepPA100_table\np-value = 0.3844\nalternative hypothesis: two.sided\n\n\n\n\n\nComments\n\n\n\nNote that there is no test statistic\nThere is also no CI\nThis is always a two-sided test\nThere is no continuity correction since the hypergeometric distribution is discrete"
  },
  {
    "objectID": "slides/Day13_bsta511.html#simulate-p-values-another-option-for-small-expected-counts",
    "href": "slides/Day13_bsta511.html#simulate-p-values-another-option-for-small-expected-counts",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Simulate p-values: another option for small expected counts",
    "text": "Simulate p-values: another option for small expected counts\nFrom the chisq.test help file:\n\nSimulation is done by random sampling from the set of all contingency tables with the same margin totals\n\nworks only if the margin totals are strictly positive.\n\nFor each simulation, a \\(\\chi^2\\) test statistic is calculated\nP-value is the proportion of simulations that have a test statistic at least as big as the observed one.\nNo continuity correction\n\n\nset.seed(567)\nchisq.test(DepPA100_table, simulate.p.value = TRUE) \n\n\n    Pearson's Chi-squared test with simulated p-value (based on 2000\n    replicates)\n\ndata:  DepPA100_table\nX-squared = 2.2195, df = NA, p-value = 0.3893"
  },
  {
    "objectID": "slides/Day13_bsta511.html#chi2-test-vs.-testing-differences-in-proportions",
    "href": "slides/Day13_bsta511.html#chi2-test-vs.-testing-differences-in-proportions",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "\\(\\chi^2\\) test vs. testing differences in proportions",
    "text": "\\(\\chi^2\\) test vs. testing differences in proportions\nIf there are only 2 levels in both of the categorical variables being tested, then the p-value from the \\(\\chi^2\\) test is equal to the p-value from the differences in proportions test.\n\n\nExample: Previously we tested whether the proportion who had participated in sports betting was the same for college and noncollege young adults:\n\\[\\begin{align}\nH_0:& ~p_{coll} - p_{noncoll} = 0\\\\\nH_A:& ~p_{coll} - p_{noncoll} \\neq 0\n\\end{align}\\]\n\n\n\nSportsBet_table &lt;- matrix(\n  c(175, 94, 137, 77), \n  nrow = 2, ncol = 2, byrow = T)\n\ndimnames(SportsBet_table) &lt;- list(\n  \"Group\" = c(\"College\", \"NonCollege\"), # row names\n  \"Bet\" = c(\"No\", \"Yes\"))  # column names\n\nSportsBet_table\n\n            Bet\nGroup         No Yes\n  College    175  94\n  NonCollege 137  77\n\n\n\n\n\n\nchisq.test(SportsBet_table) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      statistic\n      p.value\n      parameter\n      method\n    \n  \n  \n    0.01987511\n0.8878864\n1\nPearson's Chi-squared test with Yates' continuity correction\n  \n  \n  \n\n\n\nprop.test(SportsBet_table) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.6505576\n0.6401869\n0.01987511\n0.8878864\n1\n-0.07973918\n0.1004806\n2-sample test for equality of proportions with continuity correction\ntwo.sided\n  \n  \n  \n\n\n\n2*pnorm(sqrt(0.0199), lower.tail=F) # p-value\n\n[1] 0.8878167"
  },
  {
    "objectID": "slides/Day13_bsta511.html#test-of-homogeneity",
    "href": "slides/Day13_bsta511.html#test-of-homogeneity",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Test of Homogeneity",
    "text": "Test of Homogeneity\n\nRunning the sports betting example as a chi-squared test is actually an example of a test of homogeneity\nIn a test of homogeneity, proportions can be compared between many groups\n\n\\[\\begin{align}\nH_0:&~ p_1 = p_2 = p_2 = \\ldots = p_n\\\\\nH_A:&~ p_i \\neq p_j \\textrm{for at least one pair of } i, j\n\\end{align}\\]\n\nIt’s an extension of a two proportions test.\nThe test statistic & p-value are calculated the same was as a chi-squared test of association (independence)\nWhen we fix the margins (whether row or columns) of one of the “variables” (such as in a cohort or case-control study)\n\nthe chi-squared test is called a Test of Homogeneity"
  },
  {
    "objectID": "slides/Day13_bsta511.html#overview-of-tests-with-categorical-outcome",
    "href": "slides/Day13_bsta511.html#overview-of-tests-with-categorical-outcome",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Overview of tests with categorical outcome",
    "text": "Overview of tests with categorical outcome"
  },
  {
    "objectID": "slides/Day13_bsta511.html#chi-squared-tests-of-independence-vs.-homogeneity-vs.-goodness-of-fit",
    "href": "slides/Day13_bsta511.html#chi-squared-tests-of-independence-vs.-homogeneity-vs.-goodness-of-fit",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Chi-squared Tests of Independence vs. Homogeneity vs. Goodness-of-fit",
    "text": "Chi-squared Tests of Independence vs. Homogeneity vs. Goodness-of-fit\n\n\nSee YouTube video from TileStats for a good explanation of how these three tests are different: https://www.youtube.com/watch?v=TyD-_1JUhxw\nUCLA’s INSPIRE website has a good summary too: http://inspire.stat.ucla.edu/unit_13/"
  },
  {
    "objectID": "slides/Day13_bsta511.html#whats-next",
    "href": "slides/Day13_bsta511.html#whats-next",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "What’s next?",
    "text": "What’s next?"
  },
  {
    "objectID": "slides/Day14_bsta511.html#where-are-we",
    "href": "slides/Day14_bsta511.html#where-are-we",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "slides/Day14_bsta511.html#where-are-we-continuous-outcome-zoomed-in",
    "href": "slides/Day14_bsta511.html#where-are-we-continuous-outcome-zoomed-in",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Where are we? Continuous outcome zoomed in",
    "text": "Where are we? Continuous outcome zoomed in"
  },
  {
    "objectID": "slides/Day14_bsta511.html#goals-for-today-section-5.5",
    "href": "slides/Day14_bsta511.html#goals-for-today-section-5.5",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Goals for today (Section 5.5)",
    "text": "Goals for today (Section 5.5)\n\nAnalysis of Variance (ANOVA)\nWhen to use an ANOVA\nHypotheses\nANOVA table\nDifferent sources of variation in ANOVA\nANOVA conditions\nF-distribution\nPost-hoc testing of differences in means\nRunning an ANOVA in R"
  },
  {
    "objectID": "slides/Day14_bsta511.html#disability-discrimination-example",
    "href": "slides/Day14_bsta511.html#disability-discrimination-example",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Disability Discrimination Example",
    "text": "Disability Discrimination Example\n\n\n\nThe U.S. Rehabilitation Act of 1973 prohibited discrimination against people with physical disabilities.\n\nThe act defined a disabled person as any individual who has a physical or mental impairment that limits the person’s major life activities.\n\nA 1980’s study examined whether physical disabilities affect people’s perceptions of employment qualifications\n\n(Cesare, Tannenbaum, & Dalessio, 1990).\n\n\n\n\nResearchers prepared recorded job interviews, using same actors and script each time.\nOnly difference: job applicant appeared with different disabilities.\n\nNo disability\nLeg amputation\nCrutches\nHearing impairment\nWheelchair confinement\n\n70 undergrad students were randomly assigned to view one of the videotapes,\n\nthen rated the candidate’s qualifications on a 1-10 scale.\n\n\n\n\n\nThe research question: are qualifications evaluated differently depending on the applicant’s presented disability?"
  },
  {
    "objectID": "slides/Day14_bsta511.html#load-interview-data-from-.txt-file",
    "href": "slides/Day14_bsta511.html#load-interview-data-from-.txt-file",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Load interview data from .txt file",
    "text": "Load interview data from .txt file\n\n.txt (text) files are usually tab-deliminated files\n\n.csv files are comma-separated files\n\nread_delim is from the readr package, just like read_csv, and loads with other tidyverse packages\n\n\nemploy &lt;- read_delim(\n  file = here::here(\"data\", \"DisabilityEmployment.txt\"), \n  delim = \"\\t\",   # tab delimited\n  trim_ws = TRUE)\n\ntrim_ws: specify whether leading and trailing white space should be trimmed from each field before parsing it\n\nglimpse(employ)\n\nRows: 70\nColumns: 2\n$ disability &lt;chr&gt; \"none\", \"none\", \"none\", \"none\", \"none\", \"none\", \"none\", \"no…\n$ score      &lt;dbl&gt; 1.9, 2.5, 3.0, 3.6, 4.1, 4.2, 4.9, 5.1, 5.4, 5.9, 6.1, 6.7,…\n\n\n\n\n\nsummary(employ)\n\n  disability            score      \n Length:70          Min.   :1.400  \n Class :character   1st Qu.:3.700  \n Mode  :character   Median :5.050  \n                    Mean   :4.929  \n                    3rd Qu.:6.100  \n                    Max.   :8.500  \n\n\n\n\nemploy %&gt;% tabyl(disability)\n\n disability  n percent\n    amputee 14     0.2\n   crutches 14     0.2\n    hearing 14     0.2\n       none 14     0.2\n wheelchair 14     0.2"
  },
  {
    "objectID": "slides/Day14_bsta511.html#moritzs-tip-of-the-day",
    "href": "slides/Day14_bsta511.html#moritzs-tip-of-the-day",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "MoRitz’s tip of the day",
    "text": "MoRitz’s tip of the day\nRead OHSU’s Inclusive Language Guide (below is from pgs. 22-25)\n“… an evolving tool to help OHSU members learn about and use inclusive language…”\nSections on: Race and ethnicity, Immigration status, Gender and sexual orientation, and Ability (including physical, mental and chronological attributes)"
  },
  {
    "objectID": "slides/Day14_bsta511.html#factor-variable-make-disability-a-factor-variable",
    "href": "slides/Day14_bsta511.html#factor-variable-make-disability-a-factor-variable",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Factor variable: Make disability a factor variable",
    "text": "Factor variable: Make disability a factor variable\n\n\n\nglimpse(employ)\n\nRows: 70\nColumns: 2\n$ disability &lt;chr&gt; \"none\", \"none\", \"none\", \"none\", \"none\", \"none\", \"none\", \"no…\n$ score      &lt;dbl&gt; 1.9, 2.5, 3.0, 3.6, 4.1, 4.2, 4.9, 5.1, 5.4, 5.9, 6.1, 6.7,…\n\n\n\n\nsummary(employ)\n\n  disability            score      \n Length:70          Min.   :1.400  \n Class :character   1st Qu.:3.700  \n Mode  :character   Median :5.050  \n                    Mean   :4.929  \n                    3rd Qu.:6.100  \n                    Max.   :8.500  \n\n\n\n\nMake disability a factor variable:\n\nemploy &lt;- employ %&gt;% \n  mutate(disability = factor(disability))\n\n\nWhat’s different now?\n\n\n\nglimpse(employ)\n\nRows: 70\nColumns: 2\n$ disability &lt;fct&gt; none, none, none, none, none, none, none, none, none, none,…\n$ score      &lt;dbl&gt; 1.9, 2.5, 3.0, 3.6, 4.1, 4.2, 4.9, 5.1, 5.4, 5.9, 6.1, 6.7,…\n\n\n\n\nsummary(employ)\n\n      disability     score      \n amputee   :14   Min.   :1.400  \n crutches  :14   1st Qu.:3.700  \n hearing   :14   Median :5.050  \n none      :14   Mean   :4.929  \n wheelchair:14   3rd Qu.:6.100  \n                 Max.   :8.500"
  },
  {
    "objectID": "slides/Day14_bsta511.html#factor-variable-change-order-name-of-disability-levels",
    "href": "slides/Day14_bsta511.html#factor-variable-change-order-name-of-disability-levels",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Factor variable: Change order & name of disability levels",
    "text": "Factor variable: Change order & name of disability levels\nWhat are the current level names and order?\n\nlevels(employ$disability)\n\n[1] \"amputee\"    \"crutches\"   \"hearing\"    \"none\"       \"wheelchair\"\n\n\nWhat changes are being made below?\n\nemploy &lt;- employ %&gt;% \n  mutate(\n    # make \"none\" the first level\n    # by only listing the level none, all other levels will be in original order\n    disability = fct_relevel(disability, \"none\"),\n    # change the level name amputee to amputation\n    disability = fct_recode(disability, amputation = \"amputee\")\n    )\n\n\nfct_relevel() and fct_recode() are from the forcats package: https://forcats.tidyverse.org/index.html.\nforcats is loaded with library(tidyverse).\n\nNew order & names:\n\nlevels(employ$disability) # note the new order and new name\n\n[1] \"none\"       \"amputation\" \"crutches\"   \"hearing\"    \"wheelchair\""
  },
  {
    "objectID": "slides/Day14_bsta511.html#data-viz-12",
    "href": "slides/Day14_bsta511.html#data-viz-12",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Data viz (1/2)",
    "text": "Data viz (1/2)\n\nWhat are the score distribution shapes within each group?\nAny unusual values?\n\n\n\n\nggplot(employ, aes(x=score)) +\n  geom_density() +\n  facet_wrap(~ disability)\n\n\n\n\n\n\nlibrary(ggridges) \nggplot(employ, \n       aes(x=score,\n           y = disability,\n           fill = disability)) + \n  geom_density_ridges(alpha = 0.4) +\n  theme(legend.position=\"none\")"
  },
  {
    "objectID": "slides/Day14_bsta511.html#data-viz-22",
    "href": "slides/Day14_bsta511.html#data-viz-22",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Data viz (2/2)",
    "text": "Data viz (2/2)\n\nCompare the score measures of center and spread between the groups\n\n\n\n\nggplot(employ, \n       aes(y=score, \n           x = disability,\n           fill = disability)) +\n  geom_boxplot(alpha = 0.3) +\n  coord_flip() +\n  geom_jitter(width = 0.1, \n              alpha = 0.3) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nggplot(employ, \n       aes(x = disability, \n           y=score, \n           fill=disability, \n           color=disability)) +\n  geom_dotplot(binaxis = \"y\", alpha = 0.5) +\n  geom_hline(aes(yintercept = mean(score)), \n             lty = \"dashed\") +\n  stat_summary(fun =\"mean\", geom=\"point\", \n    size = 3, color = \"grey33\", alpha = 1) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "slides/Day14_bsta511.html#hypotheses",
    "href": "slides/Day14_bsta511.html#hypotheses",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Hypotheses",
    "text": "Hypotheses\nTo test for a difference in means across k groups:\n\\[\\begin{align}\nH_0 &: \\mu_1 = \\mu_2 = ... = \\mu_k\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j \\text{ for } i \\neq j\n\\end{align}\\]\nHypothetical examples:\nIn which set (A or B) do you believe the evidence will be stronger that at least one population differs from the others?"
  },
  {
    "objectID": "slides/Day14_bsta511.html#comparing-means",
    "href": "slides/Day14_bsta511.html#comparing-means",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Comparing means",
    "text": "Comparing means\nWhether or not two means are significantly different depends on:\n\nHow far apart the means are\nHow much variability there is within each group\n\nQuestions:\n\nHow to measure variability between groups?\nHow to measure variability within groups?\nHow to compare the two measures of variability?\nHow to determine significance?"
  },
  {
    "objectID": "slides/Day14_bsta511.html#anova-in-base-r",
    "href": "slides/Day14_bsta511.html#anova-in-base-r",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA in base R",
    "text": "ANOVA in base R\n\nThere are several options to run an ANOVA model in R\nTwo most common are lm and aov\n\nlm = linear model; will be using frequently in BSTA 512\n\n\n\nlm(score ~ disability, data = employ) %&gt;% anova()\n\nAnalysis of Variance Table\n\nResponse: score\n           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ndisability  4  30.521  7.6304  2.8616 0.03013 *\nResiduals  65 173.321  2.6665                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\naov(score ~ disability, data = employ) %&gt;% summary()\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ndisability   4  30.52   7.630   2.862 0.0301 *\nResiduals   65 173.32   2.666                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHypotheses:\n\\[\\begin{align}\nH_0 &: \\mu_{none} = \\mu_{amputation} = \\mu_{crutches} = \\mu_{hearing} =  \\mu_{wheelchair}\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j \\text{ for } i \\neq j\n\\end{align}\\]\nDo we reject or fail to reject \\(H_0\\)?"
  },
  {
    "objectID": "slides/Day14_bsta511.html#anova-tables",
    "href": "slides/Day14_bsta511.html#anova-tables",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA tables",
    "text": "ANOVA tables\nDisability example ANOVA table from R:\n\n\nlm(score ~ disability, data = employ) %&gt;% anova()\n\nAnalysis of Variance Table\n\nResponse: score\n           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ndisability  4  30.521  7.6304  2.8616 0.03013 *\nResiduals  65 173.321  2.6665                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nGeneric ANOVA table:"
  },
  {
    "objectID": "slides/Day14_bsta511.html#anova-analysis-of-variance",
    "href": "slides/Day14_bsta511.html#anova-analysis-of-variance",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA: Analysis of Variance",
    "text": "ANOVA: Analysis of Variance\nANOVA compares the variability between groups to the variability within groups"
  },
  {
    "objectID": "slides/Day14_bsta511.html#anova-analysis-of-variance-1",
    "href": "slides/Day14_bsta511.html#anova-analysis-of-variance-1",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA: Analysis of Variance",
    "text": "ANOVA: Analysis of Variance\nAnalysis of Variance (ANOVA) compares the variability between groups to the variability within groups\n\n\n\n\n\n\\[\\sum_{i = 1}^k \\sum_{j = 1}^{n_i}(x_{ij} -\\bar{x})^2 \\ \\\n= \\ \\sum_{i = 1}^k n_i(\\bar{x}_{i}-\\bar{x})^2 \\ \\\n+ \\ \\ \\sum_{i = 1}^k\\sum_{j = 1}^{n_i}(x_{ij}-\\bar{x}_{i})^2\\]"
  },
  {
    "objectID": "slides/Day14_bsta511.html#notation",
    "href": "slides/Day14_bsta511.html#notation",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Notation",
    "text": "Notation\n\n\n\nk groups\n\\(n_i\\) observations in each of the k groups\nTotal sample size is \\(N=\\sum_{i=1}^{k}n_i\\)\n\\(\\bar{x}_{i}\\) = mean of observations in group i\n\\(\\bar{x}\\) = mean of all observations\n\\(s_{i}\\) = sd of observations in group i\n\\(s\\) = sd of all observations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\ni = 1\ni = 2\ni = 3\n\\(\\ldots\\)\ni = k\noverall\n\n\n\n\nj = 1\n\\(x_{11}\\)\n\\(x_{21}\\)\n\\(x_{31}\\)\n\\(\\ldots\\)\n\\(x_{k1}\\)\n\n\n\nj = 2\n\\(x_{12}\\)\n\\(x_{22}\\)\n\\(x_{32}\\)\n\\(\\ldots\\)\n\\(x_{k2}\\)\n\n\n\nj = 3\n\\(x_{13}\\)\n\\(x_{23}\\)\n\\(x_{33}\\)\n\\(\\ldots\\)\n\\(x_{k3}\\)\n\n\n\nj = 4\n\\(x_{14}\\)\n\\(x_{24}\\)\n\\(x_{34}\\)\n\\(\\ldots\\)\n\\(x_{k4}\\)\n\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\n\n\nj = \\(n_i\\)\n\\(x_{1n_1}\\)\n\\(x_{2n_2}\\)\n\\(x_{3n_3}\\)\n\\(\\ldots\\)\n\\(x_{kn_k}\\)\n\n\n\nMeans\n\\(\\bar{x}_{1}\\)\n\\(\\bar{x}_{2}\\)\n\\(\\bar{x}_{3}\\)\n\\(\\ldots\\)\n\\(\\bar{x}_{k}\\)\n\\(\\bar{x}\\)\n\n\nVariance\n\\({s}^2_{1}\\)\n\\({s}^2_{2}\\)\n\\({s}^2_{3}\\)\n\\(\\ldots\\)\n\\({s}^2_{k}\\)\n\\({s}^2\\)"
  },
  {
    "objectID": "slides/Day14_bsta511.html#total-sums-of-squares-visually",
    "href": "slides/Day14_bsta511.html#total-sums-of-squares-visually",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Total Sums of Squares Visually",
    "text": "Total Sums of Squares Visually\n\n\n\n\n\n\n\n\nTotal Sums of Squares:\n\\[SST = \\sum_{i = 1}^k \\sum_{j = 1}^{n_i}(x_{ij} -\\bar{x})^2 = (N-1)s^2\\]\n\nwhere\n\n\\(N=\\sum_{i=1}^{k}n_i\\) is the total sample size and\n\\(s^2\\) is the grand standard deviation of all the observations\n\nThis is the sum of the squared differences between each observed \\(x_{ij}\\) value and the grand mean, \\(\\bar{x}\\).\nThat is, it is the total deviation of the \\(x_{ij}\\)’s from the grand mean."
  },
  {
    "objectID": "slides/Day14_bsta511.html#calculate-total-sums-of-squares",
    "href": "slides/Day14_bsta511.html#calculate-total-sums-of-squares",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Calculate Total Sums of Squares",
    "text": "Calculate Total Sums of Squares\nTotal Sums of Squares:\n\\[SST = \\sum_{i = 1}^k \\sum_{j = 1}^{n_i}(x_{ij} -\\bar{x})^2 = (N-1)s^2\\]\n\n\n\nwhere\n\n\\(N=\\sum_{i=1}^{k}n_i\\) is the total sample size and\n\\(s^2\\) is the grand standard deviation of all the observations\n\n\n\nTotal sample size \\(N\\):\n\n(Ns &lt;- employ %&gt;% group_by(disability) %&gt;% count())\n\n# A tibble: 5 × 2\n# Groups:   disability [5]\n  disability     n\n  &lt;fct&gt;      &lt;int&gt;\n1 none          14\n2 amputation    14\n3 crutches      14\n4 hearing       14\n5 wheelchair    14\n\n\n\\(SST\\):\n\n(SST &lt;- (sum(Ns$n) - 1) * sd(employ$score)^2)\n\n[1] 203.8429"
  },
  {
    "objectID": "slides/Day14_bsta511.html#anova-analysis-of-variance-2",
    "href": "slides/Day14_bsta511.html#anova-analysis-of-variance-2",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA: Analysis of Variance",
    "text": "ANOVA: Analysis of Variance\nANOVA compares the variability between groups to the variability within groups"
  },
  {
    "objectID": "slides/Day14_bsta511.html#sums-of-squares-due-to-groups-visually-between-groups",
    "href": "slides/Day14_bsta511.html#sums-of-squares-due-to-groups-visually-between-groups",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Sums of Squares due to Groups Visually (“between” groups)",
    "text": "Sums of Squares due to Groups Visually (“between” groups)\n\n\n\n\n\n\n\n\nSums of Squares due to Groups:\n\\[SSG = \\sum_{i = 1}^k n_i(\\bar{x}_{i}-\\bar{x})^2\\]\n\nThis is the sum of the squared differences between each group mean, \\(\\bar{x}_{i}\\), and the grand mean, \\(\\bar{x}\\).\nThat is, it is the deviation of the group means from the grand mean.\nAlso called the Model SS, or \\(SS_{model}.\\)"
  },
  {
    "objectID": "slides/Day14_bsta511.html#calculate-sums-of-squares-due-to-groups-between-groups",
    "href": "slides/Day14_bsta511.html#calculate-sums-of-squares-due-to-groups-between-groups",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Calculate Sums of Squares due to Groups (“between” groups)",
    "text": "Calculate Sums of Squares due to Groups (“between” groups)\n\\[SSG = \\sum_{i = 1}^k n_i(\\bar{x}_{i}-\\bar{x})^2\\]\n\n\n\n\n\n\n\n\nCalculate means \\(\\bar{x}_i\\) for each group:\n\nxbar_groups &lt;- employ %&gt;% \n  group_by(disability) %&gt;% \n  summarise(mean = mean(score))\nxbar_groups\n\n# A tibble: 5 × 2\n  disability  mean\n  &lt;fct&gt;      &lt;dbl&gt;\n1 none        4.9 \n2 amputation  4.43\n3 crutches    5.92\n4 hearing     4.05\n5 wheelchair  5.34\n\n\nCalculate \\(SSG\\):\n\n(SSG &lt;- sum(Ns$n *\n  (xbar_groups$mean - mean(employ$score))^2))\n\n[1] 30.52143"
  },
  {
    "objectID": "slides/Day14_bsta511.html#anova-analysis-of-variance-3",
    "href": "slides/Day14_bsta511.html#anova-analysis-of-variance-3",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA: Analysis of Variance",
    "text": "ANOVA: Analysis of Variance\nANOVA compares the variability between groups to the variability within groups"
  },
  {
    "objectID": "slides/Day14_bsta511.html#sums-of-squares-error-visually-within-groups",
    "href": "slides/Day14_bsta511.html#sums-of-squares-error-visually-within-groups",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Sums of Squares Error Visually (within groups)",
    "text": "Sums of Squares Error Visually (within groups)\n\n\n\n\n\n\n\n\nSums of Squares Error:\n\\[SSE = \\sum_{i = 1}^k\\sum_{j = 1}^{n_i}(x_{ij}-\\bar{x}_{i})^2 = \\sum_{i = 1}^k(n_i-1)s_{i}^2\\] where \\(s_{i}\\) is the standard deviation of the \\(i^{th}\\) group\n\nThis is the sum of the squared differences between each observed \\(x_{ij}\\) value and its group mean \\(\\bar{x}_{i}\\).\nThat is, it is the deviation of the \\(x_{ij}\\)’s from the predicted score by group.\nAlso called the residual sums of squares, or \\(SS_{residual}.\\)"
  },
  {
    "objectID": "slides/Day14_bsta511.html#calculate-sums-of-squares-error-within-groups",
    "href": "slides/Day14_bsta511.html#calculate-sums-of-squares-error-within-groups",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Calculate Sums of Squares Error (within groups)",
    "text": "Calculate Sums of Squares Error (within groups)\n\\[SSE = \\sum_{i = 1}^k\\sum_{j = 1}^{n_i}(x_{ij}-\\bar{x}_{i})^2 = \\sum_{i = 1}^k(n_i-1)s_{i}^2\\] where \\(s_{i}\\) is the standard deviation of the \\(i^{th}\\) group\n\n\n\n\n\n\n\n\nCalculate sd’s \\(s_i\\) for each group:\n\nsd_groups &lt;- employ %&gt;% \n  group_by(disability) %&gt;% \n  summarise(SD = sd(score))\nsd_groups\n\n# A tibble: 5 × 2\n  disability    SD\n  &lt;fct&gt;      &lt;dbl&gt;\n1 none        1.79\n2 amputation  1.59\n3 crutches    1.48\n4 hearing     1.53\n5 wheelchair  1.75\n\n\nCalculate \\(SSE\\):\n\n(SSE &lt;- sum(\n  (Ns$n-1)*sd_groups$SD^2))\n\n[1] 173.3214"
  },
  {
    "objectID": "slides/Day14_bsta511.html#verify-sst-ssg-sse",
    "href": "slides/Day14_bsta511.html#verify-sst-ssg-sse",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Verify SST = SSG + SSE",
    "text": "Verify SST = SSG + SSE\nANOVA compares the variability between groups to the variability within groups\n\n\n\n\n\n\\[\\sum_{i = 1}^k \\sum_{j = 1}^{n_i}(x_{ij} -\\bar{x})^2 \\ \\ = \\ \\ n_i\\sum_{i = 1}^k(\\bar{x}_{i}-\\bar{x})^2 \\ \\\n+ \\ \\ \\sum_{i = 1}^k\\sum_{j = 1}^{n_i}(x_{ij}-\\bar{x}_{i})^2\\]\n\\[(N-1)s^2 \\ \\\n= \\ \\sum_{i = 1}^k n_i(\\bar{x}_{i}-\\bar{x})^2 \\ \\\n+ \\ \\ \\sum_{i = 1}^k(n_i-1)s_{i}^2\\]\n\n\n\n\n\n\n\n\nSST\n\n[1] 203.8429\n\n\n\n\nSSG + SSE\n\n[1] 203.8429"
  },
  {
    "objectID": "slides/Day14_bsta511.html#anova-table",
    "href": "slides/Day14_bsta511.html#anova-table",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA table",
    "text": "ANOVA table"
  },
  {
    "objectID": "slides/Day14_bsta511.html#thinking-about-the-f-statistic",
    "href": "slides/Day14_bsta511.html#thinking-about-the-f-statistic",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Thinking about the F-statistic",
    "text": "Thinking about the F-statistic\n\n\nIf the groups are actually different, then which of these is more accurate?\n\nThe variability between groups should be higher than the variability within groups\nThe variability within groups should be higher than the variability between groups\n\n\nIf there really is a difference between the groups, we would expect the F-statistic to be which of these:\n\nHigher than we would observe by random chance\nLower than we would observe by random chance\n\n\n\n\n\n\\[F = \\frac{MSG}{MSE}\\]"
  },
  {
    "objectID": "slides/Day14_bsta511.html#anova-in-base-r-1",
    "href": "slides/Day14_bsta511.html#anova-in-base-r-1",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA in base R",
    "text": "ANOVA in base R\n\n# Note that I'm saving the tidy anova table\n# Will be pulling p-value from this on future slide\n\nempl_lm &lt;- lm(score ~ disability, data = employ) %&gt;% \n  anova() %&gt;% \n  tidy()\n\nempl_lm %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    disability\n4\n30.52143\n7.630357\n2.86158\n0.03012686\n    Residuals\n65\n173.32143\n2.666484\nNA\nNA\n  \n  \n  \n\n\n\n\nHypotheses:\n\\[\\begin{align}\nH_0 &: \\mu_{none} = \\mu_{amputation} = \\mu_{crutches} = \\mu_{hearing} =  \\mu_{wheelchair}\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j \\text{ for } i \\neq j\n\\end{align}\\]\nDo we reject or fail to reject \\(H_0\\)?"
  },
  {
    "objectID": "slides/Day14_bsta511.html#conclusion-to-hypothesis-test",
    "href": "slides/Day14_bsta511.html#conclusion-to-hypothesis-test",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Conclusion to hypothesis test",
    "text": "Conclusion to hypothesis test\n\\[\\begin{align}\nH_0 &: \\mu_{none} = \\mu_{amputation} = \\mu_{crutches} = \\mu_{hearing} =  \\mu_{wheelchair}\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j \\text{ for } i \\neq j\n\\end{align}\\]\n\n\n\nempl_lm  # tidy anova output\n\n# A tibble: 2 × 6\n  term          df sumsq meansq statistic p.value\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 disability     4  30.5   7.63      2.86  0.0301\n2 Residuals     65 173.    2.67     NA    NA     \n\n# Note that this is a vector:\nempl_lm$p.value\n\n[1] 0.03012686         NA\n\n\nPull the p-value using base R:\n\nround(empl_lm$p.value[1],2)\n\n[1] 0.03\n\n\nPull the p-value using tidyverse:\n\nempl_lm %&gt;% \n  filter(term == \"disability\") %&gt;% \n  pull(p.value) %&gt;% \n  round(2)\n\n[1] 0.03\n\n\n\n\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nThere is sufficient evidence that at least one of the disability groups has a mean employment score statistically different from the other groups. ( \\(p\\)-value = 0.03)."
  },
  {
    "objectID": "slides/Day14_bsta511.html#conditions-for-anova",
    "href": "slides/Day14_bsta511.html#conditions-for-anova",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Conditions for ANOVA",
    "text": "Conditions for ANOVA\nIF ALL of the following conditions hold:\n\nThe null hypothesis is true\nSample sizes in each group group are large (each \\(n \\ge 30\\))\n\nOR the data are relatively normally distributed in each group\n\n\n\n\n\nVariability is “similar” in all group groups:\n\nIs the within group variability about the same for each group?\nAs a rough rule of thumb, this condition is violated if the standard deviation of one group is more than double the standard deviation of another group\n\n\n\n\n\nChecking the equal variance condition:\n\nsd_groups # previously defined\n\n# A tibble: 5 × 2\n  disability    SD\n  &lt;fct&gt;      &lt;dbl&gt;\n1 none        1.79\n2 amputation  1.59\n3 crutches    1.48\n4 hearing     1.53\n5 wheelchair  1.75\n\nmax(sd_groups$SD) / min(sd_groups$SD)\n\n[1] 1.210425\n\n\n\n\nTHEN the sampling distribution of the F-statistic is an F-distribution"
  },
  {
    "objectID": "slides/Day14_bsta511.html#testing-variances-condition-3",
    "href": "slides/Day14_bsta511.html#testing-variances-condition-3",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Testing variances (Condition 3)",
    "text": "Testing variances (Condition 3)\nBartlett’s test for equal variances\n\n\\(H_0:\\) population variances of group levels are equal\n\\(H_A:\\) population variances of group levels are NOT equal\n\nNote: \\(H_A\\) is same as saying that at least one of the group levels has a different variance\n\n\n\n\n\n\nCaution\n\n\n\nBartlett’s test assumes the data in each group are normally distributed.\nDo not use if data do not satisfy the normality condition.\n\n\n\n\n\nbartlett.test(score ~ disability, data = employ)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  score by disability\nBartlett's K-squared = 0.7016, df = 4, p-value = 0.9511\n\n\n\n\n\n\n\n\nTip\n\n\nLevene’s test for equality of variances is not as restrictive: see https://www.statology.org/levenes-test-r/"
  },
  {
    "objectID": "slides/Day14_bsta511.html#the-f-distribution",
    "href": "slides/Day14_bsta511.html#the-f-distribution",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "The F-distribution",
    "text": "The F-distribution\n\nThe F-distribution is skewed right.\nThe F-distribution has two different degrees of freedom:\n\none for the numerator of the ratio (k – 1) and\none for the denominator (N – k)\n\n\\(p\\)-value\n\nis always the upper tail\n(the area as extreme or more extreme)\n\n\n\n\n\n\n\n\n\n\n\nempl_lm %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    disability\n4\n30.52143\n7.630357\n2.86158\n0.03012686\n    Residuals\n65\n173.32143\n2.666484\nNA\nNA\n  \n  \n  \n\n\n\n# p-value using F-distribution\n\npf(2.86158, df1=5-1, df2=70-5, \n   lower.tail = FALSE)\n\n[1] 0.03012688"
  },
  {
    "objectID": "slides/Day14_bsta511.html#which-groups-are-statistically-different",
    "href": "slides/Day14_bsta511.html#which-groups-are-statistically-different",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Which groups are statistically different?",
    "text": "Which groups are statistically different?\n\n\n\n\nSo far we’ve only determined that at least one of the groups is different from the others,\n\nbut we don’t know which.\n\n\n\n\nWhat’s your guess?"
  },
  {
    "objectID": "slides/Day14_bsta511.html#post-hoc-testing-pairwise-t-tests",
    "href": "slides/Day14_bsta511.html#post-hoc-testing-pairwise-t-tests",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Post-hoc testing: pairwise t-tests",
    "text": "Post-hoc testing: pairwise t-tests\n\n\n\nIn post-hoc testing we run all the pairwise t-tests comparing the means from each pair of groups.\nWith 5 groups, this involves doing \\({5 \\choose 2} = \\frac{5!}{2!3!} = \\frac{5\\cdot 4}{2}= 10\\) different pairwise tests.\n\nProblem:\n\nAlthough the ANOVA test has an \\(\\alpha\\) chance of a Type I error (finding a difference between a pair that aren’t different),\nthe overall Type I error rate will be much higher when running many tests simultaneously.\n\n\n\n\n\n\\[\\begin{align}\nP(\\text{making an error}) = & \\alpha\\\\\nP(\\text{not making an error}) = & 1-\\alpha\\\\\nP(\\text{not making an error in m tests}) = & (1-\\alpha)^m\\\\\nP(\\text{making at least 1 error in m tests}) = & 1-(1-\\alpha)^m\n\\end{align}\\]"
  },
  {
    "objectID": "slides/Day14_bsta511.html#the-bonferroni-correction-12",
    "href": "slides/Day14_bsta511.html#the-bonferroni-correction-12",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "The Bonferroni Correction (1/2)",
    "text": "The Bonferroni Correction (1/2)\n\n\nA very conservative (but very popular) approach is to divide the \\(\\alpha\\) level by how many tests \\(m\\) are being done:\n\\[\\alpha_{Bonf} = \\frac{\\alpha}{m}\\]\n\nThis is equivalent to multiplying the\np-values by m:\n\n\\[p\\textrm{-value} &lt; \\alpha_{Bonf} = \\frac{\\alpha}{m}\\] is the same as \\[m \\cdot (p\\textrm{-value}) &lt; \\alpha\\] The Bonferroni correction is popular since it’s very easy to implement.\n\n\nThe plot below shows the likelihood of making at least one Type I error depending on how may tests are done.\nNotice the likelihood decreases very quickly\n\nUnfortunately the likelihood of a Type II error is increasing as well\nIt becomes “harder” and harder to reject \\(H_0\\) if doing many tests."
  },
  {
    "objectID": "slides/Day14_bsta511.html#the-bonferroni-correction-22",
    "href": "slides/Day14_bsta511.html#the-bonferroni-correction-22",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "The Bonferroni Correction (2/2)",
    "text": "The Bonferroni Correction (2/2)\n\n\nPairwise t-tests without any p-value adjustments:\n\npairwise.t.test(employ$score, \n                employ$disability, \n                p.adj=\"none\") \n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  employ$score and employ$disability \n\n           none   amputation crutches hearing\namputation 0.4477 -          -        -      \ncrutches   0.1028 0.0184     -        -      \nhearing    0.1732 0.5418     0.0035   -      \nwheelchair 0.4756 0.1433     0.3520   0.0401 \n\nP value adjustment method: none \n\n\n\nPairwise t-tests with Bonferroni p-value adjustments:\n\npairwise.t.test(employ$score,  \n                employ$disability, \n                p.adj=\"bonferroni\")  \n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  employ$score and employ$disability \n\n           none  amputation crutches hearing\namputation 1.000 -          -        -      \ncrutches   1.000 0.184      -        -      \nhearing    1.000 1.000      0.035    -      \nwheelchair 1.000 1.000      1.000    0.401  \n\nP value adjustment method: bonferroni \n\n\n\nSince there were 10 tests, all the p-values were multiplied by 10.\nAre there any significant pairwise differences?"
  },
  {
    "objectID": "slides/Day14_bsta511.html#tukeys-honest-significance-test-hsd",
    "href": "slides/Day14_bsta511.html#tukeys-honest-significance-test-hsd",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Tukey’s Honest Significance Test (HSD)",
    "text": "Tukey’s Honest Significance Test (HSD)\n\nTukey’s Honest Significance Test (HSD) controls the “family-wise probability” of making a Type I error using a much less conservative method than Bonferroni\n\nIt is specific to ANOVA\n\nIn addition to adjusted p-values, it also calculates Tukey adjusted CI’s for all pairwise differences\nThe function TukeyHSD() creates a set of confidence intervals of the differences between means with the specified family-wise probability of coverage.\n\n\n\n\n# need to run the model using `aov` instead of `lm`\nempl_aov &lt;- aov(score ~ disability, data = employ) \n\nTukeyHSD(x=empl_aov, conf.level = 0.95) \n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = score ~ disability, data = employ)\n\n$disability\n                            diff        lwr        upr     p adj\namputation-none       -0.4714286 -2.2031613  1.2603042 0.9399911\ncrutches-none          1.0214286 -0.7103042  2.7531613 0.4686233\nhearing-none          -0.8500000 -2.5817328  0.8817328 0.6442517\nwheelchair-none        0.4428571 -1.2888756  2.1745899 0.9517374\ncrutches-amputation    1.4928571 -0.2388756  3.2245899 0.1232819\nhearing-amputation    -0.3785714 -2.1103042  1.3531613 0.9724743\nwheelchair-amputation  0.9142857 -0.8174470  2.6460185 0.5781165\nhearing-crutches      -1.8714286 -3.6031613 -0.1396958 0.0277842\nwheelchair-crutches   -0.5785714 -2.3103042  1.1531613 0.8812293\nwheelchair-hearing     1.2928571 -0.4388756  3.0245899 0.2348141\n\n\n\n\nplot(TukeyHSD(x=empl_aov, \n        conf.level = 0.95))"
  },
  {
    "objectID": "slides/Day14_bsta511.html#there-are-many-more-multiple-testing-adjustment-procedures",
    "href": "slides/Day14_bsta511.html#there-are-many-more-multiple-testing-adjustment-procedures",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "There are many more multiple testing adjustment procedures",
    "text": "There are many more multiple testing adjustment procedures\n\n\n\nBonferroni is popular because it’s so easy to apply\nTukey’s HSD is usually used for ANOVA\nCode below used Holm’s adjustment\n\n\n# default is Holm's adjustments\npairwise.t.test(employ$score, \n                employ$disability) \n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  employ$score and employ$disability \n\n           none  amputation crutches hearing\namputation 1.000 -          -        -      \ncrutches   0.719 0.165      -        -      \nhearing    0.866 1.000      0.035    -      \nwheelchair 1.000 0.860      1.000    0.321  \n\nP value adjustment method: holm \n\n\n\n\n\n\nFalse discovery rate (fdr) p-value adjustments are popular in omics, or whenever there are many tests being run:\n\n\npairwise.t.test(employ$score, \n                employ$disability, \n                p.adj=\"fdr\") \n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  employ$score and employ$disability \n\n           none  amputation crutches hearing\namputation 0.528 -          -        -      \ncrutches   0.257 0.092      -        -      \nhearing    0.289 0.542      0.035    -      \nwheelchair 0.528 0.287      0.503    0.134  \n\nP value adjustment method: fdr"
  },
  {
    "objectID": "slides/Day14_bsta511.html#multiple-testing-controlling-the-type-i-error-rate",
    "href": "slides/Day14_bsta511.html#multiple-testing-controlling-the-type-i-error-rate",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Multiple testing: controlling the Type I error rate",
    "text": "Multiple testing: controlling the Type I error rate\n\n\n\nThe multiple testing issue is not unique to ANOVA post-hoc testing.\nIt is also a concern when running separate tests for many related outcomes.\nBeware of p-hacking!\n\nProblem:\n\nAlthough one test has an \\(\\alpha\\) chance of a Type I error (finding a difference between a pair that aren’t different),\nthe overall Type I error rate will be much higher when running many tests simultaneously.\n\n\n\n\n\n\\[\\begin{align}\nP(\\text{making an error}) = & \\alpha\\\\\nP(\\text{not making an error}) = & 1-\\alpha\\\\\nP(\\text{not making an error in m tests}) = & (1-\\alpha)^m\\\\\nP(\\text{making at least 1 error in m tests}) = & 1-(1-\\alpha)^m\n\\end{align}\\]"
  },
  {
    "objectID": "slides/Day14_bsta511.html#anova-summary",
    "href": "slides/Day14_bsta511.html#anova-summary",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA Summary",
    "text": "ANOVA Summary\n\n\\[\\begin{align}\nH_0 &: \\mu_1 = \\mu_2 = ... = \\mu_k\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j \\text{ for } i \\neq j\n\\end{align}\\]\n\nANOVA table in R:\n\n\nlm(score ~ disability, data = employ) %&gt;% anova()\n\nAnalysis of Variance Table\n\nResponse: score\n           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ndisability  4  30.521  7.6304  2.8616 0.03013 *\nResiduals  65 173.321  2.6665                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nANOVA table\n\n\n\n\n\nPost-hoc testing\n\nF-distribution & p-value"
  },
  {
    "objectID": "slides/Day14_bsta511.html#whats-next",
    "href": "slides/Day14_bsta511.html#whats-next",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "What’s next?",
    "text": "What’s next?"
  },
  {
    "objectID": "slides/Day15_bsta511.html",
    "href": "slides/Day15_bsta511.html",
    "title": "Day 15: Simple Linear Regression (Sections 6.1-6.2)",
    "section": "",
    "text": "Download pdf of slides"
  },
  {
    "objectID": "slides/Day17_bsta511.html",
    "href": "slides/Day17_bsta511.html",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "",
    "text": "Download pdf of slides\n12/2/23: See updates made to code file for creating ranks using R’s rank() function that has an option ties.method to specify how to calculate ties.\n\nSearch “New:” for updates made."
  },
  {
    "objectID": "slides/Day16_bsta511.html#goals-for-today-sections-6.3-6.4",
    "href": "slides/Day16_bsta511.html#goals-for-today-sections-6.3-6.4",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Goals for today (Sections 6.3-6.4)",
    "text": "Goals for today (Sections 6.3-6.4)\nSimple Linear Regression Part 2\n\nReview of\n\nbest-fit line (aka regression line or least-squares line)\nresiduals\npopulation model\n\nLINE conditions and how to assess them\n\nNew diagnostic tools:\n\nNormal QQ plots of residuals\nResidual plots\n\n\nCoefficient of determination (\\(R^2\\))\nRegression inference\n\nInference for population slope \\(\\beta_1\\)\n\nCI & hypothesis test\n\nCI for mean response \\(\\mu_{Y|x^*}\\)\nPrediction interval for predicting individual observations\n\n\nConfidence bands vs. predictions bands"
  },
  {
    "objectID": "slides/Day16_bsta511.html#life-expectancy-vs.-female-adult-literacy-rate",
    "href": "slides/Day16_bsta511.html#life-expectancy-vs.-female-adult-literacy-rate",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Life expectancy vs. female adult literacy rate",
    "text": "Life expectancy vs. female adult literacy rate\nhttps://www.gapminder.org/tools/#$model$markers$bubble$encoding$x$data$concept=literacy_rate_adult_female_percent_of_females_ages_15_above&source=sg&space@=country&=time;;&scale$domain:null&zoomed:null&type:null;;&frame$value=2011;;;;;&chart-type=bubbles&url=v1"
  },
  {
    "objectID": "slides/Day16_bsta511.html#dataset-description",
    "href": "slides/Day16_bsta511.html#dataset-description",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Dataset description",
    "text": "Dataset description\n\nData file: lifeexp_femlit_water_2011.csv\nData were downloaded from https://www.gapminder.org/data/\n2011 is the most recent year with the most complete data\nLife expectancy = the average number of years a newborn child would live if current mortality patterns were to stay the same. Source: https://www.gapminder.org/data/documentation/gd004/\nAdult literacy rate is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life. Source: http://data.uis.unesco.org/\nAt least basic water source (%) = the percentage of people using at least basic water services. This indicator encompasses both people using basic water services as well as those using safely managed water services. Basic drinking water services is defined as drinking water from an improved source, provided collection time is not more than 30 minutes for a round trip. Improved water sources include piped water, boreholes or tubewells, protect dug wells, protected springs, and packaged or delivered water."
  },
  {
    "objectID": "slides/Day16_bsta511.html#get-to-know-the-data",
    "href": "slides/Day16_bsta511.html#get-to-know-the-data",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Get to know the data",
    "text": "Get to know the data\nLoad data\n\ngapm_original &lt;- read_csv(here::here(\"data\", \"lifeexp_femlit_water_2011.csv\"))\n\nGlimpse of the data\n\nglimpse(gapm_original)\n\nRows: 194\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andor…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 76.7, 82.6, 60.9, 76.9, 76.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, NA, NA, 58.6, 99.4, 97.9, 99.5,…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 92.6, 100.0, 40.3, 97.0, 99.5, …\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q2\", \"Q4\", \"Q1\", \"Q3\", \"Q4\", \"…\n\n\nNote the missing values for our variables of interest\n\ngapm_original %&gt;% select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  get_summary_stats()\n\n# A tibble: 2 × 13\n  variable        n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 life_expec…   187  47.5  82.9   72.7  64.3  76.9  12.6  9.04  70.7  8.44 0.617\n2 female_lit…    80  13    99.8   91.6  71.0  98.0  27.0 11.4   81.7 22.0  2.45 \n# ℹ 1 more variable: ci &lt;dbl&gt;"
  },
  {
    "objectID": "slides/Day16_bsta511.html#remove-missing-values",
    "href": "slides/Day16_bsta511.html#remove-missing-values",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Remove missing values",
    "text": "Remove missing values\nRemove rows with missing data for life expectancy and female literacy rate\n\ngapm &lt;- gapm_original %&gt;% \n  drop_na(life_expectancy_years_2011, female_literacy_rate_2011)\n\nglimpse(gapm)\n\nRows: 80\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Angola\", \"Antigu…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 40.3, 97.0, 99.5, 97.8, 96.7, 9…\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q1\", \"Q3\", \"Q4\", \"Q3\", \"Q3\", \"…\n\n\nNo missing values now for our variables of interest\n\ngapm %&gt;% select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  get_summary_stats()\n\n# A tibble: 2 × 13\n  variable        n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 life_expec…    80    48  81.8   72.4  65.9  75.8  9.95  6.30  69.9  7.95 0.889\n2 female_lit…    80    13  99.8   91.6  71.0  98.0 27.0  11.4   81.7 22.0  2.45 \n# ℹ 1 more variable: ci &lt;dbl&gt;\n\n\n\n\n\n\n\n\nImportant\n\n\n\nRemoving the rows with missing data was not needed to run the regression model.\nI did this step since later we will be calculating the standard deviations of the explanatory and response variables for just the values included in the regression model. It’ll be easier to do this if we remove the missing values now."
  },
  {
    "objectID": "slides/Day16_bsta511.html#regression-line-best-fit-line",
    "href": "slides/Day16_bsta511.html#regression-line-best-fit-line",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Regression line = best-fit line",
    "text": "Regression line = best-fit line\n\n\n\\[\\widehat{y} = b_0 + b_1 \\cdot x \\]\n\n\\(\\hat{y}\\) is the predicted outcome for a specific value of \\(x\\).\n\\(b_0\\) is the intercept\n\\(b_1\\) is the slope of the line, i.e., the increase in \\(\\hat{y}\\) for every increase of one (unit increase) in \\(x\\).\n\nslope = rise over run\n\n\n\n\n\n\n\n\n\n\n\nIntercept\n\nThe expected outcome for the \\(y\\)-variable when the \\(x\\)-variable is 0.\n\nSlope\n\nFor every increase of 1 unit in the \\(x\\)-variable, there is an expected increase of, on average, \\(b_1\\) units in the \\(y\\)-variable.\nWe only say that there is an expected increase and not necessarily a causal increase."
  },
  {
    "objectID": "slides/Day16_bsta511.html#regression-in-r-lm-summary-tidy",
    "href": "slides/Day16_bsta511.html#regression-in-r-lm-summary-tidy",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Regression in R: lm(), summary(), & tidy()",
    "text": "Regression in R: lm(), summary(), & tidy()\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\ntidy(model1) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\nRegression equation for our model:\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot \\textrm{female literacy rate} \\]"
  },
  {
    "objectID": "slides/Day16_bsta511.html#residuals",
    "href": "slides/Day16_bsta511.html#residuals",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Residuals",
    "text": "Residuals\n\n\n\nObserved values \\(y_i\\)\n\nthe values in the dataset\n\nFitted values \\(\\widehat{y}_i\\)\n\nthe values that fall on the best-fit line for a specific \\(x_i\\)\n\nResiduals \\(e_i = y_i - \\widehat{y}_i\\)\n\nthe differences between the observed and fitted values"
  },
  {
    "objectID": "slides/Day16_bsta511.html#the-population-regresison-model",
    "href": "slides/Day16_bsta511.html#the-population-regresison-model",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "The (population) regresison model",
    "text": "The (population) regresison model\n\n\n\nThe (population) regression model is denoted by\n\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\epsilon\\]\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown population parameters\n\\(\\epsilon\\) (epsilon) is the error about the line\n\nIt is assumed to be a random variable:\n\n\\(\\epsilon \\sim N(0, \\sigma^2)\\)\nvariance \\(\\sigma^2\\) is constant\n\n\n\n\n\n\n\n\n\n\nhttps://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions\n\n\n\n\nThe line is the average (expected) value of \\(Y\\) given a value of \\(x\\): \\(E(Y|x)\\).\nThe point estimates for \\(\\beta_0\\) and \\(\\beta_1\\) based on a sample are denoted by \\(b_0, b_1, s_{residuals}^2\\)\n\nNote: also common notation is \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\sigma}^2\\)"
  },
  {
    "objectID": "slides/Day16_bsta511.html#what-are-the-line-conditions",
    "href": "slides/Day16_bsta511.html#what-are-the-line-conditions",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "What are the LINE conditions?",
    "text": "What are the LINE conditions?\nFor “good” model fit and to be able to make inferences and predictions based on our models, 4 conditions need to be satisfied.\nBriefly:\n\nL inearity of relationship between variables\nI ndependence of the Y values\nN ormality of the residuals\nE quality of variance of the residuals (homoscedasticity)\n\nMore in depth:\n\nL : there is a linear relationship between the mean response (Y) and the explanatory variable (X),\nI : the errors are independent—there’s no connection between how far any two points lie from the regression line,\nN : the responses are normally distributed at each level of X, and\nE : the variance or, equivalently, the standard deviation of the responses is equal for all levels of X."
  },
  {
    "objectID": "slides/Day16_bsta511.html#l-linearity-of-relationship-between-variables",
    "href": "slides/Day16_bsta511.html#l-linearity-of-relationship-between-variables",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "L: Linearity of relationship between variables",
    "text": "L: Linearity of relationship between variables\nIs the association between the variables linear?\n\nDiagnostic tools:\n\nScatterplot\nResidual plot (see later section for E : Equality of variance of the residuals)"
  },
  {
    "objectID": "slides/Day16_bsta511.html#i-independence-of-the-residuals-y-values",
    "href": "slides/Day16_bsta511.html#i-independence-of-the-residuals-y-values",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "I: Independence of the residuals (\\(Y\\) values)",
    "text": "I: Independence of the residuals (\\(Y\\) values)\n\nAre the data points independent of each other?\nExamples of when they are not independent, include\n\nrepeated measures (such as baseline, 3 months, 6 months)\ndata from clusters, such as different hospitals or families\n\nThis condition is checked by reviewing the study design and not by inspecting the data\nHow to analyze data using regression models when the \\(Y\\)-values are not independent is covered in BSTA 519 (Longitudinal data)"
  },
  {
    "objectID": "slides/Day16_bsta511.html#n-normality-of-the-residuals-1",
    "href": "slides/Day16_bsta511.html#n-normality-of-the-residuals-1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "N: Normality of the residuals",
    "text": "N: Normality of the residuals\n\nThe responses Y are normally distributed at each level of x\n\n\n\nhttps://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions"
  },
  {
    "objectID": "slides/Day16_bsta511.html#extract-models-residuals-in-r",
    "href": "slides/Day16_bsta511.html#extract-models-residuals-in-r",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Extract model’s residuals in R",
    "text": "Extract model’s residuals in R\n\nFirst extract the residuals’ values from the model output using the augment() function from the broom package.\nGet a tibble with the orginal data, as well as the residuals and some other important values.\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011, \n                data = gapm)\naug1 &lt;- augment(model1) \n\nglimpse(aug1)\n\nRows: 80\nColumns: 8\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…"
  },
  {
    "objectID": "slides/Day16_bsta511.html#check-normality-with-usual-distribution-plots",
    "href": "slides/Day16_bsta511.html#check-normality-with-usual-distribution-plots",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Check normality with “usual” distribution plots",
    "text": "Check normality with “usual” distribution plots\nNote that below I save each figure, and then combine them together in one row of output using grid.arrange() from the gridExtra package.\n\nhist1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_histogram()\n\ndensity1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_density()\n\nbox1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_boxplot()\n\nlibrary(gridExtra) # NEW!!!\ngrid.arrange(hist1, density1, box1, nrow = 1)"
  },
  {
    "objectID": "slides/Day16_bsta511.html#normal-qq-plots-qq-quantile-quantile",
    "href": "slides/Day16_bsta511.html#normal-qq-plots-qq-quantile-quantile",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Normal QQ plots (QQ = quantile-quantile)",
    "text": "Normal QQ plots (QQ = quantile-quantile)\n\nIt can be tricky to eyeball with a histogram or density plot whether the residuals are normal or not\nQQ plots are often used to help with this\n\n\n\n\nVertical axis: data quantiles\n\ndata points are sorted in order and\nassigned quantiles based on how many data points there are\n\nHorizontal axis: theoretical quantiles\n\nmean and standard deviation (SD) calculated from the data points\ntheoretical quantiles are calculated for each point, assuming the data are modeled by a normal distribution with the mean and SD of the data\n\n\n\n\n\n\n\n\n\n\n\nData are approximately normal if points fall on a line.\n\nSee more info at https://data.library.virginia.edu/understanding-QQ-plots/"
  },
  {
    "objectID": "slides/Day16_bsta511.html#examples-of-normal-qq-plots-15",
    "href": "slides/Day16_bsta511.html#examples-of-normal-qq-plots-15",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of Normal QQ plots (1/5)",
    "text": "Examples of Normal QQ plots (1/5)\n\nData:\n\nBody measurements from 507 physically active individuals\nin their 20’s or early 30’s\nwithin normal weight range."
  },
  {
    "objectID": "slides/Day16_bsta511.html#examples-of-normal-qq-plots-25",
    "href": "slides/Day16_bsta511.html#examples-of-normal-qq-plots-25",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of Normal QQ plots (2/5)",
    "text": "Examples of Normal QQ plots (2/5)\nSkewed right distribution"
  },
  {
    "objectID": "slides/Day16_bsta511.html#examples-of-normal-qq-plots-35",
    "href": "slides/Day16_bsta511.html#examples-of-normal-qq-plots-35",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of Normal QQ plots (3/5)",
    "text": "Examples of Normal QQ plots (3/5)\nLong tails in distribution"
  },
  {
    "objectID": "slides/Day16_bsta511.html#examples-of-normal-qq-plots-45",
    "href": "slides/Day16_bsta511.html#examples-of-normal-qq-plots-45",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of Normal QQ plots (4/5)",
    "text": "Examples of Normal QQ plots (4/5)\nBimodal distribution"
  },
  {
    "objectID": "slides/Day16_bsta511.html#examples-of-normal-qq-plots-55",
    "href": "slides/Day16_bsta511.html#examples-of-normal-qq-plots-55",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of Normal QQ plots (5/5)",
    "text": "Examples of Normal QQ plots (5/5)"
  },
  {
    "objectID": "slides/Day16_bsta511.html#qq-plot-of-residuals-of-model1",
    "href": "slides/Day16_bsta511.html#qq-plot-of-residuals-of-model1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "QQ plot of residuals of model1",
    "text": "QQ plot of residuals of model1\n\n\n\n\n\n\n\n\n\nggplot(aug1, aes(sample = .resid)) + \n  stat_qq() +     # points\n  stat_qq_line()  # line"
  },
  {
    "objectID": "slides/Day16_bsta511.html#compare-to-randomly-generated-normal-qq-plots",
    "href": "slides/Day16_bsta511.html#compare-to-randomly-generated-normal-qq-plots",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Compare to randomly generated Normal QQ plots",
    "text": "Compare to randomly generated Normal QQ plots\nHow “good” we can expect a QQ plot to look depends on the sample size.\n\nThe QQ plots on the next slides are randomly generated\n\nusing random samples from actual standard normal distributions \\(N(0,1)\\).\n\nThus, all the points in the QQ plots should theoretically fall in a line\nHowever, there is sampling variability…"
  },
  {
    "objectID": "slides/Day16_bsta511.html#randomly-generated-normal-qq-plots-n100",
    "href": "slides/Day16_bsta511.html#randomly-generated-normal-qq-plots-n100",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Randomly generated Normal QQ plots: n=100",
    "text": "Randomly generated Normal QQ plots: n=100\n\nNote that stat_qq_line() doesn’t work with randomly generated samples, and thus the code below manually creates the line that the points should be on (which is \\(y=x\\) in this case.)\n\n\n\n\n\nsamplesize &lt;- 100\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/Day16_bsta511.html#examples-of-simulated-normal-qq-plots-n10",
    "href": "slides/Day16_bsta511.html#examples-of-simulated-normal-qq-plots-n10",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of simulated Normal QQ plots: n=10",
    "text": "Examples of simulated Normal QQ plots: n=10\nWith fewer data points,\n\nsimulated QQ plots are more likely to look “less normal”\neven though the data points were sampled from normal distributions.\n\n\n\n\n\nsamplesize &lt;- 10  # only change made to code!\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/Day16_bsta511.html#examples-of-simulated-normal-qq-plots-n1000",
    "href": "slides/Day16_bsta511.html#examples-of-simulated-normal-qq-plots-n1000",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of simulated Normal QQ plots: n=1,000",
    "text": "Examples of simulated Normal QQ plots: n=1,000\nWith more data points,\n\nsimulated QQ plots are more likely to look “more normal”\n\n\n\n\n\nsamplesize &lt;- 1000 # only change made to code!\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/Day16_bsta511.html#back-to-our-example",
    "href": "slides/Day16_bsta511.html#back-to-our-example",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Back to our example",
    "text": "Back to our example\n\n\nResiduals from Life Expectancy vs. Female Literacy Rate Regression\n\nggplot(aug1, \n      aes(sample = .resid)) + \n  stat_qq() + \n  stat_qq_line() \n\n\n\n\n\n\n\n\n\nSimulated QQ plot of Normal Residuals with n = 80\n\n\n\n# number of observations \n# in fitted model\nnobs(model1) \n\n[1] 80\n\n\n\nggplot() +\n  stat_qq(aes(\n    sample = rnorm(80))) + \n  geom_abline(\n    intercept = 0, slope = 1, \n    color = \"blue\")"
  },
  {
    "objectID": "slides/Day16_bsta511.html#residual-plot",
    "href": "slides/Day16_bsta511.html#residual-plot",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Residual plot",
    "text": "Residual plot\n\n\\(x\\) = explanatory variable from regression model\n\n(or the fitted values for a multiple regression)\n\n\\(y\\) = residuals from regression model\n\n\n\n\nnames(aug1)\n\n[1] \"life_expectancy_years_2011\" \"female_literacy_rate_2011\" \n[3] \".fitted\"                    \".resid\"                    \n[5] \".hat\"                       \".sigma\"                    \n[7] \".cooksd\"                    \".std.resid\"                \n\n\n\n\nggplot(aug1, \n       aes(x = female_literacy_rate_2011, \n           y = .resid)) + \n  geom_point() +\n  geom_abline(\n    intercept = 0, \n    slope = 0, \n    color = \"orange\") +\n  labs(title = \"Residual plot\")"
  },
  {
    "objectID": "slides/Day16_bsta511.html#e-equality-of-variance-of-the-residuals-homoscedasticity",
    "href": "slides/Day16_bsta511.html#e-equality-of-variance-of-the-residuals-homoscedasticity",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "E: Equality of variance of the residuals (Homoscedasticity)",
    "text": "E: Equality of variance of the residuals (Homoscedasticity)\n\nThe variance or, equivalently, the standard deviation of the responses is equal for all values of x.\nThis is called homoskedasticity (top row)\nIf there is heteroskedasticity (bottom row), then the assumption is not met."
  },
  {
    "objectID": "slides/Day16_bsta511.html#r2-coefficient-of-determination-12",
    "href": "slides/Day16_bsta511.html#r2-coefficient-of-determination-12",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "\\(R^2\\) = Coefficient of determination (1/2)",
    "text": "\\(R^2\\) = Coefficient of determination (1/2)\n\nRecall that the correlation coefficient \\(r\\) measures the strength of the linear relationship between two numerical variables\n\\(R^2\\) is usually used to measure the strength of a linear fit\n\nFor a simple linear regression model (one numerical predictor), \\(R^2\\) is just the square of the correlation coefficient\n\nIn general, \\(R^2\\) is the proportion of the variability of the dependent variable that is explained by the independent variable(s)\n\n\\[R^2 = \\frac{\\textrm{variance of predicted y-values}}\n{\\textrm{variance of observed y-values}} = \\frac{\\sum_{i=1}^n(\\widehat{y}_i-\\bar{y})^2}\n{\\sum_{i=1}^n(y_i-\\bar{y})^2}\n= \\frac{s_y^2 - s_{\\textrm{residuals}}^2}\n{s_y^2}\\] \\[R^2 = 1- \\frac{s_{\\textrm{residuals}}^2}\n{s_y^2}\\] where \\(\\frac{s_{\\textrm{residuals}}^2}{s_y^2}\\) is the proportion of “unexplained” variability in the \\(y\\) values,\nand thus \\(R^2 = 1- \\frac{s_{\\textrm{residuls}}^2}{s_y^2}\\) is the proportion of “explained” variability in the \\(y\\) values"
  },
  {
    "objectID": "slides/Day16_bsta511.html#r2-coefficient-of-determination-22",
    "href": "slides/Day16_bsta511.html#r2-coefficient-of-determination-22",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "\\(R^2\\) = Coefficient of determination (2/2)",
    "text": "\\(R^2\\) = Coefficient of determination (2/2)\n\nRecall, \\(-1&lt;r&lt;1\\)\nThus, \\(0&lt;R^2&lt;1\\)\nIn practice, we want “high” \\(R^2\\) values, i.e. \\(R^2\\) as close to 1 as possible.\n\nCalculating \\(R^2\\) in R using glance() from the broom package:\n\nglance(model1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(model1)$r.squared\n\n[1] 0.4109366\n\n\n\n\n\n\n\n\nWarning\n\n\n\nA model can have a high \\(R^2\\) value when there is a curved pattern.\nAlways first check whether a linear model is reasonable or not."
  },
  {
    "objectID": "slides/Day16_bsta511.html#r2-in-summary-r-output",
    "href": "slides/Day16_bsta511.html#r2-in-summary-r-output",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "\\(R^2\\) in summary() R output",
    "text": "\\(R^2\\) in summary() R output\n\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\n\nCompare to the square of the correlation coefficient \\(r\\):\n\nr &lt;- cor(x = gapm$life_expectancy_years_2011, \n    y = gapm$female_literacy_rate_2011,\n    use =  \"complete.obs\")\nr\n\n[1] 0.6410434\n\nr^2\n\n[1] 0.4109366"
  },
  {
    "objectID": "slides/Day16_bsta511.html#inference-for-population-slope-beta_1",
    "href": "slides/Day16_bsta511.html#inference-for-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Inference for population slope \\(\\beta_1\\)",
    "text": "Inference for population slope \\(\\beta_1\\)\n\n# Fit regression model:\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)\n# Get regression table:\ntidy(model1, conf.int = TRUE) %&gt;% gt() # conf.int = TRUE part is new! \n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\n\\[\\begin{align}\n\\widehat{y} =& b_0 + b_1 \\cdot x\\\\\n\\widehat{\\text{life expectancy}} =& 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{align}\\]\n\nWhat are \\(H_0\\) and \\(H_A\\)?\nHow do we calculate the standard error, statistic, p-value, and CI?\n\n\n\n\n\n\n\nNote\n\n\n\nWe can also test & calculate CI for the population intercept\nThis will be covered in BSTA 512"
  },
  {
    "objectID": "slides/Day16_bsta511.html#inference-for-the-population-slope-ci-and-hypothesis-test",
    "href": "slides/Day16_bsta511.html#inference-for-the-population-slope-ci-and-hypothesis-test",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Inference for the population slope: CI and hypothesis test",
    "text": "Inference for the population slope: CI and hypothesis test\n\n\nPopulation model\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma)\\)\n\\(\\sigma\\) is the variability (SD) of the residuals\n\nSample best-fit (least-squares) line:\n\\[\\widehat{y} = b_0 + b_1 \\cdot x \\]\nNote: Some sources use \\(\\widehat{\\beta}\\) instead of \\(b\\).\n\n\nConstruct a 95% confidence interval for the population slope \\(\\beta_1\\)\n\n\n\nConduct the hypothesis test\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote: R reports p-values for 2-sided tests"
  },
  {
    "objectID": "slides/Day16_bsta511.html#ci-for-population-slope-beta_1",
    "href": "slides/Day16_bsta511.html#ci-for-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "CI for population slope \\(\\beta_1\\)",
    "text": "CI for population slope \\(\\beta_1\\)\nRecall the general CI formula:\n\\[\\textrm{Point Estimate} \\pm t^*\\cdot SE_{\\textrm{Point Estimate}}\\]\nFor the CI of the coefficient \\(b_1\\) this translates to\n\\[b_1 \\pm t^*\\cdot SE_{b_1}\\] where \\(t^*\\) is the critical value from a \\(t\\)-distribution with \\(df = n -2\\).\n\nHow is \\(\\text{SE}_{b_1}\\) calculated? See next slide.\n\n\ntidy(model1, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term                  estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)             50.9      2.66       19.1  3.33e-31   45.6      56.2  \n2 female_literacy_rate…    0.232    0.0315      7.38 1.50e-10    0.170     0.295"
  },
  {
    "objectID": "slides/Day16_bsta511.html#standard-error-of-fitted-slope-b_1",
    "href": "slides/Day16_bsta511.html#standard-error-of-fitted-slope-b_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Standard error of fitted slope \\(b_1\\)",
    "text": "Standard error of fitted slope \\(b_1\\)\n\n\n\\[\\text{SE}_{b_1} = \\frac{s_{\\textrm{residuals}}}{s_x\\sqrt{n-1}}\\]\n\n\\(\\text{SE}_{b_1}\\) is the variability of the statistic \\(b_1\\)\n\n\n\n\n\n\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\n\n\n\n\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\n\n\n\n\n\\(n\\) is the sample size, or the number of (complete) pairs of points\n\n\n\n\n\nglance(model1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# standard deviation of the residuals (Residual standard error in summary() output)\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n# standard deviation of x's\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] 21.95371\n\n# number of pairs of complete observations\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(se_b1 &lt;- s_resid/(s_x * sqrt(n-1))) # compare to SE in regression output\n\n[1] 0.03147744"
  },
  {
    "objectID": "slides/Day16_bsta511.html#calculate-ci-for-population-slope-beta_1",
    "href": "slides/Day16_bsta511.html#calculate-ci-for-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Calculate CI for population slope \\(\\beta_1\\)",
    "text": "Calculate CI for population slope \\(\\beta_1\\)\n\n\n\\[b_1 \\pm t^*\\cdot SE_{b_1}\\]\n\nwhere \\(t^*\\) is the \\(t\\)-distribution critical value with \\(df = n -2\\).\n\n\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\nSave regression output for the row with the slope’s information:\n\nmodel1_b1 &lt;-tidy(model1) %&gt;% filter(term == \"female_literacy_rate_2011\")\nmodel1_b1 %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n\n\nSave values needed for CI:\n\nb1 &lt;- model1_b1$estimate\nSE_b1 &lt;- model1_b1$std.error\n\n\nnobs(model1) # sample size n\n\n[1] 80\n\n(tstar &lt;- qt(.975, df = 80-2))\n\n[1] 1.990847\n\n\n\nCompare CI bounds below with the ones in the regression table above.\n\n(CI_LB &lt;- b1 - tstar*SE_b1)\n\n[1] 0.1695284\n\n(CI_UB &lt;- b1 + tstar*SE_b1)\n\n[1] 0.2948619"
  },
  {
    "objectID": "slides/Day16_bsta511.html#hypothesis-test-for-population-slope-beta_1",
    "href": "slides/Day16_bsta511.html#hypothesis-test-for-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Hypothesis test for population slope \\(\\beta_1\\)",
    "text": "Hypothesis test for population slope \\(\\beta_1\\)\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nThe test statistic for \\(b_1\\) is\n\\[t = \\frac{ b_1 - \\beta_1}{ \\text{SE}_{b_1}} = \\frac{ b_1}{ \\text{SE}_{b_1}}\\]\nwhen we assume \\(H_0: \\beta_1 = 0\\) is true.\n\n\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\nCalculate the test statistic using the values in the regression table:\n\n# recall model1_b1 is regression table restricted to b1 row\n(TestStat &lt;- model1_b1$estimate / model1_b1$std.error)\n\n[1] 7.376557\n\n\nCompare this test statistic value to the one from the regression table above"
  },
  {
    "objectID": "slides/Day16_bsta511.html#p-value-for-testing-population-slope-beta_1",
    "href": "slides/Day16_bsta511.html#p-value-for-testing-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "\\(p\\)-value for testing population slope \\(\\beta_1\\)",
    "text": "\\(p\\)-value for testing population slope \\(\\beta_1\\)\n\nAs usual, the \\(p\\)-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\nTo calculate the \\(p\\)-value, we need to know the probability distribution of the test statistic (the null distribution) assuming \\(H_0\\) is true.\nStatistical theory tells us that the test statistic \\(t\\) can be modeled by a \\(t\\)-distribution with \\(df = n-2\\).\nRecall that this is a 2-sided test:\n\n\n(pv = 2*pt(TestStat, df=80-2, lower.tail=F))\n\n[1] 1.501286e-10\n\n\nCompare the \\(p\\)-value to the one from the regression table below\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()  # compare p-value calculated above to p-value in table\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619"
  },
  {
    "objectID": "slides/Day16_bsta511.html#prediction-with-regression-line",
    "href": "slides/Day16_bsta511.html#prediction-with-regression-line",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction with regression line",
    "text": "Prediction with regression line\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot \\textrm{female literacy rate} \\]\nWhat is the predicted life expectancy for a country with female literacy rate 60%?\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot 60 = 64.82\\]\n\n(y_60 &lt;- 50.9 + 0.232*60)\n\n[1] 64.82\n\n\n\n\nHow do we interpret the predicted value?\nHow variable is it?"
  },
  {
    "objectID": "slides/Day16_bsta511.html#prediction-with-regression-line-1",
    "href": "slides/Day16_bsta511.html#prediction-with-regression-line-1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction with regression line",
    "text": "Prediction with regression line\n\n\nRecall the population model:\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma)\\)\n\\(\\sigma\\) is the variability (SD) of the residuals\n\n\nWhen we take the expected value, at a given value \\(x^*\\), we have that the predicted response is the average expected response at \\(x^*\\):\n\n\\[\\widehat{E[Y|x^*]} = b_0 + b_1 x^*\\]\n\n\n\n\n\n\n\n\n\nThese are the points on the regression line.\nThe mean responses has variability, and we can calculate a CI for it, for every value of \\(x^*\\)."
  },
  {
    "objectID": "slides/Day16_bsta511.html#ci-for-mean-response-mu_yx",
    "href": "slides/Day16_bsta511.html#ci-for-mean-response-mu_yx",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "CI for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "CI for mean response \\(\\mu_{Y|x^*}\\)\n\\[\\widehat{E[Y|x^*]} \\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\]\n\n\\(SE_{\\widehat{E[Y|x^*]}}\\) is calculated using\n\n\\[SE_{\\widehat{E[Y|x^*]}} = s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\]\n\n\\(\\widehat{E[Y|x^*]}\\) is the predicted value at the specified point \\(x^*\\) of the explanatory variable\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\\(n\\) is the sample size, or the number of (complete) pairs of points\n\\(\\bar{x}\\) is the sample mean of the explanatory variable \\(x\\)\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\n\n\nRecall that \\(t_{n-2}^*\\) is calculated using qt() and depends on the confidence level."
  },
  {
    "objectID": "slides/Day16_bsta511.html#example-ci-for-mean-response-mu_yx",
    "href": "slides/Day16_bsta511.html#example-ci-for-mean-response-mu_yx",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Example: CI for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Example: CI for mean response \\(\\mu_{Y|x^*}\\)\nFind the 95% CI for the mean life expectancy when the female literacy rate is 60.\n\n\\[\\begin{align}\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\\\\n64.8596 &\\pm 1.990847 \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 6.142157 \\sqrt{\\frac{1}{80} + \\frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 0.9675541\\\\\n64.8596 &\\pm 1.926252\\\\\n(62.93335 &, 66.78586)\n\\end{align}\\]\n\n\n\n\n\n(Y60 &lt;- 50.9278981 + 0.2321951 * 60)\n\n[1] 64.8596\n\n(tstar &lt;- qt(.975, df = 78))\n\n[1] 1.990847\n\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n\n\n\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(mx &lt;- mean(gapm$female_literacy_rate_2011))\n\n[1] 81.65375\n\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] 21.95371\n\n\n\n\n\n(SE_Yx &lt;- s_resid *sqrt(1/n + (60 - mx)^2/((n-1)*s_x^2)))\n\n[1] 0.9675541\n\n\n\n\n\n(MOE_Yx &lt;- SE_Yx*tstar)\n\n[1] 1.926252\n\n\n\n\n\n\nY60 - MOE_Yx\n\n[1] 62.93335\n\n\n\n\n\n\nY60 + MOE_Yx\n\n[1] 66.78586"
  },
  {
    "objectID": "slides/Day16_bsta511.html#example-using-r-for-ci-for-mean-response-mu_yx",
    "href": "slides/Day16_bsta511.html#example-using-r-for-ci-for-mean-response-mu_yx",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Example: Using R for CI for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Example: Using R for CI for mean response \\(\\mu_{Y|x^*}\\)\nFind the 95% CI’s for the mean life expectancy when the female literacy rate is 40, 60, and 80.\n\nUse the base R predict() function\nRequires specification of a newdata “value”\n\nThe newdata value is \\(x^*\\)\nThis has to be in the format of a data frame though\nwith column name identical to the predictor variable in the model\n\n\n\nnewdata &lt;- data.frame(female_literacy_rate_2011 = c(40, 60, 80)) \nnewdata\n\n  female_literacy_rate_2011\n1                        40\n2                        60\n3                        80\n\n\n\n\n\npredict(model1, \n        newdata=newdata, \n        interval=\"confidence\")\n\n       fit      lwr      upr\n1 60.21570 57.26905 63.16236\n2 64.85961 62.93335 66.78586\n3 69.50351 68.13244 70.87457\n\n\n\nInterpretation\nWe are 95% confident that the average life expectancy for a country with a 60% female literacy rate will be between 62.9 and 66.8 years."
  },
  {
    "objectID": "slides/Day16_bsta511.html#confidence-bands-for-mean-response-mu_yx",
    "href": "slides/Day16_bsta511.html#confidence-bands-for-mean-response-mu_yx",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Confidence bands for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Confidence bands for mean response \\(\\mu_{Y|x^*}\\)\n\nOften we plot the CI for many values of X, creating confidence bands\nThe confidence bands are what ggplot creates when we set se = TRUE within geom_smooth\nFor what values of x are the confidence bands (intervals) narrowest?\n\n\nggplot(gapm,\n       aes(x=female_literacy_rate_2011, \n           y=life_expectancy_years_2011)) +\n  geom_point()+\n  geom_smooth(method = lm, se=TRUE)+\n  ggtitle(\"Life expectancy vs. female literacy rate\")"
  },
  {
    "objectID": "slides/Day16_bsta511.html#width-of-confidence-bands-for-mean-response-mu_yx",
    "href": "slides/Day16_bsta511.html#width-of-confidence-bands-for-mean-response-mu_yx",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Width of confidence bands for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Width of confidence bands for mean response \\(\\mu_{Y|x^*}\\)\n\nFor what values of \\(x^*\\) are the confidence bands (intervals) narrowest? widest?\n\n\\[\\begin{align}\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\\\\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\n\\end{align}\\]"
  },
  {
    "objectID": "slides/Day16_bsta511.html#prediction-interval-for-predicting-individual-observations",
    "href": "slides/Day16_bsta511.html#prediction-interval-for-predicting-individual-observations",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction interval for predicting individual observations",
    "text": "Prediction interval for predicting individual observations\n\nWe do not call this interval a CI since \\(Y\\) is a random variable instead of a parameter\nThe form is similar to a CI though:\n\n\\[\\widehat{Y|x^*} \\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\]\n\nNote that the only difference to the CI for a mean value of y is the additional 1+ under the square root.\n\nThus the width is wider!"
  },
  {
    "objectID": "slides/Day16_bsta511.html#example-prediction-interval",
    "href": "slides/Day16_bsta511.html#example-prediction-interval",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Example: Prediction interval",
    "text": "Example: Prediction interval\nFind the 95% prediction interval for the life expectancy when the female literacy rate is 60.\n\\[\\begin{align}\n\\widehat{Y|x^*} &\\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 6.142157 \\sqrt{1+\\frac{1}{80} + \\frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\\\\n(52.48072 &, 77.23849)\n\\end{align}\\]\n\n\n\n\n(Y60 &lt;- 50.9278981 + 0.2321951 * 60)\n\n[1] 64.8596\n\n(tstar &lt;- qt(.975, df = 78))\n\n[1] 1.990847\n\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n\n\n\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(mx &lt;- mean(gapm$female_literacy_rate_2011))\n\n[1] 81.65375\n\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] 21.95371\n\n\n\n\n\n(SE_Ypred &lt;- s_resid *sqrt(1 + 1/n + (60 - mx)^2/((n-1)*s_x^2)))\n\n[1] 6.217898\n\n\n\n\n\n(MOE_Ypred &lt;- SE_Ypred*tstar)\n\n[1] 12.37888\n\n\n\n\n\n\nY60 - MOE_Ypred\n\n[1] 52.48072\n\n\n\n\n\n\nY60 + MOE_Ypred\n\n[1] 77.23849"
  },
  {
    "objectID": "slides/Day16_bsta511.html#example-using-r-for-prediction-interval",
    "href": "slides/Day16_bsta511.html#example-using-r-for-prediction-interval",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Example: Using R for prediction interval",
    "text": "Example: Using R for prediction interval\nFind the 95% prediction intervals for the life expectancy when the female literacy rate is 40, 60, and 80.\n\nnewdata  # previously defined for CI's\n\n  female_literacy_rate_2011\n1                        40\n2                        60\n3                        80\n\npredict(model1, \n        newdata=newdata, \n        interval=\"prediction\")  # prediction instead of \"confidence\"\n\n       fit      lwr      upr\n1 60.21570 47.63758 72.79382\n2 64.85961 52.48072 77.23849\n3 69.50351 57.19879 81.80823\n\n\n\nInterpretation\nWe are 95% confident that a new selected country with a 60% female literacy rate will have a life expectancy between 52.5 and 77.2 years."
  },
  {
    "objectID": "slides/Day16_bsta511.html#prediction-bands-vs.-confidence-bands-12",
    "href": "slides/Day16_bsta511.html#prediction-bands-vs.-confidence-bands-12",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction bands vs. confidence bands (1/2)",
    "text": "Prediction bands vs. confidence bands (1/2)\nCreate a scatterplot with the regression line, 95% confidence bands, and 95% prediction bands.\n\nFirst create a data frame with the original data points (both x and y values), their respective predicted values, andtheir respective prediction intervals\nCan do this with augment() from the broom package.\n\n\nmodel1_pred_bands &lt;- augment(model1, interval = \"prediction\")\n\n# take a look at new object:\nnames(model1_pred_bands) \n\n [1] \"life_expectancy_years_2011\" \"female_literacy_rate_2011\" \n [3] \".fitted\"                    \".lower\"                    \n [5] \".upper\"                     \".resid\"                    \n [7] \".hat\"                       \".sigma\"                    \n [9] \".cooksd\"                    \".std.resid\"                \n\n# glimpse of select variables of interest:\nmodel1_pred_bands %&gt;% \n  select(life_expectancy_years_2011, female_literacy_rate_2011, \n         .fitted:.upper) %&gt;% \n  glimpse()\n\nRows: 80\nColumns: 5\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .lower                     &lt;dbl&gt; 40.91166, 60.81324, 52.14572, 61.65365, 61.…\n$ .upper                     &lt;dbl&gt; 66.98121, 85.48470, 76.92334, 86.36253, 86.…"
  },
  {
    "objectID": "slides/Day16_bsta511.html#prediction-bands-vs.-confidence-bands-22",
    "href": "slides/Day16_bsta511.html#prediction-bands-vs.-confidence-bands-22",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction bands vs. confidence bands (2/2)",
    "text": "Prediction bands vs. confidence bands (2/2)\n\nnames(model1_pred_bands) \n\n [1] \"life_expectancy_years_2011\" \"female_literacy_rate_2011\" \n [3] \".fitted\"                    \".lower\"                    \n [5] \".upper\"                     \".resid\"                    \n [7] \".hat\"                       \".sigma\"                    \n [9] \".cooksd\"                    \".std.resid\"                \n\n\n\nggplot(model1_pred_bands, \n       aes(x=female_literacy_rate_2011, y=life_expectancy_years_2011)) +\n  geom_point() +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), # prediction bands\n              alpha = 0.2, fill = \"red\") +\n  geom_smooth(method=lm) +  # confidence bands\n  labs(title = \"SLR with Confidence & Prediction Bands\")"
  },
  {
    "objectID": "slides/Day16_bsta511.html#corrrelation-doesnt-imply-causation",
    "href": "slides/Day16_bsta511.html#corrrelation-doesnt-imply-causation",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Corrrelation doesn’t imply causation*!",
    "text": "Corrrelation doesn’t imply causation*!\n\nThis might seem obvious, but make sure to not write your analysis results in a way that implies causation if the study design doesn’t warrant it (such as an observational study).\nBeware of spurious correlations: http://www.tylervigen.com/spurious-correlations\n\n\n\n*Caveat: there is a whole field of statistics/epidemiology on causal inference. https://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf"
  },
  {
    "objectID": "slides/Day16_bsta511.html#whats-next",
    "href": "slides/Day16_bsta511.html#whats-next",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "What’s next?",
    "text": "What’s next?"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#goals-for-today-sections-6.3-6.4",
    "href": "slides_md/Day16_bsta511_md.html#goals-for-today-sections-6.3-6.4",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Goals for today (Sections 6.3-6.4)",
    "text": "Goals for today (Sections 6.3-6.4)\nSimple Linear Regression Part 2\n\nReview of\n\nbest-fit line (aka regression line or least-squares line)\nresiduals\npopulation model\n\nLINE conditions and how to assess them\n\nNew diagnostic tools:\n\nNormal QQ plots of residuals\nResidual plots\n\n\nCoefficient of determination (\\(R^2\\))\nRegression inference\n\nInference for population slope \\(\\beta_1\\)\n\nCI & hypothesis test\n\nCI for mean response \\(\\mu_{Y|x^*}\\)\nPrediction interval for predicting individual observations\n\n\nConfidence bands vs. predictions bands"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#life-expectancy-vs.-female-adult-literacy-rate",
    "href": "slides_md/Day16_bsta511_md.html#life-expectancy-vs.-female-adult-literacy-rate",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Life expectancy vs. female adult literacy rate",
    "text": "Life expectancy vs. female adult literacy rate\nhttps://www.gapminder.org/tools/#$model$markers$bubble$encoding$x$data$concept=literacy_rate_adult_female_percent_of_females_ages_15_above&source=sg&space@=country&=time;;&scale$domain:null&zoomed:null&type:null;;&frame$value=2011;;;;;&chart-type=bubbles&url=v1"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#dataset-description",
    "href": "slides_md/Day16_bsta511_md.html#dataset-description",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Dataset description",
    "text": "Dataset description\n\nData file: lifeexp_femlit_water_2011.csv\nData were downloaded from https://www.gapminder.org/data/\n2011 is the most recent year with the most complete data\nLife expectancy = the average number of years a newborn child would live if current mortality patterns were to stay the same. Source: https://www.gapminder.org/data/documentation/gd004/\nAdult literacy rate is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life. Source: http://data.uis.unesco.org/\nAt least basic water source (%) = the percentage of people using at least basic water services. This indicator encompasses both people using basic water services as well as those using safely managed water services. Basic drinking water services is defined as drinking water from an improved source, provided collection time is not more than 30 minutes for a round trip. Improved water sources include piped water, boreholes or tubewells, protect dug wells, protected springs, and packaged or delivered water."
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#get-to-know-the-data",
    "href": "slides_md/Day16_bsta511_md.html#get-to-know-the-data",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Get to know the data",
    "text": "Get to know the data\nLoad data\n\ngapm_original &lt;- read_csv(here::here(\"data\", \"lifeexp_femlit_water_2011.csv\"))\n\nRows: 194 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, water_2011_quart\ndbl (3): life_expectancy_years_2011, female_literacy_rate_2011, water_basic_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nGlimpse of the data\n\nglimpse(gapm_original)\n\nRows: 194\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andor…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 76.7, 82.6, 60.9, 76.9, 76.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, NA, NA, 58.6, 99.4, 97.9, 99.5,…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 92.6, 100.0, 40.3, 97.0, 99.5, …\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q2\", \"Q4\", \"Q1\", \"Q3\", \"Q4\", \"…\n\n\nNote the missing values for our variables of interest\n\ngapm_original %&gt;% select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  get_summary_stats()\n\n# A tibble: 2 × 13\n  variable        n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 life_expec…   187  47.5  82.9   72.7  64.3  76.9  12.6  9.04  70.7  8.44 0.617\n2 female_lit…    80  13    99.8   91.6  71.0  98.0  27.0 11.4   81.7 22.0  2.45 \n# ℹ 1 more variable: ci &lt;dbl&gt;"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#remove-missing-values",
    "href": "slides_md/Day16_bsta511_md.html#remove-missing-values",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Remove missing values",
    "text": "Remove missing values\nRemove rows with missing data for life expectancy and female literacy rate\n\ngapm &lt;- gapm_original %&gt;% \n  drop_na(life_expectancy_years_2011, female_literacy_rate_2011)\n\nglimpse(gapm)\n\nRows: 80\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Angola\", \"Antigu…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 40.3, 97.0, 99.5, 97.8, 96.7, 9…\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q1\", \"Q3\", \"Q4\", \"Q3\", \"Q3\", \"…\n\n\nNo missing values now for our variables of interest\n\ngapm %&gt;% select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  get_summary_stats()\n\n# A tibble: 2 × 13\n  variable        n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 life_expec…    80    48  81.8   72.4  65.9  75.8  9.95  6.30  69.9  7.95 0.889\n2 female_lit…    80    13  99.8   91.6  71.0  98.0 27.0  11.4   81.7 22.0  2.45 \n# ℹ 1 more variable: ci &lt;dbl&gt;\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nRemoving the rows with missing data was not needed to run the regression model.\nI did this step since later we will be calculating the standard deviations of the explanatory and response variables for just the values included in the regression model. It’ll be easier to do this if we remove the missing values now."
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#regression-line-best-fit-line",
    "href": "slides_md/Day16_bsta511_md.html#regression-line-best-fit-line",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Regression line = best-fit line",
    "text": "Regression line = best-fit line\n\n\n\\[\\widehat{y} = b_0 + b_1 \\cdot x \\]\n\n\\(\\hat{y}\\) is the predicted outcome for a specific value of \\(x\\).\n\\(b_0\\) is the intercept\n\\(b_1\\) is the slope of the line, i.e., the increase in \\(\\hat{y}\\) for every increase of one (unit increase) in \\(x\\).\n\nslope = rise over run\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nIntercept\n\nThe expected outcome for the \\(y\\)-variable when the \\(x\\)-variable is 0.\n\nSlope\n\nFor every increase of 1 unit in the \\(x\\)-variable, there is an expected increase of, on average, \\(b_1\\) units in the \\(y\\)-variable.\nWe only say that there is an expected increase and not necessarily a causal increase."
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#regression-in-r-lm-summary-tidy",
    "href": "slides_md/Day16_bsta511_md.html#regression-in-r-lm-summary-tidy",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Regression in R: lm(), summary(), & tidy()",
    "text": "Regression in R: lm(), summary(), & tidy()\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\ntidy(model1) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\nRegression equation for our model:\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot \\textrm{female literacy rate} \\]"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#residuals",
    "href": "slides_md/Day16_bsta511_md.html#residuals",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Residuals",
    "text": "Residuals\n\n\n\nObserved values \\(y_i\\)\n\nthe values in the dataset\n\nFitted values \\(\\widehat{y}_i\\)\n\nthe values that fall on the best-fit line for a specific \\(x_i\\)\n\nResiduals \\(e_i = y_i - \\widehat{y}_i\\)\n\nthe differences between the observed and fitted values\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#the-population-regresison-model",
    "href": "slides_md/Day16_bsta511_md.html#the-population-regresison-model",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "The (population) regresison model",
    "text": "The (population) regresison model\n\n\n\nThe (population) regression model is denoted by\n\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\epsilon\\]\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown population parameters\n\\(\\epsilon\\) (epsilon) is the error about the line\n\nIt is assumed to be a random variable:\n\n\\(\\epsilon \\sim N(0, \\sigma^2)\\)\nvariance \\(\\sigma^2\\) is constant\n\n\n\n\n\n\n\n\n\n\nhttps://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions\n\n\n\n\nThe line is the average (expected) value of \\(Y\\) given a value of \\(x\\): \\(E(Y|x)\\).\nThe point estimates for \\(\\beta_0\\) and \\(\\beta_1\\) based on a sample are denoted by \\(b_0, b_1, s_{residuals}^2\\)\n\nNote: also common notation is \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\sigma}^2\\)"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#what-are-the-line-conditions",
    "href": "slides_md/Day16_bsta511_md.html#what-are-the-line-conditions",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "What are the LINE conditions?",
    "text": "What are the LINE conditions?\nFor “good” model fit and to be able to make inferences and predictions based on our models, 4 conditions need to be satisfied.\nBriefly:\n\nL inearity of relationship between variables\nI ndependence of the Y values\nN ormality of the residuals\nE quality of variance of the residuals (homoscedasticity)\n\nMore in depth:\n\nL : there is a linear relationship between the mean response (Y) and the explanatory variable (X),\nI : the errors are independent—there’s no connection between how far any two points lie from the regression line,\nN : the responses are normally distributed at each level of X, and\nE : the variance or, equivalently, the standard deviation of the responses is equal for all levels of X."
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#l-linearity-of-relationship-between-variables",
    "href": "slides_md/Day16_bsta511_md.html#l-linearity-of-relationship-between-variables",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "L: Linearity of relationship between variables",
    "text": "L: Linearity of relationship between variables\nIs the association between the variables linear?\n\nDiagnostic tools:\n\nScatterplot\nResidual plot (see later section for E : Equality of variance of the residuals)\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#i-independence-of-the-residuals-y-values",
    "href": "slides_md/Day16_bsta511_md.html#i-independence-of-the-residuals-y-values",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "I: Independence of the residuals (\\(Y\\) values)",
    "text": "I: Independence of the residuals (\\(Y\\) values)\n\nAre the data points independent of each other?\nExamples of when they are not independent, include\n\nrepeated measures (such as baseline, 3 months, 6 months)\ndata from clusters, such as different hospitals or families\n\nThis condition is checked by reviewing the study design and not by inspecting the data\nHow to analyze data using regression models when the \\(Y\\)-values are not independent is covered in BSTA 519 (Longitudinal data)"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#n-normality-of-the-residuals-1",
    "href": "slides_md/Day16_bsta511_md.html#n-normality-of-the-residuals-1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "N: Normality of the residuals",
    "text": "N: Normality of the residuals\n\nThe responses Y are normally distributed at each level of x\n\n\n\n\n\n\n\nhttps://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#extract-models-residuals-in-r",
    "href": "slides_md/Day16_bsta511_md.html#extract-models-residuals-in-r",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Extract model’s residuals in R",
    "text": "Extract model’s residuals in R\n\nFirst extract the residuals’ values from the model output using the augment() function from the broom package.\nGet a tibble with the orginal data, as well as the residuals and some other important values.\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011, \n                data = gapm)\naug1 &lt;- augment(model1) \n\nglimpse(aug1)\n\nRows: 80\nColumns: 8\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#check-normality-with-usual-distribution-plots",
    "href": "slides_md/Day16_bsta511_md.html#check-normality-with-usual-distribution-plots",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Check normality with “usual” distribution plots",
    "text": "Check normality with “usual” distribution plots\nNote that below I save each figure, and then combine them together in one row of output using grid.arrange() from the gridExtra package.\n\nhist1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_histogram()\n\ndensity1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_density()\n\nbox1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_boxplot()\n\nlibrary(gridExtra) # NEW!!!\ngrid.arrange(hist1, density1, box1, nrow = 1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#normal-qq-plots-qq-quantile-quantile",
    "href": "slides_md/Day16_bsta511_md.html#normal-qq-plots-qq-quantile-quantile",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Normal QQ plots (QQ = quantile-quantile)",
    "text": "Normal QQ plots (QQ = quantile-quantile)\n\nIt can be tricky to eyeball with a histogram or density plot whether the residuals are normal or not\nQQ plots are often used to help with this\n\n\n\n\nVertical axis: data quantiles\n\ndata points are sorted in order and\nassigned quantiles based on how many data points there are\n\nHorizontal axis: theoretical quantiles\n\nmean and standard deviation (SD) calculated from the data points\ntheoretical quantiles are calculated for each point, assuming the data are modeled by a normal distribution with the mean and SD of the data\n\n\n\n\n\n\n\n\n\n\n\nData are approximately normal if points fall on a line.\n\nSee more info at https://data.library.virginia.edu/understanding-QQ-plots/"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#examples-of-normal-qq-plots-15",
    "href": "slides_md/Day16_bsta511_md.html#examples-of-normal-qq-plots-15",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of Normal QQ plots (1/5)",
    "text": "Examples of Normal QQ plots (1/5)\n\nData:\n\nBody measurements from 507 physically active individuals\nin their 20’s or early 30’s\nwithin normal weight range."
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#examples-of-normal-qq-plots-25",
    "href": "slides_md/Day16_bsta511_md.html#examples-of-normal-qq-plots-25",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of Normal QQ plots (2/5)",
    "text": "Examples of Normal QQ plots (2/5)\nSkewed right distribution"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#examples-of-normal-qq-plots-35",
    "href": "slides_md/Day16_bsta511_md.html#examples-of-normal-qq-plots-35",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of Normal QQ plots (3/5)",
    "text": "Examples of Normal QQ plots (3/5)\nLong tails in distribution"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#examples-of-normal-qq-plots-45",
    "href": "slides_md/Day16_bsta511_md.html#examples-of-normal-qq-plots-45",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of Normal QQ plots (4/5)",
    "text": "Examples of Normal QQ plots (4/5)\nBimodal distribution"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#examples-of-normal-qq-plots-55",
    "href": "slides_md/Day16_bsta511_md.html#examples-of-normal-qq-plots-55",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of Normal QQ plots (5/5)",
    "text": "Examples of Normal QQ plots (5/5)"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#qq-plot-of-residuals-of-model1",
    "href": "slides_md/Day16_bsta511_md.html#qq-plot-of-residuals-of-model1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "QQ plot of residuals of model1",
    "text": "QQ plot of residuals of model1\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nggplot(aug1, aes(sample = .resid)) + \n  stat_qq() +     # points\n  stat_qq_line()  # line"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#compare-to-randomly-generated-normal-qq-plots",
    "href": "slides_md/Day16_bsta511_md.html#compare-to-randomly-generated-normal-qq-plots",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Compare to randomly generated Normal QQ plots",
    "text": "Compare to randomly generated Normal QQ plots\nHow “good” we can expect a QQ plot to look depends on the sample size.\n\nThe QQ plots on the next slides are randomly generated\n\nusing random samples from actual standard normal distributions \\(N(0,1)\\).\n\nThus, all the points in the QQ plots should theoretically fall in a line\nHowever, there is sampling variability…"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#randomly-generated-normal-qq-plots-n100",
    "href": "slides_md/Day16_bsta511_md.html#randomly-generated-normal-qq-plots-n100",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Randomly generated Normal QQ plots: n=100",
    "text": "Randomly generated Normal QQ plots: n=100\n\nNote that stat_qq_line() doesn’t work with randomly generated samples, and thus the code below manually creates the line that the points should be on (which is \\(y=x\\) in this case.)\n\n\n\n\n\nsamplesize &lt;- 100\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#examples-of-simulated-normal-qq-plots-n10",
    "href": "slides_md/Day16_bsta511_md.html#examples-of-simulated-normal-qq-plots-n10",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of simulated Normal QQ plots: n=10",
    "text": "Examples of simulated Normal QQ plots: n=10\nWith fewer data points,\n\nsimulated QQ plots are more likely to look “less normal”\neven though the data points were sampled from normal distributions.\n\n\n\n\n\nsamplesize &lt;- 10  # only change made to code!\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#examples-of-simulated-normal-qq-plots-n1000",
    "href": "slides_md/Day16_bsta511_md.html#examples-of-simulated-normal-qq-plots-n1000",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of simulated Normal QQ plots: n=1,000",
    "text": "Examples of simulated Normal QQ plots: n=1,000\nWith more data points,\n\nsimulated QQ plots are more likely to look “more normal”\n\n\n\n\n\nsamplesize &lt;- 1000 # only change made to code!\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#back-to-our-example",
    "href": "slides_md/Day16_bsta511_md.html#back-to-our-example",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Back to our example",
    "text": "Back to our example\n\n\nResiduals from Life Expectancy vs. Female Literacy Rate Regression\n\nggplot(aug1, \n      aes(sample = .resid)) + \n  stat_qq() + \n  stat_qq_line() \n\n\n\n\n\n\n\n\n\nSimulated QQ plot of Normal Residuals with n = 80\n\n\n\n# number of observations \n# in fitted model\nnobs(model1) \n\n[1] 80\n\n\n\nggplot() +\n  stat_qq(aes(\n    sample = rnorm(80))) + \n  geom_abline(\n    intercept = 0, slope = 1, \n    color = \"blue\")"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#residual-plot",
    "href": "slides_md/Day16_bsta511_md.html#residual-plot",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Residual plot",
    "text": "Residual plot\n\n\\(x\\) = explanatory variable from regression model\n\n(or the fitted values for a multiple regression)\n\n\\(y\\) = residuals from regression model\n\n\n\n\nnames(aug1)\n\n[1] \"life_expectancy_years_2011\" \"female_literacy_rate_2011\" \n[3] \".fitted\"                    \".resid\"                    \n[5] \".hat\"                       \".sigma\"                    \n[7] \".cooksd\"                    \".std.resid\"                \n\n\n\n\nggplot(aug1, \n       aes(x = female_literacy_rate_2011, \n           y = .resid)) + \n  geom_point() +\n  geom_abline(\n    intercept = 0, \n    slope = 0, \n    color = \"orange\") +\n  labs(title = \"Residual plot\")"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#e-equality-of-variance-of-the-residuals-homoscedasticity",
    "href": "slides_md/Day16_bsta511_md.html#e-equality-of-variance-of-the-residuals-homoscedasticity",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "E: Equality of variance of the residuals (Homoscedasticity)",
    "text": "E: Equality of variance of the residuals (Homoscedasticity)\n\nThe variance or, equivalently, the standard deviation of the responses is equal for all values of x.\nThis is called homoskedasticity (top row)\nIf there is heteroskedasticity (bottom row), then the assumption is not met."
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#r2-coefficient-of-determination-12",
    "href": "slides_md/Day16_bsta511_md.html#r2-coefficient-of-determination-12",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "\\(R^2\\) = Coefficient of determination (1/2)",
    "text": "\\(R^2\\) = Coefficient of determination (1/2)\n\nRecall that the correlation coefficient \\(r\\) measures the strength of the linear relationship between two numerical variables\n\\(R^2\\) is usually used to measure the strength of a linear fit\n\nFor a simple linear regression model (one numerical predictor), \\(R^2\\) is just the square of the correlation coefficient\n\nIn general, \\(R^2\\) is the proportion of the variability of the dependent variable that is explained by the independent variable(s)\n\n\\[R^2 = \\frac{\\textrm{variance of predicted y-values}}\n{\\textrm{variance of observed y-values}} = \\frac{\\sum_{i=1}^n(\\widehat{y}_i-\\bar{y})^2}\n{\\sum_{i=1}^n(y_i-\\bar{y})^2}\n= \\frac{s_y^2 - s_{\\textrm{residuals}}^2}\n{s_y^2}\\] \\[R^2 = 1- \\frac{s_{\\textrm{residuals}}^2}\n{s_y^2}\\] where \\(\\frac{s_{\\textrm{residuals}}^2}{s_y^2}\\) is the proportion of “unexplained” variability in the \\(y\\) values,\nand thus \\(R^2 = 1- \\frac{s_{\\textrm{residuls}}^2}{s_y^2}\\) is the proportion of “explained” variability in the \\(y\\) values"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#r2-coefficient-of-determination-22",
    "href": "slides_md/Day16_bsta511_md.html#r2-coefficient-of-determination-22",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "\\(R^2\\) = Coefficient of determination (2/2)",
    "text": "\\(R^2\\) = Coefficient of determination (2/2)\n\nRecall, \\(-1&lt;r&lt;1\\)\nThus, \\(0&lt;R^2&lt;1\\)\nIn practice, we want “high” \\(R^2\\) values, i.e. \\(R^2\\) as close to 1 as possible.\n\nCalculating \\(R^2\\) in R using glance() from the broom package:\n\nglance(model1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(model1)$r.squared\n\n[1] 0.4109366\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nA model can have a high \\(R^2\\) value when there is a curved pattern.\nAlways first check whether a linear model is reasonable or not."
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#r2-in-summary-r-output",
    "href": "slides_md/Day16_bsta511_md.html#r2-in-summary-r-output",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "\\(R^2\\) in summary() R output",
    "text": "\\(R^2\\) in summary() R output\n\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\n\nCompare to the square of the correlation coefficient \\(r\\):\n\nr &lt;- cor(x = gapm$life_expectancy_years_2011, \n    y = gapm$female_literacy_rate_2011,\n    use =  \"complete.obs\")\nr\n\n[1] 0.6410434\n\nr^2\n\n[1] 0.4109366"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#inference-for-population-slope-beta_1",
    "href": "slides_md/Day16_bsta511_md.html#inference-for-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Inference for population slope \\(\\beta_1\\)",
    "text": "Inference for population slope \\(\\beta_1\\)\n\n# Fit regression model:\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)\n# Get regression table:\ntidy(model1, conf.int = TRUE) %&gt;% gt() # conf.int = TRUE part is new! \n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\n\\[\\begin{align}\n\\widehat{y} =& b_0 + b_1 \\cdot x\\\\\n\\widehat{\\text{life expectancy}} =& 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{align}\\]\n\nWhat are \\(H_0\\) and \\(H_A\\)?\nHow do we calculate the standard error, statistic, p-value, and CI?\n\n\n\n\n\n\n\nNote\n\n\n\n\nWe can also test & calculate CI for the population intercept\nThis will be covered in BSTA 512"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#inference-for-the-population-slope-ci-and-hypothesis-test",
    "href": "slides_md/Day16_bsta511_md.html#inference-for-the-population-slope-ci-and-hypothesis-test",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Inference for the population slope: CI and hypothesis test",
    "text": "Inference for the population slope: CI and hypothesis test\n\n\nPopulation model\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma)\\)\n\\(\\sigma\\) is the variability (SD) of the residuals\n\nSample best-fit (least-squares) line:\n\\[\\widehat{y} = b_0 + b_1 \\cdot x \\]\nNote: Some sources use \\(\\widehat{\\beta}\\) instead of \\(b\\).\n\n\nConstruct a 95% confidence interval for the population slope \\(\\beta_1\\)\n\n\n\nConduct the hypothesis test\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote: R reports p-values for 2-sided tests"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#ci-for-population-slope-beta_1",
    "href": "slides_md/Day16_bsta511_md.html#ci-for-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "CI for population slope \\(\\beta_1\\)",
    "text": "CI for population slope \\(\\beta_1\\)\nRecall the general CI formula:\n\\[\\textrm{Point Estimate} \\pm t^*\\cdot SE_{\\textrm{Point Estimate}}\\]\nFor the CI of the coefficient \\(b_1\\) this translates to\n\\[b_1 \\pm t^*\\cdot SE_{b_1}\\] where \\(t^*\\) is the critical value from a \\(t\\)-distribution with \\(df = n -2\\).\n\nHow is \\(\\text{SE}_{b_1}\\) calculated? See next slide.\n\n\ntidy(model1, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term                  estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)             50.9      2.66       19.1  3.33e-31   45.6      56.2  \n2 female_literacy_rate…    0.232    0.0315      7.38 1.50e-10    0.170     0.295"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#standard-error-of-fitted-slope-b_1",
    "href": "slides_md/Day16_bsta511_md.html#standard-error-of-fitted-slope-b_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Standard error of fitted slope \\(b_1\\)",
    "text": "Standard error of fitted slope \\(b_1\\)\n\n\n\\[\\text{SE}_{b_1} = \\frac{s_{\\textrm{residuals}}}{s_x\\sqrt{n-1}}\\]\n\n\\(\\text{SE}_{b_1}\\) is the variability of the statistic \\(b_1\\)\n\n\n\n\n\n\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\n\n\n\n\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\n\n\n\n\n\\(n\\) is the sample size, or the number of (complete) pairs of points\n\n\n\n\n\nglance(model1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# standard deviation of the residuals (Residual standard error in summary() output)\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n# standard deviation of x's\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] 21.95371\n\n# number of pairs of complete observations\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(se_b1 &lt;- s_resid/(s_x * sqrt(n-1))) # compare to SE in regression output\n\n[1] 0.03147744"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#calculate-ci-for-population-slope-beta_1",
    "href": "slides_md/Day16_bsta511_md.html#calculate-ci-for-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Calculate CI for population slope \\(\\beta_1\\)",
    "text": "Calculate CI for population slope \\(\\beta_1\\)\n\n\n\\[b_1 \\pm t^*\\cdot SE_{b_1}\\]\n\nwhere \\(t^*\\) is the \\(t\\)-distribution critical value with \\(df = n -2\\).\n\n\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\nSave regression output for the row with the slope’s information:\n\nmodel1_b1 &lt;-tidy(model1) %&gt;% filter(term == \"female_literacy_rate_2011\")\nmodel1_b1 %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n\n\nSave values needed for CI:\n\nb1 &lt;- model1_b1$estimate\nSE_b1 &lt;- model1_b1$std.error\n\n\nnobs(model1) # sample size n\n\n[1] 80\n\n(tstar &lt;- qt(.975, df = 80-2))\n\n[1] 1.990847\n\n\n\nCompare CI bounds below with the ones in the regression table above.\n\n(CI_LB &lt;- b1 - tstar*SE_b1)\n\n[1] 0.1695284\n\n(CI_UB &lt;- b1 + tstar*SE_b1)\n\n[1] 0.2948619"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#hypothesis-test-for-population-slope-beta_1",
    "href": "slides_md/Day16_bsta511_md.html#hypothesis-test-for-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Hypothesis test for population slope \\(\\beta_1\\)",
    "text": "Hypothesis test for population slope \\(\\beta_1\\)\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nThe test statistic for \\(b_1\\) is\n\\[t = \\frac{ b_1 - \\beta_1}{ \\text{SE}_{b_1}} = \\frac{ b_1}{ \\text{SE}_{b_1}}\\]\nwhen we assume \\(H_0: \\beta_1 = 0\\) is true.\n\n\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\nCalculate the test statistic using the values in the regression table:\n\n# recall model1_b1 is regression table restricted to b1 row\n(TestStat &lt;- model1_b1$estimate / model1_b1$std.error)\n\n[1] 7.376557\n\n\nCompare this test statistic value to the one from the regression table above"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#p-value-for-testing-population-slope-beta_1",
    "href": "slides_md/Day16_bsta511_md.html#p-value-for-testing-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "\\(p\\)-value for testing population slope \\(\\beta_1\\)",
    "text": "\\(p\\)-value for testing population slope \\(\\beta_1\\)\n\nAs usual, the \\(p\\)-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\nTo calculate the \\(p\\)-value, we need to know the probability distribution of the test statistic (the null distribution) assuming \\(H_0\\) is true.\nStatistical theory tells us that the test statistic \\(t\\) can be modeled by a \\(t\\)-distribution with \\(df = n-2\\).\nRecall that this is a 2-sided test:\n\n\n(pv = 2*pt(TestStat, df=80-2, lower.tail=F))\n\n[1] 1.501286e-10\n\n\nCompare the \\(p\\)-value to the one from the regression table below\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()  # compare p-value calculated above to p-value in table\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#prediction-with-regression-line",
    "href": "slides_md/Day16_bsta511_md.html#prediction-with-regression-line",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction with regression line",
    "text": "Prediction with regression line\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot \\textrm{female literacy rate} \\]\nWhat is the predicted life expectancy for a country with female literacy rate 60%?\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot 60 = 64.82\\]\n\n(y_60 &lt;- 50.9 + 0.232*60)\n\n[1] 64.82\n\n\n\n\nHow do we interpret the predicted value?\nHow variable is it?"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#prediction-with-regression-line-1",
    "href": "slides_md/Day16_bsta511_md.html#prediction-with-regression-line-1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction with regression line",
    "text": "Prediction with regression line\n\n\nRecall the population model:\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma)\\)\n\\(\\sigma\\) is the variability (SD) of the residuals\n\n\nWhen we take the expected value, at a given value \\(x^*\\), we have that the predicted response is the average expected response at \\(x^*\\):\n\n\\[\\widehat{E[Y|x^*]} = b_0 + b_1 x^*\\]\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nThese are the points on the regression line.\nThe mean responses has variability, and we can calculate a CI for it, for every value of \\(x^*\\)."
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#ci-for-mean-response-mu_yx",
    "href": "slides_md/Day16_bsta511_md.html#ci-for-mean-response-mu_yx",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "CI for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "CI for mean response \\(\\mu_{Y|x^*}\\)\n\\[\\widehat{E[Y|x^*]} \\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\]\n\n\\(SE_{\\widehat{E[Y|x^*]}}\\) is calculated using\n\n\\[SE_{\\widehat{E[Y|x^*]}} = s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\]\n\n\\(\\widehat{E[Y|x^*]}\\) is the predicted value at the specified point \\(x^*\\) of the explanatory variable\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\\(n\\) is the sample size, or the number of (complete) pairs of points\n\\(\\bar{x}\\) is the sample mean of the explanatory variable \\(x\\)\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\n\n\nRecall that \\(t_{n-2}^*\\) is calculated using qt() and depends on the confidence level."
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#example-ci-for-mean-response-mu_yx",
    "href": "slides_md/Day16_bsta511_md.html#example-ci-for-mean-response-mu_yx",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Example: CI for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Example: CI for mean response \\(\\mu_{Y|x^*}\\)\nFind the 95% CI for the mean life expectancy when the female literacy rate is 60.\n\n\\[\\begin{align}\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\\\\n64.8596 &\\pm 1.990847 \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 6.142157 \\sqrt{\\frac{1}{80} + \\frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 0.9675541\\\\\n64.8596 &\\pm 1.926252\\\\\n(62.93335 &, 66.78586)\n\\end{align}\\]\n\n\n\n\n\n(Y60 &lt;- 50.9278981 + 0.2321951 * 60)\n\n[1] 64.8596\n\n(tstar &lt;- qt(.975, df = 78))\n\n[1] 1.990847\n\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n\n\n\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(mx &lt;- mean(gapm$female_literacy_rate_2011))\n\n[1] 81.65375\n\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] 21.95371\n\n\n\n\n\n(SE_Yx &lt;- s_resid *sqrt(1/n + (60 - mx)^2/((n-1)*s_x^2)))\n\n[1] 0.9675541\n\n\n\n\n\n(MOE_Yx &lt;- SE_Yx*tstar)\n\n[1] 1.926252\n\n\n\n\n\n\nY60 - MOE_Yx\n\n[1] 62.93335\n\n\n\n\n\n\nY60 + MOE_Yx\n\n[1] 66.78586"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#example-using-r-for-ci-for-mean-response-mu_yx",
    "href": "slides_md/Day16_bsta511_md.html#example-using-r-for-ci-for-mean-response-mu_yx",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Example: Using R for CI for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Example: Using R for CI for mean response \\(\\mu_{Y|x^*}\\)\nFind the 95% CI’s for the mean life expectancy when the female literacy rate is 40, 60, and 80.\n\nUse the base R predict() function\nRequires specification of a newdata “value”\n\nThe newdata value is \\(x^*\\)\nThis has to be in the format of a data frame though\nwith column name identical to the predictor variable in the model\n\n\n\nnewdata &lt;- data.frame(female_literacy_rate_2011 = c(40, 60, 80)) \nnewdata\n\n  female_literacy_rate_2011\n1                        40\n2                        60\n3                        80\n\n\n\n\n\npredict(model1, \n        newdata=newdata, \n        interval=\"confidence\")\n\n       fit      lwr      upr\n1 60.21570 57.26905 63.16236\n2 64.85961 62.93335 66.78586\n3 69.50351 68.13244 70.87457\n\n\n\n\nInterpretation\nWe are 95% confident that the average life expectancy for a country with a 60% female literacy rate will be between 62.9 and 66.8 years."
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#confidence-bands-for-mean-response-mu_yx",
    "href": "slides_md/Day16_bsta511_md.html#confidence-bands-for-mean-response-mu_yx",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Confidence bands for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Confidence bands for mean response \\(\\mu_{Y|x^*}\\)\n\nOften we plot the CI for many values of X, creating confidence bands\nThe confidence bands are what ggplot creates when we set se = TRUE within geom_smooth\nFor what values of x are the confidence bands (intervals) narrowest?\n\n\nggplot(gapm,\n       aes(x=female_literacy_rate_2011, \n           y=life_expectancy_years_2011)) +\n  geom_point()+\n  geom_smooth(method = lm, se=TRUE)+\n  ggtitle(\"Life expectancy vs. female literacy rate\") \n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#width-of-confidence-bands-for-mean-response-mu_yx",
    "href": "slides_md/Day16_bsta511_md.html#width-of-confidence-bands-for-mean-response-mu_yx",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Width of confidence bands for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Width of confidence bands for mean response \\(\\mu_{Y|x^*}\\)\n\nFor what values of \\(x^*\\) are the confidence bands (intervals) narrowest? widest?\n\n\\[\\begin{align}\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\\\\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\n\\end{align}\\]\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#prediction-interval-for-predicting-individual-observations",
    "href": "slides_md/Day16_bsta511_md.html#prediction-interval-for-predicting-individual-observations",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction interval for predicting individual observations",
    "text": "Prediction interval for predicting individual observations\n\nWe do not call this interval a CI since \\(Y\\) is a random variable instead of a parameter\nThe form is similar to a CI though:\n\n\\[\\widehat{Y|x^*} \\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\]\n\nNote that the only difference to the CI for a mean value of y is the additional 1+ under the square root.\n\nThus the width is wider!"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#example-prediction-interval",
    "href": "slides_md/Day16_bsta511_md.html#example-prediction-interval",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Example: Prediction interval",
    "text": "Example: Prediction interval\nFind the 95% prediction interval for the life expectancy when the female literacy rate is 60.\n\\[\\begin{align}\n\\widehat{Y|x^*} &\\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 6.142157 \\sqrt{1+\\frac{1}{80} + \\frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\\\\n(52.48072 &, 77.23849)\n\\end{align}\\]\n\n\n\n\n(Y60 &lt;- 50.9278981 + 0.2321951 * 60)\n\n[1] 64.8596\n\n(tstar &lt;- qt(.975, df = 78))\n\n[1] 1.990847\n\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n\n\n\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(mx &lt;- mean(gapm$female_literacy_rate_2011))\n\n[1] 81.65375\n\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] 21.95371\n\n\n\n\n\n(SE_Ypred &lt;- s_resid *sqrt(1 + 1/n + (60 - mx)^2/((n-1)*s_x^2)))\n\n[1] 6.217898\n\n\n\n\n\n(MOE_Ypred &lt;- SE_Ypred*tstar)\n\n[1] 12.37888\n\n\n\n\n\n\nY60 - MOE_Ypred\n\n[1] 52.48072\n\n\n\n\n\n\nY60 + MOE_Ypred\n\n[1] 77.23849"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#example-using-r-for-prediction-interval",
    "href": "slides_md/Day16_bsta511_md.html#example-using-r-for-prediction-interval",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Example: Using R for prediction interval",
    "text": "Example: Using R for prediction interval\nFind the 95% prediction intervals for the life expectancy when the female literacy rate is 40, 60, and 80.\n\nnewdata  # previously defined for CI's\n\n  female_literacy_rate_2011\n1                        40\n2                        60\n3                        80\n\npredict(model1, \n        newdata=newdata, \n        interval=\"prediction\")  # prediction instead of \"confidence\"\n\n       fit      lwr      upr\n1 60.21570 47.63758 72.79382\n2 64.85961 52.48072 77.23849\n3 69.50351 57.19879 81.80823\n\n\n\n\nInterpretation\nWe are 95% confident that a new selected country with a 60% female literacy rate will have a life expectancy between 52.5 and 77.2 years."
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#prediction-bands-vs.-confidence-bands-12",
    "href": "slides_md/Day16_bsta511_md.html#prediction-bands-vs.-confidence-bands-12",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction bands vs. confidence bands (1/2)",
    "text": "Prediction bands vs. confidence bands (1/2)\nCreate a scatterplot with the regression line, 95% confidence bands, and 95% prediction bands.\n\nFirst create a data frame with the original data points (both x and y values), their respective predicted values, andtheir respective prediction intervals\nCan do this with augment() from the broom package.\n\n\nmodel1_pred_bands &lt;- augment(model1, interval = \"prediction\")\n\n# take a look at new object:\nnames(model1_pred_bands) \n\n [1] \"life_expectancy_years_2011\" \"female_literacy_rate_2011\" \n [3] \".fitted\"                    \".lower\"                    \n [5] \".upper\"                     \".resid\"                    \n [7] \".hat\"                       \".sigma\"                    \n [9] \".cooksd\"                    \".std.resid\"                \n\n# glimpse of select variables of interest:\nmodel1_pred_bands %&gt;% \n  select(life_expectancy_years_2011, female_literacy_rate_2011, \n         .fitted:.upper) %&gt;% \n  glimpse()\n\nRows: 80\nColumns: 5\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .lower                     &lt;dbl&gt; 40.91166, 60.81324, 52.14572, 61.65365, 61.…\n$ .upper                     &lt;dbl&gt; 66.98121, 85.48470, 76.92334, 86.36253, 86.…"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#prediction-bands-vs.-confidence-bands-22",
    "href": "slides_md/Day16_bsta511_md.html#prediction-bands-vs.-confidence-bands-22",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction bands vs. confidence bands (2/2)",
    "text": "Prediction bands vs. confidence bands (2/2)\n\nnames(model1_pred_bands) \n\n [1] \"life_expectancy_years_2011\" \"female_literacy_rate_2011\" \n [3] \".fitted\"                    \".lower\"                    \n [5] \".upper\"                     \".resid\"                    \n [7] \".hat\"                       \".sigma\"                    \n [9] \".cooksd\"                    \".std.resid\"                \n\n\n\nggplot(model1_pred_bands, \n       aes(x=female_literacy_rate_2011, y=life_expectancy_years_2011)) +\n  geom_point() +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), # prediction bands\n              alpha = 0.2, fill = \"red\") +\n  geom_smooth(method=lm) +  # confidence bands\n  labs(title = \"SLR with Confidence & Prediction Bands\") \n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#corrrelation-doesnt-imply-causation",
    "href": "slides_md/Day16_bsta511_md.html#corrrelation-doesnt-imply-causation",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Corrrelation doesn’t imply causation*!",
    "text": "Corrrelation doesn’t imply causation*!\n\nThis might seem obvious, but make sure to not write your analysis results in a way that implies causation if the study design doesn’t warrant it (such as an observational study).\nBeware of spurious correlations: http://www.tylervigen.com/spurious-correlations\n\n\n\n*Caveat: there is a whole field of statistics/epidemiology on causal inference. https://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf"
  },
  {
    "objectID": "slides_md/Day16_bsta511_md.html#whats-next",
    "href": "slides_md/Day16_bsta511_md.html#whats-next",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "What’s next?",
    "text": "What’s next?"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#where-are-we-continuous-outcome-zoomed-in",
    "href": "slides_md/Day14_bsta511_md.html#where-are-we-continuous-outcome-zoomed-in",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Where are we? Continuous outcome zoomed in",
    "text": "Where are we? Continuous outcome zoomed in"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#goals-for-today-section-5.5",
    "href": "slides_md/Day14_bsta511_md.html#goals-for-today-section-5.5",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Goals for today (Section 5.5)",
    "text": "Goals for today (Section 5.5)\n\nAnalysis of Variance (ANOVA)\nWhen to use an ANOVA\nHypotheses\nANOVA table\nDifferent sources of variation in ANOVA\nANOVA conditions\nF-distribution\nPost-hoc testing of differences in means\nRunning an ANOVA in R"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#disability-discrimination-example",
    "href": "slides_md/Day14_bsta511_md.html#disability-discrimination-example",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Disability Discrimination Example",
    "text": "Disability Discrimination Example\n\n\n\nThe U.S. Rehabilitation Act of 1973 prohibited discrimination against people with physical disabilities.\n\nThe act defined a disabled person as any individual who has a physical or mental impairment that limits the person’s major life activities.\n\nA 1980’s study examined whether physical disabilities affect people’s perceptions of employment qualifications\n\n(Cesare, Tannenbaum, & Dalessio, 1990).\n\n\n\n\nResearchers prepared recorded job interviews, using same actors and script each time.\nOnly difference: job applicant appeared with different disabilities.\n\nNo disability\nLeg amputation\nCrutches\nHearing impairment\nWheelchair confinement\n\n70 undergrad students were randomly assigned to view one of the videotapes,\n\nthen rated the candidate’s qualifications on a 1-10 scale.\n\n\n\n\n\nThe research question: are qualifications evaluated differently depending on the applicant’s presented disability?"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#load-interview-data-from-.txt-file",
    "href": "slides_md/Day14_bsta511_md.html#load-interview-data-from-.txt-file",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Load interview data from .txt file",
    "text": "Load interview data from .txt file\n\n.txt (text) files are usually tab-deliminated files\n\n.csv files are comma-separated files\n\nread_delim is from the readr package, just like read_csv, and loads with other tidyverse packages\n\n\nemploy &lt;- read_delim(\n  file = here::here(\"data\", \"DisabilityEmployment.txt\"), \n  delim = \"\\t\",   # tab delimited\n  trim_ws = TRUE)\n\nRows: 70 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): disability\ndbl (1): score\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\ntrim_ws: specify whether leading and trailing white space should be trimmed from each field before parsing it\n\nglimpse(employ)\n\nRows: 70\nColumns: 2\n$ disability &lt;chr&gt; \"none\", \"none\", \"none\", \"none\", \"none\", \"none\", \"none\", \"no…\n$ score      &lt;dbl&gt; 1.9, 2.5, 3.0, 3.6, 4.1, 4.2, 4.9, 5.1, 5.4, 5.9, 6.1, 6.7,…\n\n\n\n\n\nsummary(employ)\n\n  disability            score      \n Length:70          Min.   :1.400  \n Class :character   1st Qu.:3.700  \n Mode  :character   Median :5.050  \n                    Mean   :4.929  \n                    3rd Qu.:6.100  \n                    Max.   :8.500  \n\n\n\n\nemploy %&gt;% tabyl(disability)\n\n disability  n percent\n    amputee 14     0.2\n   crutches 14     0.2\n    hearing 14     0.2\n       none 14     0.2\n wheelchair 14     0.2"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#moritzs-tip-of-the-day",
    "href": "slides_md/Day14_bsta511_md.html#moritzs-tip-of-the-day",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "MoRitz’s tip of the day",
    "text": "MoRitz’s tip of the day\nRead OHSU’s Inclusive Language Guide (below is from pgs. 22-25)\n“… an evolving tool to help OHSU members learn about and use inclusive language…”\nSections on: Race and ethnicity, Immigration status, Gender and sexual orientation, and Ability (including physical, mental and chronological attributes)"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#factor-variable-make-disability-a-factor-variable",
    "href": "slides_md/Day14_bsta511_md.html#factor-variable-make-disability-a-factor-variable",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Factor variable: Make disability a factor variable",
    "text": "Factor variable: Make disability a factor variable\n\n\n\nglimpse(employ)\n\nRows: 70\nColumns: 2\n$ disability &lt;chr&gt; \"none\", \"none\", \"none\", \"none\", \"none\", \"none\", \"none\", \"no…\n$ score      &lt;dbl&gt; 1.9, 2.5, 3.0, 3.6, 4.1, 4.2, 4.9, 5.1, 5.4, 5.9, 6.1, 6.7,…\n\n\n\n\nsummary(employ)\n\n  disability            score      \n Length:70          Min.   :1.400  \n Class :character   1st Qu.:3.700  \n Mode  :character   Median :5.050  \n                    Mean   :4.929  \n                    3rd Qu.:6.100  \n                    Max.   :8.500  \n\n\n\n\nMake disability a factor variable:\n\nemploy &lt;- employ %&gt;% \n  mutate(disability = factor(disability))\n\n\nWhat’s different now?\n\n\n\nglimpse(employ)\n\nRows: 70\nColumns: 2\n$ disability &lt;fct&gt; none, none, none, none, none, none, none, none, none, none,…\n$ score      &lt;dbl&gt; 1.9, 2.5, 3.0, 3.6, 4.1, 4.2, 4.9, 5.1, 5.4, 5.9, 6.1, 6.7,…\n\n\n\n\nsummary(employ)\n\n      disability     score      \n amputee   :14   Min.   :1.400  \n crutches  :14   1st Qu.:3.700  \n hearing   :14   Median :5.050  \n none      :14   Mean   :4.929  \n wheelchair:14   3rd Qu.:6.100  \n                 Max.   :8.500"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#factor-variable-change-order-name-of-disability-levels",
    "href": "slides_md/Day14_bsta511_md.html#factor-variable-change-order-name-of-disability-levels",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Factor variable: Change order & name of disability levels",
    "text": "Factor variable: Change order & name of disability levels\nWhat are the current level names and order?\n\nlevels(employ$disability)\n\n[1] \"amputee\"    \"crutches\"   \"hearing\"    \"none\"       \"wheelchair\"\n\n\nWhat changes are being made below?\n\nemploy &lt;- employ %&gt;% \n  mutate(\n    # make \"none\" the first level\n    # by only listing the level none, all other levels will be in original order\n    disability = fct_relevel(disability, \"none\"),\n    # change the level name amputee to amputation\n    disability = fct_recode(disability, amputation = \"amputee\")\n    )\n\n\nfct_relevel() and fct_recode() are from the forcats package: https://forcats.tidyverse.org/index.html.\nforcats is loaded with library(tidyverse).\n\nNew order & names:\n\nlevels(employ$disability) # note the new order and new name\n\n[1] \"none\"       \"amputation\" \"crutches\"   \"hearing\"    \"wheelchair\""
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#data-viz-12",
    "href": "slides_md/Day14_bsta511_md.html#data-viz-12",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Data viz (1/2)",
    "text": "Data viz (1/2)\n\nWhat are the score distribution shapes within each group?\nAny unusual values?\n\n\n\n\nggplot(employ, aes(x=score)) +\n  geom_density() +\n  facet_wrap(~ disability)\n\n\n\n\n\n\nlibrary(ggridges) \nggplot(employ, \n       aes(x=score,\n           y = disability,\n           fill = disability)) + \n  geom_density_ridges(alpha = 0.4) +\n  theme(legend.position=\"none\")\n\nPicking joint bandwidth of 0.801"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#data-viz-22",
    "href": "slides_md/Day14_bsta511_md.html#data-viz-22",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Data viz (2/2)",
    "text": "Data viz (2/2)\n\nCompare the score measures of center and spread between the groups\n\n\n\n\nggplot(employ, \n       aes(y=score, \n           x = disability,\n           fill = disability)) +\n  geom_boxplot(alpha = 0.3) +\n  coord_flip() +\n  geom_jitter(width = 0.1, \n              alpha = 0.3) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nggplot(employ, \n       aes(x = disability, \n           y=score, \n           fill=disability, \n           color=disability)) +\n  geom_dotplot(binaxis = \"y\", alpha = 0.5) +\n  geom_hline(aes(yintercept = mean(score)), \n             lty = \"dashed\") +\n  stat_summary(fun =\"mean\", geom=\"point\", \n    size = 3, color = \"grey33\", alpha = 1) +\n  theme(legend.position = \"none\")\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`."
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#hypotheses",
    "href": "slides_md/Day14_bsta511_md.html#hypotheses",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Hypotheses",
    "text": "Hypotheses\nTo test for a difference in means across k groups:\n\\[\\begin{align}\nH_0 &: \\mu_1 = \\mu_2 = ... = \\mu_k\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j \\text{ for } i \\neq j\n\\end{align}\\]\nHypothetical examples:\nIn which set (A or B) do you believe the evidence will be stronger that at least one population differs from the others?"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#comparing-means",
    "href": "slides_md/Day14_bsta511_md.html#comparing-means",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Comparing means",
    "text": "Comparing means\nWhether or not two means are significantly different depends on:\n\nHow far apart the means are\nHow much variability there is within each group\n\nQuestions:\n\nHow to measure variability between groups?\nHow to measure variability within groups?\nHow to compare the two measures of variability?\nHow to determine significance?"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#anova-in-base-r",
    "href": "slides_md/Day14_bsta511_md.html#anova-in-base-r",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA in base R",
    "text": "ANOVA in base R\n\nThere are several options to run an ANOVA model in R\nTwo most common are lm and aov\n\nlm = linear model; will be using frequently in BSTA 512\n\n\n\nlm(score ~ disability, data = employ) %&gt;% anova()\n\nAnalysis of Variance Table\n\nResponse: score\n           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ndisability  4  30.521  7.6304  2.8616 0.03013 *\nResiduals  65 173.321  2.6665                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\naov(score ~ disability, data = employ) %&gt;% summary()\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ndisability   4  30.52   7.630   2.862 0.0301 *\nResiduals   65 173.32   2.666                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHypotheses:\n\\[\\begin{align}\nH_0 &: \\mu_{none} = \\mu_{amputation} = \\mu_{crutches} = \\mu_{hearing} =  \\mu_{wheelchair}\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j \\text{ for } i \\neq j\n\\end{align}\\]\nDo we reject or fail to reject \\(H_0\\)?"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#anova-tables",
    "href": "slides_md/Day14_bsta511_md.html#anova-tables",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA tables",
    "text": "ANOVA tables\nDisability example ANOVA table from R:\n\n\nlm(score ~ disability, data = employ) %&gt;% anova()\n\nAnalysis of Variance Table\n\nResponse: score\n           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ndisability  4  30.521  7.6304  2.8616 0.03013 *\nResiduals  65 173.321  2.6665                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nGeneric ANOVA table:"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#anova-analysis-of-variance",
    "href": "slides_md/Day14_bsta511_md.html#anova-analysis-of-variance",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA: Analysis of Variance",
    "text": "ANOVA: Analysis of Variance\nANOVA compares the variability between groups to the variability within groups"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#anova-analysis-of-variance-1",
    "href": "slides_md/Day14_bsta511_md.html#anova-analysis-of-variance-1",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA: Analysis of Variance",
    "text": "ANOVA: Analysis of Variance\nAnalysis of Variance (ANOVA) compares the variability between groups to the variability within groups\n\n\n\n\n\n\\[\\sum_{i = 1}^k \\sum_{j = 1}^{n_i}(x_{ij} -\\bar{x})^2 \\ \\\n= \\ \\sum_{i = 1}^k n_i(\\bar{x}_{i}-\\bar{x})^2 \\ \\\n+ \\ \\ \\sum_{i = 1}^k\\sum_{j = 1}^{n_i}(x_{ij}-\\bar{x}_{i})^2\\]"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#notation",
    "href": "slides_md/Day14_bsta511_md.html#notation",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Notation",
    "text": "Notation\n\n\n\nk groups\n\\(n_i\\) observations in each of the k groups\nTotal sample size is \\(N=\\sum_{i=1}^{k}n_i\\)\n\\(\\bar{x}_{i}\\) = mean of observations in group i\n\\(\\bar{x}\\) = mean of all observations\n\\(s_{i}\\) = sd of observations in group i\n\\(s\\) = sd of all observations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\ni = 1\ni = 2\ni = 3\n\\(\\ldots\\)\ni = k\noverall\n\n\n\n\nj = 1\n\\(x_{11}\\)\n\\(x_{21}\\)\n\\(x_{31}\\)\n\\(\\ldots\\)\n\\(x_{k1}\\)\n\n\n\nj = 2\n\\(x_{12}\\)\n\\(x_{22}\\)\n\\(x_{32}\\)\n\\(\\ldots\\)\n\\(x_{k2}\\)\n\n\n\nj = 3\n\\(x_{13}\\)\n\\(x_{23}\\)\n\\(x_{33}\\)\n\\(\\ldots\\)\n\\(x_{k3}\\)\n\n\n\nj = 4\n\\(x_{14}\\)\n\\(x_{24}\\)\n\\(x_{34}\\)\n\\(\\ldots\\)\n\\(x_{k4}\\)\n\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\n\n\nj = \\(n_i\\)\n\\(x_{1n_1}\\)\n\\(x_{2n_2}\\)\n\\(x_{3n_3}\\)\n\\(\\ldots\\)\n\\(x_{kn_k}\\)\n\n\n\nMeans\n\\(\\bar{x}_{1}\\)\n\\(\\bar{x}_{2}\\)\n\\(\\bar{x}_{3}\\)\n\\(\\ldots\\)\n\\(\\bar{x}_{k}\\)\n\\(\\bar{x}\\)\n\n\nVariance\n\\({s}^2_{1}\\)\n\\({s}^2_{2}\\)\n\\({s}^2_{3}\\)\n\\(\\ldots\\)\n\\({s}^2_{k}\\)\n\\({s}^2\\)"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#total-sums-of-squares-visually",
    "href": "slides_md/Day14_bsta511_md.html#total-sums-of-squares-visually",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Total Sums of Squares Visually",
    "text": "Total Sums of Squares Visually\n\n\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\nTotal Sums of Squares:\n\\[SST = \\sum_{i = 1}^k \\sum_{j = 1}^{n_i}(x_{ij} -\\bar{x})^2 = (N-1)s^2\\]\n\nwhere\n\n\\(N=\\sum_{i=1}^{k}n_i\\) is the total sample size and\n\\(s^2\\) is the grand standard deviation of all the observations\n\nThis is the sum of the squared differences between each observed \\(x_{ij}\\) value and the grand mean, \\(\\bar{x}\\).\nThat is, it is the total deviation of the \\(x_{ij}\\)’s from the grand mean."
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#calculate-total-sums-of-squares",
    "href": "slides_md/Day14_bsta511_md.html#calculate-total-sums-of-squares",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Calculate Total Sums of Squares",
    "text": "Calculate Total Sums of Squares\nTotal Sums of Squares:\n\\[SST = \\sum_{i = 1}^k \\sum_{j = 1}^{n_i}(x_{ij} -\\bar{x})^2 = (N-1)s^2\\]\n\n\n\nwhere\n\n\\(N=\\sum_{i=1}^{k}n_i\\) is the total sample size and\n\\(s^2\\) is the grand standard deviation of all the observations\n\n\n\nTotal sample size \\(N\\):\n\n(Ns &lt;- employ %&gt;% group_by(disability) %&gt;% count())\n\n# A tibble: 5 × 2\n# Groups:   disability [5]\n  disability     n\n  &lt;fct&gt;      &lt;int&gt;\n1 none          14\n2 amputation    14\n3 crutches      14\n4 hearing       14\n5 wheelchair    14\n\n\n\\(SST\\):\n\n(SST &lt;- (sum(Ns$n) - 1) * sd(employ$score)^2)\n\n[1] 203.8429"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#anova-analysis-of-variance-2",
    "href": "slides_md/Day14_bsta511_md.html#anova-analysis-of-variance-2",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA: Analysis of Variance",
    "text": "ANOVA: Analysis of Variance\nANOVA compares the variability between groups to the variability within groups"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#sums-of-squares-due-to-groups-visually-between-groups",
    "href": "slides_md/Day14_bsta511_md.html#sums-of-squares-due-to-groups-visually-between-groups",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Sums of Squares due to Groups Visually (“between” groups)",
    "text": "Sums of Squares due to Groups Visually (“between” groups)\n\n\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\nSums of Squares due to Groups:\n\\[SSG = \\sum_{i = 1}^k n_i(\\bar{x}_{i}-\\bar{x})^2\\]\n\nThis is the sum of the squared differences between each group mean, \\(\\bar{x}_{i}\\), and the grand mean, \\(\\bar{x}\\).\nThat is, it is the deviation of the group means from the grand mean.\nAlso called the Model SS, or \\(SS_{model}.\\)"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#calculate-sums-of-squares-due-to-groups-between-groups",
    "href": "slides_md/Day14_bsta511_md.html#calculate-sums-of-squares-due-to-groups-between-groups",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Calculate Sums of Squares due to Groups (“between” groups)",
    "text": "Calculate Sums of Squares due to Groups (“between” groups)\n\\[SSG = \\sum_{i = 1}^k n_i(\\bar{x}_{i}-\\bar{x})^2\\]\n\n\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\nCalculate means \\(\\bar{x}_i\\) for each group:\n\nxbar_groups &lt;- employ %&gt;% \n  group_by(disability) %&gt;% \n  summarise(mean = mean(score))\nxbar_groups\n\n# A tibble: 5 × 2\n  disability  mean\n  &lt;fct&gt;      &lt;dbl&gt;\n1 none        4.9 \n2 amputation  4.43\n3 crutches    5.92\n4 hearing     4.05\n5 wheelchair  5.34\n\n\nCalculate \\(SSG\\):\n\n(SSG &lt;- sum(Ns$n *\n  (xbar_groups$mean - mean(employ$score))^2))\n\n[1] 30.52143"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#anova-analysis-of-variance-3",
    "href": "slides_md/Day14_bsta511_md.html#anova-analysis-of-variance-3",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA: Analysis of Variance",
    "text": "ANOVA: Analysis of Variance\nANOVA compares the variability between groups to the variability within groups"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#sums-of-squares-error-visually-within-groups",
    "href": "slides_md/Day14_bsta511_md.html#sums-of-squares-error-visually-within-groups",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Sums of Squares Error Visually (within groups)",
    "text": "Sums of Squares Error Visually (within groups)\n\n\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\nSums of Squares Error:\n\\[SSE = \\sum_{i = 1}^k\\sum_{j = 1}^{n_i}(x_{ij}-\\bar{x}_{i})^2 = \\sum_{i = 1}^k(n_i-1)s_{i}^2\\] where \\(s_{i}\\) is the standard deviation of the \\(i^{th}\\) group\n\nThis is the sum of the squared differences between each observed \\(x_{ij}\\) value and its group mean \\(\\bar{x}_{i}\\).\nThat is, it is the deviation of the \\(x_{ij}\\)’s from the predicted score by group.\nAlso called the residual sums of squares, or \\(SS_{residual}.\\)"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#calculate-sums-of-squares-error-within-groups",
    "href": "slides_md/Day14_bsta511_md.html#calculate-sums-of-squares-error-within-groups",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Calculate Sums of Squares Error (within groups)",
    "text": "Calculate Sums of Squares Error (within groups)\n\\[SSE = \\sum_{i = 1}^k\\sum_{j = 1}^{n_i}(x_{ij}-\\bar{x}_{i})^2 = \\sum_{i = 1}^k(n_i-1)s_{i}^2\\] where \\(s_{i}\\) is the standard deviation of the \\(i^{th}\\) group\n\n\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\nCalculate sd’s \\(s_i\\) for each group:\n\nsd_groups &lt;- employ %&gt;% \n  group_by(disability) %&gt;% \n  summarise(SD = sd(score))\nsd_groups\n\n# A tibble: 5 × 2\n  disability    SD\n  &lt;fct&gt;      &lt;dbl&gt;\n1 none        1.79\n2 amputation  1.59\n3 crutches    1.48\n4 hearing     1.53\n5 wheelchair  1.75\n\n\nCalculate \\(SSE\\):\n\n(SSE &lt;- sum(\n  (Ns$n-1)*sd_groups$SD^2))\n\n[1] 173.3214"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#verify-sst-ssg-sse",
    "href": "slides_md/Day14_bsta511_md.html#verify-sst-ssg-sse",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Verify SST = SSG + SSE",
    "text": "Verify SST = SSG + SSE\nANOVA compares the variability between groups to the variability within groups\n\n\n\n\n\n\\[\\sum_{i = 1}^k \\sum_{j = 1}^{n_i}(x_{ij} -\\bar{x})^2 \\ \\ = \\ \\ n_i\\sum_{i = 1}^k(\\bar{x}_{i}-\\bar{x})^2 \\ \\\n+ \\ \\ \\sum_{i = 1}^k\\sum_{j = 1}^{n_i}(x_{ij}-\\bar{x}_{i})^2\\]\n\\[(N-1)s^2 \\ \\\n= \\ \\sum_{i = 1}^k n_i(\\bar{x}_{i}-\\bar{x})^2 \\ \\\n+ \\ \\ \\sum_{i = 1}^k(n_i-1)s_{i}^2\\]\n\n\n\n\n\n\n\n\nSST\n\n[1] 203.8429\n\n\n\n\nSSG + SSE\n\n[1] 203.8429"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#anova-table",
    "href": "slides_md/Day14_bsta511_md.html#anova-table",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA table",
    "text": "ANOVA table"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#thinking-about-the-f-statistic",
    "href": "slides_md/Day14_bsta511_md.html#thinking-about-the-f-statistic",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Thinking about the F-statistic",
    "text": "Thinking about the F-statistic\n\n\nIf the groups are actually different, then which of these is more accurate?\n\nThe variability between groups should be higher than the variability within groups\nThe variability within groups should be higher than the variability between groups\n\n\nIf there really is a difference between the groups, we would expect the F-statistic to be which of these:\n\nHigher than we would observe by random chance\nLower than we would observe by random chance\n\n\n\n\n\n\\[F = \\frac{MSG}{MSE}\\]"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#anova-in-base-r-1",
    "href": "slides_md/Day14_bsta511_md.html#anova-in-base-r-1",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA in base R",
    "text": "ANOVA in base R\n\n# Note that I'm saving the tidy anova table\n# Will be pulling p-value from this on future slide\n\nempl_lm &lt;- lm(score ~ disability, data = employ) %&gt;% \n  anova() %&gt;% \n  tidy()\n\nempl_lm %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    disability\n4\n30.52143\n7.630357\n2.86158\n0.03012686\n    Residuals\n65\n173.32143\n2.666484\nNA\nNA\n  \n  \n  \n\n\n\n\nHypotheses:\n\\[\\begin{align}\nH_0 &: \\mu_{none} = \\mu_{amputation} = \\mu_{crutches} = \\mu_{hearing} =  \\mu_{wheelchair}\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j \\text{ for } i \\neq j\n\\end{align}\\]\nDo we reject or fail to reject \\(H_0\\)?"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#conclusion-to-hypothesis-test",
    "href": "slides_md/Day14_bsta511_md.html#conclusion-to-hypothesis-test",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Conclusion to hypothesis test",
    "text": "Conclusion to hypothesis test\n\\[\\begin{align}\nH_0 &: \\mu_{none} = \\mu_{amputation} = \\mu_{crutches} = \\mu_{hearing} =  \\mu_{wheelchair}\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j \\text{ for } i \\neq j\n\\end{align}\\]\n\n\n\nempl_lm  # tidy anova output\n\n# A tibble: 2 × 6\n  term          df sumsq meansq statistic p.value\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 disability     4  30.5   7.63      2.86  0.0301\n2 Residuals     65 173.    2.67     NA    NA     \n\n# Note that this is a vector:\nempl_lm$p.value\n\n[1] 0.03012686         NA\n\n\nPull the p-value using base R:\n\nround(empl_lm$p.value[1],2)\n\n[1] 0.03\n\n\nPull the p-value using tidyverse:\n\nempl_lm %&gt;% \n  filter(term == \"disability\") %&gt;% \n  pull(p.value) %&gt;% \n  round(2)\n\n[1] 0.03\n\n\n\n\nUse \\(\\alpha\\) = 0.05.\nDo we reject or fail to reject \\(H_0\\)?\n\nConclusion statement:\n\nThere is sufficient evidence that at least one of the disability groups has a mean employment score statistically different from the other groups. ( \\(p\\)-value = 0.03)."
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#conditions-for-anova",
    "href": "slides_md/Day14_bsta511_md.html#conditions-for-anova",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Conditions for ANOVA",
    "text": "Conditions for ANOVA\nIF ALL of the following conditions hold:\n\nThe null hypothesis is true\nSample sizes in each group group are large (each \\(n \\ge 30\\))\n\nOR the data are relatively normally distributed in each group\n\n\n\n\n\nVariability is “similar” in all group groups:\n\nIs the within group variability about the same for each group?\nAs a rough rule of thumb, this condition is violated if the standard deviation of one group is more than double the standard deviation of another group\n\n\n\n\n\nChecking the equal variance condition:\n\nsd_groups # previously defined\n\n# A tibble: 5 × 2\n  disability    SD\n  &lt;fct&gt;      &lt;dbl&gt;\n1 none        1.79\n2 amputation  1.59\n3 crutches    1.48\n4 hearing     1.53\n5 wheelchair  1.75\n\nmax(sd_groups$SD) / min(sd_groups$SD)\n\n[1] 1.210425\n\n\n\n\nTHEN the sampling distribution of the F-statistic is an F-distribution"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#testing-variances-condition-3",
    "href": "slides_md/Day14_bsta511_md.html#testing-variances-condition-3",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Testing variances (Condition 3)",
    "text": "Testing variances (Condition 3)\nBartlett’s test for equal variances\n\n\\(H_0:\\) population variances of group levels are equal\n\\(H_A:\\) population variances of group levels are NOT equal\n\nNote: \\(H_A\\) is same as saying that at least one of the group levels has a different variance\n\n\n\n\n\n\nCaution\n\n\n\n\nBartlett’s test assumes the data in each group are normally distributed.\nDo not use if data do not satisfy the normality condition.\n\n\n\n\nbartlett.test(score ~ disability, data = employ)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  score by disability\nBartlett's K-squared = 0.7016, df = 4, p-value = 0.9511\n\n\n\n\n\n\n\n\nTip\n\n\n\nLevene’s test for equality of variances is not as restrictive: see https://www.statology.org/levenes-test-r/"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#the-f-distribution",
    "href": "slides_md/Day14_bsta511_md.html#the-f-distribution",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "The F-distribution",
    "text": "The F-distribution\n\nThe F-distribution is skewed right.\nThe F-distribution has two different degrees of freedom:\n\none for the numerator of the ratio (k – 1) and\none for the denominator (N – k)\n\n\\(p\\)-value\n\nis always the upper tail\n(the area as extreme or more extreme)\n\n\n\n\n\n\n\n\n\n\n\nempl_lm %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    disability\n4\n30.52143\n7.630357\n2.86158\n0.03012686\n    Residuals\n65\n173.32143\n2.666484\nNA\nNA\n  \n  \n  \n\n\n\n# p-value using F-distribution\n\npf(2.86158, df1=5-1, df2=70-5, \n   lower.tail = FALSE)\n\n[1] 0.03012688"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#which-groups-are-statistically-different",
    "href": "slides_md/Day14_bsta511_md.html#which-groups-are-statistically-different",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Which groups are statistically different?",
    "text": "Which groups are statistically different?\n\n\n\n\nSo far we’ve only determined that at least one of the groups is different from the others,\n\nbut we don’t know which.\n\n\n\n\nWhat’s your guess?\n\n\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`."
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#post-hoc-testing-pairwise-t-tests",
    "href": "slides_md/Day14_bsta511_md.html#post-hoc-testing-pairwise-t-tests",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Post-hoc testing: pairwise t-tests",
    "text": "Post-hoc testing: pairwise t-tests\n\n\n\nIn post-hoc testing we run all the pairwise t-tests comparing the means from each pair of groups.\nWith 5 groups, this involves doing \\({5 \\choose 2} = \\frac{5!}{2!3!} = \\frac{5\\cdot 4}{2}= 10\\) different pairwise tests.\n\nProblem:\n\nAlthough the ANOVA test has an \\(\\alpha\\) chance of a Type I error (finding a difference between a pair that aren’t different),\nthe overall Type I error rate will be much higher when running many tests simultaneously.\n\n\n\n\n\n\\[\\begin{align}\nP(\\text{making an error}) = & \\alpha\\\\\nP(\\text{not making an error}) = & 1-\\alpha\\\\\nP(\\text{not making an error in m tests}) = & (1-\\alpha)^m\\\\\nP(\\text{making at least 1 error in m tests}) = & 1-(1-\\alpha)^m\n\\end{align}\\]"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#the-bonferroni-correction-12",
    "href": "slides_md/Day14_bsta511_md.html#the-bonferroni-correction-12",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "The Bonferroni Correction (1/2)",
    "text": "The Bonferroni Correction (1/2)\n\n\nA very conservative (but very popular) approach is to divide the \\(\\alpha\\) level by how many tests \\(m\\) are being done:\n\\[\\alpha_{Bonf} = \\frac{\\alpha}{m}\\]\n\nThis is equivalent to multiplying the\np-values by m:\n\n\\[p\\textrm{-value} &lt; \\alpha_{Bonf} = \\frac{\\alpha}{m}\\] is the same as \\[m \\cdot (p\\textrm{-value}) &lt; \\alpha\\] The Bonferroni correction is popular since it’s very easy to implement.\n\n\nThe plot below shows the likelihood of making at least one Type I error depending on how may tests are done.\nNotice the likelihood decreases very quickly\n\nUnfortunately the likelihood of a Type II error is increasing as well\nIt becomes “harder” and harder to reject \\(H_0\\) if doing many tests."
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#the-bonferroni-correction-22",
    "href": "slides_md/Day14_bsta511_md.html#the-bonferroni-correction-22",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "The Bonferroni Correction (2/2)",
    "text": "The Bonferroni Correction (2/2)\n\n\nPairwise t-tests without any p-value adjustments:\n\npairwise.t.test(employ$score, \n                employ$disability, \n                p.adj=\"none\") \n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  employ$score and employ$disability \n\n           none   amputation crutches hearing\namputation 0.4477 -          -        -      \ncrutches   0.1028 0.0184     -        -      \nhearing    0.1732 0.5418     0.0035   -      \nwheelchair 0.4756 0.1433     0.3520   0.0401 \n\nP value adjustment method: none \n\n\n\nPairwise t-tests with Bonferroni p-value adjustments:\n\npairwise.t.test(employ$score,  \n                employ$disability, \n                p.adj=\"bonferroni\")  \n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  employ$score and employ$disability \n\n           none  amputation crutches hearing\namputation 1.000 -          -        -      \ncrutches   1.000 0.184      -        -      \nhearing    1.000 1.000      0.035    -      \nwheelchair 1.000 1.000      1.000    0.401  \n\nP value adjustment method: bonferroni \n\n\n\nSince there were 10 tests, all the p-values were multiplied by 10.\nAre there any significant pairwise differences?"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#tukeys-honest-significance-test-hsd",
    "href": "slides_md/Day14_bsta511_md.html#tukeys-honest-significance-test-hsd",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Tukey’s Honest Significance Test (HSD)",
    "text": "Tukey’s Honest Significance Test (HSD)\n\nTukey’s Honest Significance Test (HSD) controls the “family-wise probability” of making a Type I error using a much less conservative method than Bonferroni\n\nIt is specific to ANOVA\n\nIn addition to adjusted p-values, it also calculates Tukey adjusted CI’s for all pairwise differences\nThe function TukeyHSD() creates a set of confidence intervals of the differences between means with the specified family-wise probability of coverage.\n\n\n\n\n# need to run the model using `aov` instead of `lm`\nempl_aov &lt;- aov(score ~ disability, data = employ) \n\nTukeyHSD(x=empl_aov, conf.level = 0.95) \n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = score ~ disability, data = employ)\n\n$disability\n                            diff        lwr        upr     p adj\namputation-none       -0.4714286 -2.2031613  1.2603042 0.9399911\ncrutches-none          1.0214286 -0.7103042  2.7531613 0.4686233\nhearing-none          -0.8500000 -2.5817328  0.8817328 0.6442517\nwheelchair-none        0.4428571 -1.2888756  2.1745899 0.9517374\ncrutches-amputation    1.4928571 -0.2388756  3.2245899 0.1232819\nhearing-amputation    -0.3785714 -2.1103042  1.3531613 0.9724743\nwheelchair-amputation  0.9142857 -0.8174470  2.6460185 0.5781165\nhearing-crutches      -1.8714286 -3.6031613 -0.1396958 0.0277842\nwheelchair-crutches   -0.5785714 -2.3103042  1.1531613 0.8812293\nwheelchair-hearing     1.2928571 -0.4388756  3.0245899 0.2348141\n\n\n\n\nplot(TukeyHSD(x=empl_aov, \n        conf.level = 0.95))"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#there-are-many-more-multiple-testing-adjustment-procedures",
    "href": "slides_md/Day14_bsta511_md.html#there-are-many-more-multiple-testing-adjustment-procedures",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "There are many more multiple testing adjustment procedures",
    "text": "There are many more multiple testing adjustment procedures\n\n\n\nBonferroni is popular because it’s so easy to apply\nTukey’s HSD is usually used for ANOVA\nCode below used Holm’s adjustment\n\n\n# default is Holm's adjustments\npairwise.t.test(employ$score, \n                employ$disability) \n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  employ$score and employ$disability \n\n           none  amputation crutches hearing\namputation 1.000 -          -        -      \ncrutches   0.719 0.165      -        -      \nhearing    0.866 1.000      0.035    -      \nwheelchair 1.000 0.860      1.000    0.321  \n\nP value adjustment method: holm \n\n\n\n\n\n\nFalse discovery rate (fdr) p-value adjustments are popular in omics, or whenever there are many tests being run:\n\n\npairwise.t.test(employ$score, \n                employ$disability, \n                p.adj=\"fdr\") \n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  employ$score and employ$disability \n\n           none  amputation crutches hearing\namputation 0.528 -          -        -      \ncrutches   0.257 0.092      -        -      \nhearing    0.289 0.542      0.035    -      \nwheelchair 0.528 0.287      0.503    0.134  \n\nP value adjustment method: fdr"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#multiple-testing-controlling-the-type-i-error-rate",
    "href": "slides_md/Day14_bsta511_md.html#multiple-testing-controlling-the-type-i-error-rate",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Multiple testing: controlling the Type I error rate",
    "text": "Multiple testing: controlling the Type I error rate\n\n\n\nThe multiple testing issue is not unique to ANOVA post-hoc testing.\nIt is also a concern when running separate tests for many related outcomes.\nBeware of p-hacking!\n\nProblem:\n\nAlthough one test has an \\(\\alpha\\) chance of a Type I error (finding a difference between a pair that aren’t different),\nthe overall Type I error rate will be much higher when running many tests simultaneously.\n\n\n\n\n\n\\[\\begin{align}\nP(\\text{making an error}) = & \\alpha\\\\\nP(\\text{not making an error}) = & 1-\\alpha\\\\\nP(\\text{not making an error in m tests}) = & (1-\\alpha)^m\\\\\nP(\\text{making at least 1 error in m tests}) = & 1-(1-\\alpha)^m\n\\end{align}\\]"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#anova-summary",
    "href": "slides_md/Day14_bsta511_md.html#anova-summary",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA Summary",
    "text": "ANOVA Summary\n\n\\[\\begin{align}\nH_0 &: \\mu_1 = \\mu_2 = ... = \\mu_k\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j \\text{ for } i \\neq j\n\\end{align}\\]\n\nANOVA table in R:\n\n\nlm(score ~ disability, data = employ) %&gt;% anova()\n\nAnalysis of Variance Table\n\nResponse: score\n           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ndisability  4  30.521  7.6304  2.8616 0.03013 *\nResiduals  65 173.321  2.6665                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nANOVA table\n\n\n\n\n\nPost-hoc testing\n\nF-distribution & p-value"
  },
  {
    "objectID": "slides_md/Day14_bsta511_md.html#whats-next",
    "href": "slides_md/Day14_bsta511_md.html#whats-next",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "What’s next?",
    "text": "What’s next?"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html",
    "href": "slides_md/Day13_bsta511_md.html",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "",
    "text": "Add text to a plot using annotate():\n\nggplot(NULL, aes(c(0,4))) +  # no dataset, create axes for x from 0 to 4\n  geom_area(stat = \"function\", fun = dchisq, args = list(df=2), \n            fill = \"blue\", xlim = c(0, 1.0414)) +\n  geom_area(stat = \"function\", fun = dchisq, args = list(df=2),\n            fill = \"violet\", xlim = c(1.0414, 4)) +\n  geom_vline(xintercept = 1.0414) +  # vertical line at x = 1.0414\n  annotate(\"text\", x = 1.1, y = .4, # add text at specified (x,y) coordinate\n           label = \"chi-squared = 1.0414\", hjust=0, size=6) + \n  annotate(\"text\", x = 1.3, y = .1, \n           label = \"p-value = 0.59\", hjust=0, size=6)"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#moritzs-tip-of-the-day",
    "href": "slides_md/Day13_bsta511_md.html#moritzs-tip-of-the-day",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "",
    "text": "Add text to a plot using annotate():\n\nggplot(NULL, aes(c(0,4))) +  # no dataset, create axes for x from 0 to 4\n  geom_area(stat = \"function\", fun = dchisq, args = list(df=2), \n            fill = \"blue\", xlim = c(0, 1.0414)) +\n  geom_area(stat = \"function\", fun = dchisq, args = list(df=2),\n            fill = \"violet\", xlim = c(1.0414, 4)) +\n  geom_vline(xintercept = 1.0414) +  # vertical line at x = 1.0414\n  annotate(\"text\", x = 1.1, y = .4, # add text at specified (x,y) coordinate\n           label = \"chi-squared = 1.0414\", hjust=0, size=6) + \n  annotate(\"text\", x = 1.3, y = .1, \n           label = \"p-value = 0.59\", hjust=0, size=6)"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#where-are-we",
    "href": "slides_md/Day13_bsta511_md.html#where-are-we",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#where-are-we-categorical-outcome-zoomed-in",
    "href": "slides_md/Day13_bsta511_md.html#where-are-we-categorical-outcome-zoomed-in",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Where are we? Categorical outcome zoomed in",
    "text": "Where are we? Categorical outcome zoomed in"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#goals-for-today-sections-8.3-8.4",
    "href": "slides_md/Day13_bsta511_md.html#goals-for-today-sections-8.3-8.4",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Goals for today (Sections 8.3-8.4)",
    "text": "Goals for today (Sections 8.3-8.4)\n\nStatistical inference for categorical data when either are\n\ncomparing more than two groups,\nor have categorical outcomes that have more than 2 levels,\nor both\n\nChi-squared tests of association (independence)\n\nHypotheses\ntest statistic\nChi-squared distribution\np-value\ntechnical conditions (assumptions)\nconclusion\nR: chisq.test()\n\nFisher’s Exact Test\nChi-squared test vs. testing difference in proportions\n\nTest of Homogeneity"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#is-there-an-association-between-depression-and-being-physically-active",
    "href": "slides_md/Day13_bsta511_md.html#is-there-an-association-between-depression-and-being-physically-active",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Is there an association between depression and being physically active?",
    "text": "Is there an association between depression and being physically active?\n\nData sampled from the NHANES R package:\n\nAmerican National Health and Nutrition Examination Surveys\nCollected 2009-2012 by US National Center for Health Statistics (NCHS)\nNHANES dataset: 10,000 rows, resampled from NHANESraw to undo oversampling effects\n\nTreat it as a simple random sample from the US population (for pedagogical purposes)\n\n\nDepressed\n\nSelf-reported number of days where participant felt down, depressed or hopeless.\nOne of None, Several, or Most (more than half the days).\nReported for participants aged 18 years or older.\n\nPhysActive\n\nParticipant does moderate or vigorous-intensity sports, fitness or recreational activities (Yes or No).\nReported for participants 12 years or older."
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#hypotheses-for-a-chi-squared-test-of-association-independence",
    "href": "slides_md/Day13_bsta511_md.html#hypotheses-for-a-chi-squared-test-of-association-independence",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Hypotheses for a Chi-squared test of association (independence)",
    "text": "Hypotheses for a Chi-squared test of association (independence)\n\n\nGeneric wording:\nTest of “association” wording\n\n\\(H_0\\): There is no association between the two variables\n\\(H_A\\): There is an association between the two variables\n\nTest of “independence” wording\n\n\\(H_0\\): The variables are independent\n\\(H_A\\): The variables are not independent\n\n\n\n\nFor our example:\nTest of “association” wording\n\n\\(H_0\\): There is no association between depression and physical activity\n\\(H_A\\): There is an association between depression and physical activity\n\nTest of “independence” wording\n\n\\(H_0\\): The variables depression and physical activity are independent\n\\(H_A\\): The variables depression and physical activity are not independent\n\n\n\n\n\n\n\n\n\nNo symbols\n\n\n\nFor chi-squared test hypotheses we do not have versions using “symbols” like we do with tests of means or proportions."
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#data-from-nhanes",
    "href": "slides_md/Day13_bsta511_md.html#data-from-nhanes",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Data from NHANES",
    "text": "Data from NHANES\n\nResults below are from\n\na random sample of 400 adults (≥ 18 yrs old)\nwith data for both the depression Depressed and physically active (PhysActive) variables.\n\n\n\n\n\n\n\n\nWhat does it mean for the variables to be independent?"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#h_0-variables-are-independent",
    "href": "slides_md/Day13_bsta511_md.html#h_0-variables-are-independent",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "\\(H_0\\): Variables are Independent",
    "text": "\\(H_0\\): Variables are Independent\n\n\n\nRecall from Chapter 2, that events \\(A\\) and \\(B\\) are independent if and only if\n\n\\[P(A~and~B)=P(A)P(B)\\]\n\nIf depression and being physically active are independent variables, then theoretically this condition needs to hold for every combination of levels, i.e.\n\n\n\\[\\begin{align}\nP(None~and~Yes) &= P(None)P(Yes)\\\\\nP(None~and~No) &= P(None)P(No)\\\\\nP(Several~and~Yes) &= P(Several)P(Yes)\\\\\nP(Several~and~No) &= P(Several)P(No)\\\\\nP(Most~and~Yes) &= P(Most)P(Yes)\\\\\nP(Most~and~No) &= P(Most)P(No)\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\nP(None~and~Yes) &= \\frac{314}{400}\\cdot\\frac{226}{400}\\\\\n& ...\\\\\nP(Most~and~No) &= \\frac{28}{400}\\cdot\\frac{174}{400}\n\\end{align}\\]\n\nWith these probabilities, for each cell of the table we calculate the expected counts for each cell under the \\(H_0\\) hypothesis that the variables are independent"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#expected-counts-if-variables-are-independent",
    "href": "slides_md/Day13_bsta511_md.html#expected-counts-if-variables-are-independent",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Expected counts (if variables are independent)",
    "text": "Expected counts (if variables are independent)\n\n\n\nThe expected counts (if \\(H_0\\) is true & the variables are independent) for each cell are\n\n\\(np\\) = total table size \\(\\cdot\\) probability of cell\n\n\nExpected count of Yes & None:\n\\[\\begin{align}\n400 \\cdot & P(None~and~Yes)\\\\\n&= 400 \\cdot P(None)P(Yes)\\\\\n&= 400 \\cdot\\frac{314}{400}\\cdot\\frac{226}{400}\\\\\n&= \\frac{314\\cdot 226}{400} \\\\\n&=  177.41\\\\\n&= \\frac{\\text{column total}\\cdot \\text{row total}}{\\text{table total}}\n\\end{align}\\]\n\n\n\n\n\n\n\nIf depression and being physically active are independent variables\n\n(as assumed by \\(H_0\\)),\n\nthen the observed counts should be close to the expected counts for each cell of the table"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#observed-vs.-expected-counts",
    "href": "slides_md/Day13_bsta511_md.html#observed-vs.-expected-counts",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Observed vs. Expected counts",
    "text": "Observed vs. Expected counts\n\n\n\nThe observed counts are the counts in the 2-way table summarizing the data\n\n\n\n\n\n\n\nExpected count for cell \\(i,j\\) :\n\n\nThe expected counts are the counts the we would expect to see in the 2-way table if there was no association between depression and being physically activity\n\n\n\n\n\n\n\n\n\\[\\textrm{Expected Count}_{\\textrm{row } i,\\textrm{ col }j}=\\frac{(\\textrm{row}~i~ \\textrm{total})\\cdot(\\textrm{column}~j~ \\textrm{total})}{\\textrm{table total}}\\]"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#the-chi2-test-statistic",
    "href": "slides_md/Day13_bsta511_md.html#the-chi2-test-statistic",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "The \\(\\chi^2\\) test statistic",
    "text": "The \\(\\chi^2\\) test statistic\n\n\nTest statistic for a test of association (independence):\n\\[\\chi^2 = \\sum_{\\textrm{all cells}} \\frac{(\\textrm{observed} - \\text{expected})^2}{\\text{expected}}\\]\n\nWhen the variables are independent, the observed and expected counts should be close to each other\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\n\\chi^2 &=  \\sum\\frac{(O-E)^2}{E} \\\\\n&= \\frac{(199-177.41)^2}{177.41} + \\frac{(26-32.77)^2}{32.77} + \\ldots + \\frac{(27-12.18)^2}{12.18} \\\\\n&=  41.2\n\\end{align}\\]\n\nIs this value big? Big enough to reject \\(H_0\\)?"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#the-chi2-distribution-calculating-the-p-value",
    "href": "slides_md/Day13_bsta511_md.html#the-chi2-distribution-calculating-the-p-value",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "The \\(\\chi^2\\) distribution & calculating the p-value",
    "text": "The \\(\\chi^2\\) distribution & calculating the p-value\n\n\nThe \\(\\chi^2\\) distribution shape depends on its degrees of freedom\n\nIt’s skewed right for smaller df,\n\ngets more symmetric for larger df\n\ndf = (# rows-1) x (# columns-1)\n\n\n\n\n\n\n\n\nThe p-value is always the area to the right of the test statistic for a \\(\\chi^2\\) test.\nWe can use the pchisq function in R to calculate the probability of being at least as big as the \\(\\chi^2\\) test statistic:\n\n\npv &lt;- pchisq(41.2, df = 2, \n       lower.tail = FALSE)\npv\n\n[1] 1.131185e-09\n\n\nWhat’s the conclusion to the \\(\\chi^2\\) test?"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#conclusion",
    "href": "slides_md/Day13_bsta511_md.html#conclusion",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Conclusion",
    "text": "Conclusion\nRecall the hypotheses to our \\(\\chi^2\\) test:\n\n\\(H_0\\): There is no association between depression and being physically activity\n\\(H_A\\): There is an association between depression and being physically activity\n\n\n\n\n\n\n\n\n\nConclusion:\nBased a random sample of 400 US adults from 2009-2012, there is sufficient evidence that there is an association between depression and being physically activity (p-value &lt; 0.001).\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf we fail to reject, we DO NOT have evidence of no association."
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#technical-conditions",
    "href": "slides_md/Day13_bsta511_md.html#technical-conditions",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Technical conditions",
    "text": "Technical conditions\n\nIndependence\n\nEach case (person) that contributes a count to the table must be independent of all the other cases in the table\n\nIn particular, observational units cannot be represented in more than one cell.\nFor example, someone cannot choose both “Several” and “Most” for depression status. They have to choose exactly one option for each variable.\n\n\n\n\n\n\n\nSample size\n\nIn order for the distribution of the test statistic to be appropriately modeled by a chi-squared distribution we need\n2 \\(\\times\\) 2 table:\n\nexpected counts are at least 10 for each cell\n\nlarger tables:\n\nno more than 1/5 of the expected counts are less than 5, and\nall expected counts are greater than 1"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#depression-vs.-physical-activity-dataset",
    "href": "slides_md/Day13_bsta511_md.html#depression-vs.-physical-activity-dataset",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Depression vs. physical activity dataset",
    "text": "Depression vs. physical activity dataset\nCreate dataset based on results table:\n\n\n\nDepPA &lt;- tibble(\n  Depression = c(rep(\"None\", 314), \n         rep(\"Several\", 58),\n         rep(\"Most\", 28)),\n  PA = c(rep(\"Yes\", 199),  # None\n          rep(\"No\", 115),\n          rep(\"Yes\", 26), # Several\n          rep(\"No\", 32),\n          rep(\"Yes\", 1), # Most\n          rep(\"No\", 27))\n)\n\n\n\n\n\n\n\n\n\nSummary table of data:\n\n\n\nDepPA %&gt;% \n  tabyl(Depression, PA)\n\n Depression  No Yes\n       Most  27   1\n       None 115 199\n    Several  32  26\n\n\n\n\n# base R:\ntable(DepPA)\n\n          PA\nDepression  No Yes\n   Most     27   1\n   None    115 199\n   Several  32  26"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#chi2-test-in-r-using-dataset",
    "href": "slides_md/Day13_bsta511_md.html#chi2-test-in-r-using-dataset",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "\\(\\chi^2\\) test in R using dataset",
    "text": "\\(\\chi^2\\) test in R using dataset\n\n\nIf only have 2 columns in the dataset:\n\n(ChisqTest_DepPA &lt;- \n   chisq.test(table(DepPA)))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(DepPA)\nX-squared = 41.171, df = 2, p-value = 1.148e-09\n\n\nIf have &gt;2 columns in the dataset, we need to specify which columns to table:\n\n(ChisqTest_DepPA &lt;- \n   chisq.test(table(\n     DepPA$Depression, DepPA$PA)))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(DepPA$Depression, DepPA$PA)\nX-squared = 41.171, df = 2, p-value = 1.148e-09\n\n\n\nThe tidyverse way (fewer parentheses)\n\ntable(DepPA$Depression, DepPA$PA) %&gt;% \n  chisq.test() \n\n\n    Pearson's Chi-squared test\n\ndata:  .\nX-squared = 41.171, df = 2, p-value = 1.148e-09\n\n\ntidy() the output (from broom package):\n\ntable(DepPA$Depression, DepPA$PA) %&gt;% \n  chisq.test() %&gt;% \n  tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      statistic\n      p.value\n      parameter\n      method\n    \n  \n  \n    41.17067\n1.147897e-09\n2\nPearson's Chi-squared test\n  \n  \n  \n\n\n\n\nPull p-value\n\ntable(DepPA$Depression, DepPA$PA) %&gt;% \n  chisq.test() %&gt;% \n  tidy() %&gt;% pull(p.value)\n\n[1] 1.147897e-09"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#observed-expected-counts-in-r",
    "href": "slides_md/Day13_bsta511_md.html#observed-expected-counts-in-r",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Observed & expected counts in R",
    "text": "Observed & expected counts in R\n\n\nYou can see what the observed and expected counts are from the saved chi-squared test results:\n\nChisqTest_DepPA$observed\n\n         \n           No Yes\n  Most     27   1\n  None    115 199\n  Several  32  26\n\nChisqTest_DepPA$expected\n\n         \n              No    Yes\n  Most     12.18  15.82\n  None    136.59 177.41\n  Several  25.23  32.77\n\n\n\n\n\n\n\n\n\nWhy is it important to look at the expected counts?\nWhat are we looking for in the expected counts?"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#chi2-test-in-r-with-2-way-table",
    "href": "slides_md/Day13_bsta511_md.html#chi2-test-in-r-with-2-way-table",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "\\(\\chi^2\\) test in R with 2-way table",
    "text": "\\(\\chi^2\\) test in R with 2-way table\nCreate a base R table of the results:\n\n(DepPA_table &lt;- matrix(c(199, 26, 1, 115, 32, 27), nrow = 2, ncol = 3, byrow = T))\n\n     [,1] [,2] [,3]\n[1,]  199   26    1\n[2,]  115   32   27\n\ndimnames(DepPA_table) &lt;- list(\"PA\" = c(\"Yes\", \"No\"),   # row names\n                              \"Depression\" = c(\"None\", \"Several\", \"Most\"))  # column names\nDepPA_table\n\n     Depression\nPA    None Several Most\n  Yes  199      26    1\n  No   115      32   27\n\n\nRun \\(\\chi^2\\) test with 2-way table:\n\nchisq.test(DepPA_table) \n\n\n    Pearson's Chi-squared test\n\ndata:  DepPA_table\nX-squared = 41.171, df = 2, p-value = 1.148e-09\n\nchisq.test(DepPA_table)$expected\n\n     Depression\nPA      None Several  Most\n  Yes 177.41   32.77 15.82\n  No  136.59   25.23 12.18"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#yates-continuity-correction",
    "href": "slides_md/Day13_bsta511_md.html#yates-continuity-correction",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "(Yates’) Continuity correction",
    "text": "(Yates’) Continuity correction\n\nFor a 2x2 contingency table,\n\nthe \\(\\chi^2\\) test has the option of including a continuity correction\njust like with the proportions test\n\nThe default includes a continuity correction\nThere is no CC for bigger tables\n\n\n(DepPA_table2x2 &lt;- matrix(c(199, 27, 115, 59), nrow = 2, ncol = 2, byrow = T))\n\n     [,1] [,2]\n[1,]  199   27\n[2,]  115   59\n\ndimnames(DepPA_table2x2) &lt;- list(\"PA\" = c(\"Yes\", \"No\"),   # row names\n                              \"Depression\" = c(\"None\", \"Several/Most\"))  # column names\nDepPA_table2x2\n\n     Depression\nPA    None Several/Most\n  Yes  199           27\n  No   115           59\n\n\n\n\nOutput without a CC\n\nchisq.test(DepPA_table2x2, correct = FALSE) \n\n\n    Pearson's Chi-squared test\n\ndata:  DepPA_table2x2\nX-squared = 28.093, df = 1, p-value = 1.156e-07\n\n\n\nCompare to output with CC:\n\nchisq.test(DepPA_table2x2) \n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  DepPA_table2x2\nX-squared = 26.807, df = 1, p-value = 2.248e-07"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#example-with-smaller-sample-size",
    "href": "slides_md/Day13_bsta511_md.html#example-with-smaller-sample-size",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Example with smaller sample size",
    "text": "Example with smaller sample size\n\nSuppose that instead of taking a random sample of 400 adults (from the NHANES data), a study takes a random sample of 100 such that\n\n50 people that are physically active and\n50 people that are not physically active\n\n\n\n(DepPA100_table &lt;- matrix(c(43, 5, 2, 40, 4, 6), nrow = 2, ncol = 3, byrow = T))\n\n     [,1] [,2] [,3]\n[1,]   43    5    2\n[2,]   40    4    6\n\ndimnames(DepPA100_table) &lt;- list(\"PA\" = c(\"Yes\", \"No\"),   # row names\n                              \"Depression\" = c(\"None\", \"Several\", \"Most\"))  # column names\n\nDepPA100_table\n\n     Depression\nPA    None Several Most\n  Yes   43       5    2\n  No    40       4    6"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#chi-squared-test-warning",
    "href": "slides_md/Day13_bsta511_md.html#chi-squared-test-warning",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Chi-squared test warning",
    "text": "Chi-squared test warning\n\nchisq.test(DepPA100_table) \n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  DepPA100_table\nX-squared = 2.2195, df = 2, p-value = 0.3296\n\nchisq.test(DepPA100_table)$expected\n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\n\n     Depression\nPA    None Several Most\n  Yes 41.5     4.5    4\n  No  41.5     4.5    4\n\n\n\nRecall the sample size condition\n\nIn order for the test statistic to be modeled by a chi-squared distribution we need\n2 \\(\\times\\) 2 table: expected counts are at least 10 for each cell\nlarger tables:\n\nno more than 1/5 of the expected counts are less than 5, and\nall expected counts are greater than 1"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#fishers-exact-test",
    "href": "slides_md/Day13_bsta511_md.html#fishers-exact-test",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Fisher’s Exact Test",
    "text": "Fisher’s Exact Test\n\nCalled an exact test since it\n\ncalculates an exact probability for the p-value\n\ninstead of using an asymptotic approximation, such as the normal, t, or chi-squared distributions\n\nFor 2x2 tables the p-value is calculated using the hypergeometric probability distribution (see book for details)\n\n\n\nfisher.test(DepPA100_table)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  DepPA100_table\np-value = 0.3844\nalternative hypothesis: two.sided\n\n\n\n\n\n\n\n\nComments\n\n\n\n\nNote that there is no test statistic\nThere is also no CI\nThis is always a two-sided test\nThere is no continuity correction since the hypergeometric distribution is discrete"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#simulate-p-values-another-option-for-small-expected-counts",
    "href": "slides_md/Day13_bsta511_md.html#simulate-p-values-another-option-for-small-expected-counts",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Simulate p-values: another option for small expected counts",
    "text": "Simulate p-values: another option for small expected counts\nFrom the chisq.test help file:\n\nSimulation is done by random sampling from the set of all contingency tables with the same margin totals\n\nworks only if the margin totals are strictly positive.\n\nFor each simulation, a \\(\\chi^2\\) test statistic is calculated\nP-value is the proportion of simulations that have a test statistic at least as big as the observed one.\nNo continuity correction\n\n\nset.seed(567)\nchisq.test(DepPA100_table, simulate.p.value = TRUE) \n\n\n    Pearson's Chi-squared test with simulated p-value (based on 2000\n    replicates)\n\ndata:  DepPA100_table\nX-squared = 2.2195, df = NA, p-value = 0.3893"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#chi2-test-vs.-testing-differences-in-proportions",
    "href": "slides_md/Day13_bsta511_md.html#chi2-test-vs.-testing-differences-in-proportions",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "\\(\\chi^2\\) test vs. testing differences in proportions",
    "text": "\\(\\chi^2\\) test vs. testing differences in proportions\nIf there are only 2 levels in both of the categorical variables being tested, then the p-value from the \\(\\chi^2\\) test is equal to the p-value from the differences in proportions test.\n\n\nExample: Previously we tested whether the proportion who had participated in sports betting was the same for college and noncollege young adults:\n\\[\\begin{align}\nH_0:& ~p_{coll} - p_{noncoll} = 0\\\\\nH_A:& ~p_{coll} - p_{noncoll} \\neq 0\n\\end{align}\\]\n\n\n\nSportsBet_table &lt;- matrix(\n  c(175, 94, 137, 77), \n  nrow = 2, ncol = 2, byrow = T)\n\ndimnames(SportsBet_table) &lt;- list(\n  \"Group\" = c(\"College\", \"NonCollege\"), # row names\n  \"Bet\" = c(\"No\", \"Yes\"))  # column names\n\nSportsBet_table\n\n            Bet\nGroup         No Yes\n  College    175  94\n  NonCollege 137  77\n\n\n\n\n\n\nchisq.test(SportsBet_table) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      statistic\n      p.value\n      parameter\n      method\n    \n  \n  \n    0.01987511\n0.8878864\n1\nPearson's Chi-squared test with Yates' continuity correction\n  \n  \n  \n\n\n\nprop.test(SportsBet_table) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.6505576\n0.6401869\n0.01987511\n0.8878864\n1\n-0.07973918\n0.1004806\n2-sample test for equality of proportions with continuity correction\ntwo.sided\n  \n  \n  \n\n\n\n2*pnorm(sqrt(0.0199), lower.tail=F) # p-value\n\n[1] 0.8878167"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#test-of-homogeneity",
    "href": "slides_md/Day13_bsta511_md.html#test-of-homogeneity",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Test of Homogeneity",
    "text": "Test of Homogeneity\n\nRunning the sports betting example as a chi-squared test is actually an example of a test of homogeneity\nIn a test of homogeneity, proportions can be compared between many groups\n\n\\[\\begin{align}\nH_0:&~ p_1 = p_2 = p_2 = \\ldots = p_n\\\\\nH_A:&~ p_i \\neq p_j \\textrm{for at least one pair of } i, j\n\\end{align}\\]\n\nIt’s an extension of a two proportions test.\nThe test statistic & p-value are calculated the same was as a chi-squared test of association (independence)\nWhen we fix the margins (whether row or columns) of one of the “variables” (such as in a cohort or case-control study)\n\nthe chi-squared test is called a Test of Homogeneity"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#overview-of-tests-with-categorical-outcome",
    "href": "slides_md/Day13_bsta511_md.html#overview-of-tests-with-categorical-outcome",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Overview of tests with categorical outcome",
    "text": "Overview of tests with categorical outcome"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#chi-squared-tests-of-independence-vs.-homogeneity-vs.-goodness-of-fit",
    "href": "slides_md/Day13_bsta511_md.html#chi-squared-tests-of-independence-vs.-homogeneity-vs.-goodness-of-fit",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Chi-squared Tests of Independence vs. Homogeneity vs. Goodness-of-fit",
    "text": "Chi-squared Tests of Independence vs. Homogeneity vs. Goodness-of-fit\n\n\n\n\n\n\nSee YouTube video from TileStats for a good explanation of how these three tests are different: https://www.youtube.com/watch?v=TyD-_1JUhxw\nUCLA’s INSPIRE website has a good summary too: http://inspire.stat.ucla.edu/unit_13/"
  },
  {
    "objectID": "slides_md/Day13_bsta511_md.html#whats-next",
    "href": "slides_md/Day13_bsta511_md.html#whats-next",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "What’s next?",
    "text": "What’s next?"
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html",
    "href": "slides_code/Day16_bsta511_code.html",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) \nlibrary(here) \nlibrary(pwr) \nlibrary(gridExtra) # NEW!!!\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#dataset-description",
    "href": "slides_code/Day16_bsta511_code.html#dataset-description",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Dataset description",
    "text": "Dataset description\n\nData file: lifeexp_femlit_water_2011.csv\nData were downloaded from https://www.gapminder.org/data/\n2011 is the most recent year with the most complete data\nLife expectancy = the average number of years a newborn child would live if current mortality patterns were to stay the same. Source: https://www.gapminder.org/data/documentation/gd004/\nAdult literacy rate is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life. Source: http://data.uis.unesco.org/\nAt least basic water source (%) = the percentage of people using at least basic water services. This indicator encompasses both people using basic water services as well as those using safely managed water services. Basic drinking water services is defined as drinking water from an improved source, provided collection time is not more than 30 minutes for a round trip. Improved water sources include piped water, boreholes or tubewells, protect dug wells, protected springs, and packaged or delivered water."
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#get-to-know-the-data",
    "href": "slides_code/Day16_bsta511_code.html#get-to-know-the-data",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Get to know the data",
    "text": "Get to know the data\nLoad data\n\ngapm_original &lt;- read_csv(here::here(\"data\", \"lifeexp_femlit_water_2011.csv\"))\n\nGlimpse of the data\n\nglimpse(gapm_original)\n\nRows: 194\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andor…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 76.7, 82.6, 60.9, 76.9, 76.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, NA, NA, 58.6, 99.4, 97.9, 99.5,…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 92.6, 100.0, 40.3, 97.0, 99.5, …\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q2\", \"Q4\", \"Q1\", \"Q3\", \"Q4\", \"…\n\n\nNote the missing values for our variables of interest\n\ngapm_original %&gt;% select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  get_summary_stats()\n\n# A tibble: 2 × 13\n  variable        n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 life_expec…   187  47.5  82.9   72.7  64.3  76.9  12.6  9.04  70.7  8.44 0.617\n2 female_lit…    80  13    99.8   91.6  71.0  98.0  27.0 11.4   81.7 22.0  2.45 \n# ℹ 1 more variable: ci &lt;dbl&gt;"
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#remove-missing-values",
    "href": "slides_code/Day16_bsta511_code.html#remove-missing-values",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Remove missing values",
    "text": "Remove missing values\nRemove rows with missing data for life expectancy and female literacy rate\n\ngapm &lt;- gapm_original %&gt;% \n  drop_na(life_expectancy_years_2011, female_literacy_rate_2011)\n\nglimpse(gapm)\n\nRows: 80\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Angola\", \"Antigu…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 40.3, 97.0, 99.5, 97.8, 96.7, 9…\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q1\", \"Q3\", \"Q4\", \"Q3\", \"Q3\", \"…\n\n\nNo missing values now for our variables of interest\n\ngapm %&gt;% select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  get_summary_stats()\n\n# A tibble: 2 × 13\n  variable        n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 life_expec…    80    48  81.8   72.4  65.9  75.8  9.95  6.30  69.9  7.95 0.889\n2 female_lit…    80    13  99.8   91.6  71.0  98.0 27.0  11.4   81.7 22.0  2.45 \n# ℹ 1 more variable: ci &lt;dbl&gt;\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nRemoving the rows with missing data was not needed to run the regression model.\nI did this step since later we will be calculating the standard deviations of the explanatory and response variables for just the values included in the regression model. It’ll be easier to do this if we remove the missing values now."
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#regression-in-r-lm-summary-tidy",
    "href": "slides_code/Day16_bsta511_code.html#regression-in-r-lm-summary-tidy",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Regression in R: lm(), summary(), & tidy()",
    "text": "Regression in R: lm(), summary(), & tidy()\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\ntidy(model1) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\nRegression equation for our model:\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot \\textrm{female literacy rate} \\]"
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#residuals",
    "href": "slides_code/Day16_bsta511_code.html#residuals",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Residuals",
    "text": "Residuals\n\nObserved values \\(y_i\\)\n\nthe values in the dataset\n\nFitted values \\(\\widehat{y}_i\\)\n\nthe values that fall on the best-fit line for a specific \\(x_i\\)\n\nResiduals \\(e_i = y_i - \\widehat{y}_i\\)\n\nthe differences between the observed and fitted values"
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#the-population-regresison-model",
    "href": "slides_code/Day16_bsta511_code.html#the-population-regresison-model",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "The (population) regresison model",
    "text": "The (population) regresison model\n\nThe (population) regression model is denoted by\n\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\epsilon\\]\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown population parameters\n\\(\\epsilon\\) (epsilon) is the error about the line\n\nIt is assumed to be a random variable:\n\n\\(\\epsilon \\sim N(0, \\sigma^2)\\)\nvariance \\(\\sigma^2\\) is constant\n\n\n\nSee slides for image.\n\nThe line is the average (expected) value of \\(Y\\) given a value of \\(x\\): \\(E(Y|x)\\).\nThe point estimates for \\(\\beta_0\\) and \\(\\beta_1\\) based on a sample are denoted by \\(b_0, b_1, s_{residuals}^2\\)\n\nNote: also common notation is \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\sigma}^2\\)"
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#l-linearity-of-relationship-between-variables",
    "href": "slides_code/Day16_bsta511_code.html#l-linearity-of-relationship-between-variables",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "L: Linearity of relationship between variables",
    "text": "L: Linearity of relationship between variables\nIs the association between the variables linear?\n\nDiagnostic tools:\n\nScatterplot\nResidual plot (see later section for E : Equality of variance of the residuals)"
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#i-independence-of-the-residuals-y-values",
    "href": "slides_code/Day16_bsta511_code.html#i-independence-of-the-residuals-y-values",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "I: Independence of the residuals (\\(Y\\) values)",
    "text": "I: Independence of the residuals (\\(Y\\) values)\n\nAre the data points independent of each other?\nExamples of when they are not independent, include\n\nrepeated measures (such as baseline, 3 months, 6 months)\ndata from clusters, such as different hospitals or families\n\nThis condition is checked by reviewing the study design and not by inspecting the data\nHow to analyze data using regression models when the \\(Y\\)-values are not independent is covered in BSTA 519 (Longitudinal data)"
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#n-normality-of-the-residuals-1",
    "href": "slides_code/Day16_bsta511_code.html#n-normality-of-the-residuals-1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "N: Normality of the residuals",
    "text": "N: Normality of the residuals\n\nThe responses Y are normally distributed at each level of x\n\nSee slides for image."
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#extract-models-residuals-in-r",
    "href": "slides_code/Day16_bsta511_code.html#extract-models-residuals-in-r",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Extract model’s residuals in R",
    "text": "Extract model’s residuals in R\n\nFirst extract the residuals’ values from the model output using the augment() function from the broom package.\nGet a tibble with the orginal data, as well as the residuals and some other important values.\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011, \n                data = gapm)\naug1 &lt;- augment(model1) \n\nglimpse(aug1)\n\nRows: 80\nColumns: 8\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…"
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#check-normality-with-usual-distribution-plots",
    "href": "slides_code/Day16_bsta511_code.html#check-normality-with-usual-distribution-plots",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Check normality with “usual” distribution plots",
    "text": "Check normality with “usual” distribution plots\nNote that below I save each figure, and then combine them together in one row of output using grid.arrange() from the gridExtra package.\n\nhist1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_histogram()\n\ndensity1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_density()\n\nbox1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_boxplot()\n\nlibrary(gridExtra) # NEW!!!\ngrid.arrange(hist1, density1, box1, nrow = 1)"
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#normal-qq-plots-qq-quantile-quantile",
    "href": "slides_code/Day16_bsta511_code.html#normal-qq-plots-qq-quantile-quantile",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Normal QQ plots (QQ = quantile-quantile)",
    "text": "Normal QQ plots (QQ = quantile-quantile)\n\nIt can be tricky to eyeball with a histogram or density plot whether the residuals are normal or not\nQQ plots are often used to help with this\nVertical axis: data quantiles\n\ndata points are sorted in order and\nassigned quantiles based on how many data points there are\n\nHorizontal axis: theoretical quantiles\n\nmean and standard deviation (SD) calculated from the data points\ntheoretical quantiles are calculated for each point, assuming the data are modeled by a normal distribution with the mean and SD of the data\n\n\n\n\n\n\n\n\nData are approximately normal if points fall on a line.\n\nSee more info at https://data.library.virginia.edu/understanding-QQ-plots/\n\nExamples of Normal QQ plots\nSee slides for examples."
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#qq-plot-of-residuals-of-model1",
    "href": "slides_code/Day16_bsta511_code.html#qq-plot-of-residuals-of-model1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "QQ plot of residuals of model1",
    "text": "QQ plot of residuals of model1\n\ngrid.arrange(hist1, density1, box1, nrow = 1)\n\n\n\n\n\nggplot(aug1, aes(sample = .resid)) + \n  stat_qq() +     # points\n  stat_qq_line()  # line"
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#compare-to-randomly-generated-normal-qq-plots",
    "href": "slides_code/Day16_bsta511_code.html#compare-to-randomly-generated-normal-qq-plots",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Compare to randomly generated Normal QQ plots",
    "text": "Compare to randomly generated Normal QQ plots\nHow “good” we can expect a QQ plot to look depends on the sample size.\n\nThe QQ plots on the next slides are randomly generated\n\nusing random samples from actual standard normal distributions \\(N(0,1)\\).\n\nThus, all the points in the QQ plots should theoretically fall in a line\nHowever, there is sampling variability…\n\n\nRandomly generated Normal QQ plots: n=100\n\nNote that stat_qq_line() doesn’t work with randomly generated samples, and thus the code below manually creates the line that the points should be on (which is \\(y=x\\) in this case.)\n\n\nsamplesize &lt;- 100\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)\n\n\n\n\n\n\nExamples of simulated Normal QQ plots: n=10\nWith fewer data points,\n\nsimulated QQ plots are more likely to look “less normal”\neven though the data points were sampled from normal distributions.\n\n\nsamplesize &lt;- 10  # only change made to code!\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)\n\n\n\n\n\n\nExamples of simulated Normal QQ plots: n=1,000\nWith more data points,\n\nsimulated QQ plots are more likely to look “more normal”\n\n\nsamplesize &lt;- 1000 # only change made to code!\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#back-to-our-example",
    "href": "slides_code/Day16_bsta511_code.html#back-to-our-example",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Back to our example",
    "text": "Back to our example\nResiduals from Life Expectancy vs. Female Literacy Rate Regression\n\nggplot(aug1, \n      aes(sample = .resid)) + \n  stat_qq() + \n  stat_qq_line() \n\n\n\n\nSimulated QQ plot of Normal Residuals with n = 80\n\n# number of observations \n# in fitted model\nnobs(model1) \n\n[1] 80\n\nggplot() +\n  stat_qq(aes(\n    sample = rnorm(80))) + \n  geom_abline(\n    intercept = 0, slope = 1, \n    color = \"blue\")"
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#residual-plot",
    "href": "slides_code/Day16_bsta511_code.html#residual-plot",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Residual plot",
    "text": "Residual plot\n\n\\(x\\) = explanatory variable from regression model\n\n(or the fitted values for a multiple regression)\n\n\\(y\\) = residuals from regression model\n\n\nnames(aug1)\n\n[1] \"life_expectancy_years_2011\" \"female_literacy_rate_2011\" \n[3] \".fitted\"                    \".resid\"                    \n[5] \".hat\"                       \".sigma\"                    \n[7] \".cooksd\"                    \".std.resid\"                \n\nggplot(aug1, \n       aes(x = female_literacy_rate_2011, \n           y = .resid)) + \n  geom_point() +\n  geom_abline(\n    intercept = 0, \n    slope = 0, \n    color = \"orange\") +\n  labs(title = \"Residual plot\")"
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#e-equality-of-variance-of-the-residuals-homoscedasticity",
    "href": "slides_code/Day16_bsta511_code.html#e-equality-of-variance-of-the-residuals-homoscedasticity",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "E: Equality of variance of the residuals (Homoscedasticity)",
    "text": "E: Equality of variance of the residuals (Homoscedasticity)\n\nThe variance or, equivalently, the standard deviation of the responses is equal for all values of x.\nThis is called homoskedasticity (top row)\nIf there is heteroskedasticity (bottom row), then the assumption is not met.\n\nSee slides for image."
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#r2-coefficient-of-determination-12",
    "href": "slides_code/Day16_bsta511_code.html#r2-coefficient-of-determination-12",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "\\(R^2\\) = Coefficient of determination (1/2)",
    "text": "\\(R^2\\) = Coefficient of determination (1/2)\n\nRecall that the correlation coefficient \\(r\\) measures the strength of the linear relationship between two numerical variables\n\\(R^2\\) is usually used to measure the strength of a linear fit\n\nFor a simple linear regression model (one numerical predictor), \\(R^2\\) is just the square of the correlation coefficient\n\nIn general, \\(R^2\\) is the proportion of the variability of the dependent variable that is explained by the independent variable(s)\n\n\\[R^2 = \\frac{\\textrm{variance of predicted y-values}}\n{\\textrm{variance of observed y-values}} = \\frac{\\sum_{i=1}^n(\\widehat{y}_i-\\bar{y})^2}\n{\\sum_{i=1}^n(y_i-\\bar{y})^2}\n= \\frac{s_y^2 - s_{\\textrm{residuals}}^2}\n{s_y^2}\\] \\[R^2 = 1- \\frac{s_{\\textrm{residuals}}^2}\n{s_y^2}\\] where \\(\\frac{s_{\\textrm{residuals}}^2}{s_y^2}\\) is the proportion of “unexplained” variability in the \\(y\\) values,\nand thus \\(R^2 = 1- \\frac{s_{\\textrm{residuls}}^2}{s_y^2}\\) is the proportion of “explained” variability in the \\(y\\) values"
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#r2-coefficient-of-determination-22",
    "href": "slides_code/Day16_bsta511_code.html#r2-coefficient-of-determination-22",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "\\(R^2\\) = Coefficient of determination (2/2)",
    "text": "\\(R^2\\) = Coefficient of determination (2/2)\n\nRecall, \\(-1&lt;r&lt;1\\)\nThus, \\(0&lt;R^2&lt;1\\)\nIn practice, we want “high” \\(R^2\\) values, i.e. \\(R^2\\) as close to 1 as possible.\n\nCalculating \\(R^2\\) in R using glance() from the broom package:\n\nglance(model1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(model1)$r.squared\n\n[1] 0.4109366\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nA model can have a high \\(R^2\\) value when there is a curved pattern.\nAlways first check whether a linear model is reasonable or not."
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#r2-in-summary-r-output",
    "href": "slides_code/Day16_bsta511_code.html#r2-in-summary-r-output",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "\\(R^2\\) in summary() R output",
    "text": "\\(R^2\\) in summary() R output\n\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\n\nCompare to the square of the correlation coefficient \\(r\\):\n\nr &lt;- cor(x = gapm$life_expectancy_years_2011, \n    y = gapm$female_literacy_rate_2011,\n    use =  \"complete.obs\")\nr\n\n[1] 0.6410434\n\nr^2\n\n[1] 0.4109366"
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#inference-for-population-slope-beta_1",
    "href": "slides_code/Day16_bsta511_code.html#inference-for-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Inference for population slope \\(\\beta_1\\)",
    "text": "Inference for population slope \\(\\beta_1\\)\n\n# Fit regression model:\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)\n# Get regression table:\ntidy(model1, conf.int = TRUE) %&gt;% gt() # conf.int = TRUE part is new! \n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\n\\[\\begin{align}\n\\widehat{y} =& b_0 + b_1 \\cdot x\\\\\n\\widehat{\\text{life expectancy}} =& 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{align}\\]\n\nWhat are \\(H_0\\) and \\(H_A\\)?\nHow do we calculate the standard error, statistic, p-value, and CI?\n\n\n\n\n\n\n\nNote\n\n\n\n\nWe can also test & calculate CI for the population intercept\nThis will be covered in BSTA 512\n\n\n\n\nInference for the population slope: CI and hypothesis test\nPopulation model\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma)\\)\n\\(\\sigma\\) is the variability (SD) of the residuals\n\nSample best-fit (least-squares) line:\n\\[\\widehat{y} = b_0 + b_1 \\cdot x \\]\nNote: Some sources use \\(\\widehat{\\beta}\\) instead of \\(b\\).\n\nConstruct a 95% confidence interval for the population slope \\(\\beta_1\\)\n\n\n\nConduct the hypothesis test\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote: R reports p-values for 2-sided tests\n\n\nCI for population slope \\(\\beta_1\\)\nRecall the general CI formula:\n\\[\\textrm{Point Estimate} \\pm t^*\\cdot SE_{\\textrm{Point Estimate}}\\]\nFor the CI of the coefficient \\(b_1\\) this translates to\n\\[b_1 \\pm t^*\\cdot SE_{b_1}\\] where \\(t^*\\) is the critical value from a \\(t\\)-distribution with \\(df = n -2\\).\n\nHow is \\(\\text{SE}_{b_1}\\) calculated? See next slide.\n\ntidy(model1, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term                  estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)             50.9      2.66       19.1  3.33e-31   45.6      56.2  \n2 female_literacy_rate…    0.232    0.0315      7.38 1.50e-10    0.170     0.295\n\n\n\n\nStandard error of fitted slope \\(b_1\\)\n\\[\\text{SE}_{b_1} = \\frac{s_{\\textrm{residuals}}}{s_x\\sqrt{n-1}}\\]\n\\(\\text{SE}_{b_1}\\) is the variability of the statistic \\(b_1\\)\n\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\\(n\\) is the sample size, or the number of (complete) pairs of points\n\n\nglance(model1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# standard deviation of the residuals (Residual standard error in summary() output)\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n# standard deviation of x's\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] 21.95371\n\n# number of pairs of complete observations\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(se_b1 &lt;- s_resid/(s_x * sqrt(n-1))) # compare to SE in regression output\n\n[1] 0.03147744\n\n\n\n\nCalculate CI for population slope \\(\\beta_1\\)\n\\[b_1 \\pm t^*\\cdot SE_{b_1}\\]\nwhere \\(t^*\\) is the \\(t\\)-distribution critical value with \\(df = n -2\\).\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\nSave regression output for the row with the slope’s information:\n\nmodel1_b1 &lt;-tidy(model1) %&gt;% filter(term == \"female_literacy_rate_2011\")\nmodel1_b1 %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\nSave values needed for CI:\n\nb1 &lt;- model1_b1$estimate\nSE_b1 &lt;- model1_b1$std.error\n\nnobs(model1) # sample size n\n\n[1] 80\n\n(tstar &lt;- qt(.975, df = 80-2))\n\n[1] 1.990847\n\n\nCompare CI bounds below with the ones in the regression table above.\n\n(CI_LB &lt;- b1 - tstar*SE_b1)\n\n[1] 0.1695284\n\n(CI_UB &lt;- b1 + tstar*SE_b1)\n\n[1] 0.2948619\n\n\n\n\nHypothesis test for population slope \\(\\beta_1\\)\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\nThe test statistic for \\(b_1\\) is\n\\[t = \\frac{ b_1 - \\beta_1}{ \\text{SE}_{b_1}} = \\frac{ b_1}{ \\text{SE}_{b_1}}\\]\nwhen we assume \\(H_0: \\beta_1 = 0\\) is true.\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\nCalculate the test statistic using the values in the regression table:\n\n# recall model1_b1 is regression table restricted to b1 row\n(TestStat &lt;- model1_b1$estimate / model1_b1$std.error)\n\n[1] 7.376557\n\n\nCompare this test statistic value to the one from the regression table above\n\n\n\\(p\\)-value for testing population slope \\(\\beta_1\\)\n\nAs usual, the \\(p\\)-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\nTo calculate the \\(p\\)-value, we need to know the probability distribution of the test statistic (the null distribution) assuming \\(H_0\\) is true.\nStatistical theory tells us that the test statistic \\(t\\) can be modeled by a \\(t\\)-distribution with \\(df = n-2\\).\nRecall that this is a 2-sided test:\n\n\n(pv = 2*pt(TestStat, df=80-2, lower.tail=F))\n\n[1] 1.501286e-10\n\n\nCompare the \\(p\\)-value to the one from the regression table below\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()  # compare p-value calculated above to p-value in table\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619"
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#prediction-with-regression-line",
    "href": "slides_code/Day16_bsta511_code.html#prediction-with-regression-line",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction with regression line",
    "text": "Prediction with regression line\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot \\textrm{female literacy rate} \\]\nWhat is the predicted life expectancy for a country with female literacy rate 60%?\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot 60 = 64.82\\]\n\n(y_60 &lt;- 50.9 + 0.232*60)\n\n[1] 64.82\n\n\n\n\nHow do we interpret the predicted value?\nHow variable is it?\n\n\nPrediction with regression line\nRecall the population model:\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma)\\)\n\\(\\sigma\\) is the variability (SD) of the residuals\n\n\nWhen we take the expected value, at a given value \\(x^*\\), we have that the predicted response is the average expected response at \\(x^*\\):\n\n\\[\\widehat{E[Y|x^*]} = b_0 + b_1 x^*\\]\n\nggplot(gapm, aes(x = female_literacy_rate_2011,\n                 y = life_expectancy_years_2011)) +\n  geom_point() +\n  labs(x = \"female literacy rate\", \n       y = \"life expectancy\",\n       title = \"Life expectancy vs. female literacy rate\") +  \n  geom_smooth(method = \"lm\", se = TRUE) +\n  geom_vline(xintercept = 60, color = \"green3\")\n\n\n\n\n\nThese are the points on the regression line.\nThe mean responses has variability, and we can calculate a CI for it, for every value of \\(x^*\\).\n\n\n\nCI for mean response \\(\\mu_{Y|x^*}\\)\n\\[\\widehat{E[Y|x^*]} \\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\]\n\n\\(SE_{\\widehat{E[Y|x^*]}}\\) is calculated using\n\n\\[SE_{\\widehat{E[Y|x^*]}} = s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\]\n\n\\(\\widehat{E[Y|x^*]}\\) is the predicted value at the specified point \\(x^*\\) of the explanatory variable\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\\(n\\) is the sample size, or the number of (complete) pairs of points\n\\(\\bar{x}\\) is the sample mean of the explanatory variable \\(x\\)\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\n\n\nRecall that \\(t_{n-2}^*\\) is calculated using qt() and depends on the confidence level.\n\n\n\nExample: CI for mean response \\(\\mu_{Y|x^*}\\)\nFind the 95% CI for the mean life expectancy when the female literacy rate is 60.\n\\[\\begin{align}\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\\\\n64.8596 &\\pm 1.990847 \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 6.142157 \\sqrt{\\frac{1}{80} + \\frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 0.9675541\\\\\n64.8596 &\\pm 1.926252\\\\\n(62.93335 &, 66.78586)\n\\end{align}\\]\n\n(Y60 &lt;- 50.9278981 + 0.2321951 * 60)\n\n[1] 64.8596\n\n(tstar &lt;- qt(.975, df = 78))\n\n[1] 1.990847\n\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(mx &lt;- mean(gapm$female_literacy_rate_2011))\n\n[1] 81.65375\n\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] 21.95371\n\n(SE_Yx &lt;- s_resid *sqrt(1/n + (60 - mx)^2/((n-1)*s_x^2)))\n\n[1] 0.9675541\n\n(MOE_Yx &lt;- SE_Yx*tstar)\n\n[1] 1.926252\n\nY60 - MOE_Yx\n\n[1] 62.93335\n\nY60 + MOE_Yx\n\n[1] 66.78586\n\n\n\n\nExample: Using R for CI for mean response \\(\\mu_{Y|x^*}\\)\nFind the 95% CI’s for the mean life expectancy when the female literacy rate is 40, 60, and 80.\n\nUse the base R predict() function\nRequires specification of a newdata “value”\n\nThe newdata value is \\(x^*\\)\nThis has to be in the format of a data frame though\nwith column name identical to the predictor variable in the model\n\n\n\nnewdata &lt;- data.frame(female_literacy_rate_2011 = c(40, 60, 80)) \nnewdata\n\n  female_literacy_rate_2011\n1                        40\n2                        60\n3                        80\n\npredict(model1, \n        newdata=newdata, \n        interval=\"confidence\")\n\n       fit      lwr      upr\n1 60.21570 57.26905 63.16236\n2 64.85961 62.93335 66.78586\n3 69.50351 68.13244 70.87457\n\n\n\n\nInterpretation\nWe are 95% confident that the average life expectancy for a country with a 60% female literacy rate will be between 62.9 and 66.8 years.\n\n\nConfidence bands for mean response \\(\\mu_{Y|x^*}\\)\n\nOften we plot the CI for many values of X, creating confidence bands\nThe confidence bands are what ggplot creates when we set se = TRUE within geom_smooth\nFor what values of x are the confidence bands (intervals) narrowest?\n\n\nggplot(gapm,\n       aes(x=female_literacy_rate_2011, \n           y=life_expectancy_years_2011)) +\n  geom_point()+\n  geom_smooth(method = lm, se=TRUE)+\n  ggtitle(\"Life expectancy vs. female literacy rate\") \n\n\n\n\n\n\nWidth of confidence bands for mean response \\(\\mu_{Y|x^*}\\)\n\nFor what values of \\(x^*\\) are the confidence bands (intervals) narrowest? widest?\n\n\\[\\begin{align}\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\\\\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\n\\end{align}\\]"
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#prediction-interval-for-predicting-individual-observations",
    "href": "slides_code/Day16_bsta511_code.html#prediction-interval-for-predicting-individual-observations",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction interval for predicting individual observations",
    "text": "Prediction interval for predicting individual observations\n\nWe do not call this interval a CI since \\(Y\\) is a random variable instead of a parameter\nThe form is similar to a CI though:\n\n\\[\\widehat{Y|x^*} \\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\]\n\nNote that the only difference to the CI for a mean value of y is the additional 1+ under the square root.\n\nThus the width is wider!\n\n\n\nExample: Prediction interval\nFind the 95% prediction interval for the life expectancy when the female literacy rate is 60.\n\\[\\begin{align}\n\\widehat{Y|x^*} &\\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 6.142157 \\sqrt{1+\\frac{1}{80} + \\frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\\\\n(52.48072 &, 77.23849)\n\\end{align}\\]\n\n(Y60 &lt;- 50.9278981 + 0.2321951 * 60)\n\n[1] 64.8596\n\n(tstar &lt;- qt(.975, df = 78))\n\n[1] 1.990847\n\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(mx &lt;- mean(gapm$female_literacy_rate_2011))\n\n[1] 81.65375\n\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] 21.95371\n\n(SE_Ypred &lt;- s_resid *sqrt(1 + 1/n + (60 - mx)^2/((n-1)*s_x^2)))\n\n[1] 6.217898\n\n(MOE_Ypred &lt;- SE_Ypred*tstar)\n\n[1] 12.37888\n\nY60 - MOE_Ypred\n\n[1] 52.48072\n\nY60 + MOE_Ypred\n\n[1] 77.23849\n\n\n\n\nExample: Using R for prediction interval\nFind the 95% prediction intervals for the life expectancy when the female literacy rate is 40, 60, and 80.\n\nnewdata  # previously defined for CI's\n\n  female_literacy_rate_2011\n1                        40\n2                        60\n3                        80\n\npredict(model1, \n        newdata=newdata, \n        interval=\"prediction\")  # prediction instead of \"confidence\"\n\n       fit      lwr      upr\n1 60.21570 47.63758 72.79382\n2 64.85961 52.48072 77.23849\n3 69.50351 57.19879 81.80823\n\n\n\n\nInterpretation\nWe are 95% confident that a new selected country with a 60% female literacy rate will have a life expectancy between 52.5 and 77.2 years."
  },
  {
    "objectID": "slides_code/Day16_bsta511_code.html#prediction-bands-vs.-confidence-bands-12",
    "href": "slides_code/Day16_bsta511_code.html#prediction-bands-vs.-confidence-bands-12",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction bands vs. confidence bands (1/2)",
    "text": "Prediction bands vs. confidence bands (1/2)\nCreate a scatterplot with the regression line, 95% confidence bands, and 95% prediction bands.\n\nFirst create a data frame with the original data points (both x and y values), their respective predicted values, andtheir respective prediction intervals\nCan do this with augment() from the broom package.\n\n\nmodel1_pred_bands &lt;- augment(model1, interval = \"prediction\")\n\n# take a look at new object:\nnames(model1_pred_bands) \n\n [1] \"life_expectancy_years_2011\" \"female_literacy_rate_2011\" \n [3] \".fitted\"                    \".lower\"                    \n [5] \".upper\"                     \".resid\"                    \n [7] \".hat\"                       \".sigma\"                    \n [9] \".cooksd\"                    \".std.resid\"                \n\n# glimpse of select variables of interest:\nmodel1_pred_bands %&gt;% \n  select(life_expectancy_years_2011, female_literacy_rate_2011, \n         .fitted:.upper) %&gt;% \n  glimpse()\n\nRows: 80\nColumns: 5\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .lower                     &lt;dbl&gt; 40.91166, 60.81324, 52.14572, 61.65365, 61.…\n$ .upper                     &lt;dbl&gt; 66.98121, 85.48470, 76.92334, 86.36253, 86.…\n\n\n\nPrediction bands vs. confidence bands (2/2)\n\nnames(model1_pred_bands) \n\n [1] \"life_expectancy_years_2011\" \"female_literacy_rate_2011\" \n [3] \".fitted\"                    \".lower\"                    \n [5] \".upper\"                     \".resid\"                    \n [7] \".hat\"                       \".sigma\"                    \n [9] \".cooksd\"                    \".std.resid\"                \n\n\n\nggplot(model1_pred_bands, \n       aes(x=female_literacy_rate_2011, y=life_expectancy_years_2011)) +\n  geom_point() +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), # prediction bands\n              alpha = 0.2, fill = \"red\") +\n  geom_smooth(method=lm) +  # confidence bands\n  labs(title = \"SLR with Confidence & Prediction Bands\")"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html",
    "href": "slides_code/Day17_bsta511_code.html",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) \nlibrary(here) \nlibrary(pwr) \nlibrary(BSDA)  # NEW package!!!\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#load-packages",
    "href": "slides_code/Day17_bsta511_code.html#load-packages",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) \nlibrary(here) \nlibrary(pwr) \nlibrary(BSDA)  # NEW package!!!\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#additional-resource",
    "href": "slides_code/Day17_bsta511_code.html#additional-resource",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Additional resource",
    "text": "Additional resource\n\nChapter 13: Nonparametric tests of Pagano’s Principles of Biostatistics, 2022 edition\nCan download chapter from OHSU library eBook at https://ebookcentral.proquest.com/lib/ohsu/detail.action?docID=6950388&pq-origsite=primo"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#background-parametric-vs-nonparametric",
    "href": "slides_code/Day17_bsta511_code.html#background-parametric-vs-nonparametric",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Background: parametric vs nonparametric",
    "text": "Background: parametric vs nonparametric\n\nPrior inference of means methods had conditions assuming the underlying population(s) was (were) normal (or at least approximately normal).\nNormal distribution is completely described (parameterized) by two parameters: \\(\\mu\\) and \\(\\sigma\\).\nThe first was often the parameter of interest, while the latter was assumed known ( \\(Z\\)-test) or estimated ( \\(t\\)-tests).\nThe above are therefore described as parametric procedures.\nNonparametric procedures\n\nMake fewer assumptions about the structure of the underlying population from which the samples were collected.\nWork well when distributional assumptions are in doubt.\n\n\n\nThe good and the bad about nonparametric tests\nGood\n\nFewer assumptions\nTests are based on ranks\n\nTherefore outliers have no greater influence than non-outliers.\nSince tests are based on ranks we can apply these procedures to ordinal data\n\n(where means and standard deviations are not meaningful).\n\n\n\nDrawbacks\n\nLess powerful than parametric tests (due to loss of information when data are converted to ranks)\nWhile the test is easy, it may require substantial (computer) work to develop a confidence interval [by “inverting” the test].\nTheory was developed for continuous data (where ties are not possible); if population or data contain many ties, then a correction to the procedures must be implemented.\nSome procedures have “large” and “small” sample versions; the small sample versions require special tables or a computer."
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#example-intraocular-pressure-of-glaucoma-patients",
    "href": "slides_code/Day17_bsta511_code.html#example-intraocular-pressure-of-glaucoma-patients",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Example: Intraocular pressure of glaucoma patients",
    "text": "Example: Intraocular pressure of glaucoma patients\n\nIntraocular pressure of glaucoma patients is often reduced by treatment with adrenaline.\nA new synthetic drug is being considered, but it is more expensive than the current adrenaline alternative.\n7 glaucoma patients were treated with both drugs:\n\none eye with adrenaline and\nthe other with the synthetic drug\n\nReduction in pressure was recorded in each eye after following treatment (larger numbers indicate greater reduction)\n\n\nIOP_table &lt;- tibble(\n  Patient = 1:7,\n  Adren = c(3.5, 2.6, 3, 1.9, 2.9, 2.4, 2),\n  Synth = c(3.2, 3.1, 3.3, 2.4, 2.9, 2.8, 2.6)\n  ) %&gt;% \n  mutate(\n    d = Synth - Adren,\n    Sign = case_when(\n      d &lt; 0 ~ \"-\",\n      d &gt; 0 ~ \"+\"\n      )\n    )\nIOP_table %&gt;% gt()\n\n\n\n\n\n  \n    \n      Patient\n      Adren\n      Synth\n      d\n      Sign\n    \n  \n  \n    1\n3.5\n3.2\n-0.3\n-\n    2\n2.6\n3.1\n0.5\n+\n    3\n3.0\n3.3\n0.3\n+\n    4\n1.9\n2.4\n0.5\n+\n    5\n2.9\n2.9\n0.0\nNA\n    6\n2.4\n2.8\n0.4\n+\n    7\n2.0\n2.6\n0.6\n+\n  \n  \n  \n\n\n\n\n\nd is the difference in reduction of pressure: Synth - Adren\nSign is + if the difference is positive and - if the difference is negative\n\n\nVisualize the differences\nVisualize the differences in reduction of pressure \\(d\\) : Synth - Adren\n\nggplot(IOP_table, aes(x = d)) +\n  geom_dotplot()\n\n\n\n\n\nggplot(IOP_table, aes(x = d)) +\n  geom_density()"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#hypotheses-statistic-sign-test",
    "href": "slides_code/Day17_bsta511_code.html#hypotheses-statistic-sign-test",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Hypotheses & “statistic” (Sign test)",
    "text": "Hypotheses & “statistic” (Sign test)\nHypotheses\n\\(H_0:\\) The median difference in the population is 0\n\\(H_a:\\) The median difference in the population is NOT 0\n\n“Statistic”\n\\(D^+\\) = number of positive differences\n\\(D^-\\) = number of negative differences\n\nWhat are \\(D^+\\) and \\(D^-\\) for our example?\n\nIOP_table %&gt;% gt()\n\n\n\n\n\n  \n    \n      Patient\n      Adren\n      Synth\n      d\n      Sign\n    \n  \n  \n    1\n3.5\n3.2\n-0.3\n-\n    2\n2.6\n3.1\n0.5\n+\n    3\n3.0\n3.3\n0.3\n+\n    4\n1.9\n2.4\n0.5\n+\n    5\n2.9\n2.9\n0.0\nNA\n    6\n2.4\n2.8\n0.4\n+\n    7\n2.0\n2.6\n0.6\n+"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#exact-p-value-sign-test",
    "href": "slides_code/Day17_bsta511_code.html#exact-p-value-sign-test",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Exact p-value (Sign test)",
    "text": "Exact p-value (Sign test)\n\nIf the median difference is 0 ( \\(H_0\\) is true) , then * half the population consists of positive differences * while half have negative differences.\nLet \\(p=P(\\textrm{neg. diff.})=P(\\textrm{pos. diff.})= 0.5\\)\n\n\n\nIf the median difference is 0 ( \\(H_0\\) is true), * then a sample of \\(n\\) differences * behaves like \\(n\\) trials in a binomial experiment * where “success” is analogous to seeing a positive difference. * By symmetry ( \\(p=0.5\\) ), the same distribution applies to negative differences, i.e.,\n\n\\[D^+ \\textrm{ and } D^- \\sim \\textrm{Bin}(n,p=0.5)\\]\n\n\nThus the (exact) p-value is calculated using the Binomial distribution"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#glaucoma-example-exact-p-value",
    "href": "slides_code/Day17_bsta511_code.html#glaucoma-example-exact-p-value",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Glaucoma example (exact) p-value",
    "text": "Glaucoma example (exact) p-value\n\n7 differences:\n\n1 negative ( \\(D^-\\) )\n5 were positive ( \\(D^+\\) )\n1 difference is 0 and is discarded\n\nThus the effective sample size is \\(n=6\\).\n\n\nOne-sided p-value = probability that we would see 1 or fewer negative signs among the \\(n=6\\) differences, if the median difference is really 0\n\nTwo-sided p-value = 2 \\(\\times\\) One-sided p-value\n\n# 2-sided p-value: 2*P(D^- &lt;= 1)\n2*pbinom(1, size = 6, p = 0.5)\n\n[1] 0.21875\n\n\n\\[D^- \\sim \\textrm{Bin}(n=6,p=0.5)\\]\n\n\n\\[p-value = P(D^- \\leq 1) \\\\\n= P(D^-=0) + P(D^-=1) \\\\\n= \\frac{6!}{0!6!} (0.5)^6 + \\frac{6!}{1!5!} (0.5)^6 \\\\\n\\approx 0.1094\\]\n\n\\[p-value \\times 2 \\approx 0.2188\\]"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#sign-test-in-r-glaucoma-example",
    "href": "slides_code/Day17_bsta511_code.html#sign-test-in-r-glaucoma-example",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Sign test in R: Glaucoma example",
    "text": "Sign test in R: Glaucoma example\nBelow we create the dataset as a tibble (and add the signs):\n\nIOP &lt;- tibble(\n  Patient = 1:7,\n  Adren = c(3.5, 2.6, 3, 1.9, 2.9, 2.4, 2),\n  Synth = c(3.2, 3.1, 3.3, 2.4, 2.9, 2.8, 2.6)\n  ) %&gt;% \n  mutate(d = Synth - Adren,\n    Sign = case_when(\n      d &lt; 0 ~ \"-\",\n      d &gt; 0 ~ \"+\"))\n\nIOP %&gt;% gt()\n\n\n\n\n\n  \n    \n      Patient\n      Adren\n      Synth\n      d\n      Sign\n    \n  \n  \n    1\n3.5\n3.2\n-0.3\n-\n    2\n2.6\n3.1\n0.5\n+\n    3\n3.0\n3.3\n0.3\n+\n    4\n1.9\n2.4\n0.5\n+\n    5\n2.9\n2.9\n0.0\nNA\n    6\n2.4\n2.8\n0.4\n+\n    7\n2.0\n2.6\n0.6\n+\n  \n  \n  \n\n\n\n\nRecall we’re testing the population median.\nHere’s the sample median:\n\nmedian(IOP$d)\n\n[1] 0.4"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#sign-test-in-r-glaucoma-example-specifying-both-columns",
    "href": "slides_code/Day17_bsta511_code.html#sign-test-in-r-glaucoma-example-specifying-both-columns",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Sign test in R: Glaucoma example (specifying both columns)",
    "text": "Sign test in R: Glaucoma example (specifying both columns)\n\nlibrary(BSDA)  # new package!! Make sure to first install it\n# Can't \"tidy\" the output\nSIGN.test(x = IOP$Synth, y = IOP$Adren, alternative = \"two.sided\", conf.level = 0.95)\n\n\n    Dependent-samples Sign-Test\n\ndata:  IOP$Synth and IOP$Adren\nS = 5, p-value = 0.2187\nalternative hypothesis: true median difference is not equal to 0\n95 percent confidence interval:\n -0.2057143  0.5685714\nsample estimates:\nmedian of x-y \n          0.4 \n\nAchieved and Interpolated Confidence Intervals: \n\n                  Conf.Level  L.E.pt U.E.pt\nLower Achieved CI     0.8750  0.0000 0.5000\nInterpolated CI       0.9500 -0.2057 0.5686\nUpper Achieved CI     0.9844 -0.3000 0.6000\n\n\n\nSign test in R: Glaucoma example (specifying differences)\n\n# Note output calls this a \"One-sample Sign-Test\"\nSIGN.test(x = IOP$d, alternative = \"two.sided\", conf.level = 0.95)\n\n\n    One-sample Sign-Test\n\ndata:  IOP$d\ns = 5, p-value = 0.2187\nalternative hypothesis: true median is not equal to 0\n95 percent confidence interval:\n -0.2057143  0.5685714\nsample estimates:\nmedian of x \n        0.4 \n\nAchieved and Interpolated Confidence Intervals: \n\n                  Conf.Level  L.E.pt U.E.pt\nLower Achieved CI     0.8750  0.0000 0.5000\nInterpolated CI       0.9500 -0.2057 0.5686\nUpper Achieved CI     0.9844 -0.3000 0.6000"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#conclusion",
    "href": "slides_code/Day17_bsta511_code.html#conclusion",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Conclusion",
    "text": "Conclusion\nRecall the hypotheses to the sign test:\n\\(H_0:\\) The median population difference in reduction of intraocular pressure in treatment with adrenaline vs. new synthetic drug is 0.\n\\(H_a:\\) The median population difference in reduction of intraocular pressure in treatment with adrenaline vs. new synthetic drug is NOT 0.\n\nSignificance level: \\(\\alpha\\) = 0.05\np-value = 0.2188\n\nConclusion:\nThe median difference in reduction of intraocular pressure between eyes being treated with the synthetic drug and adrenaline for seven glaucoma patients was 0.4 (95% CI: -0.2, 0.6).\nThere is insufficient evidence the reduction in intraocular pressure differs when using the synthetic drug and adrenaline (2-sided sign test \\(p\\)-value = 0.219)."
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#sign-test-with-large-samples-p-value-normal-approximation",
    "href": "slides_code/Day17_bsta511_code.html#sign-test-with-large-samples-p-value-normal-approximation",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Sign test with large samples: p-value normal approximation",
    "text": "Sign test with large samples: p-value normal approximation\n\nIf the sample size is large, say greater than 20,\n\nthen binomial probabilities can be approximated using normal probabilities\n\nNormal approximation:\n\n\\[\\mu = np =n(0.5)=n/2\\\\\n\\sigma=\\sqrt{np(1-p)}=\\sqrt{n(0.5)(0.5)}= \\sqrt{n}/2\\]\n\nThus we have the test statistic:\n\n\\[z=\\frac{D^- −n/2}{\\sqrt{n}/2}\\]\n\nWith access to a computer, it’s better to use the exact binomial probabilities instead of the normal approximation."
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#sign-test-with-one-sample",
    "href": "slides_code/Day17_bsta511_code.html#sign-test-with-one-sample",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Sign test with one sample",
    "text": "Sign test with one sample\n\nOne can use the sign test when testing just one sample.\nNote that we did this when in R, when running the sign test using just the differences.\nFor one sample, we can specify the null population median value:\n\n\\(H_0:\\) The population median is \\(m\\)\n\\(H_a:\\) The population median is NOT \\(m\\)\nExample: Run sign test for paired data with null \\(m = 0.7\\):\n\nSIGN.test(x = IOP$d, md = 0.7, alternative = \"two.sided\", conf.level = 0.95)\n\n\n    One-sample Sign-Test\n\ndata:  IOP$d\ns = 0, p-value = 0.01563\nalternative hypothesis: true median is not equal to 0.7\n95 percent confidence interval:\n -0.2057143  0.5685714\nsample estimates:\nmedian of x \n        0.4 \n\nAchieved and Interpolated Confidence Intervals: \n\n                  Conf.Level  L.E.pt U.E.pt\nLower Achieved CI     0.8750  0.0000 0.5000\nInterpolated CI       0.9500 -0.2057 0.5686\nUpper Achieved CI     0.9844 -0.3000 0.6000"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#wilcoxon-signed-rank-test-1",
    "href": "slides_code/Day17_bsta511_code.html#wilcoxon-signed-rank-test-1",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "(Wilcoxon) Signed-rank test",
    "text": "(Wilcoxon) Signed-rank test\n\nLike the sign test, the (Wilcoxon) signed-rank test is used for\n\npaired samples (i.e., a single set of differences) or\na one-sample comparison against a specified value\n\nHowever, this test does make use of information concerning the size of the differences.\n\n\nHypotheses\n\\(H_0:\\) the population is symmetric around some value \\(\\tilde{\\mu}_0\\)\n\\(H_a:\\) the population is not symmetric around some value \\(\\tilde{\\mu}_0\\)\n\nEven if the population has a mean/median equal to \\(\\tilde{\\mu}_0\\), the test may reject the null if the population isn’t symmetric, thus only use if the data (differences) are symmetric.\nIf the population is symmetric\n\nthen the mean and median coincide,\nthus often the null hypothesis is phrased in terms of the median difference being 0"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#example-calculate-signed-ranks",
    "href": "slides_code/Day17_bsta511_code.html#example-calculate-signed-ranks",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Example: calculate signed ranks",
    "text": "Example: calculate signed ranks\n\nRank the absolute values of the differences from smallest to largest\nFor ties, take the average of the highest and lowest tied ranks\n\nI.e. if ranks 3-7 are tied, then assign (3+7)/2 = 5 as the rank\n\nThen calculate the signed ranks as +/- the rank depending on whether the sign is +/-\n\n\nIOP_ranks &lt;- IOP %&gt;% \n  mutate(\n    Rank = c(1.5, 4.5, 1.5, 4.5, NA, 3, 6),\n    Signed_rank = case_when(\n      d &lt; 0 ~ -Rank,\n      d &gt; 0 ~ Rank\n      )\n    )\nIOP_ranks %&gt;% gt()\n\n\n\n\n\n  \n    \n      Patient\n      Adren\n      Synth\n      d\n      Sign\n      Rank\n      Signed_rank\n    \n  \n  \n    1\n3.5\n3.2\n-0.3\n-\n1.5\n-1.5\n    2\n2.6\n3.1\n0.5\n+\n4.5\n4.5\n    3\n3.0\n3.3\n0.3\n+\n1.5\n1.5\n    4\n1.9\n2.4\n0.5\n+\n4.5\n4.5\n    5\n2.9\n2.9\n0.0\nNA\nNA\nNA\n    6\n2.4\n2.8\n0.4\n+\n3.0\n3.0\n    7\n2.0\n2.6\n0.6\n+\n6.0\n6.0\n  \n  \n  \n\n\n\n\n\nNew: Calculate ranks using R’s rank() function\n\nBelow I create the ranks using R’s rank() function that has an option ties.method to specify how to calculate ties.\nThis doesn’t require first sorting the data.\nHowever, it includes a difference of 0 in the ranking, and thus below I first remove the row with \\(d=0\\) from the data.\nIn the output below, I called this column Rank_R.\n\n\nIOP_Ranks_R &lt;- IOP_ranks %&gt;% \n  filter(d != 0) %&gt;% \n  mutate(\n    # create column with absolute values of differences\n    # it's the absolute values that get ranked\n    abs_d = abs(d), \n    # create column with ranks that accounts for ties using R's rank() function\n    # you don't need to first sort the data to use this command\n    Rank_R = rank(abs_d, ties.method = \"average\"),\n    Signed_Rank_R = case_when(\n      d &lt; 0 ~ -Rank_R,\n      d &gt; 0 ~ Rank_R\n      )\n    ) \n\nIOP_Ranks_R %&gt;% gt()\n\n\n\n\n\n  \n    \n      Patient\n      Adren\n      Synth\n      d\n      Sign\n      Rank\n      Signed_rank\n      abs_d\n      Rank_R\n      Signed_Rank_R\n    \n  \n  \n    1\n3.5\n3.2\n-0.3\n-\n1.5\n-1.5\n0.3\n1.5\n-1.5\n    2\n2.6\n3.1\n0.5\n+\n4.5\n4.5\n0.5\n4.5\n4.5\n    3\n3.0\n3.3\n0.3\n+\n1.5\n1.5\n0.3\n1.5\n1.5\n    4\n1.9\n2.4\n0.5\n+\n4.5\n4.5\n0.5\n4.5\n4.5\n    6\n2.4\n2.8\n0.4\n+\n3.0\n3.0\n0.4\n3.0\n3.0\n    7\n2.0\n2.6\n0.6\n+\n6.0\n6.0\n0.6\n6.0\n6.0"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#test-statistic-wilcoxon-signed-rank-test",
    "href": "slides_code/Day17_bsta511_code.html#test-statistic-wilcoxon-signed-rank-test",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Test statistic (Wilcoxon) Signed-rank test",
    "text": "Test statistic (Wilcoxon) Signed-rank test\nIf the null is true:\n\nThe population is symmetric around some point ( \\(\\tilde{\\mu}_0 = 0\\) , typically), and\nThe overall size of the positive ranks should be about the same as the overall size of negative ranks.\n\nNote:\n\nThe sum of the ranks \\(1,2,\\dotsc, n\\) is always \\(n(n+1)/2\\),\nwhich can be broken down as the\n\nsum of the positive ranks ( \\(T^+\\) )\nplus the sum of the negative ranks ( \\(T^-\\) )\n\n\nThus, any of the following can be used as a test statistic and will lead to the same conclusion:\n\n\\(T^+\\)\n\\(T^-\\)\n\\(T^+\\) - \\(T^-\\)\n\\(\\min(T^+,T^−) = T_0\\)"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#example-calculate-sums-of-signed-ranks",
    "href": "slides_code/Day17_bsta511_code.html#example-calculate-sums-of-signed-ranks",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Example: calculate sums of signed ranks",
    "text": "Example: calculate sums of signed ranks\n\nIOP_ranks %&gt;% gt()\n\n\n\n\n\n  \n    \n      Patient\n      Adren\n      Synth\n      d\n      Sign\n      Rank\n      Signed_rank\n    \n  \n  \n    1\n3.5\n3.2\n-0.3\n-\n1.5\n-1.5\n    2\n2.6\n3.1\n0.5\n+\n4.5\n4.5\n    3\n3.0\n3.3\n0.3\n+\n1.5\n1.5\n    4\n1.9\n2.4\n0.5\n+\n4.5\n4.5\n    5\n2.9\n2.9\n0.0\nNA\nNA\nNA\n    6\n2.4\n2.8\n0.4\n+\n3.0\n3.0\n    7\n2.0\n2.6\n0.6\n+\n6.0\n6.0\n  \n  \n  \n\n\n\n\n\nSum of the positive ranks\n\n\\(T^+\\) = 1.5 + 3 + 4.5 + 4.5 + 6 = 19.5\n\nSum of the negative ranks\n\n\\(T^-\\) = -1.5\n\nThe sum of the ranks \\(1,2,\\dotsc, n\\) is always \\(n(n+1)/2\\):\n\n\\(n(n+1)/2 = 6(7)/2 = 21\\)\n\\(T^+ + |T^-| = 19.5 + |-1.5| = 21\\)"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#exact-p-value-wilcoxon-signed-rank-test-fyi-12",
    "href": "slides_code/Day17_bsta511_code.html#exact-p-value-wilcoxon-signed-rank-test-fyi-12",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Exact p-value (Wilcoxon) Signed-rank test (fyi) (1/2)",
    "text": "Exact p-value (Wilcoxon) Signed-rank test (fyi) (1/2)\n\nExact p-value is preferable\n\nThis is the default method in R’s wilcox.test()\n\nif the samples contain less than 50 finite values\nand there are no ties\n\nR will automatically use normal approximation method if there are ties\n\n\n\nWe will not be calculating the exact p-value “by hand.” We will be using R for this.\n\n\n\n\\[p-value = 2 * P(\\min(T^+,T^−) \\leq t)\\]\n\n\\(t\\) is the smaller of the calculated sums of the positive and negative ranks\nTo calculate the exact p-value, we need the probability of each possible sum of ranks.\n\n\nExact p-value (Wilcoxon) Signed-rank test (fyi) (2/2)\n\nTo calculate the exact p-value, we need the probability of each possible sum of ranks:\n\nlist all possible combinations of positive and negative ranks for the sample size,\ncalculate the sum of the positive ranks ( \\(T^+\\) ) for each possible combination (or \\(T^-\\) ), and\nthen figure out the probability of each possible \\(T^+\\) (assuming all combinations are equally likely)\n\n\nExample when \\(n=3\\) : (from https://online.stat.psu.edu/stat415/lesson/20/20.2)\n[See slides for images with example]\nSee https://online.stat.psu.edu/stat415/lesson/20/20.2 for more details."
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#normal-approximation-p-value-wilcoxon-signed-rank-test-fyi",
    "href": "slides_code/Day17_bsta511_code.html#normal-approximation-p-value-wilcoxon-signed-rank-test-fyi",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Normal approximation p-value (Wilcoxon) Signed-rank test (fyi)",
    "text": "Normal approximation p-value (Wilcoxon) Signed-rank test (fyi)\n\nNormal approximation method:\n\nIf the number of non-zero differences is at least 16, then a normal approximation can be used.\nHave the option to apply a continuity correct (default) or not\n\nWe will not be calculating the p-value “by hand.” We will be using R for this.\n\n\nTest statistic:\n\\[Z_{T_{min}} = \\frac{T_{min} - \\frac{n(n+1)}{4}}{\\sqrt{\\frac{n(n+1)(2n+1)}{24}}}\\]\n\n\\(T_{min} = \\min(T^+,T^−)\\)\n\\(n\\) = sample size\nTest statistic \\(Z_{T_{min}}\\) follows a standard normal distribution \\(N(0,1)\\)\nUse normal distribution to calculate p-value\n\nSee https://online.stat.psu.edu/stat415/lesson/20/20.2 for more details."
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#wilcoxon-signed-rank-test-in-r-glaucoma-example",
    "href": "slides_code/Day17_bsta511_code.html#wilcoxon-signed-rank-test-in-r-glaucoma-example",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "(Wilcoxon) Signed-rank test in R: Glaucoma example",
    "text": "(Wilcoxon) Signed-rank test in R: Glaucoma example\n“Attempt” with exact p-value & specifying columns for paired data\n\n# Exact p-value\nwilcox.test(x = IOP$Synth, y = IOP$Adren, paired = TRUE, \n    alternative = c(\"two.sided\"), mu = 0, \n    exact = TRUE)\n\nWarning in wilcox.test.default(x = IOP$Synth, y = IOP$Adren, paired = TRUE, :\ncannot compute exact p-value with ties\n\n\nWarning in wilcox.test.default(x = IOP$Synth, y = IOP$Adren, paired = TRUE, :\ncannot compute exact p-value with zeroes\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  IOP$Synth and IOP$Adren\nV = 19.5, p-value = 0.07314\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#wilcoxon-signed-rank-test-in-r-glaucoma-example-1",
    "href": "slides_code/Day17_bsta511_code.html#wilcoxon-signed-rank-test-in-r-glaucoma-example-1",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "(Wilcoxon) Signed-rank test in R: Glaucoma example",
    "text": "(Wilcoxon) Signed-rank test in R: Glaucoma example\n“Attempt” with exact p-value & running one sample test with differences\n\n# Exact p-value\nwilcox.test(x = IOP$d, \n        alternative = c(\"two.sided\"), mu = 0, \n        exact = TRUE, correct = TRUE)\n\nWarning in wilcox.test.default(x = IOP$d, alternative = c(\"two.sided\"), :\ncannot compute exact p-value with ties\n\n\nWarning in wilcox.test.default(x = IOP$d, alternative = c(\"two.sided\"), :\ncannot compute exact p-value with zeroes\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  IOP$d\nV = 19.5, p-value = 0.07314\nalternative hypothesis: true location is not equal to 0"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#wilcoxon-signed-rank-test-in-r-glaucoma-example-2",
    "href": "slides_code/Day17_bsta511_code.html#wilcoxon-signed-rank-test-in-r-glaucoma-example-2",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "(Wilcoxon) Signed-rank test in R: Glaucoma example",
    "text": "(Wilcoxon) Signed-rank test in R: Glaucoma example\n“Attempt” with approximate p-value & specifying columns for paired data\n\n# Normal approximation with continuity correction\nwilcox.test(x = IOP$Synth, y = IOP$Adren, paired = TRUE, \n        alternative = c(\"two.sided\"), mu = 0, \n        exact = FALSE, correct = TRUE)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  IOP$Synth and IOP$Adren\nV = 19.5, p-value = 0.07314\nalternative hypothesis: true location shift is not equal to 0\n\n\nNo more warnings!! However,… should we be using the normal approximation here?"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#conclusion-1",
    "href": "slides_code/Day17_bsta511_code.html#conclusion-1",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Conclusion",
    "text": "Conclusion\nRecall the hypotheses to the (Wilcoxon) Signed-rank test:\n\\(H_0:\\) the population difference in reduction of intraocular pressure in treatment with adrenaline vs. new synthetic drug is symmetric around \\(\\tilde{\\mu}_0 =0\\)\n\\(H_a:\\) the population difference in reduction of intraocular pressure in treatment with adrenaline vs. new synthetic drug is not symmetric around \\(\\tilde{\\mu}_0 =0\\)\n\nSignificance level: \\(\\alpha\\) = 0.05\np-value = 0.07314\n\nConclusion:\nThere is insufficient evidence the differences in reduction in intraocular pressure differs between the synthetic drug and adrenaline are symmetric about 0 (2-sided Wilcoxon signed rank test \\(p\\)-value = 0.07314)\nHowever,…"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#wilcoxon-rank-sum-test-1",
    "href": "slides_code/Day17_bsta511_code.html#wilcoxon-rank-sum-test-1",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Wilcoxon rank-sum test",
    "text": "Wilcoxon rank-sum test\n\nThe nonparametric alternative to the two-sample \\(t\\)-test\n\nused to analyze two samples selected from separate (independent) populations\n\nAlso called the Mann-Whitney U test.\nUnlike the signed-rank test, there is no need to assume symmetry\nNecessary condition is that the two populations being compared\n\nhave the same shape (both right skewed, both left skewed, or both symmetric),\nso any noted difference is due to a shift in the median\n\nSince they have the same shape, when summarizing the test, we can describe the results in terms of a difference in medians.\n\nHypotheses:\n\\(H_0:\\) the two populations have the same median\n\\(H_a:\\) the two populations do NOT have the same median"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#example",
    "href": "slides_code/Day17_bsta511_code.html#example",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Example",
    "text": "Example\nDr. Priya Chaudhary (OHSU) examined the evoked membrane current of dental sensory neurons (in rats) under control conditions and a mixture of capsaicin plus capsazepine (CPZ). J. Dental Research} 80:1518–23, 2001.\n\nCPZdata &lt;- tibble(\n  control = c(3024, 2164, 864, 780, 125, 110),\n  cap_CPZ = c(426, 232, 130, 94, 75, 55)\n  ) \n\nCPZdata %&gt;% gt()\n\n\n\n\n\n  \n    \n      control\n      cap_CPZ\n    \n  \n  \n    3024\n426\n    2164\n232\n    864\n130\n    780\n94\n    125\n75\n    110\n55\n  \n  \n  \n\n\n\nCPZdata %&gt;% get_summary_stats(type = \"median\") %&gt;% gt()\n\n\n\n\n\n  \n    \n      variable\n      n\n      median\n    \n  \n  \n    control\n6\n822\n    cap_CPZ\n6\n112"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#visualize-the-data",
    "href": "slides_code/Day17_bsta511_code.html#visualize-the-data",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Visualize the data",
    "text": "Visualize the data\nDo the independent samples have the same distribution?\nControl group\n\nggplot(CPZdata, aes(x = control)) +\n  geom_dotplot()\n\n\n\nggplot(CPZdata, aes(x = control)) +\n  geom_boxplot()\n\n\n\nggplot(CPZdata, aes(x = control)) +\n  geom_density()\n\n\n\n\nCap + CPZ group\n\nggplot(CPZdata, aes(x = cap_CPZ)) +\n  geom_dotplot()\n\n\n\nggplot(CPZdata, aes(x = cap_CPZ)) +\n  geom_boxplot()\n\n\n\nggplot(CPZdata, aes(x = cap_CPZ)) +\n  geom_density()"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#calculating-ranks-and-test-statistic-w",
    "href": "slides_code/Day17_bsta511_code.html#calculating-ranks-and-test-statistic-w",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Calculating ranks and test statistic \\(W\\)",
    "text": "Calculating ranks and test statistic \\(W\\)\n\nCombine the two samples together (keep track of which observations came from each sample).\nRank the full set of \\(N=n_1 + n_2\\) observations.\n\nIf ties exist, assign average ranks to the tied values (as with the signed-rank test).\n\nSum the ranks corresponding to those observations from the smaller sample.\n\nThis is a time-saving step; you could also have used the larger sample.\nCall this sum \\(W\\).\n\nIf \\(n_1, n_2\\) are both less than 10, then use an exact test (can only be done if no ties are present)\n\notherwise use the large-sample normal approximation.\n\n\nIn our example, both groups have equal n; choose either for computing W.\n\\[W_{CPZ}=1+2+3+6+7+8 = 27\\]\n\\[W_{control}=4+5+9+10+11+12 = 51\\]\n\nCPZdata_long &lt;- CPZdata %&gt;% \n  pivot_longer(cols = c(control,cap_CPZ), \n               names_to = \"Group\",\n               values_to = \"Current\") %&gt;% \n  arrange(Current) %&gt;% \n  mutate(Rank = 1:12)\n\nCPZdata_long %&gt;% gt()\n\n\n\n\n\n  \n    \n      Group\n      Current\n      Rank\n    \n  \n  \n    cap_CPZ\n55\n1\n    cap_CPZ\n75\n2\n    cap_CPZ\n94\n3\n    control\n110\n4\n    control\n125\n5\n    cap_CPZ\n130\n6\n    cap_CPZ\n232\n7\n    cap_CPZ\n426\n8\n    control\n780\n9\n    control\n864\n10\n    control\n2164\n11\n    control\n3024\n12\n  \n  \n  \n\n\n\n\n\nNew: calculate ranks’s using rank()\n\nBelow I create the ranks using R’s rank() function that has an option ties.method to specify how to calculate ties.\nThis doesn’t require first sorting the data.\nIn the output below, I called this column Rank_R.\n\n\nCPZdata_long &lt;- CPZdata_long %&gt;% \n  mutate(\n    # create column with ranks that accounts for ties using R's rank() function\n    # you don't need to first sort the data to use this command\n    Rank_R = rank(Current, ties.method = \"average\")\n    ) \n\nCPZdata_long %&gt;% gt()\n\n\n\n\n\n  \n    \n      Group\n      Current\n      Rank\n      Rank_R\n    \n  \n  \n    cap_CPZ\n55\n1\n1\n    cap_CPZ\n75\n2\n2\n    cap_CPZ\n94\n3\n3\n    control\n110\n4\n4\n    control\n125\n5\n5\n    cap_CPZ\n130\n6\n6\n    cap_CPZ\n232\n7\n7\n    cap_CPZ\n426\n8\n8\n    control\n780\n9\n9\n    control\n864\n10\n10\n    control\n2164\n11\n11\n    control\n3024\n12\n12"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#exact-p-value-approach",
    "href": "slides_code/Day17_bsta511_code.html#exact-p-value-approach",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Exact p-value approach",
    "text": "Exact p-value approach\n\nIf \\(n_1, n_2\\) are both less than 10, then use an exact test,\n\notherwise use the large-sample normal approximation.\nHowever, exact method can only be done if no ties are present\n\np-value is the probability of getting a rank sum \\(W\\) as extreme or more extreme than the observed one.\n\nMultiply the 1-tail probability by 2 for the 2-tailed probability\n\n\n\\[p-value = 2 \\cdot P(W_{CPZ} \\leq 27)\\]\n\nTo calculate \\(P(W_{CPZ} \\leq 27)\\),\n\nwe need to enumerate all possible sets ranks for the sample size,\n\ncalculate the sum of ranks for each set of possible ranks,\nand then the probability for each sum (assuming each set of ranks is equally likely).\n\n\n\n\nWe will not be calculating the p-value “by hand.” We will be using R for this."
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#normal-approximation-approach",
    "href": "slides_code/Day17_bsta511_code.html#normal-approximation-approach",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Normal approximation approach",
    "text": "Normal approximation approach\nIf the null hypothesis is true, then the mean of the sum of the ranks from the smaller-sized group will be\n\\[\\mu_W = \\dfrac{n_s \\cdot (n_s + n_l + 1)}{2},\\] with a standard deviation of\n\\[\\sigma_W = \\sqrt{ \\dfrac{n_s\\cdot n_l \\cdot (n_s + n_l + 1)}{12} }.\\] Provided both groups are large ( \\(\\geq 10\\) ),\n\\[Z = \\frac{W - \\mu_W}{\\sigma_W}  \\approx Normal(0,1)\\]\n\nExample:\nWe have \\(W=27\\) and \\(n_l = n_s = 6\\):\n\\[\\mu_W = \\dfrac{6 \\cdot (6 + 6 +1 )}{2} = 39 \\\\\n\\sigma_W  = \\sqrt{\\dfrac{6\\cdot 6 \\cdot (6+6+1)}{12}} = \\sqrt{39} \\approx 6.2450 \\\\\nz \\approx \\frac{27-39}{6.2450} = -1.921538\\]\nThe two-sided \\(p\\)-value is \\[p=2 \\cdot P(Z &lt; -1.921538)=0.05466394\\]\n\n2 * pnorm((27-39) / sqrt(39))\n\n[1] 0.05466394"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#wilcoxon-rank-sum-test-in-r-with-wide-data",
    "href": "slides_code/Day17_bsta511_code.html#wilcoxon-rank-sum-test-in-r-with-wide-data",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Wilcoxon rank-sum test in R: with wide data",
    "text": "Wilcoxon rank-sum test in R: with wide data\n\nglimpse(CPZdata)\n\nRows: 6\nColumns: 2\n$ control &lt;dbl&gt; 3024, 2164, 864, 780, 125, 110\n$ cap_CPZ &lt;dbl&gt; 426, 232, 130, 94, 75, 55\n\n\nExact p-value\n\nwilcox.test(x = CPZdata$cap_CPZ, y = CPZdata$control, \n            alternative = c(\"two.sided\"), mu = 0, \n            exact = TRUE) \n\n\n    Wilcoxon rank sum exact test\n\ndata:  CPZdata$cap_CPZ and CPZdata$control\nW = 6, p-value = 0.06494\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#wilcoxon-rank-sum-test-in-r-with-wide-data-1",
    "href": "slides_code/Day17_bsta511_code.html#wilcoxon-rank-sum-test-in-r-with-wide-data-1",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Wilcoxon rank-sum test in R: with wide data",
    "text": "Wilcoxon rank-sum test in R: with wide data\nNormal approximation p-value without CC\n\nwilcox.test(x = CPZdata$cap_CPZ, y = CPZdata$control, \n    alternative = c(\"two.sided\"), mu = 0, \n    exact = FALSE, correct = FALSE) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      statistic\n      p.value\n      method\n      alternative\n    \n  \n  \n    6\n0.05466394\nWilcoxon rank sum test\ntwo.sided\n  \n  \n  \n\n\n\n\nNormal approximation p-value with CC\n\nwilcox.test(x = CPZdata$cap_CPZ, y = CPZdata$control, \n    alternative = c(\"two.sided\"), mu = 0, \n    exact = FALSE, correct = TRUE) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      statistic\n      p.value\n      method\n      alternative\n    \n  \n  \n    6\n0.06555216\nWilcoxon rank sum test with continuity correction\ntwo.sided"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#wilcoxon-rank-sum-test-in-r-with-long-data",
    "href": "slides_code/Day17_bsta511_code.html#wilcoxon-rank-sum-test-in-r-with-long-data",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Wilcoxon rank-sum test in R: with long data",
    "text": "Wilcoxon rank-sum test in R: with long data\nMake data long (if it’s not already long):\n\nCPZdata_long &lt;- CPZdata %&gt;% \n  pivot_longer(cols = c(control,cap_CPZ), \n               names_to = \"Group\",\n               values_to = \"Current\")\n\nhead(CPZdata_long)\n\n# A tibble: 6 × 2\n  Group   Current\n  &lt;chr&gt;     &lt;dbl&gt;\n1 control    3024\n2 cap_CPZ     426\n3 control    2164\n4 cap_CPZ     232\n5 control     864\n6 cap_CPZ     130\n\n\nExact p-value\n\nwilcox.test(Current ~ Group, \n            data = CPZdata_long, \n            alternative = c(\"two.sided\"), \n            mu = 0, \n            exact = TRUE) %&gt;% \n  tidy() %&gt;% gt() \n\n\n\n\n\n  \n    \n      statistic\n      p.value\n      method\n      alternative\n    \n  \n  \n    6\n0.06493506\nWilcoxon rank sum exact test\ntwo.sided"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#conclusion-2",
    "href": "slides_code/Day17_bsta511_code.html#conclusion-2",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Conclusion",
    "text": "Conclusion\nRecall the hypotheses to the (Wilcoxon) Signed-rank test:\n\\(H_0:\\) the control and treated populations have the same median\n\\(H_a:\\) the control and treated populations do NOT have the same median\n\nSignificance level: \\(\\alpha\\) = 0.05\np-value = 0.06494\n\nConclusion:\nThere is suggestive but inconclusive evidence that the evoked membrane current of dental sensory neurons (in rats) differs between the control group and the group exposed to a mixture of capsaicin plus capsazepine (2-sided Wilcoxon rank-sum test \\(p\\)-value = 0.06494)."
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#kruskal-wallis-test-nonparametric-anova-test",
    "href": "slides_code/Day17_bsta511_code.html#kruskal-wallis-test-nonparametric-anova-test",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Kruskal-Wallis test: nonparametric ANOVA test",
    "text": "Kruskal-Wallis test: nonparametric ANOVA test\n\nRecall that an ANOVA tests means from more than 2 groups\nConditions for ANOVA include\n\nSample sizes in each group group are large (each \\(n \\ge 30\\)),\n\nOR the data are relatively normally distributed in each group\n\nVariability is “similar” in all group groups\n\nIf these conditions are in doubt, or if the response is ordinal, then the Kruskal-Wallis test is an alternative.\n\n\\[\\begin{align}\nH_0 &: \\text{pop median}_1 = \\text{pop median}_2 = ... = \\text{pop median}_k\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\text{pop median}_i \\neq \\text{pop median}_j \\text{ for } i \\neq j\n\\end{align}\\]\n\nK-W test is an extension of the (Wilcoxon) rank-sum test to more than 2 groups\n\nWith \\(k=2\\) groups, the K-W test is the same as the rank-sum test"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#k-w-test-statistic-h-fyi",
    "href": "slides_code/Day17_bsta511_code.html#k-w-test-statistic-h-fyi",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "K-W test statistic: \\(H\\) (fyi)",
    "text": "K-W test statistic: \\(H\\) (fyi)\n\\[H = \\frac{12}{{N(N + 1)}} \\sum_{i=1}^{k} \\frac{R_i^2}{n_i} - 3(N + 1)\\]\n\n\\(k\\) is the number of groups,\n\\(n_i\\) is the number of observations in group \\(i\\)\n\\(N = n_1 + \\ldots + n_k\\) is the total number of observations across all groups,\n\\(R_i\\) is the sum of ranks for group \\(i\\)\n\nThe test statistic \\(H\\) has a Chi-squared distribution with \\(k-1\\) degrees of freedom:\n\\[H \\sim \\chi^2_{k-1}\\]\nRanks are calculated similarly to the (Wilcoxon) rank-sum test."
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#ranks-for-the-k-w-test",
    "href": "slides_code/Day17_bsta511_code.html#ranks-for-the-k-w-test",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Ranks for the K-W test",
    "text": "Ranks for the K-W test\n\nCombine the \\(k\\) samples together (keep track of which observations came from each sample).\nRank the full set of \\(N = n_1 + \\ldots + n_k\\) observations.\n\nIf ties exist, assign average ranks to the tied values (as with the signed-rank test).\n\nThen sum the ranks within each of the \\(k\\) groups\n\nLabel the sums of the ranks for each group as \\(R_1, \\ldots + R_k\\).\n\n\nIf \\(H_0\\) is true, we expect the populations to have the same medians, and thus the ranks to be similar as well."
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#example-ozone-levels-by-month",
    "href": "slides_code/Day17_bsta511_code.html#example-ozone-levels-by-month",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Example: Ozone levels by month",
    "text": "Example: Ozone levels by month\n\nairquality data included in base R - no need to load it\nDaily air quality measurements in New York, May to September 1973.\nQuestion: do ozone levels differ by month?\n\n\nglimpse(airquality)\n\nRows: 153\nColumns: 6\n$ Ozone   &lt;int&gt; 41, 36, 12, 18, NA, 28, 23, 19, 8, NA, 7, 16, 11, 14, 18, 14, …\n$ Solar.R &lt;int&gt; 190, 118, 149, 313, NA, NA, 299, 99, 19, 194, NA, 256, 290, 27…\n$ Wind    &lt;dbl&gt; 7.4, 8.0, 12.6, 11.5, 14.3, 14.9, 8.6, 13.8, 20.1, 8.6, 6.9, 9…\n$ Temp    &lt;int&gt; 67, 72, 74, 62, 56, 66, 65, 59, 61, 69, 74, 69, 66, 68, 58, 64…\n$ Month   &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Day     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n\n\n\nOz_mnth &lt;- airquality %&gt;% \n  group_by(Month) %&gt;% \n  get_summary_stats(Ozone, \n    show = c(\"n\", \"mean\", \"median\", \"sd\"))\nOz_mnth %&gt;% gt()\n\n\n\n\n\n  \n    \n      Month\n      variable\n      n\n      mean\n      median\n      sd\n    \n  \n  \n    5\nOzone\n26\n23.615\n18\n22.224\n    6\nOzone\n9\n29.444\n23\n18.208\n    7\nOzone\n26\n59.115\n60\n31.636\n    8\nOzone\n26\n59.962\n52\n39.681\n    9\nOzone\n29\n31.448\n23\n24.142\n  \n  \n  \n\n\n\nmax(Oz_mnth$sd) / min(Oz_mnth$sd)\n\n[1] 2.179317\n\n\n\nggplot(airquality,\n       aes(x = Ozone, y = factor(Month))) +\n  geom_boxplot()"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#example-calculate-ranks-fyi",
    "href": "slides_code/Day17_bsta511_code.html#example-calculate-ranks-fyi",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Example: calculate ranks (fyi)",
    "text": "Example: calculate ranks (fyi)\n\nranks_Oz_mnth &lt;- airquality %&gt;% \n  select(Ozone, Month) \n\nsummary(ranks_Oz_mnth)\n\n     Ozone            Month      \n Min.   :  1.00   Min.   :5.000  \n 1st Qu.: 18.00   1st Qu.:6.000  \n Median : 31.50   Median :7.000  \n Mean   : 42.13   Mean   :6.993  \n 3rd Qu.: 63.25   3rd Qu.:8.000  \n Max.   :168.00   Max.   :9.000  \n NA's   :37                      \n\nranks_Oz_mnth &lt;- ranks_Oz_mnth %&gt;% \n  drop_na(Ozone) %&gt;% \n  arrange(Ozone) %&gt;% \n  mutate(Rank = 1:nrow(.))\n\nRanks below do not take into account ties!!\n\nhead(ranks_Oz_mnth, 20)\n\n   Ozone Month Rank\n1      1     5    1\n2      4     5    2\n3      6     5    3\n4      7     5    4\n5      7     7    5\n6      7     9    6\n7      8     5    7\n8      9     8    8\n9      9     8    9\n10     9     9   10\n11    10     7   11\n12    11     5   12\n13    11     5   13\n14    11     5   14\n15    12     5   15\n16    12     6   16\n17    13     6   17\n18    13     9   18\n19    13     9   19\n20    13     9   20\n\n\nSum of ranks for each group: (not taking into account ties!!)\n\nranks_Oz_mnth %&gt;% \n  group_by(Month) %&gt;% \n  summarise(sumRank = sum(Rank))\n\n# A tibble: 5 × 2\n  Month sumRank\n  &lt;int&gt;   &lt;int&gt;\n1     5     939\n2     6     434\n3     7    2023\n4     8    1956\n5     9    1434\n\n\n\nNew: calculate ranks’s using rank()\n\nBelow I create the ranks using R’s rank() function that has an option ties.method to specify how to calculate ties.\nThis doesn’t require first sorting the data.\nIn the output below, I called this column Rank_R.\n\n\nranks_Oz_mnth &lt;- ranks_Oz_mnth %&gt;% \n  mutate(\n    # create column with ranks that accounts for ties using R's rank() function\n    # you don't need to first sort the data to use this command\n    Rank_R = rank(Ozone, ties.method = \"average\")\n    ) \n\n# Compare \"simple\" ranks not including ties with tie-corrected ranks\nhead(ranks_Oz_mnth, 20)\n\n   Ozone Month Rank Rank_R\n1      1     5    1    1.0\n2      4     5    2    2.0\n3      6     5    3    3.0\n4      7     5    4    5.0\n5      7     7    5    5.0\n6      7     9    6    5.0\n7      8     5    7    7.0\n8      9     8    8    9.0\n9      9     8    9    9.0\n10     9     9   10    9.0\n11    10     7   11   11.0\n12    11     5   12   13.0\n13    11     5   13   13.0\n14    11     5   14   13.0\n15    12     5   15   15.5\n16    12     6   16   15.5\n17    13     6   17   18.5\n18    13     9   18   18.5\n19    13     9   19   18.5\n20    13     9   20   18.5"
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#k-w-test-in-r",
    "href": "slides_code/Day17_bsta511_code.html#k-w-test-in-r",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "K-W test in R",
    "text": "K-W test in R\n\nkruskal.test(Ozone ~ Month, data = airquality)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Ozone by Month\nKruskal-Wallis chi-squared = 29.267, df = 4, p-value = 6.901e-06\n\nkruskal.test(Ozone ~ Month, data = airquality) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n      statistic\n      p.value\n      parameter\n      method\n    \n  \n  \n    29.26658\n6.900714e-06\n4\nKruskal-Wallis rank sum test\n  \n  \n  \n\n\n\n\nThere is sufficient evidence that the median ozone levels are different in at least two months from May - September, 1973 in New York City (p &lt; 0.001; Kruskal-Wallis test).\n\n(fyi) Since the K-W test is significant, follow-up with pairwise (Wilcoxon) rank-sum tests using a multiple comparison procedure to identify which months have different medians."
  },
  {
    "objectID": "slides_code/Day17_bsta511_code.html#permutation-tests-bootstrapping-1",
    "href": "slides_code/Day17_bsta511_code.html#permutation-tests-bootstrapping-1",
    "title": "Day 17: Nonparametric tests - Supplemental material",
    "section": "Permutation tests & bootstrapping",
    "text": "Permutation tests & bootstrapping\n\nIn some cases we saw that the conditions failed or the sample size was too small for a normal approximation and there were ties in ranks preventing us from using an exact method.\nAnother nonparametric option to consider is a permutation test or bootstrapping.\nIf you’re interested in learning more about this approach, check out the ModernDive Statistical Inference via Data Science book by Chester Ismay and Albert Kim.\n\nCh 7: Sampling\nCh 8: Bootstrapping and Confidence Intervals\nCh 9: Hypothesis Testing"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html",
    "href": "slides_code/Day13_bsta511_code.html",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n\nCode\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) \nlibrary(here) \nlibrary(pwr) # new-ish\n\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#load-packages",
    "href": "slides_code/Day13_bsta511_code.html#load-packages",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n\nCode\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) \nlibrary(here) \nlibrary(pwr) # new-ish\n\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#goals-for-today-sections-8.3-8.4",
    "href": "slides_code/Day13_bsta511_code.html#goals-for-today-sections-8.3-8.4",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Goals for today (Sections 8.3-8.4)",
    "text": "Goals for today (Sections 8.3-8.4)\n\nStatistical inference for categorical data when either are\n\ncomparing more than two groups or\nhave categorical outcomes that have more than 2 levels or\nboth\n\nChi-squared tests of association (independence)\n\nHypotheses\ntest statistic\nChi-squared distribution\np-value\ntechnical assumptions (conditions)\nconclusion\nR: chisq.test()\n\nFisher’s Exact Test\nChi-squared test vs. testing difference in proportions\n\nTest of Homogeneity"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#is-there-an-association-between-depression-and-being-physically-active",
    "href": "slides_code/Day13_bsta511_code.html#is-there-an-association-between-depression-and-being-physically-active",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Is there an association between depression and being physically active?",
    "text": "Is there an association between depression and being physically active?\n\nData sampled from the NHANES R package:\n\nAmerican National Health and Nutrition Examination Surveys\nCollected 2009-2012 by US National Center for Health Statistics (NCHS)\nNHANES dataset: 10,000 rows, resampled from NHANESraw to undo oversampling effects\n\nTreat it as a simple random sample from the US population (for pedagogical purposes)\n\n\nDepressed\n\nSelf-reported number of days where participant felt down, depressed or hopeless.\nOne of None, Several, or Most (more than half the days).\nReported for participants aged 18 years or older.\n\nPhysActive\n\nParticipant does moderate or vigorous-intensity sports, fitness or recreational activities (Yes or No).\nReported for participants 12 years or older."
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#hypotheses-for-a-chi-squared-test-of-association-independence",
    "href": "slides_code/Day13_bsta511_code.html#hypotheses-for-a-chi-squared-test-of-association-independence",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Hypotheses for a Chi-squared test of association (independence)",
    "text": "Hypotheses for a Chi-squared test of association (independence)\nGeneric wording:\nTest of “association” wording\n\n\\(H_0\\): There is no association between the two variables\n\\(H_A\\): There is an association between the two variables\n\nTest of “independence” wording\n\n\\(H_0\\): The variables are independent\n\\(H_A\\): The variables are not independent\n\nFor our example:\nTest of “association” wording\n\n\\(H_0\\): There is no association between depression and physical activity\n\\(H_A\\): There is an association between depression and physical activity\n\nTest of “independence” wording\n\n\\(H_0\\): The variables depression and physical activity are independent\n\\(H_A\\): The variables depression and physical activity are not independent\n\n\n\n\n\n\n\nNo symbols\n\n\n\nFor chi-squared test hypotheses we do not have versions using “symbols” like we do with tests of means or proportions."
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#data-from-nhanes",
    "href": "slides_code/Day13_bsta511_code.html#data-from-nhanes",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Data from NHANES",
    "text": "Data from NHANES\n\nResults below are from\n\na random sample of 400 adults (≥ 18 yrs old)\nwith data for both the depression Depressed and physically active (PhysActive) variables.\n\n\nSee slides for data table.\nBelow is table in R.\n\n\nCode\n(DepPA_table &lt;- matrix(c(199, 26, 1, 115, 32, 27), nrow = 2, ncol = 3, byrow = T))\n\n\n     [,1] [,2] [,3]\n[1,]  199   26    1\n[2,]  115   32   27\n\n\nCode\ndimnames(DepPA_table) &lt;- list(\"PA\" = c(\"Yes\", \"No\"),   # row names\n                              \"Depression\" = c(\"None\", \"Several\", \"Most\"))  # column names\nDepPA_table\n\n\n     Depression\nPA    None Several Most\n  Yes  199      26    1\n  No   115      32   27\n\n\n\nWhat does it mean for the variables to be independent?"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#h_0-variables-are-independent",
    "href": "slides_code/Day13_bsta511_code.html#h_0-variables-are-independent",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "\\(H_0\\): Variables are Independent",
    "text": "\\(H_0\\): Variables are Independent\n\nRecall from Chapter 2, that events \\(A\\) and \\(B\\) are independent if and only if\n\n\\[P(A~and~B)=P(A)P(B)\\]\n\nIf depression and being physically active are independent variables, then theoretically this condition needs to hold for every combination of levels, i.e.\n\n\\[\\begin{align}\nP(None~and~Yes) &= P(None)P(Yes)\\\\\nP(None~and~No) &= P(None)P(No)\\\\\nP(Several~and~Yes) &= P(Several)P(Yes)\\\\\nP(Several~and~No) &= P(Several)P(No)\\\\\nP(Most~and~Yes) &= P(Most)P(Yes)\\\\\nP(Most~and~No) &= P(Most)P(No)\n\\end{align}\\]\n\n\nCode\n# See slides for data table.  \nDepPA_table\n\n\n     Depression\nPA    None Several Most\n  Yes  199      26    1\n  No   115      32   27\n\n\n\\[\\begin{align}\nP(None~and~Yes) &= \\frac{314}{400}\\cdot\\frac{226}{400}\\\\\n& ...\\\\\nP(Most~and~No) &= \\frac{28}{400}\\cdot\\frac{174}{400}\n\\end{align}\\]\nWith these probabilities, for each cell of the table we calculate the expected counts for each cell under the \\(H_0\\) hypothesis that the variables are independent"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#expected-counts-if-variables-are-independent",
    "href": "slides_code/Day13_bsta511_code.html#expected-counts-if-variables-are-independent",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Expected counts (if variables are independent)",
    "text": "Expected counts (if variables are independent)\n\nThe expected counts (if \\(H_0\\) is true & the variables are independent) for each cell are\n\n\\(np\\) = total table size \\(\\cdot\\) probability of cell\n\n\nExpected count of Yes & None:\n\\[\\begin{align}\n400 \\cdot & P(None~and~Yes)\\\\\n&= 400 \\cdot P(None)P(Yes)\\\\\n&= 400 \\cdot\\frac{314}{400}\\cdot\\frac{226}{400}\\\\\n&= \\frac{314\\cdot 226}{400} \\\\\n&=  177.41\\\\\n&= \\frac{\\text{column total}\\cdot \\text{row total}}{\\text{table total}}\n\\end{align}\\]\n\n\nCode\n# See slides for data table.  \nDepPA_table\n\n\n     Depression\nPA    None Several Most\n  Yes  199      26    1\n  No   115      32   27\n\n\n\nIf depression and being physically active are independent variables\n\n(as assumed by \\(H_0\\)),\n\nthen the observed counts should be close to the expected counts for each cell of the table"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#observed-vs.-expected-counts",
    "href": "slides_code/Day13_bsta511_code.html#observed-vs.-expected-counts",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Observed vs. Expected counts",
    "text": "Observed vs. Expected counts\n\nThe observed counts are the counts in the 2-way table summarizing the data\n\n\n\nCode\n# See slides for data table.  \nDepPA_table\n\n\n     Depression\nPA    None Several Most\n  Yes  199      26    1\n  No   115      32   27\n\n\n\nThe expected counts are the counts the we would expect to see in the 2-way table if there was no association between depression and being physically activity\n\nSee slides for table with expected counts & calculations\nExpected count for cell \\(i,j\\) :\n\\[\\textrm{Expected Count}_{\\textrm{row } i,\\textrm{ col }j}=\\frac{(\\textrm{row}~i~ \\textrm{total})\\cdot(\\textrm{column}~j~ \\textrm{total})}{\\textrm{table total}}\\]"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#the-chi2-test-statistic",
    "href": "slides_code/Day13_bsta511_code.html#the-chi2-test-statistic",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "The \\(\\chi^2\\) test statistic",
    "text": "The \\(\\chi^2\\) test statistic\nTest statistic for a test of association (independence):\n\\[\\chi^2 = \\sum_{\\textrm{all cells}} \\frac{(\\textrm{observed} - \\text{expected})^2}{\\text{expected}}\\]\n\nWhen the variables are independent, the observed and expected counts should be close to each other\n\n\n\nCode\n# See slides for data table with expected counts.  \nDepPA_table\n\n\n     Depression\nPA    None Several Most\n  Yes  199      26    1\n  No   115      32   27\n\n\n\n\\[\\begin{align}\n\\chi^2 &=  \\sum\\frac{(O-E)^2}{E} \\\\\n&= \\frac{(199-177.41)^2}{177.41} + \\frac{(26-32.77)^2}{32.77} + \\ldots + \\frac{(27-12.18)^2}{12.18} \\\\\n&=  41.2\n\\end{align}\\]\nIs this value big? Big enough to reject \\(H_0\\)?"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#the-chi2-distribution-calculating-the-p-value",
    "href": "slides_code/Day13_bsta511_code.html#the-chi2-distribution-calculating-the-p-value",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "The \\(\\chi^2\\) distribution & calculating the p-value",
    "text": "The \\(\\chi^2\\) distribution & calculating the p-value\nThe \\(\\chi^2\\) distribution shape depends on its degrees of freedom\n\nIt’s skewed right for smaller df,\n\ngets more symmetric for larger df\n\ndf = (# rows-1) x (# columns-1)\n\nSee slides for Chi-squared distribution figure.\n\nThe p-value is always the area to the right of the test statistic for a \\(\\chi^2\\) test.\nWe can use the pchisq function in R to calculate the probability of being at least as big as the \\(\\chi^2\\) test statistic:\n\n\n\nCode\npv &lt;- pchisq(41.2, df = 2, lower.tail = FALSE)\npv\n\n\n[1] 1.131185e-09\n\n\nWhat’s the conclusion to the \\(\\chi^2\\) test?"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#conclusion",
    "href": "slides_code/Day13_bsta511_code.html#conclusion",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Conclusion",
    "text": "Conclusion\nRecall the hypotheses to our \\(\\chi^2\\) test:\n\n\\(H_0\\): There is no association between depression and being physically activity\n\\(H_A\\): There is an association between depression and being physically activity\n\n\n\n\n\n\nConclusion:\nBased a random sample of 400 US adults from 2009-2012, there is sufficient evidence that there is an association between depression and being physically activity (p-value &lt; 0.001).\n\n\n\n\n\n\nWarning\n\n\n\nIf we fail to reject, we DO NOT have evidence of no association."
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#technical-conditions",
    "href": "slides_code/Day13_bsta511_code.html#technical-conditions",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Technical conditions",
    "text": "Technical conditions\n\nIndependence\n\nEach case (person) that contributes a count to the table must be independent of all the other cases in the table\n\nIn particular, observational units cannot be represented in more than one cell.\nFor example, someone cannot choose both “Several” and “Most” for depression status. They have to choose exactly one option for each variable.\n\n\n\n\n\nSample size\n\nIn order for the test statistic to be modeled by a chi-squared distribution we need\n2 \\(\\times\\) 2 table: expected counts are at least 10 for each cell\nlarger tables:\n\nno more than 1/5 of the expected counts are less than 5, and\nall expected counts are greater than 1"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#depression-vs.-physical-activity-dataset",
    "href": "slides_code/Day13_bsta511_code.html#depression-vs.-physical-activity-dataset",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Depression vs. physical activity dataset",
    "text": "Depression vs. physical activity dataset\nCreate dataset based on results table:\n\n\nCode\nDepPA &lt;- tibble(\n  Depression = c(rep(\"None\", 314), \n         rep(\"Several\", 58),\n         rep(\"Most\", 28)),\n  PA = c(rep(\"Yes\", 199),  # None\n          rep(\"No\", 115),\n          rep(\"Yes\", 26), # Several\n          rep(\"No\", 32),\n          rep(\"Yes\", 1), # Most\n          rep(\"No\", 27))\n)\n\n\nSummary table of data:\n\n\nCode\nDepPA %&gt;% tabyl(Depression, PA) \n\n\n Depression  No Yes\n       Most  27   1\n       None 115 199\n    Several  32  26\n\n\nCode\n# base R:\ntable(DepPA)\n\n\n          PA\nDepression  No Yes\n   Most     27   1\n   None    115 199\n   Several  32  26"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#chi2-test-in-r-using-dataset",
    "href": "slides_code/Day13_bsta511_code.html#chi2-test-in-r-using-dataset",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "\\(\\chi^2\\) test in R using dataset",
    "text": "\\(\\chi^2\\) test in R using dataset\nIf only have 2 columns in the dataset:\n\n\nCode\n(ChisqTest_DepPA &lt;- \n   chisq.test(table(DepPA)))\n\n\n\n    Pearson's Chi-squared test\n\ndata:  table(DepPA)\nX-squared = 41.171, df = 2, p-value = 1.148e-09\n\n\nIf have &gt;2 columns in the dataset, we need to specify which columns to table:\n\n\nCode\n(ChisqTest_DepPA &lt;- \n   chisq.test(table(\n     DepPA$Depression, DepPA$PA)))\n\n\n\n    Pearson's Chi-squared test\n\ndata:  table(DepPA$Depression, DepPA$PA)\nX-squared = 41.171, df = 2, p-value = 1.148e-09\n\n\nThe tidyverse way (fewer parentheses)\n\n\nCode\ntable(DepPA$Depression, DepPA$PA) %&gt;% \n  chisq.test() \n\n\n\n    Pearson's Chi-squared test\n\ndata:  .\nX-squared = 41.171, df = 2, p-value = 1.148e-09\n\n\ntidy() the output (from broom package):\n\n\nCode\ntable(DepPA$Depression, DepPA$PA) %&gt;% \n  chisq.test() %&gt;% \n  tidy() %&gt;% gt()\n\n\n\n\n\n\n  \n    \n      statistic\n      p.value\n      parameter\n      method\n    \n  \n  \n    41.17067\n1.147897e-09\n2\nPearson's Chi-squared test\n  \n  \n  \n\n\n\n\nPull p-value\n\n\nCode\ntable(DepPA$Depression, DepPA$PA) %&gt;% \n  chisq.test() %&gt;% \n  tidy() %&gt;% pull(p.value)\n\n\n[1] 1.147897e-09"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#observed-expected-counts-in-r",
    "href": "slides_code/Day13_bsta511_code.html#observed-expected-counts-in-r",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Observed & expected counts in R",
    "text": "Observed & expected counts in R\nYou can see what the observed and expected counts are from the saved chi-squared test results:\n\n\nCode\nChisqTest_DepPA$observed\n\n\n         \n           No Yes\n  Most     27   1\n  None    115 199\n  Several  32  26\n\n\nCode\nChisqTest_DepPA$expected\n\n\n         \n              No    Yes\n  Most     12.18  15.82\n  None    136.59 177.41\n  Several  25.23  32.77\n\n\n\nWhy is it important to look at the expected counts?\nWhat are we looking for in the expected counts?"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#chi2-test-in-r-with-2-way-table",
    "href": "slides_code/Day13_bsta511_code.html#chi2-test-in-r-with-2-way-table",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "\\(\\chi^2\\) test in R with 2-way table",
    "text": "\\(\\chi^2\\) test in R with 2-way table\nCreate a base R table of the results:\n\n\nCode\n(DepPA_table &lt;- matrix(c(199, 26, 1, 115, 32, 27), nrow = 2, ncol = 3, byrow = T))\n\n\n     [,1] [,2] [,3]\n[1,]  199   26    1\n[2,]  115   32   27\n\n\nCode\ndimnames(DepPA_table) &lt;- list(\"PA\" = c(\"Yes\", \"No\"),   # row names\n                              \"Depression\" = c(\"None\", \"Several\", \"Most\"))  # column names\nDepPA_table\n\n\n     Depression\nPA    None Several Most\n  Yes  199      26    1\n  No   115      32   27\n\n\nRun \\(\\chi^2\\) test with 2-way table:\n\n\nCode\nchisq.test(DepPA_table) \n\n\n\n    Pearson's Chi-squared test\n\ndata:  DepPA_table\nX-squared = 41.171, df = 2, p-value = 1.148e-09\n\n\nCode\nchisq.test(DepPA_table)$expected\n\n\n     Depression\nPA      None Several  Most\n  Yes 177.41   32.77 15.82\n  No  136.59   25.23 12.18"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#yates-continuity-correction",
    "href": "slides_code/Day13_bsta511_code.html#yates-continuity-correction",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "(Yates’) Continuity correction",
    "text": "(Yates’) Continuity correction\n\nFor a 2x2 contingency table,\n\nthe \\(\\chi^2\\) test has the option of including a continuity correction\njust like with the proportions test\n\nThe default includes a continuity correction\nThere is no CC for bigger tables\n\n\n\nCode\n(DepPA_table2x2 &lt;- matrix(c(199, 27, 115, 59), nrow = 2, ncol = 2, byrow = T))\n\n\n     [,1] [,2]\n[1,]  199   27\n[2,]  115   59\n\n\nCode\ndimnames(DepPA_table2x2) &lt;- list(\"PA\" = c(\"Yes\", \"No\"),   # row names\n                              \"Depression\" = c(\"None\", \"Several/Most\"))  # column names\nDepPA_table2x2\n\n\n     Depression\nPA    None Several/Most\n  Yes  199           27\n  No   115           59\n\n\nOutput without a CC\n\n\nCode\nchisq.test(DepPA_table2x2, correct = FALSE) \n\n\n\n    Pearson's Chi-squared test\n\ndata:  DepPA_table2x2\nX-squared = 28.093, df = 1, p-value = 1.156e-07\n\n\nCompare to output with CC:\n\n\nCode\nchisq.test(DepPA_table2x2) \n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  DepPA_table2x2\nX-squared = 26.807, df = 1, p-value = 2.248e-07"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#example-with-smaller-sample-size",
    "href": "slides_code/Day13_bsta511_code.html#example-with-smaller-sample-size",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Example with smaller sample size",
    "text": "Example with smaller sample size\n\nSuppose that instead of taking a random sample of 400 adults (from the NHANES data), a study takes a random sample of 100 such that\n\n50 people that are physically active and\n50 people that are not physically active\n\n\n\n\nCode\n(DepPA100_table &lt;- matrix(c(43, 5, 2, 40, 4, 6), nrow = 2, ncol = 3, byrow = T))\n\n\n     [,1] [,2] [,3]\n[1,]   43    5    2\n[2,]   40    4    6\n\n\nCode\ndimnames(DepPA100_table) &lt;- list(\"PA\" = c(\"Yes\", \"No\"),   # row names\n                              \"Depression\" = c(\"None\", \"Several\", \"Most\"))  # column names\nDepPA100_table\n\n\n     Depression\nPA    None Several Most\n  Yes   43       5    2\n  No    40       4    6"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#chi-squared-test-warning",
    "href": "slides_code/Day13_bsta511_code.html#chi-squared-test-warning",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Chi-squared test warning",
    "text": "Chi-squared test warning\n\n\nCode\nchisq.test(DepPA100_table) \n\n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  DepPA100_table\nX-squared = 2.2195, df = 2, p-value = 0.3296\n\n\nCode\nchisq.test(DepPA100_table)$expected\n\n\nWarning in stats::chisq.test(x, y, ...): Chi-squared approximation may be\nincorrect\n\n\n     Depression\nPA    None Several Most\n  Yes 41.5     4.5    4\n  No  41.5     4.5    4\n\n\n\nRecall the sample size condition\n\nIn order for the test statistic to be modeled by a chi-squared distribution we need\n2 \\(\\times\\) 2 table: expected counts are at least 10 for each cell\nlarger tables:\n\nno more than 1/5 of the expected counts are less than 5, and\nall expected counts are greater than 1"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#fishers-exact-test",
    "href": "slides_code/Day13_bsta511_code.html#fishers-exact-test",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Fisher’s Exact Test",
    "text": "Fisher’s Exact Test\n\nCalled an exact test since it\n\ncalculates an exact probability for the p-value\n\ninstead of using an asymptotic approximation, such as the normal, t, or chi-squared distributions\n\nFor 2x2 tables the p-value is calculated using the hypergeometric probability distribution (see book for details)\n\n\n\n\nCode\nfisher.test(DepPA100_table)\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  DepPA100_table\np-value = 0.3844\nalternative hypothesis: two.sided\n\n\n\n\n\n\n\n\nComments\n\n\n\n\nNote that there is no test statistic\nThere is also no CI\nThis is always a two-sided test"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#simulate-p-values-another-option-for-small-expected-counts",
    "href": "slides_code/Day13_bsta511_code.html#simulate-p-values-another-option-for-small-expected-counts",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Simulate p-values: another option for small expected counts",
    "text": "Simulate p-values: another option for small expected counts\nFrom the chisq.test help file:\n\nSimulation is done by random sampling from the set of all contingency tables with the same margin totals\n\nworks only if the margin totals are strictly positive.\n\nFor each simulation, a \\(\\chi^2\\) test statistic is calculated\nP-value is the proportion of simulations that have a test statistic at least as big as the observed one.\nNo continuity correction\n\n\n\nCode\nset.seed(567)\nchisq.test(DepPA100_table, simulate.p.value = TRUE) \n\n\n\n    Pearson's Chi-squared test with simulated p-value (based on 2000\n    replicates)\n\ndata:  DepPA100_table\nX-squared = 2.2195, df = NA, p-value = 0.3893"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#chi2-test-vs.-testing-differences-in-proportions",
    "href": "slides_code/Day13_bsta511_code.html#chi2-test-vs.-testing-differences-in-proportions",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "\\(\\chi^2\\) test vs. testing differences in proportions",
    "text": "\\(\\chi^2\\) test vs. testing differences in proportions\nIf there are only 2 levels in both of the categorical variables being tested, then the p-value from the \\(\\chi^2\\) test is equal to the p-value from the differences in proportions test.\nExample: Previously we tested whether the proportion who had participated in sports betting was the same for college and noncollege young adults:\n\\[\\begin{align}\nH_0:& ~p_{coll} - p_{noncoll} = 0\\\\\nH_A:& ~p_{coll} - p_{noncoll} \\neq 0\n\\end{align}\\]\n\n\nCode\nSportsBet_table &lt;- matrix(\n  c(175, 94, 137, 77), \n  nrow = 2, ncol = 2, byrow = T)\n\ndimnames(SportsBet_table) &lt;- list(\n  \"Group\" = c(\"College\", \"NonCollege\"), # row names\n  \"Bet\" = c(\"No\", \"Yes\"))  # column names\n\nSportsBet_table\n\n\n            Bet\nGroup         No Yes\n  College    175  94\n  NonCollege 137  77\n\n\n\n\nCode\nchisq.test(SportsBet_table) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n\n  \n    \n      statistic\n      p.value\n      parameter\n      method\n    \n  \n  \n    0.01987511\n0.8878864\n1\nPearson's Chi-squared test with Yates' continuity correction\n  \n  \n  \n\n\n\n\nCode\nprop.test(SportsBet_table) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n\n  \n    \n      estimate1\n      estimate2\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.6505576\n0.6401869\n0.01987511\n0.8878864\n1\n-0.07973918\n0.1004806\n2-sample test for equality of proportions with continuity correction\ntwo.sided\n  \n  \n  \n\n\n\n\nCode\n2*pnorm(sqrt(0.0199), lower.tail=F) # p-value\n\n\n[1] 0.8878167"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#test-of-homogeneity",
    "href": "slides_code/Day13_bsta511_code.html#test-of-homogeneity",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Test of Homogeneity",
    "text": "Test of Homogeneity\n\nRunning the sports betting example as a chi-squared test is actually an example of a test of homogeneity\nIn a test of homogeneity, proportions can be compared between many groups\n\n\\[\\begin{align}\nH_0:&~ p_1 = p_2 = p_2 = \\ldots = p_n\\\\\nH_A:&~ p_i \\neq p_j \\textrm{for at least one pair of } i, j\n\\end{align}\\]\n\nIt’s an extension of a two proportions test.\nThe test statistic & p-value are calculated the same was as a chi-squared test of association (independence)\nWhen we fix the margins (whether row or columns) of one of the “variables” (such as in a cohort or case-control study)\n\nthe chi-squared test is called a Test of Homogeneity"
  },
  {
    "objectID": "slides_code/Day13_bsta511_code.html#chi-squared-tests-of-independence-vs.-homogeneity-vs.-goodness-of-fit",
    "href": "slides_code/Day13_bsta511_code.html#chi-squared-tests-of-independence-vs.-homogeneity-vs.-goodness-of-fit",
    "title": "Day 13: Chi-squared tests (Sections 8.3-8.4)",
    "section": "Chi-squared Tests of Independence vs. Homogeneity vs. Goodness-of-fit",
    "text": "Chi-squared Tests of Independence vs. Homogeneity vs. Goodness-of-fit\n\nSee YouTube video from TileStats for a good explanation of how these three tests are different: https://www.youtube.com/watch?v=TyD-_1JUhxw\nUCLA’s INSPIRE website has a good summary too: http://inspire.stat.ucla.edu/unit_13/"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html",
    "href": "slides_code/Day14_bsta511_code.html",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) \nlibrary(here) \nlibrary(pwr) # new-ish\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#load-packages",
    "href": "slides_code/Day14_bsta511_code.html#load-packages",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) \nlibrary(here) \nlibrary(pwr) # new-ish\n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#hypotheses",
    "href": "slides_code/Day14_bsta511_code.html#hypotheses",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Hypotheses",
    "text": "Hypotheses\nTo test for a difference in means across k groups:\n\\[\\begin{align}\nH_0 &: \\mu_1 = \\mu_2 = ... = \\mu_k\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j \\text{ for } i \\neq j\n\\end{align}\\]\nHypothetical examples:\nIn which set (A or B) do you believe the evidence will be stronger that at least one population differs from the others?\nSee slides"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#comparing-means",
    "href": "slides_code/Day14_bsta511_code.html#comparing-means",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Comparing means",
    "text": "Comparing means\nWhether or not two means are significantly different depends on:\n\nHow far apart the means are\nHow much variability there is within each group\n\nQuestions:\n\nHow to measure variability between groups?\nHow to measure variability within groups?\nHow to compare the two measures of variability?\nHow to determine significance?"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#anova-in-base-r",
    "href": "slides_code/Day14_bsta511_code.html#anova-in-base-r",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA in base R",
    "text": "ANOVA in base R\n\nThere are several options to run an ANOVA model in R\nTwo most common are lm and aov\n\nlm = linear model; will be using frequently in BSTA 512\n\n\n\nlm(score ~ disability, data = employ) %&gt;% anova()\n\nAnalysis of Variance Table\n\nResponse: score\n           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ndisability  4  30.521  7.6304  2.8616 0.03013 *\nResiduals  65 173.321  2.6665                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\naov(score ~ disability, data = employ) %&gt;% summary()\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ndisability   4  30.52   7.630   2.862 0.0301 *\nResiduals   65 173.32   2.666                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHypotheses:\n\\[\\begin{align}\nH_0 &: \\mu_{none} = \\mu_{amputation} = \\mu_{crutches} = \\mu_{hearing} =  \\mu_{wheelchair}\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j \\text{ for } i \\neq j\n\\end{align}\\]\nDo we reject or fail to reject \\(H_0\\)?"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#anova-analysis-of-variance",
    "href": "slides_code/Day14_bsta511_code.html#anova-analysis-of-variance",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA: Analysis of Variance",
    "text": "ANOVA: Analysis of Variance\nAnalysis of Variance (ANOVA) compares the variability between groups to the variability within groups\n\\[\\sum_{i = 1}^k \\sum_{j = 1}^{n_i}(x_{ij} -\\bar{x})^2 \\ \\\n= \\ \\sum_{i = 1}^k n_i(\\bar{x}_{i}-\\bar{x})^2 \\ \\\n+ \\ \\ \\sum_{i = 1}^k\\sum_{j = 1}^{n_i}(x_{ij}-\\bar{x}_{i})^2\\]"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#notation",
    "href": "slides_code/Day14_bsta511_code.html#notation",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Notation",
    "text": "Notation\n\nk groups\n\\(n_i\\) observations in each of the k groups\nTotal sample size is \\(N=\\sum_{i=1}^{k}n_i\\)\n\\(\\bar{x}_{i}\\) = mean of observations in group i\n\\(\\bar{x}\\) = mean of all observations\n\\(s_{i}\\) = sd of observations in group i\n\\(s\\) = sd of all observations\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\ni = 1\ni = 2\ni = 3\n\\(\\ldots\\)\ni = k\noverall\n\n\n\n\nj = 1\n\\(x_{11}\\)\n\\(x_{21}\\)\n\\(x_{31}\\)\n\\(\\ldots\\)\n\\(x_{k1}\\)\n\n\n\nj = 2\n\\(x_{12}\\)\n\\(x_{22}\\)\n\\(x_{32}\\)\n\\(\\ldots\\)\n\\(x_{k2}\\)\n\n\n\nj = 3\n\\(x_{13}\\)\n\\(x_{23}\\)\n\\(x_{33}\\)\n\\(\\ldots\\)\n\\(x_{k3}\\)\n\n\n\nj = 4\n\\(x_{14}\\)\n\\(x_{24}\\)\n\\(x_{34}\\)\n\\(\\ldots\\)\n\\(x_{k4}\\)\n\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\n\n\nj = \\(n_i\\)\n\\(x_{1n_1}\\)\n\\(x_{2n_2}\\)\n\\(x_{3n_3}\\)\n\\(\\ldots\\)\n\\(x_{kn_k}\\)\n\n\n\nMeans\n\\(\\bar{x}_{1}\\)\n\\(\\bar{x}_{2}\\)\n\\(\\bar{x}_{3}\\)\n\\(\\ldots\\)\n\\(\\bar{x}_{k}\\)\n\\(\\bar{x}\\)\n\n\nVariance\n\\({s}^2_{1}\\)\n\\({s}^2_{2}\\)\n\\({s}^2_{3}\\)\n\\(\\ldots\\)\n\\({s}^2_{k}\\)\n\\({s}^2\\)"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#total-sums-of-squares-visually",
    "href": "slides_code/Day14_bsta511_code.html#total-sums-of-squares-visually",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Total Sums of Squares Visually",
    "text": "Total Sums of Squares Visually\n\nggplot(employ, aes(x = disability, y=score, \n      fill = disability, color = disability)) +\n  geom_dotplot(binaxis = \"y\", alpha =.9) +\n  geom_hline(aes(yintercept = mean(score)), \n             lty = \"dashed\") +\n  # stat_summary(fun = \"mean\", geom = \"point\", \n  #      size = 3, color = \"grey33\", alpha =1)  +\n  theme(legend.position = \"none\")\n\n\n\n\nTotal Sums of Squares:\n\\[SST = \\sum_{i = 1}^k \\sum_{j = 1}^{n_i}(x_{ij} -\\bar{x})^2 = (N-1)s^2\\]\n\nwhere\n\n\\(N=\\sum_{i=1}^{k}n_i\\) is the total sample size and\n\\(s^2\\) is the grand standard deviation of all the observations\n\nThis is the sum of the squared differences between each observed \\(x_{ij}\\) value and the grand mean, \\(\\bar{x}\\).\nThat is, it is the total deviation of the \\(x_{ij}\\)’s from the grand mean."
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#calculate-total-sums-of-squares",
    "href": "slides_code/Day14_bsta511_code.html#calculate-total-sums-of-squares",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Calculate Total Sums of Squares",
    "text": "Calculate Total Sums of Squares\nTotal Sums of Squares:\n\\[SST = \\sum_{i = 1}^k \\sum_{j = 1}^{n_i}(x_{ij} -\\bar{x})^2 = (N-1)s^2\\]\n\nwhere\n\n\\(N=\\sum_{i=1}^{k}n_i\\) is the total sample size and\n\\(s^2\\) is the grand standard deviation of all the observations\n\n\nTotal sample size \\(N\\):\n\n(Ns &lt;- employ %&gt;% group_by(disability) %&gt;% count())\n\n# A tibble: 5 × 2\n# Groups:   disability [5]\n  disability     n\n  &lt;fct&gt;      &lt;int&gt;\n1 none          14\n2 amputation    14\n3 crutches      14\n4 hearing       14\n5 wheelchair    14\n\n\n\\(SST\\):\n\n(SST &lt;- (sum(Ns$n) - 1) * sd(employ$score)^2)\n\n[1] 203.8429"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#sums-of-squares-due-to-groups-visually-between-groups",
    "href": "slides_code/Day14_bsta511_code.html#sums-of-squares-due-to-groups-visually-between-groups",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Sums of Squares due to Groups Visually (“between” groups)",
    "text": "Sums of Squares due to Groups Visually (“between” groups)\n\nggplot(employ, aes(x = disability, y=score, \n      fill = disability, color = disability)) +\n  geom_dotplot(binaxis = \"y\", alpha =.2) +\n  geom_hline(aes(yintercept = mean(score)), \n             lty = \"dashed\") +\n  stat_summary(fun = \"mean\", geom = \"point\", \n       size = 3, color = \"grey33\", alpha =1)  +\n  theme(legend.position = \"none\")\n\n\n\n\nSums of Squares due to Groups:\n\\[SSG = \\sum_{i = 1}^k n_i(\\bar{x}_{i}-\\bar{x})^2\\]\n\nThis is the sum of the squared differences between each group mean, \\(\\bar{x}_{i}\\), and the grand mean, \\(\\bar{x}\\).\nThat is, it is the deviation of the group means from the grand mean.\nAlso called the Model SS, or \\(SS_{model}.\\)"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#calculate-sums-of-squares-due-to-groups-between-groups",
    "href": "slides_code/Day14_bsta511_code.html#calculate-sums-of-squares-due-to-groups-between-groups",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Calculate Sums of Squares due to Groups (“between” groups)",
    "text": "Calculate Sums of Squares due to Groups (“between” groups)\n\\[SSG = \\sum_{i = 1}^k n_i(\\bar{x}_{i}-\\bar{x})^2\\]\n\nggplot(employ, aes(x = disability, y=score, \n      fill = disability, color = disability)) +\n  geom_dotplot(binaxis = \"y\", alpha =.2) +\n  geom_hline(aes(yintercept = mean(score)), \n             lty = \"dashed\") +\n  stat_summary(fun = \"mean\", geom = \"point\", \n       size = 3, color = \"grey33\", alpha =1)  +\n  theme(legend.position = \"none\")\n\n\n\n\nCalculate means \\(\\bar{x}_i\\) for each group:\n\nxbar_groups &lt;- employ %&gt;% \n  group_by(disability) %&gt;% \n  summarise(mean = mean(score))\nxbar_groups\n\n# A tibble: 5 × 2\n  disability  mean\n  &lt;fct&gt;      &lt;dbl&gt;\n1 none        4.9 \n2 amputation  4.43\n3 crutches    5.92\n4 hearing     4.05\n5 wheelchair  5.34\n\n\nCalculate \\(SSG\\):\n\n(SSG &lt;- sum(Ns$n *\n  (xbar_groups$mean - mean(employ$score))^2))\n\n[1] 30.52143"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#sums-of-squares-error-visually-within-groups",
    "href": "slides_code/Day14_bsta511_code.html#sums-of-squares-error-visually-within-groups",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Sums of Squares Error Visually (within groups)",
    "text": "Sums of Squares Error Visually (within groups)\n\nggplot(employ, aes(x = disability, y=score, \n      fill = disability, color = disability)) +\n  geom_dotplot(binaxis = \"y\", alpha =.5) +\n  # geom_hline(aes(yintercept = mean(score)), \n             # lty = \"dashed\") +\n  stat_summary(fun = \"mean\", geom = \"point\", \n       size = 3, color = \"grey33\", alpha =1)  +\n  theme(legend.position = \"none\")\n\n\n\n\nSums of Squares Error:\n\\[SSE = \\sum_{i = 1}^k\\sum_{j = 1}^{n_i}(x_{ij}-\\bar{x}_{i})^2 = \\sum_{i = 1}^k(n_i-1)s_{i}^2\\] where \\(s_{i}\\) is the standard deviation of the \\(i^{th}\\) group\n\nThis is the sum of the squared differences between each observed \\(x_{ij}\\) value and its group mean \\(\\bar{x}_{i}\\).\nThat is, it is the deviation of the \\(x_{ij}\\)’s from the predicted score by group.\nAlso called the residual sums of squares, or \\(SS_{residual}.\\)"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#calculate-sums-of-squares-error-within-groups",
    "href": "slides_code/Day14_bsta511_code.html#calculate-sums-of-squares-error-within-groups",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Calculate Sums of Squares Error (within groups)",
    "text": "Calculate Sums of Squares Error (within groups)\n\\[SSE = \\sum_{i = 1}^k\\sum_{j = 1}^{n_i}(x_{ij}-\\bar{x}_{i})^2 = \\sum_{i = 1}^k(n_i-1)s_{i}^2\\] where \\(s_{i}\\) is the standard deviation of the \\(i^{th}\\) group\n\nggplot(employ, aes(x = disability, y=score, \n      fill = disability, color = disability)) +\n  geom_dotplot(binaxis = \"y\", alpha =.5) +\n  # geom_hline(aes(yintercept = mean(score)), \n             # lty = \"dashed\") +\n  stat_summary(fun = \"mean\", geom = \"point\", \n       size = 3, color = \"grey33\", alpha =1)  +\n  theme(legend.position = \"none\")\n\n\n\n\nCalculate sd’s \\(s_i\\) for each group:\n\nsd_groups &lt;- employ %&gt;% \n  group_by(disability) %&gt;% \n  summarise(SD = sd(score))\nsd_groups\n\n# A tibble: 5 × 2\n  disability    SD\n  &lt;fct&gt;      &lt;dbl&gt;\n1 none        1.79\n2 amputation  1.59\n3 crutches    1.48\n4 hearing     1.53\n5 wheelchair  1.75\n\n\nCalculate \\(SSE\\):\n\n(SSE &lt;- sum(\n  (Ns$n-1)*sd_groups$SD^2))\n\n[1] 173.3214"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#verify-sst-ssg-sse",
    "href": "slides_code/Day14_bsta511_code.html#verify-sst-ssg-sse",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Verify SST = SSG + SSE",
    "text": "Verify SST = SSG + SSE\nANOVA compares the variability between groups to the variability within groups\n\\[\\sum_{i = 1}^k \\sum_{j = 1}^{n_i}(x_{ij} -\\bar{x})^2 \\ \\ = \\ \\ n_i\\sum_{i = 1}^k(\\bar{x}_{i}-\\bar{x})^2 \\ \\\n+ \\ \\ \\sum_{i = 1}^k\\sum_{j = 1}^{n_i}(x_{ij}-\\bar{x}_{i})^2\\]\n\\[(N-1)s^2 \\ \\\n= \\ \\sum_{i = 1}^k n_i(\\bar{x}_{i}-\\bar{x})^2 \\ \\\n+ \\ \\ \\sum_{i = 1}^k(n_i-1)s_{i}^2\\]\n\nSST\n\n[1] 203.8429\n\nSSG + SSE\n\n[1] 203.8429"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#thinking-about-the-f-statistic",
    "href": "slides_code/Day14_bsta511_code.html#thinking-about-the-f-statistic",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Thinking about the F-statistic",
    "text": "Thinking about the F-statistic\nIf the groups are actually different, then which of these is more accurate?\n\nThe variability between groups should be higher than the variability within groups\nThe variability within groups should be higher than the variability between groups\n\nIf there really is a difference between the groups, we would expect the F-statistic to be which of these:\n\nHigher than we would observe by random chance\nLower than we would observe by random chance\n\n\\[F = \\frac{MSG}{MSE}\\]"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#testing-variances-condition-3",
    "href": "slides_code/Day14_bsta511_code.html#testing-variances-condition-3",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Testing variances (Condition 3)",
    "text": "Testing variances (Condition 3)\nBartlett’s test for equal variances\n\n\\(H_0:\\) population variances of group levels are equal\n\\(H_A:\\) population variances of group levels are NOT equal\n\nNote: \\(H_A\\) is same as saying that at least one of the group levels has a different variance\n\n\n\n\n\n\nCaution\n\n\n\n\nBartlett’s test assumes the data in each group are normally distributed.\nDo not use if data do not satisfy the normality condition.\n\n\n\n\nbartlett.test(score ~ disability, data = employ)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  score by disability\nBartlett's K-squared = 0.7016, df = 4, p-value = 0.9511\n\n\n\n\n\n\n\n\nTip\n\n\n\nLevene’s test for equality of variances is not as restrictive: see https://www.statology.org/levenes-test-r/"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#post-hoc-testing-for-anova",
    "href": "slides_code/Day14_bsta511_code.html#post-hoc-testing-for-anova",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Post-hoc testing for ANOVA",
    "text": "Post-hoc testing for ANOVA\ndetermining which groups are statistically different"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#post-hoc-testing-pairwise-t-tests",
    "href": "slides_code/Day14_bsta511_code.html#post-hoc-testing-pairwise-t-tests",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Post-hoc testing: pairwise t-tests",
    "text": "Post-hoc testing: pairwise t-tests\n\nIn post-hoc testing we run all the pairwise t-tests comparing the means from each pair of groups.\nWith 5 groups, this involves doing \\({5 \\choose 2} = \\frac{5!}{2!3!} = \\frac{5\\cdot 4}{2}= 10\\) different pairwise tests.\n\nProblem:\n\nAlthough the ANOVA test has an \\(\\alpha\\) chance of a Type I error (finding a difference between a pair that aren’t different),\nthe overall Type I error rate will be much higher when running many tests simultaneously.\n\n\\[\\begin{align}\nP(\\text{making an error}) = & \\alpha\\\\\nP(\\text{not making an error}) = & 1-\\alpha\\\\\nP(\\text{not making an error in m tests}) = & (1-\\alpha)^m\\\\\nP(\\text{making at least 1 error in m tests}) = & 1-(1-\\alpha)^m\n\\end{align}\\]"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#the-bonferroni-correction-12",
    "href": "slides_code/Day14_bsta511_code.html#the-bonferroni-correction-12",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "The Bonferroni Correction (1/2)",
    "text": "The Bonferroni Correction (1/2)\nA very conservative (but very popular) approach is to divide the \\(\\alpha\\) level by how many tests \\(m\\) are being done:\n\\[\\alpha_{Bonf} = \\frac{\\alpha}{m}\\]\n\nThis is equivalent to multiplying the\np-values by m:\n\n\\[p\\textrm{-value} &lt; \\alpha_{Bonf} = \\frac{\\alpha}{m}\\] is the same as \\[m \\cdot (p\\textrm{-value}) &lt; \\alpha\\] The Bonferroni correction is popular since it’s very easy to implement.\n\nThe plot below shows the likelihood of making at least one Type I error depending on how may tests are done.\nNotice the likelihood decreases very quickly\n\nUnfortunately the likelihood of a Type II error is increasing as well\nIt becomes “harder” and harder to reject \\(H_0\\) if doing many tests."
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#the-bonferroni-correction-22",
    "href": "slides_code/Day14_bsta511_code.html#the-bonferroni-correction-22",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "The Bonferroni Correction (2/2)",
    "text": "The Bonferroni Correction (2/2)\nPairwise t-tests without any p-value adjustments:\n\npairwise.t.test(employ$score, \n                employ$disability, \n                p.adj=\"none\") \n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  employ$score and employ$disability \n\n           none   amputation crutches hearing\namputation 0.4477 -          -        -      \ncrutches   0.1028 0.0184     -        -      \nhearing    0.1732 0.5418     0.0035   -      \nwheelchair 0.4756 0.1433     0.3520   0.0401 \n\nP value adjustment method: none \n\n\nPairwise t-tests with Bonferroni p-value adjustments:\n\npairwise.t.test(employ$score,  \n                employ$disability, \n                p.adj=\"bonferroni\")  \n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  employ$score and employ$disability \n\n           none  amputation crutches hearing\namputation 1.000 -          -        -      \ncrutches   1.000 0.184      -        -      \nhearing    1.000 1.000      0.035    -      \nwheelchair 1.000 1.000      1.000    0.401  \n\nP value adjustment method: bonferroni \n\n\n\nSince there were 10 tests, all the p-values were multiplied by 10.\nAre there any significant pairwise differences?"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#tukeys-honest-significance-test-hsd",
    "href": "slides_code/Day14_bsta511_code.html#tukeys-honest-significance-test-hsd",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Tukey’s Honest Significance Test (HSD)",
    "text": "Tukey’s Honest Significance Test (HSD)\n\nTukey’s Honest Significance Test (HSD) controls the “family-wise probability” of making a Type I error using a much less conservative method than Bonferroni\n\nIt is specific to ANOVA\n\nIn addition to adjusted p-values, it also calculates Tukey adjusted CI’s for all pairwise differences\nThe function TukeyHSD() creates a set of confidence intervals of the differences between means with the specified family-wise probability of coverage.\n\n\n# need to run the model using `aov` instead of `lm`\nempl_aov &lt;- aov(score ~ disability, data = employ) \n\nTukeyHSD(x=empl_aov, conf.level = 0.95)  \n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = score ~ disability, data = employ)\n\n$disability\n                            diff        lwr        upr     p adj\namputation-none       -0.4714286 -2.2031613  1.2603042 0.9399911\ncrutches-none          1.0214286 -0.7103042  2.7531613 0.4686233\nhearing-none          -0.8500000 -2.5817328  0.8817328 0.6442517\nwheelchair-none        0.4428571 -1.2888756  2.1745899 0.9517374\ncrutches-amputation    1.4928571 -0.2388756  3.2245899 0.1232819\nhearing-amputation    -0.3785714 -2.1103042  1.3531613 0.9724743\nwheelchair-amputation  0.9142857 -0.8174470  2.6460185 0.5781165\nhearing-crutches      -1.8714286 -3.6031613 -0.1396958 0.0277842\nwheelchair-crutches   -0.5785714 -2.3103042  1.1531613 0.8812293\nwheelchair-hearing     1.2928571 -0.4388756  3.0245899 0.2348141\n\nplot(TukeyHSD(x=empl_aov, conf.level = 0.95))\n\n\n\n\n\nVisualization of pairwise CI’s\nWhich pair(s) of disabilities are significant after Tukey’s adjustments?"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#multiple-testing-controlling-the-type-i-error-rate",
    "href": "slides_code/Day14_bsta511_code.html#multiple-testing-controlling-the-type-i-error-rate",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "Multiple testing: controlling the Type I error rate",
    "text": "Multiple testing: controlling the Type I error rate\n\nThe multiple testing issue is not unique to ANOVA post-hoc testing.\nIt is also a concern when running separate tests for many related outcomes.\nBeware of p-hacking!\n\nProblem:\n\nAlthough one test has an \\(\\alpha\\) chance of a Type I error (finding a difference between a pair that aren’t different),\nthe overall Type I error rate will be much higher when running many tests simultaneously.\n\n\\[\\begin{align}\nP(\\text{making an error}) = & \\alpha\\\\\nP(\\text{not making an error}) = & 1-\\alpha\\\\\nP(\\text{not making an error in m tests}) = & (1-\\alpha)^m\\\\\nP(\\text{making at least 1 error in m tests}) = & 1-(1-\\alpha)^m\n\\end{align}\\]"
  },
  {
    "objectID": "slides_code/Day14_bsta511_code.html#anova-summary",
    "href": "slides_code/Day14_bsta511_code.html#anova-summary",
    "title": "Day 14: Comparing Means with ANOVA (Section 5.5)",
    "section": "ANOVA Summary",
    "text": "ANOVA Summary\n\\[\\begin{align}\nH_0 &: \\mu_1 = \\mu_2 = ... = \\mu_k\\\\\n\\text{vs. } H_A&: \\text{At least one pair } \\mu_i \\neq \\mu_j \\text{ for } i \\neq j\n\\end{align}\\]\n\nANOVA table in R:\n\n\nlm(score ~ disability, data = employ) %&gt;% anova()\n\nAnalysis of Variance Table\n\nResponse: score\n           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ndisability  4  30.521  7.6304  2.8616 0.03013 *\nResiduals  65 173.321  2.6665                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nANOVA table\nF-distribution & p-value\n\n\n\n\n\n\n\nPost-hoc testing"
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html",
    "href": "slides_code/Day15_bsta511_code.html",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) \nlibrary(here) \nlibrary(pwr) \n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#load-packages",
    "href": "slides_code/Day15_bsta511_code.html#load-packages",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "",
    "text": "Packages need to be loaded every time you restart R or render an Qmd file\n\n\n# run these every time you open Rstudio\nlibrary(tidyverse)    \nlibrary(oibiostat)\nlibrary(janitor)\nlibrary(rstatix)\nlibrary(knitr)\nlibrary(gtsummary)\nlibrary(moderndive)\nlibrary(gt)\nlibrary(broom) \nlibrary(here) \nlibrary(pwr) \n\n\nYou can check whether a package has been loaded or not\n\nby looking at the Packages tab and\nseeing whether it has been checked off or not"
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#dataset-description",
    "href": "slides_code/Day15_bsta511_code.html#dataset-description",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "Dataset description",
    "text": "Dataset description\n\nData file: lifeexp_femlit_water_2011.csv\nData were downloaded from https://www.gapminder.org/data/\n2011 is the most recent year with the most complete data\nLife expectancy = the average number of years a newborn child would live if current mortality patterns were to stay the same. Source: https://www.gapminder.org/data/documentation/gd004/\nAdult literacy rate is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life. Source: http://data.uis.unesco.org/\nAt least basic water source (%) = the percentage of people using at least basic water services. This indicator encompasses both people using basic water services as well as those using safely managed water services. Basic drinking water services is defined as drinking water from an improved source, provided collection time is not more than 30 minutes for a round trip. Improved water sources include piped water, boreholes or tubewells, protect dug wells, protected springs, and packaged or delivered water."
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#get-to-know-the-data",
    "href": "slides_code/Day15_bsta511_code.html#get-to-know-the-data",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "Get to know the data",
    "text": "Get to know the data\n\ngapm &lt;- read_csv(here::here(\"data\", \"lifeexp_femlit_water_2011.csv\"))\n\nglimpse(gapm)\n\nRows: 194\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andor…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 76.7, 82.6, 60.9, 76.9, 76.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, NA, NA, 58.6, 99.4, 97.9, 99.5,…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 92.6, 100.0, 40.3, 97.0, 99.5, …\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q2\", \"Q4\", \"Q1\", \"Q3\", \"Q4\", \"…\n\nsummary(gapm)\n\n   country          life_expectancy_years_2011 female_literacy_rate_2011\n Length:194         Min.   :47.50              Min.   :13.00            \n Class :character   1st Qu.:64.30              1st Qu.:70.97            \n Mode  :character   Median :72.70              Median :91.60            \n                    Mean   :70.66              Mean   :81.65            \n                    3rd Qu.:76.90              3rd Qu.:98.03            \n                    Max.   :82.90              Max.   :99.80            \n                    NA's   :7                  NA's   :114              \n water_basic_source_2011 water_2011_quart  \n Min.   : 18.30          Length:194        \n 1st Qu.: 74.90          Class :character  \n Median : 93.50          Mode  :character  \n Mean   : 84.84                            \n 3rd Qu.: 99.08                            \n Max.   :100.00"
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#association-between-life-expectancy-and-female-literacy-rate",
    "href": "slides_code/Day15_bsta511_code.html#association-between-life-expectancy-and-female-literacy-rate",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "Association between life expectancy and female literacy rate",
    "text": "Association between life expectancy and female literacy rate\n\nggplot(gapm, \n       aes(x = female_literacy_rate_2011,\n           y = life_expectancy_years_2011)) +\n  geom_point() +\n  labs(x = \"female literacy rate\", \n       y = \"life expectancy\",\n       title = \"Life expectancy vs. \n             female literacy rate in 2011\") +  \n  geom_smooth(method = \"lm\", \n              se = FALSE)\n\nWarning: Removed 114 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 114 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\nIs there a relationship between the two variables?\nIs it positive or negative?\nStrong, moderate, or weak?\nIs it linear?"
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#dependent-vs.-independent-variables",
    "href": "slides_code/Day15_bsta511_code.html#dependent-vs.-independent-variables",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "Dependent vs. independent variables",
    "text": "Dependent vs. independent variables\n\n\nWarning: Removed 114 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 114 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\\(y\\) = dependent variable (DV)\n\nalso called the outcome or response variable\n\n\\(x\\) = independent variable (IV)\n\nalso called the predictor variable\nor regressor in a regression analysis\n\nHow to determine which is which?"
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#correlation-between-life-expectancy-and-female-literacy-rate",
    "href": "slides_code/Day15_bsta511_code.html#correlation-between-life-expectancy-and-female-literacy-rate",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "Correlation between life expectancy and female literacy rate",
    "text": "Correlation between life expectancy and female literacy rate\n\nThe base R function for calculating the correlation coefficient is cor().\nThis can be used within summarize.\n\n\ngapm %&gt;% \n  summarize(correlation = cor(life_expectancy_years_2011, \n                              female_literacy_rate_2011,\n                              use =  \"complete.obs\"))\n\n# A tibble: 1 × 1\n  correlation\n        &lt;dbl&gt;\n1       0.641\n\n# base R:\ncor(gapm$life_expectancy_years_2011, \n    gapm$female_literacy_rate_2011,\n    use =  \"complete.obs\")\n\n[1] 0.6410434"
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#pearson-correlation-coefficient-r",
    "href": "slides_code/Day15_bsta511_code.html#pearson-correlation-coefficient-r",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "(Pearson) Correlation coefficient (r)",
    "text": "(Pearson) Correlation coefficient (r)\n\nA bivariate summary statistic since calculated using two variables\n-1 indicates a perfect negative linear relationship: As one variable increases, the value of the other variable tends to go down, following a straight line.\n0 indicates no linear relationship: The values of both variables go up/down independently of each other.\n1 indicates a__ perfect positive linear relationship__: As the value of one variable goes up, the value of the other variable tends to go up as well in a linear fashion.\n\n\\[r = \\frac{1}{n-1}\\sum_{i=1}^{n}\\Big(\\frac{x_i - \\bar{x}}{s_x}\\Big)\\Big(\\frac{y_i - \\bar{y}}{s_y}\\Big)\\]"
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#regression-line-best-fit-line",
    "href": "slides_code/Day15_bsta511_code.html#regression-line-best-fit-line",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "Regression line = best-fit line",
    "text": "Regression line = best-fit line\n\\[\\widehat{y} = b_0 + b_1 \\cdot x \\]\n\n\\(\\hat{y}\\) is the predicted outcome for a specific value of \\(x\\).\n\\(b_0\\) is the intercept\n\\(b_1\\) is the slope of the line, i.e., the increase in \\(\\hat{y}\\) for every increase of one (unit increase) in \\(x\\).\n\nslope = rise over run\n\n\n\n\nWarning: Removed 114 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 114 rows containing missing values (`geom_point()`).\n\n\n\n\n\n]\nInterpreting coefficients\n\nIntercept\n\nThe expected outcome for the \\(y\\)-variable when the \\(x\\)-variable is 0.\n\nSlope\n\nFor every increase of 1 unit in the \\(x\\)-variable, there is an expected increase of, on average, \\(b_1\\) units in the \\(y\\)-variable.\nWe only say that there is an expected increase and not necessarily a causal increase."
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#correlation-coefficient-vs.-slope",
    "href": "slides_code/Day15_bsta511_code.html#correlation-coefficient-vs.-slope",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "Correlation coefficient vs. slope",
    "text": "Correlation coefficient vs. slope\n\nNot the same!!!\nDirectly related though:\n\n\\[b_1 = r\\frac{s_y}{s_x}\\]\nwhere \\(s_x\\) and \\(s_y\\) are the standard deviations of the \\(x\\) and \\(y\\) variables.\nThe correlation coefficient can be computed using the formula below.\n\\[r = \\frac{1}{n-1}\\sum_{i=1}^{n}\\Big(\\frac{x_i - \\bar{x}}{s_x}\\Big)\\Big(\\frac{y_i - \\bar{y}}{s_y}\\Big)\\]\nWe will be using R to do these calculations."
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#how-is-the-best-fit-line-calculated-23",
    "href": "slides_code/Day15_bsta511_code.html#how-is-the-best-fit-line-calculated-23",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "How is the best-fit line calculated? (2/3)",
    "text": "How is the best-fit line calculated? (2/3)\n\nObserved values \\(y_i\\) are the values in the dataset\nFitted values \\(\\widehat{y}_i\\) are the values that fall on the best-fit line for a specific \\(x_i\\)\nResiduals \\(e_i = y_i - \\widehat{y}_i\\) are the differences between the two.\n\nThe best-line is minimizing “how far” the residuals are from the best-fit line * However, if we add all the residuals we get 0. ( \\(\\sum_{i=1}^n e_i = 0\\) ) * Thus instead, we add the squares of the residuals, which is always positive, and minimize the sums of the squares ( \\(\\sum_{i=1}^n e_i^2 \\geq 0\\) ) - which is why the best-fit line is often called the least-squares line * This is the same as minimizing the combined area of all the “residual squares” we were looking at in the applet."
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#regression-model-with-residuals",
    "href": "slides_code/Day15_bsta511_code.html#regression-model-with-residuals",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "Regression model with residuals",
    "text": "Regression model with residuals\n\nObserved values \\(y_i\\)\n\nthe values in the dataset\n\nFitted values \\(\\widehat{y}_i\\)\n\nthe values that fall on the best-fit line for a specific \\(x_i\\)\n\nResiduals \\(e_i = y_i - \\widehat{y}_i\\)\n\nthe differences between the observed and fitted values\n\n\n\n# code from https://drsimonj.svbtle.com/visualising-residuals\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)\nregression_points &lt;- augment(model1)\n\nggplot(regression_points, \n       aes(x = female_literacy_rate_2011,\n                 y = life_expectancy_years_2011)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +\n  geom_segment(aes(\n    xend = female_literacy_rate_2011, \n    yend = .fitted), \n    alpha = .2) +\n  # &gt; Color adjustments made here...\n  geom_point(aes(color = .resid), size = 4) +  # Color mapped here\n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +  # Colors to use here\n  guides(color = \"none\") +\n  geom_point(aes(y = .fitted), shape = 1, size = 3) +\n  labs(x = \"Female literacy rate\", \n       y = \"Average life expectancy\",\n       title = \"Regression line with residuals\") +\n  theme_bw() + \n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#how-is-the-best-fit-line-calculated-33",
    "href": "slides_code/Day15_bsta511_code.html#how-is-the-best-fit-line-calculated-33",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "How is the best-fit line calculated? (3/3)",
    "text": "How is the best-fit line calculated? (3/3)\nIn math-speak, we need to find the values \\(b_0\\) and \\(b_1\\) that minimize the equation:\n\\[\\sum_{i=1}^n e_i^2 =\n\\sum_{i=1}^n (y_i - \\hat{y_i})^2 =\n\\sum_{i=1}^n (y_i - b_0 -b_1 x_i)^2\\]\nwhere\n\n\\(y_i\\) are the values in the dataset\nwith \\(n\\) datapoints and\n\\(\\widehat{y_i}\\) are the fitted values that fall on the best-fit line for a specific value \\(x_i\\).\n\nHow do we find the minimum values of an equation???\nIf you want to see the mathematical details, check out pages 222-223 (pdf pages 10-11) of https://www.stat.cmu.edu/~hseltman/309/Book/chapter9.pdf.\n\\[b_0 = \\bar{y} - b_1\\bar{x}, \\ \\ \\ \\ \\ \\ \\ \\ \\\nb_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = r\\frac{s_y}{s_x}\\]"
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#tidy-model",
    "href": "slides_code/Day15_bsta511_code.html#tidy-model",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "tidy() model",
    "text": "tidy() model\n\ntidy(model1) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10"
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#regression-equation",
    "href": "slides_code/Day15_bsta511_code.html#regression-equation",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "Regression equation",
    "text": "Regression equation\nThe values in the estimate column are the intercept and slope of the regression model.\n\ntidy(model1) %&gt;% gt()\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\nGeneric regression equation:\n\\[\\widehat{y} = b_0 + b_1 \\cdot x \\]\nRegression equation for our model:\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot \\textrm{female literacy rate} \\]"
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#interpretation-of-coefficients",
    "href": "slides_code/Day15_bsta511_code.html#interpretation-of-coefficients",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "Interpretation of coefficients",
    "text": "Interpretation of coefficients\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot \\textrm{female literacy rate} \\]\nInterpretation of the intercept for this example. Is it meaningful?\n\nThe average life expectancy in 2011 for countries with no literate female adults was on average 50.9 years.\n\nInterpretation of the slope for this example.\n\nFor every 1 point increase in a country’s female literacy rate, we expect an average 0.232 year increase in the country’s average life expectancy."
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#the-regresison-model",
    "href": "slides_code/Day15_bsta511_code.html#the-regresison-model",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "The regresison model",
    "text": "The regresison model\n\nThe (population) regression model is denoted by\n\n\\[y = \\beta_0 + \\beta_1 \\cdot x + \\epsilon\\]\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown population parameters\n\\(\\epsilon\\) (epsilon) is the error about the line\n\nIt is assumed to be a random variable with a\n\nnormal distribution with\nmean 0 and\nconstant variance \\(\\sigma^2\\)\n\n\nThe line is the average (expected) value of \\(Y\\) given a value of \\(x\\): \\(E(Y|x)\\).\nThe point estimates based on a sample are denoted by \\(b_0, b_1, s_{residuals}^2\\)\n\nNote: also common notation is \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\sigma}^2\\)"
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#the-regresison-model-visually",
    "href": "slides_code/Day15_bsta511_code.html#the-regresison-model-visually",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "The regresison model visually",
    "text": "The regresison model visually\n\\[y = \\beta_0 + \\beta_1 \\cdot x + \\epsilon, \\text{ where } \\epsilon \\sim N(0, \\sigma^2)\\]\nSee slides for figure."
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#l-linearity-of-relationship-between-variables",
    "href": "slides_code/Day15_bsta511_code.html#l-linearity-of-relationship-between-variables",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "L: Linearity of relationship between variables",
    "text": "L: Linearity of relationship between variables\nIs the association between the variables linear?\n\nggplot(gapm, aes(x = female_literacy_rate_2011,\n                 y = life_expectancy_years_2011)) +\n  geom_point() +\n  labs(x = \"female literacy rate\", \n       y = \"life expectancy\",\n       title = \"Life expectancy vs. female literacy rate in 2011\") +  \n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_smooth(se = FALSE, color = \"black\")\n\nWarning: Removed 114 rows containing non-finite values (`stat_smooth()`).\nRemoved 114 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 114 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#i-independence-of-the-residuals",
    "href": "slides_code/Day15_bsta511_code.html#i-independence-of-the-residuals",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "I: Independence of the residuals",
    "text": "I: Independence of the residuals\n\nAre the data points independent of each other?\nExamples of when they are not independent, include\n\nrepeated measures (such as baseline, 3 months, 6 months)\ndata from clusters, such as different hospitals or families\n\nThis condition is checked by reviewing the study design and not by inspecting the data\nHow to analyze data using regression models when the \\(Y\\)-values are not independent is covered in BSTA 519 (Longitudinal data)"
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#n-normality-of-the-residuals",
    "href": "slides_code/Day15_bsta511_code.html#n-normality-of-the-residuals",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "N: Normality of the residuals",
    "text": "N: Normality of the residuals\n\nThe responses Y are normally distributed at each level of x\nNext time we will go over how to assess this\n\nSee slides for figure."
  },
  {
    "objectID": "slides_code/Day15_bsta511_code.html#e-equality-of-variance-of-the-residuals",
    "href": "slides_code/Day15_bsta511_code.html#e-equality-of-variance-of-the-residuals",
    "title": "Day 15: Simple Linear Regression (part 1) (Sections 6.1-6.2)",
    "section": "E: Equality of variance of the residuals",
    "text": "E: Equality of variance of the residuals\n\nThe variance or, equivalently, the standard deviation of the responses is equal for all values of x.\nThis is called homoskedasticity (top row)\nIf there is heteroskedasticity (bottom row), then the assumption is not met.\n\nSee slides for figure.\nWe will discuss how to assess this next time."
  },
  {
    "objectID": "slides/Day16_bsta511.html#where-are-we",
    "href": "slides/Day16_bsta511.html#where-are-we",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "homework/HW_5_F24_bsta511.html",
    "href": "homework/HW_5_F24_bsta511.html",
    "title": "HW 5: BSTA 511/611 F24",
    "section": "",
    "text": "Due 11/16/24\nDownload the .qmd file for this assignment from https://github.com/niederhausen/BSTA_511_F24/blob/main/homework/HW_5_F24_bsta511.qmd"
  },
  {
    "objectID": "homework/HW_5_F24_bsta511.html#graded-exercises",
    "href": "homework/HW_5_F24_bsta511.html#graded-exercises",
    "title": "HW 5: BSTA 511/611 F24",
    "section": "Graded exercises",
    "text": "Graded exercises\nThe exercises listed below will be graded for this assignment. You are strongly encouraged to complete the entire assignment. You will receive feedback on exercises you turn in that are not being graded.\n\nBook exercises\n\n5.26 (parts a-b), 8.8, 8.24\n\nPSS\n\nPSS2\n\nR exercises\n\nR1: DDS expenditures by ethnicity (parts a-l)"
  },
  {
    "objectID": "homework/HW_5_F24_bsta511.html#directions",
    "href": "homework/HW_5_F24_bsta511.html#directions",
    "title": "HW 5: BSTA 511/611 F24",
    "section": "Directions",
    "text": "Directions\n\n\n\n\n\n\nImportant\n\n\n\n\nComplete all exercises in this assignment using Quarto.\nI highly recommend using LaTeX to format equations.\n\nSee the .qmd files from class notes for LaTeX code to make it easier to show your work in computations.\nFor instructions on creating equations in the Visual editor, check out https://quarto.org/docs/get-started/authoring/rstudio.html#equations. html\nAlso check out examples of LaTeX formatting for statistics created by recent biostats alum Ariel Weingarten.\nIf you have difficulty rendering the LaTeX equations, I recommend installing and running the R package tinytex. See this website for instructions.\n\n\n\n\n\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file.\n\nUse the assignment .qmd file linked to above as a template for your own assignment.\n\nPlease always use the following naming convention for submitting your files:\n\nLastname_Firstname_HWx.qmd, such as Niederhausen_Meike_HW2.qmd\nLastname_Firstname_HWx.html, such as Niederhausen_Meike_HW2.html\n\nFor each question, make sure to show all of your work.\n\nThis includes all code and resulting output in the html file to support your answers for exercises requiring work done in R (including any arithmetic calculations).\nFor non-calculation questions, this includes an explanation of your answer (why did you choose your answer?).\n\nFor each question, include a sentence summarizing the answer for that question in the context of the research question.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your Qmd file and rendering frequently helps you catch your errors more quickly.\n\n\n\nHypothesis test & CI instructions\n\n\n\n\n\n\nImportant\n\n\n\n\nFor book exercises, make sure to include all steps in a hypothesis test (where applicable) as outlined in the class notes.\nDo not forget to include a discussion on whether you think the test (or CI) assumptions have been satisfied. Are there assumptions you need to make in order for them to be satisfied? Whether you believe they are satisfied or not, continue to run the hypothesis test (or CI) as instructed."
  },
  {
    "objectID": "homework/HW_5_F24_bsta511.html#egg-volume",
    "href": "homework/HW_5_F24_bsta511.html#egg-volume",
    "title": "HW 5: BSTA 511/611 F24",
    "section": "5.26 Egg volume",
    "text": "5.26 Egg volume"
  },
  {
    "objectID": "homework/HW_5_F24_bsta511.html#placebos-without-deception",
    "href": "homework/HW_5_F24_bsta511.html#placebos-without-deception",
    "title": "HW 5: BSTA 511/611 F24",
    "section": "5.34 Placebos without deception",
    "text": "5.34 Placebos without deception"
  },
  {
    "objectID": "homework/HW_5_F24_bsta511.html#young-americans-part-i",
    "href": "homework/HW_5_F24_bsta511.html#young-americans-part-i",
    "title": "HW 5: BSTA 511/611 F24",
    "section": "8.2 Young Americans, Part I",
    "text": "8.2 Young Americans, Part I"
  },
  {
    "objectID": "homework/HW_5_F24_bsta511.html#legalization-of-marijuana-part-i",
    "href": "homework/HW_5_F24_bsta511.html#legalization-of-marijuana-part-i",
    "title": "HW 5: BSTA 511/611 F24",
    "section": "8.8 Legalization of marijuana, Part I",
    "text": "8.8 Legalization of marijuana, Part I\nAdditional instructions:\n\n(b): Calculate the CI both using the formula and using the appropriate R statistical test.\nAdd parts (e) & (f) as instructed below.\n\n\n(a)\n\n\n(b)\n\n\n(c)\n\n\n(d)\n\n\n(e)\nTest whether the proportion of US residents who think marijuana should be made legal is different than 0.586. Do the test using both the formulas and running it in R (without a continuity correction).\n\n\n(f)\nAre the results from CI and hypothesis test consistent? Why or why not?"
  },
  {
    "objectID": "homework/HW_5_F24_bsta511.html#legalize-marijuana-part-ii",
    "href": "homework/HW_5_F24_bsta511.html#legalize-marijuana-part-ii",
    "title": "HW 5: BSTA 511/611 F24",
    "section": "8.10 Legalize Marijuana, Part II",
    "text": "8.10 Legalize Marijuana, Part II"
  },
  {
    "objectID": "homework/HW_5_F24_bsta511.html#healthcare-law",
    "href": "homework/HW_5_F24_bsta511.html#healthcare-law",
    "title": "HW 5: BSTA 511/611 F24",
    "section": "8.14 2010 Healthcare Law",
    "text": "8.14 2010 Healthcare Law"
  },
  {
    "objectID": "homework/HW_5_F24_bsta511.html#prenatal-vitamins-and-autism",
    "href": "homework/HW_5_F24_bsta511.html#prenatal-vitamins-and-autism",
    "title": "HW 5: BSTA 511/611 F24",
    "section": "8.24 Prenatal vitamins and Autism",
    "text": "8.24 Prenatal vitamins and Autism\nAdditional instructions:\n\n(b): Do the hypothesis test both using the formula and using the appropriate R statistical test.\nAdd part (d) as instructed below.\n\n\n(a)\n\n\n(b)\n\n\n(c)\n\n\n(d)\nCalculate and interpret the 95% confidence interval for the difference in proportions using the formula. Is it consistent with CI from the R output of the hypothesis test?"
  },
  {
    "objectID": "homework/HW_5_F24_bsta511.html#an-apple-a-day-keeps-the-doctor-away",
    "href": "homework/HW_5_F24_bsta511.html#an-apple-a-day-keeps-the-doctor-away",
    "title": "HW 5: BSTA 511/611 F24",
    "section": "8.26 An apple a day keeps the doctor away",
    "text": "8.26 An apple a day keeps the doctor away"
  },
  {
    "objectID": "homework/HW_5_F24_bsta511.html#pss1-4.22-testing-for-food-safety.",
    "href": "homework/HW_5_F24_bsta511.html#pss1-4.22-testing-for-food-safety.",
    "title": "HW 5: BSTA 511/611 F24",
    "section": "PSS1: 4.22 Testing for food safety.",
    "text": "PSS1: 4.22 Testing for food safety.\nDo exercise 4.22 from textbook."
  },
  {
    "objectID": "homework/HW_5_F24_bsta511.html#pss2-auto-exhaust-and-lead-exposure-revisited.",
    "href": "homework/HW_5_F24_bsta511.html#pss2-auto-exhaust-and-lead-exposure-revisited.",
    "title": "HW 5: BSTA 511/611 F24",
    "section": "PSS2: Auto exhaust and lead exposure revisited.",
    "text": "PSS2: Auto exhaust and lead exposure revisited.\n\n(a) Power\nIn exercise 5.12, we tested whether police officers appear to have been exposed to a higher concentration of lead than 35. Calculate the power for the hypothesis test and include an interpretation of the power in the context of the research question. Was it sufficiently powered?\n\n\n(b) Sample size\nFor the same test, what sample size would be needed for 80% power? How about 90% power? Would it be reasonable to conduct the study with these sample sizes? Why or why not?\n\n\n(c) Effect size\nSuppose the study has resources to include 30 people. What minimum effect size would they be able to detect with 85% power assuming the same sample mean and standard deviation. Use \\(\\alpha\\) = 0.05.\n\n\n(d) 2-sided vs. 1-sided\nContinuing with the previous question, what happens to the effect size they can detect if the test is two sided instead of one-sided?"
  },
  {
    "objectID": "homework/HW_5_F24_bsta511.html#pss3-legalize-marijuana-part-iii.",
    "href": "homework/HW_5_F24_bsta511.html#pss3-legalize-marijuana-part-iii.",
    "title": "HW 5: BSTA 511/611 F24",
    "section": "PSS3: Legalize Marijuana, Part III.",
    "text": "PSS3: Legalize Marijuana, Part III.\n\n(a) Power\nIn exercise 8.8 (e), we tested whether the proportion of US residents who think marijuana should be made legal is different than 0.586. Calculate the power for the hypothesis test and include an interpretation of the power in the context of the research question. Was it sufficiently powered?\n\n\n(b) Sample size\nFor the same test, what sample size would be needed for 80% power? How about 90% power?\n\n\n(c) Effect size\nIf we increase the sample size, and keep the power and significance level the same, does the effect size increase or decrease? Why?"
  },
  {
    "objectID": "homework/HW_5_F24_bsta511.html#load-packages",
    "href": "homework/HW_5_F24_bsta511.html#load-packages",
    "title": "HW 5: BSTA 511/611 F24",
    "section": "Load packages",
    "text": "Load packages\nLoad all the packages you need in the first code chunk of the file that starts with #| label: \"setup\"."
  },
  {
    "objectID": "homework/HW_5_F24_bsta511.html#r1-dds-expenditures-by-ethnicity",
    "href": "homework/HW_5_F24_bsta511.html#r1-dds-expenditures-by-ethnicity",
    "title": "HW 5: BSTA 511/611 F24",
    "section": "R1: DDS expenditures by ethnicity",
    "text": "R1: DDS expenditures by ethnicity\n\nIn these exercises you will use R to work through the discrimination in developmental disability support example from Section 5.3.4 (pg. 253) in the textbook.\nThe data are in the oibiostats package and called dds.discr.\n\n\n(a) New dataset\nCreate a new dataset that only includes the White (non Hispanic) and Hispanic ethnicities. Use this new dataset for the following questions.\n\n\n(b) Data viz\nCreate density plots and box plots of the expenditures stratified by ethnicity. Comment on the distribution shapes. Are there any outliers?\n\n\n(c) t-test conditions\nAre the conditions for a t-test comparing the mean expenditures of the two ethnicities satisfied?\n\n\n(d) Log-transformation\nThe book recommends log-transforming the expenditure values before testing. Create a new column in the dataset with the transformed values. The R command for the natural logarithm is log(). \n\n\n(e) Data viz: log-transformed expenditures\nCreate density plots and box plots of the log-transformed expenditures stratified by ethnicity. Comment on the distribution shapes. Are there any outliers?\n\n\n(f) t-test conditions: log-transformed expenditures\nAre the conditions for a t-test comparing the mean log-transformed expenditures of the two ethnicities satisfied?\n\n\n(g) Summary stats: log-transformed expenditures\nCalculate the means, standard deviations, and sample sizes for the log-transformed expenditures stratified by ethnicity, and compare them to the ones in the book. Which group had a larger mean?\n\n\n(h) Test\nRun the appropriate statistical test in R to verify the test statistic in the text and get the actual p-value. In which order was the difference in means calculated, and is this same as in the book? Use inline R code to pull these values from the test output when writing up your comparison of these values to the book’s values.\n\n\n(i) df\nHow do the degrees of freedom (df) from the hypothesis test compare to the df used by the book? Why are they different? Which degrees df (book vs. test output) leads to a bigger p-value?\n\n\n(j) CI\nWhat is the 95% CI? Write an interpretation of the CI in the context of the research question. \n\n\n(k) Test original expenditure values\nRun the appropriate statistical test in R using the original expenditure values. What are the test statistic and p-value? Does the conclusion of the test change?\n\n\n(l) CI using original expenditure values\nWhat is the 95% CI? Write an interpretation of the CI in the context of the research question. Which of the CI’s (log-transformed vs not) is easier to interpret?\n\n\n(m) Age groups\nThe book’s example goes on to analyze the data stratified by age groups, since age is a confounder in expenditure amounts. Create two new datasets restricted to the age groups 13-17 and 22-50, respectively.\n\n\n(n) Data viz by age groups\nCreate density plots and box plots of the expenditures stratified by ethnicity for each of the age groups separately. Comment on the distribution shapes. Are there any outliers?\n\n\n(o) t-test conditions: age groups\nAre the assumptions for a t-test comparing the mean expenditures of the two ethnicities satisfied for either or both of the age groups?\n\n\n(p) Summary stats: age groups\nCalculate the means, standard deviations, and sample sizes for the expenditures stratified by ethnicity and the age groups, and compare them to the ones in the book. Which group had a larger mean?\n\n\n(q) t-test: age groups\nRun the appropriate statistical tests for both age groups in R to verify the test statistics, df’s, and p-values in the text. In which order were the differences in means calculated, and are they the same as in the book? Use inline R code to pull these values from the test output when writing up your comparison of these values to the book’s values.\n\n\n(r) CI: age groups\nWhat are the 95% CI’s for each of the age groups? Write interpretations of the CI’s in the context of the research question. Does they suggest there are differences in expenditures between the two ethnicities? Why or why not?\n\n\n(s) Discrimination in DDS expenditures?\nEven though the p-values for the age-stratified tests were not significant, is it possible that there was discrimination in DDS expenditures?"
  },
  {
    "objectID": "weeks/week_08.html",
    "href": "weeks/week_08.html",
    "title": "Week 8",
    "section": "",
    "text": "Chi-squared tests of association and homogeneity\nFisher’s exact test\nSections 8.3-8.4\n\n\n\n\n\nANOVA = Analysis of Variance\nComparing more than 2 means\nSection 5.5"
  },
  {
    "objectID": "weeks/week_08.html#overview-of-week-8",
    "href": "weeks/week_08.html#overview-of-week-8",
    "title": "Week 8",
    "section": "",
    "text": "Chi-squared tests of association and homogeneity\nFisher’s exact test\nSections 8.3-8.4\n\n\n\n\n\nANOVA = Analysis of Variance\nComparing more than 2 means\nSection 5.5"
  },
  {
    "objectID": "weeks/week_08.html#slides-recordings",
    "href": "weeks/week_08.html#slides-recordings",
    "title": "Week 8",
    "section": "Slides & Recordings",
    "text": "Slides & Recordings\n\nPre-recorded lessons are on Echo Cloud (aka echo 360 or echo video).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nTopic\nSlides: html\nSlides: pdf\nSlides: web-page\nSlides with notes\nRecording Link\nDuration\nCode: qmd\nCode: html\n\n\n\n\n13\nChi-sqr intro\nDay 13: slides 1-10\n\n\n\nDay 13 Part 1\n18 min\n\n\n\n\n\nChi-sqr test: 8.3.1 - 8.3.4\nDay 13: slides 11-17\nsame\n\nsame\nDay 13 Part 2\n23 min\nsame\n\n\n\n\nChi-sqr in R, Fisher’s Exact Test, Chi-sqr vs. prop’s\nDay 13: R tip, slides 18-33\nsame\n\nsame\nDay 13 Part 3\n57 min\nsame\n\n\n\n14\nANOVA intro\nDay 14: slides 1-15\n\n\n\nDay 14 Part 1\n29 min\n\n\n\n\n\nANOVA table\nDay 14: slides 16-28\nsame\n\nsame\nDay 14 Part 2\n27 min\nsame\n\n\n\n\nANOVA in R, conditions, post-hoc testing\nDay 14: slides 29-47\nsame\n\nsame\nDay 14 Part 3\n39 min\nsame"
  },
  {
    "objectID": "weeks/week_08.html#datasets",
    "href": "weeks/week_08.html#datasets",
    "title": "Week 8",
    "section": "Datasets",
    "text": "Datasets\nAll datasets can be found on GitHub\n\nDay 14: DisabilityEmployment.txt"
  },
  {
    "objectID": "weeks/week_08.html#class-discussion",
    "href": "weeks/week_08.html#class-discussion",
    "title": "Week 8",
    "section": "Class discussion",
    "text": "Class discussion\nDuring class you will be working in groups discussing the following:\n\nDay 13\n\nQuestions from HW 6\n\nBook: 8.28 (warm-up), 8.32, 8.34, 8.38 (modified; see assignment)\n\n\n\n\nDay 14\n\nSlide 12: “Hypothetical examples” question\nSlide 29 questions\nQuestions from HW 6\n\nBook 5.44, 5.48\nR1: Palmer Penguins ANOVA"
  },
  {
    "objectID": "weeks/week_08.html#homework",
    "href": "weeks/week_08.html#homework",
    "title": "Week 8",
    "section": "Homework",
    "text": "Homework\n\nHW 6 covers Days 13 -14 and is due on Sat, 11/23\nCheck out this spreadsheet for typos in book."
  },
  {
    "objectID": "homework/HW_6_F24_bsta511.html",
    "href": "homework/HW_6_F24_bsta511.html",
    "title": "HW 6: BSTA 511/611 F24",
    "section": "",
    "text": "Due 11/23/24\nDownload the .qmd file for this assignment from https://github.com/niederhausen/BSTA_511_F24/blob/main/homework/HW_6_F24_bsta511.qmd"
  },
  {
    "objectID": "homework/HW_6_F24_bsta511.html#graded-exercises",
    "href": "homework/HW_6_F24_bsta511.html#graded-exercises",
    "title": "HW 6: BSTA 511/611 F24",
    "section": "Graded exercises",
    "text": "Graded exercises\nThe exercises listed below will be graded for this assignment. You are strongly encouraged to complete the entire assignment. You will receive feedback on exercises you turn in that are not being graded.\n\nBook exercises\n\n8.32, 8.34, 8.38 (modified; see below), 5.44\n\nR exercises\n\nR1: Palmer Penguins ANOVA"
  },
  {
    "objectID": "homework/HW_6_F24_bsta511.html#directions",
    "href": "homework/HW_6_F24_bsta511.html#directions",
    "title": "HW 6: BSTA 511/611 F24",
    "section": "Directions",
    "text": "Directions\n\n\n\n\n\n\nImportant\n\n\n\n\nComplete all exercises in this assignment using Quarto.\nI highly recommend using LaTeX to format equations.\n\nSee the .qmd files from class notes for LaTeX code to make it easier to show your work in computations.\nFor instructions on creating equations in the Visual editor, check out https://quarto.org/docs/get-started/authoring/rstudio.html#equations. html\nAlso check out examples of LaTeX formatting for statistics created by recent biostats alum Ariel Weingarten.\nIf you have difficulty rendering the LaTeX equations, I recommend installing and running the R package tinytex. See this website for instructions.\n\n\n\n\n\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file.\n\nUse the assignment .qmd file linked to above as a template for your own assignment.\n\nPlease always use the following naming convention for submitting your files:\n\nLastname_Firstname_HWx.qmd, such as Niederhausen_Meike_HW2.qmd\nLastname_Firstname_HWx.html, such as Niederhausen_Meike_HW2.html\n\nFor each question, make sure to show all of your work.\n\nThis includes all code and resulting output in the html file to support your answers for exercises requiring work done in R (including any arithmetic calculations).\nFor non-calculation questions, this includes an explanation of your answer (why did you choose your answer?).\n\nFor each question, include a sentence summarizing the answer for that question in the context of the research question.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your Qmd file and rendering frequently helps you catch your errors more quickly.\n\n\n\nHypothesis test instructions\n\n\n\n\n\n\nImportant\n\n\n\n\nFor book exercises, make sure to include all steps in a hypothesis test (where applicable) as outlined in the class notes.\nDo not forget to include a discussion on whether you think the test (or CI) assumptions have been satisfied. Are there assumptions you need to make in order for them to be satisfied? Whether you believe they are satisfied or not, continue to run the hypothesis test (or CI) as instructed."
  },
  {
    "objectID": "homework/HW_6_F24_bsta511.html#true-or-false-part-ii",
    "href": "homework/HW_6_F24_bsta511.html#true-or-false-part-ii",
    "title": "HW 6: BSTA 511/611 F24",
    "section": "8.28 True or false, Part II",
    "text": "8.28 True or false, Part II"
  },
  {
    "objectID": "homework/HW_6_F24_bsta511.html#diabetes-and-unemployment",
    "href": "homework/HW_6_F24_bsta511.html#diabetes-and-unemployment",
    "title": "HW 6: BSTA 511/611 F24",
    "section": "8.32 Diabetes and unemployment",
    "text": "8.32 Diabetes and unemployment\n\n(a)\n\n\n(b)\n\n\n(c)\nIn addition to answering the question in (c), run the 2 proportions test in R and calculated the “z” test statistic and p-value using the normal distribution based on the \\(\\chi^2\\) output in R’s test.\n\n\n(d) extra part\nRun the test as a \\(\\chi^2\\) as well, and compare your results to those in (c)."
  },
  {
    "objectID": "homework/HW_6_F24_bsta511.html#coffee-and-depression",
    "href": "homework/HW_6_F24_bsta511.html#coffee-and-depression",
    "title": "HW 6: BSTA 511/611 F24",
    "section": "8.34 Coffee and Depression",
    "text": "8.34 Coffee and Depression\n\nDo not create a dataset and run the test in R. Use just the information given in the problem.\nFor (e), calculate the p-value “directly” (not using \\(\\chi^2\\) test command in R). Also comment on whether you think the sample size condition would be met without computing the expected cell counts."
  },
  {
    "objectID": "homework/HW_6_F24_bsta511.html#a-extra-salt-intake-and-cvd",
    "href": "homework/HW_6_F24_bsta511.html#a-extra-salt-intake-and-cvd",
    "title": "HW 6: BSTA 511/611 F24",
    "section": "8.38 (a) & (extra) Salt intake and CVD",
    "text": "8.38 (a) & (extra) Salt intake and CVD\nDo not do parts (b)-(c) in the book\n\n(a)\n\nYou can use the expected cell counts from R’s chi-squared test (you do not need to compute them using the formula).\nComment on whether the sample size condition is met or not for these data.\n\n\n\n(extra)\nRun a Fisher’s Exact test. Include the hypotheses and a conclusion in the context of the problem."
  },
  {
    "objectID": "homework/HW_6_F24_bsta511.html#work-hours-and-education",
    "href": "homework/HW_6_F24_bsta511.html#work-hours-and-education",
    "title": "HW 6: BSTA 511/611 F24",
    "section": "5.44 Work hours and education",
    "text": "5.44 Work hours and education"
  },
  {
    "objectID": "homework/HW_6_F24_bsta511.html#child-care-hours",
    "href": "homework/HW_6_F24_bsta511.html#child-care-hours",
    "title": "HW 6: BSTA 511/611 F24",
    "section": "5.46 Child care hours",
    "text": "5.46 Child care hours"
  },
  {
    "objectID": "homework/HW_6_F24_bsta511.html#truefalse-anova-part-ii",
    "href": "homework/HW_6_F24_bsta511.html#truefalse-anova-part-ii",
    "title": "HW 6: BSTA 511/611 F24",
    "section": "5.48 True/False: ANOVA, Part II",
    "text": "5.48 True/False: ANOVA, Part II"
  },
  {
    "objectID": "homework/HW_6_F24_bsta511.html#load-packages",
    "href": "homework/HW_6_F24_bsta511.html#load-packages",
    "title": "HW 6: BSTA 511/611 F24",
    "section": "Load packages",
    "text": "Load packages\nLoad all the packages you need in the first code chunk of the file that starts with #| label: \"setup\"."
  },
  {
    "objectID": "homework/HW_6_F24_bsta511.html#r1-palmer-penguins-anova",
    "href": "homework/HW_6_F24_bsta511.html#r1-palmer-penguins-anova",
    "title": "HW 6: BSTA 511/611 F24",
    "section": "R1: Palmer Penguins ANOVA",
    "text": "R1: Palmer Penguins ANOVA\n\nUse the penguins data from the palmerpenguins package.\n\nDon’t forget to first install the palmerpenguins package\n\nYou can learn more about the Palmer penguins data at https://allisonhorst.github.io/palmerpenguins/\nWe will test whether there are differences in penguins’ mean bill depths when comparing different species.\n\n\nlibrary(palmerpenguins)\ndata(penguins)\n\n\n(a) Dotplots\nMake a dotplot of the penguins’ bill depths stratified by species type. Include points for the mean of each species type as well as a horizontal dashed line for the overall mean. See example from class for the plot I’m describing.\n\n\n(b) Which groups significantly different?\nBased on the figure, which pairs of species look like they have significantly different mean bill depths?\n\n\n(c) Hypotheses in words\nWrite out in words the null and alternative hypotheses.\n\n\n(d) Hypotheses in symbols\nWrite out in symbols the null and alternative hypotheses.\n\n\n(e) Run ANOVA in R\nUsing R, run the hypothesis test and display the output.\n\n\n(f) SST\nUsing the values from the ANOVA table, calculate the value of the SST (total sum of squares).\n\n\n(g) MSG & MSE\nUsing the values from the ANOVA table, verify (calculate) the values of the MSG (mean square groups) and MSE (mean square error).\n\n\n(h) F statistic\nUsing the values from the ANOVA table, verify (calculate) the value of the F statistic.\n\n\n(i) p-value\nUsing the values from the ANOVA table, verify (calculate) the p-value.\n\n\n(j) Decision?\nBased on the p-value, will we reject or fail to reject the null hypothesis? Why?\n\n\n(k) Conclusion\nWrite a conclusion to the hypothesis test in the context of the problem.\n\n\n(l) Technical conditions\nInvestigate whether the technical conditions for using an ANOVA been satisfied.\n\n\n(m) Post-hoc pairwise t-tests: no correction\nRun post-hoc pairwise t-tests using NO p-value correction. Which pairs of species have significantly different bill depths?\n\n\n(n) Post-hoc pairwise t-tests: Bonferroni correction\nRun post-hoc pairwise t-tests using a Bonferroni correction. Which pairs of species have significantly different bill depths?\n\n\n(o) Hypothetical Bonferroni correction\nIf hypothetically the p-value comparing the mean bill depths of the Adelie and Chinstrap species were 0.03 without any p-value adjustment, what would the p-value be after running the post-hoc pairwise t-tests using a Bonferroni correction?\n\n\n(p) Post-hoc pairwise t-tests: Tukey’s Honest Significance Test correction\nRun post-hoc pairwise t-tests using Tukey’s Honest Significance Test correction. Which pairs of species have significantly different bill depths?\n\n\n(q) Tukey confidence intervals\nMake a plot showing the 95% family-wise Tukey confidence intervals. How does this plot visually confirm the which pairs of species have significantly different bill depths?"
  },
  {
    "objectID": "weeks/week_07.html#sec-Exam2info",
    "href": "weeks/week_07.html#sec-Exam2info",
    "title": "Week 7",
    "section": "Exam 2 information",
    "text": "Exam 2 information\n\nExam 2 will be on Mon, Oct. 30th\nSamples of past exam questions and answers (on Sakai)\nMaterial will cover Days 8-13,\n\nwhich is approximately Ch 4, 5.1-5.4, 8.1-8.4 from the textbook, and supplementary material.\n\nThe exam will be in-class and handwritten. Bring a calculator.\nYou may bring one page of notes on an 8.5” x 11” sheet of paper.\n\nYou may use both sides.\nYou may type your notes.\nYou may not use screenshots of the notes or textbook\nYou will be turning in the page of notes with the exam, so please add your name to the sheet."
  },
  {
    "objectID": "weeks/week_08.html#sec-Exam2info",
    "href": "weeks/week_08.html#sec-Exam2info",
    "title": "Week 8",
    "section": "Exam 2 information",
    "text": "Exam 2 information\n\nExam 2 will be on Mon, Oct. 30th\nSamples of past exam questions and answers (on Sakai)\nMaterial will cover Days 8-13,\n\nwhich is approximately Ch 4, 5.1-5.4, 8.1-8.4 from the textbook, and supplementary material.\n\nThe exam will be in-class and handwritten. Bring a calculator.\nYou may bring one page of notes on an 8.5” x 11” sheet of paper.\n\nYou may use both sides.\nYou may type your notes.\nYou may not use screenshots of the notes or textbook\nYou will be turning in the page of notes with the exam, so please add your name to the sheet."
  },
  {
    "objectID": "weeks/week_09.html",
    "href": "weeks/week_09.html",
    "title": "Week 9",
    "section": "",
    "text": "See the Exam 2 information section below for more information\n\n\n\n\n\nSimple Linear Regression (part 1)\nSections 6.1, 6.2"
  },
  {
    "objectID": "weeks/week_09.html#overview-of-week-8",
    "href": "weeks/week_09.html#overview-of-week-8",
    "title": "Week 8",
    "section": "",
    "text": "Chi-squared tests of association and homogeneity\nFisher’s exact test\nSections 8.3-8.4\n\n\n\n\n\nANOVA = Analysis of Variance\nComparing more than 2 means\nSection 5.5"
  },
  {
    "objectID": "weeks/week_09.html#slides-recordings",
    "href": "weeks/week_09.html#slides-recordings",
    "title": "Week 9",
    "section": "Slides & Recordings",
    "text": "Slides & Recordings\n\nPre-recorded lessons are on Echo Cloud (aka echo 360 or echo video).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nTopic\nSlides: html\nSlides: pdf\nSlides: web-page\nSlides with notes\nRecording Link\nDuration\nCode: qmd\nCode: html\n\n\n\n\n15\nSimple linear regression: 6.1-6.2\n\n\n\n\nDay 15\n46 min"
  },
  {
    "objectID": "weeks/week_09.html#datasets",
    "href": "weeks/week_09.html#datasets",
    "title": "Week 9",
    "section": "Datasets",
    "text": "Datasets\nAll datasets can be found on GitHub\n\nDay 15: lifeexp_femlit_water_2011.csv"
  },
  {
    "objectID": "weeks/week_09.html#class-discussion",
    "href": "weeks/week_09.html#class-discussion",
    "title": "Week 9",
    "section": "Class discussion",
    "text": "Class discussion\nDuring class you will be working in groups discussing the following:\n\nDay 15\n\nBook exercises\n\nWarm-up: 6.2, 6.6\n6.10\n\nR1: Palmer Penguins SLR"
  },
  {
    "objectID": "weeks/week_09.html#homework",
    "href": "weeks/week_09.html#homework",
    "title": "Week 9",
    "section": "Homework",
    "text": "Homework\n\nHW 7 covers Days 15 -17 and is due on Sat, 12/7\nCheck out this spreadsheet for typos in book."
  },
  {
    "objectID": "weeks/week_09.html#sec-Exam2info",
    "href": "weeks/week_09.html#sec-Exam2info",
    "title": "Week 9",
    "section": "Exam 2 information",
    "text": "Exam 2 information\n\nExam 2 will be on Mon, Nov. 25th\nSamples of past exam questions and answers (on Sakai)\nMaterial will cover Days 8-13,\n\nwhich is approximately Ch 4, 5.1-5.4, 8.1-8.4 from the textbook, and supplementary material.\n\nThe exam will be in-class and handwritten. Bring a calculator.\nYou may bring one page of notes on an 8.5” x 11” sheet of paper.\n\nYou may use both sides.\nYou may type your notes.\nYou may not use screenshots of the notes or textbook\nYou will be turning in the page of notes with the exam, so please add your name to the sheet."
  },
  {
    "objectID": "weeks/week_09.html#overview-of-week-9",
    "href": "weeks/week_09.html#overview-of-week-9",
    "title": "Week 9",
    "section": "",
    "text": "See the Exam 2 information section below for more information\n\n\n\n\n\nSimple Linear Regression (part 1)\nSections 6.1, 6.2"
  },
  {
    "objectID": "homework/HW_7_F24_bsta511.html",
    "href": "homework/HW_7_F24_bsta511.html",
    "title": "HW 7: BSTA 511/611 F24",
    "section": "",
    "text": "Due 12/7/24\nDownload the .qmd file for this assignment from https://github.com/niederhausen/BSTA_511_F24/blob/main/homework/HW_7_F24_bsta511.qmd"
  },
  {
    "objectID": "homework/HW_7_F24_bsta511.html#graded-exercises",
    "href": "homework/HW_7_F24_bsta511.html#graded-exercises",
    "title": "HW 7: BSTA 511/611 F24",
    "section": "Graded exercises",
    "text": "Graded exercises\nThe exercises listed below will be graded for this assignment. You are strongly encouraged to complete the entire assignment. You will receive feedback on exercises you turn in that are not being graded.\n\nBook exercises\n\n6.10, 6.28, 6.32\n\nR exercises\n\nR1: Palmer Penguins SLR\nR2: Life expectancy vs. CO2 emissions parts (a)-(j)\n\nNonparametric exercises\n\nNPT 1, NPT 2, NPT 3"
  },
  {
    "objectID": "homework/HW_7_F24_bsta511.html#directions",
    "href": "homework/HW_7_F24_bsta511.html#directions",
    "title": "HW 7: BSTA 511/611 F24",
    "section": "Directions",
    "text": "Directions\n\n\n\n\n\n\nImportant\n\n\n\n\nComplete all exercises in this assignment using Quarto.\nI highly recommend using LaTeX to format equations.\n\nSee the .qmd files from class notes for LaTeX code to make it easier to show your work in computations.\nFor instructions on creating equations in the Visual editor, check out https://quarto.org/docs/get-started/authoring/rstudio.html#equations. html\nAlso check out examples of LaTeX formatting for statistics created by recent biostats alum Ariel Weingarten.\nIf you have difficulty rendering the LaTeX equations, I recommend installing and running the R package tinytex. See this website for instructions.\n\n\n\n\n\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file.\n\nUse the assignment .qmd file linked to above as a template for your own assignment.\n\nPlease always use the following naming convention for submitting your files:\n\nLastname_Firstname_HWx.qmd, such as Niederhausen_Meike_HW2.qmd\nLastname_Firstname_HWx.html, such as Niederhausen_Meike_HW2.html\n\nFor each question, make sure to show all of your work.\n\nThis includes all code and resulting output in the html file to support your answers for exercises requiring work done in R (including any arithmetic calculations).\nFor non-calculation questions, this includes an explanation of your answer (why did you choose your answer?).\n\nFor each question, include a sentence summarizing the answer for that question in the context of the research question.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your Qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW_7_F24_bsta511.html#identify-relationships-part-ii",
    "href": "homework/HW_7_F24_bsta511.html#identify-relationships-part-ii",
    "title": "HW 7: BSTA 511/611 F24",
    "section": "6.2 Identify relationships, Part II",
    "text": "6.2 Identify relationships, Part II"
  },
  {
    "objectID": "homework/HW_7_F24_bsta511.html#over-under-part-ii",
    "href": "homework/HW_7_F24_bsta511.html#over-under-part-ii",
    "title": "HW 7: BSTA 511/611 F24",
    "section": "6.6 Over-under, Part II",
    "text": "6.6 Over-under, Part II"
  },
  {
    "objectID": "homework/HW_7_F24_bsta511.html#guppies-part-i",
    "href": "homework/HW_7_F24_bsta511.html#guppies-part-i",
    "title": "HW 7: BSTA 511/611 F24",
    "section": "6.10 Guppies, Part I",
    "text": "6.10 Guppies, Part I"
  },
  {
    "objectID": "homework/HW_7_F24_bsta511.html#trends-in-the-residuals",
    "href": "homework/HW_7_F24_bsta511.html#trends-in-the-residuals",
    "title": "HW 7: BSTA 511/611 F24",
    "section": "6.12 Trends in the residuals",
    "text": "6.12 Trends in the residuals"
  },
  {
    "objectID": "homework/HW_7_F24_bsta511.html#guppies-part-iv",
    "href": "homework/HW_7_F24_bsta511.html#guppies-part-iv",
    "title": "HW 7: BSTA 511/611 F24",
    "section": "6.20 Guppies, Part IV",
    "text": "6.20 Guppies, Part IV"
  },
  {
    "objectID": "homework/HW_7_F24_bsta511.html#a-e-helmets-and-lunches",
    "href": "homework/HW_7_F24_bsta511.html#a-e-helmets-and-lunches",
    "title": "HW 7: BSTA 511/611 F24",
    "section": "6.26 (a, e) Helmets and lunches",
    "text": "6.26 (a, e) Helmets and lunches\nSkip parts (b)-(d). To complete (e), use that the slope from part (b) is −0.537, and the intercept is 55.34.\nNote: if you have time, it would be good practice to calculate the regression line as well. This was covered on the previous assignment."
  },
  {
    "objectID": "homework/HW_7_F24_bsta511.html#guppies-part-v",
    "href": "homework/HW_7_F24_bsta511.html#guppies-part-v",
    "title": "HW 7: BSTA 511/611 F24",
    "section": "6.28 Guppies, Part V",
    "text": "6.28 Guppies, Part V"
  },
  {
    "objectID": "homework/HW_7_F24_bsta511.html#a-b-guppies-part-vi",
    "href": "homework/HW_7_F24_bsta511.html#a-b-guppies-part-vi",
    "title": "HW 7: BSTA 511/611 F24",
    "section": "6.32 (a, b) Guppies, Part VI",
    "text": "6.32 (a, b) Guppies, Part VI\nSkip part (c)."
  },
  {
    "objectID": "homework/HW_7_F24_bsta511.html#load-packages",
    "href": "homework/HW_7_F24_bsta511.html#load-packages",
    "title": "HW 7: BSTA 511/611 F24",
    "section": "Load packages",
    "text": "Load packages\nLoad all the packages you need in the first code chunk of the file that starts with #| label: \"setup\"."
  },
  {
    "objectID": "homework/HW_7_F24_bsta511.html#r1-palmer-penguins-slr",
    "href": "homework/HW_7_F24_bsta511.html#r1-palmer-penguins-slr",
    "title": "HW 7: BSTA 511/611 F24",
    "section": "R1: Palmer Penguins SLR",
    "text": "R1: Palmer Penguins SLR\n\n\n\n\n\n\nImportant\n\n\n\nBelow I frequently use the terminology variable1 vs. variable2. When we write this, the first variable is \\(y\\) (vertical axis) and the second is \\(x\\) (horizontal axis). Thus it’s always \\(y\\) vs. \\(x\\) (NOT \\(x\\) vs. \\(y\\)).\n\n\n\n(a) Scatterplots\n\nFor each of the following pairs of variables, make a scatterplot showing the best fit line and describe the relationship between the variables.\nIn particular address\n\nwhether the association is linear,\nhow strong it is (based purely on the plot), and\nwhat direction (positive, negative, or neither).\n\n\n\nbody mass vs. flipper length\nbill depth vs. flipper length\nbill depth vs. bill length\n\n\n\n(b) Correlations\n\nFor each of the following pairs of variables, find the correlation coefficient \\(r\\).\n\n\nbody mass vs. flipper length\nbill depth vs. flipper length\nbill depth vs. bill length\n\n\n\n(c) Compare associations\nWhich pair of variables has the strongest association? Which has the weakest? Explain how you determined this.\n\n\n(d) Body mass vs. flipper length SLR\nRun the simple linear regression model for body mass vs. flipper length, and display the regression table output.\n\n\n(e) Regression equation\nWrite out the regression equation for this model, using the variable names instead of the generic \\(x\\) and \\(y\\), and inserting the regression coefficient values.\n\n\n(f) \\(b_1\\) calculation\nVery that the formula \\(b_1 = r \\cdot \\frac{s_y}{s_x}\\) holds for this example using the values of the statistics.\n\n\n(g) Interpret intercept\nWrite a sentence interpreting the intercept for this example. Is it meaningful in this example?\n\n\n(h) Interpret slope\nWrite a sentence interpreting the slope for this example.\n\n\n(i) Prediction\nWhat is the expected body mass of a penguin with flipper length 210 mm based on the model?"
  },
  {
    "objectID": "homework/HW_7_F24_bsta511.html#r2-life-expectancy-vs.-co2-emissions",
    "href": "homework/HW_7_F24_bsta511.html#r2-life-expectancy-vs.-co2-emissions",
    "title": "HW 7: BSTA 511/611 F24",
    "section": "R2: Life expectancy vs. CO2 emissions",
    "text": "R2: Life expectancy vs. CO2 emissions\nUse the dataset Gapminder_2011_LifeExp_CO2.csv\nData were downloaded from https://www.gapminder.org/data/.\n\nLife expectancy = the average number of years a newborn child would live if current mortality patterns were to stay the same. Source: https://www.gminder.org/data/documentation/gd004/\nCO2 emissions (tons per person) = Carbon dioxide emissions from the burning of fossil fuels (metric tons of CO2 per person). Source: https://cdiac.ess-dive.lbl.gov/\n2011 is the most recent year with the most complete data\n\n\n(a) Load data\nLoad the dataset Gapminder_2011_LifeExp_CO2.csv and do a quick inspection of it. What are the dimensions? Variable names?\n\n\n(b) Linear association?\nMake a scatterplot of life expectancy vs. CO2 emissions per person showing the best fit line and describe the relationship between the variables. In addition comment on whether the relationship looks linear or not.\n\n\n(c) SLR\nRun the simple linear regression of life expectancy vs.CO2 emissions per person, and write out the corresponding regression equation. \n\n\n(d) Prediction\nUsing the regression equation, what is the expected life expectancy for a country with \\(CO_2\\) emissions 20 metric tons per person?\n\n\n(e) Independent data points?\nExplain whether you think the data point are independent of each other or not.\n\n\n(f) Normality of residuals?\nMake a histogram, density plot, and boxplot of the model’s residuals. What is the distribution shape of the residuals? What shape do we want them to have?\n\n\n(g) QQ plot\nMake a QQ plot of the model’s residuals.Explain whether or not the distribution of the residuals deviates from normality and how you made that conclusion based on the QQ plot.\n\n\n(h) Random Normal QQ plots\nMake 5 QQ plots with points randomly generated from a normal distribution, where the number of points matches the sample size used in the linear model.\n\n\n(i) QQ plot comparison\nCompare the QQ plot of the model’s residuals to the randomly generated QQ plots. Is the QQ plot of the residuals similar to the randomly generated plots or different? Based on the these QQ plots, do you think it’s possible that the residuals could be normally distributed?\n\n\n(j) Equality of variance of the residuals?\nMake a residual plot. Describe what the plot looks like and whether there are any patterns in the residuals, and whether the equality of variance assumption the residuals seems to be satisfied or not.\n\n\n(k) Transformation: log(x)\nAdd a new variable to the dataset for the natural logarithm (log()) of the CO2 emissions per person values.\n\n\n(l) Linear association (with transformation)?\nMake a scatterplot of life expectancy vs. log of CO2 emissions per person showing the best fit line and describe the relationship between the variables. In addition comment on whether the relationship looks linear or not.\n\n\n(m) SLR (with transformation)\nRun the simple linear regression of life expectancy vs.CO2 emissions per person, and write out the corresponding regression equation.\n\n\n(n) Prediction (with transformation)\nUsing the regression equation, what is the expected life expectancy for a country with \\(CO_2\\) emissions 20 metric tons per person?\n\n\n(o) Compare predictions from without and with transformation\nCompare the predicted values from the models with and without the transformation. Which is bigger and why? Explain in terms of visually comparing the respective regression lines on the scatterplots.\n\n\n(p) Normality of residuals (with transformation)?\nMake a histogram, density plot, and boxplot of the model’s residuals. What is the distribution shape of the residuals? What shape do we want them to have?\n\n\n(q) QQ plot (with transformation)\nMake a QQ plot of the model’s residuals.Explain whether or not the distribution of the residuals deviates from normality and how you made that conclusion based on the QQ plot.\n\n\n(r) Random Normal QQ plots (with transformation)\nCompare the QQ plot of the model’s residuals to the randomly generated QQ plots (use the ones you generated above). Is the QQ plot of the residuals similar to the randomly generated plots or different? Based on the these QQ plots, do you think it’s possible that the residuals could be normally distributed?\n\n\n(s) Equality of variance of the residuals (with transformation)?\nMake a residual plot. Describe what the plot looks like and whether there are any patterns in the residuals, and whether the equality of variance assumption the residuals seems to be satisfied or not.\n\n\n(t) Comparison of models without and with transformation\nWhich of the models do you think has a better fit? Make sure your explanation comments on each of the LINE assumptions, and also compare the \\(R^2\\) values from the models."
  },
  {
    "objectID": "homework/HW_7_F24_bsta511.html#npt-1-sign-test",
    "href": "homework/HW_7_F24_bsta511.html#npt-1-sign-test",
    "title": "HW 7: BSTA 511/611 F24",
    "section": "NPT 1: Sign test",
    "text": "NPT 1: Sign test\nVegetarian diet and cholesterol levels\nWhen covering paired t-tests on Day 10 Part 2, the class notes used the example of testing whether a vegetarian diet changed cholesterol levels. The data are in the file chol213.csv at https://niederhausen.github.io/BSTA_511_F23/data/chol213.csv. In this exercise we will use non-parametric tests to test for a change and compare the results to the paired t-test.\n\n(a) Hypotheses\nWhat are the hypotheses for the (Wilcoxon) Signed-rank test (2-sided) in the context of the problem?\n\n\n(b) \\(D^+\\) and \\(D^-\\)\nCalculate \\(D^+\\) and \\(D^-\\), the number of positive and negative differences when the differences are calculated as After - Before.\n\n\n(c) Probability distribution\nWhat probability distribution can be used to model the number of positive differences? Make sure to specify its parameters.\n\n\n(d) Exact probability\nFind the exact probability that there were at most 3 positive differences.\n\n\n(e) Sign test in R\nRun the sign test in R. What is the sign test p-value and how does it compare to the p-value of the paired t-test (check the class notes for this)? \n\n\n(f) S\nThe sign test output includes the value for S. What is S and what does it measure?\n\n\n(g) p-value\nDoes the probability that there were at most 3 positive differences match the p-value from the R output? Why or why not?\n\n\n(h) Normal approximation\nWould it be appropriate to use a normal approximation to calculate the p-value for this test? Why or why not?"
  },
  {
    "objectID": "homework/HW_7_F24_bsta511.html#npt-2-wilcoxon-signed-rank-test",
    "href": "homework/HW_7_F24_bsta511.html#npt-2-wilcoxon-signed-rank-test",
    "title": "HW 7: BSTA 511/611 F24",
    "section": "NPT 2: (Wilcoxon) Signed-rank test",
    "text": "NPT 2: (Wilcoxon) Signed-rank test\nVegetarian diet and cholesterol levels\nWhen covering paired t-tests on Day 10 Part 2, the class notes used the example of testing whether a vegetarian diet changed cholesterol levels. The data are in the file chol213.csv at https://niederhausen.github.io/BSTA_511_F23/data/chol213.csv. In this exercise we will use non-parametric tests to test for a change and compare the results to the paired t-test.\n\n(a) Hypotheses\nWhat are the hypotheses for the sign test (2-sided) in the context of the problem?\n\n\n(b) Signed ranks\nFind the signed ranks. Make sure to account for ties when doing so.\n\n\n(c) \\(T^+\\)\nCalculate the sum of the positive ranks ( \\(T^+\\) ) \n\n\n(d) Exact p-value\nCan an exact p-value for the (Wilcoxon) Signed-rank test be calculated? Why or why not?\n\n\n(e) ormal approximation\nIs it appropriate to use the normal approximation method in this case?\n\n\n(f) Test in R\nRun the (Wilcoxon) Signed-rank test in R. What is the p-value and how does it compare to the p-value of the sign test and the paired t-test (check the class notes for this)?\n\n\n(g) Condition\nThere’s one more condition that should be satisfied to use the (Wilcoxon) Signed-rank test that has not been asked about yet in these questions. What is it and do you think it’s satisfied?"
  },
  {
    "objectID": "homework/HW_7_F24_bsta511.html#npt-3-wilcoxon-rank-sum-test",
    "href": "homework/HW_7_F24_bsta511.html#npt-3-wilcoxon-rank-sum-test",
    "title": "HW 7: BSTA 511/611 F24",
    "section": "NPT 3: Wilcoxon rank-sum test",
    "text": "NPT 3: Wilcoxon rank-sum test\nDoes caffeine increase finger taps/min?\nWhen covering 2-sample t-tests on Day 11, the class notes used the example of testing whether caffeine increases finger taps/min. The data are in the file CaffeineTaps.csv at https://niederhausen.github.io/BSTA_511_F23/data/CaffeineTaps.csv. In this exercise we will use a non-parametric test and compare the results to the paired t-test.\n\n(a) Condition\nWhat condition needs to be satisfied to apply the Wilcoxon rank-sum test and is it satisfied for these data?\nAnswer the following questions using the Wilcoxon rank-sum test whether you think the condition has been satisfied or not.\n\n\n(b) Why Wilcoxon rank-sum test?\nHow would we know to use the Wilcoxon rank-sum test instead of the sign test or (Wilcoxon) Signed-rank test?\n\n\n(c) Hypotheses\nWhat are the hypotheses for the Wilcoxon rank-sum test (1-sided) in the context of the problem? \n\n\n(d) Exact test in R\nRun the exact Wilcoxon rank-sum test in R. What warning(s) does it give you?\n\n\n(e) Normal approximation test in R\nRun the Wilcoxon rank-sum test in R with the normal approximation. Comment on whether it is appropriate to use the normal approximation or not in this case.\n\n\n(f) p-value\nWhat is the p-value and how does it compare to the p-value of the 2-sample t-test (check the class notes for this)?\n\n\n(g) Conclusion\nWrite a conclusion to the test in the context of the problem."
  },
  {
    "objectID": "weeks/week_10.html",
    "href": "weeks/week_10.html",
    "title": "Week 10",
    "section": "",
    "text": "Simple Linear Regression Part 2\nSections 6.3-6.4\n\n\n\n\n\nNonparametric tests\nThis material is not in our textbook. Please see Chapter 13: Nonparametric tests of Pagano’s Principles of Biostatistics, 2022 edition as a supplementary resource.\n\nThis chapter can be downloaded from the OHSU library’s eBook at https://ebookcentral.proquest.com/lib/ohsu/detail.action?docID=6950388&pq-origsite=primo"
  },
  {
    "objectID": "weeks/week_10.html#overview-of-week-10",
    "href": "weeks/week_10.html#overview-of-week-10",
    "title": "Week 10",
    "section": "",
    "text": "Simple Linear Regression Part 2\nSections 6.3-6.4\n\n\n\n\n\nNonparametric tests\nThis material is not in our textbook. Please see Chapter 13: Nonparametric tests of Pagano’s Principles of Biostatistics, 2022 edition as a supplementary resource.\n\nThis chapter can be downloaded from the OHSU library’s eBook at https://ebookcentral.proquest.com/lib/ohsu/detail.action?docID=6950388&pq-origsite=primo"
  },
  {
    "objectID": "weeks/week_10.html#slides-recordings",
    "href": "weeks/week_10.html#slides-recordings",
    "title": "Week 10",
    "section": "Slides & Recordings",
    "text": "Slides & Recordings\n\nPre-recorded lessons are on Echo Cloud (aka echo 360 or echo video).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nTopic\nSlides: html\nSlides: pdf\nSlides: web-page\nSlides with notes\nRecording Link\nDuration\nCode: qmd\nCode: html\n\n\n\n\n16\nSimple Linear Regression cont’d: 6.3-6.4\nDay 16\n\n\n\nDay 16 *\n1 hr 26 min\n\n\n\n\n17\nSign test & Sign-rank test: Slides 1-31\n\n\n\n\nDay 17 Part 1\n56 min\n\n\n\n\n\nWilcoxon rank-sum test, Kruskal-Wallis test: Slides 32-53\n\nsame\n\nsame\nDay 17 Part 2\n41 min\nsame\n\n\n\n\n*Edited live recording from F23 class"
  },
  {
    "objectID": "weeks/week_10.html#datasets",
    "href": "weeks/week_10.html#datasets",
    "title": "Week 10",
    "section": "Datasets",
    "text": "Datasets\nAll datasets can be found on GitHub\n\nDay 16: lifeexp_femlit_water_2011.csv"
  },
  {
    "objectID": "weeks/week_10.html#class-discussion",
    "href": "weeks/week_10.html#class-discussion",
    "title": "Week 10",
    "section": "Class discussion",
    "text": "Class discussion\nDuring class you will be working in groups discussing the following:\n\nDay 16\n\nQuestions from HW 7\n\nBook\n\nWarm-up: 6.12\n6.28, 6.32\n\nR2 (a) - (j): Life expectancy vs. CO2 emissions\n\n\n\n\nDay 17\n\nQuestions from HW 7\n\nNonparametric Tests: NPT 1-3"
  },
  {
    "objectID": "weeks/week_10.html#homework",
    "href": "weeks/week_10.html#homework",
    "title": "Week 10",
    "section": "Homework",
    "text": "Homework\n\nHW 7 covers Days 15 -17 and is due on Sat, 12/7\nCheck out this spreadsheet for typos in book."
  },
  {
    "objectID": "weeks/week_11.html",
    "href": "weeks/week_11.html",
    "title": "Week 11",
    "section": "",
    "text": "Open office hours during class\n\nIn-person: VPT 627 Note different room!\nOnline: Webex (see link below)\n\n\n\n\n\n\n\nOpen office hours during class\n\nOnline: Webex (see link below)"
  },
  {
    "objectID": "weeks/week_11.html#overview-of-week-11",
    "href": "weeks/week_11.html#overview-of-week-11",
    "title": "Week 11",
    "section": "",
    "text": "Open office hours during class\n\nIn-person: VPT 627 Note different room!\nOnline: Webex (see link below)\n\n\n\n\n\n\n\nOpen office hours during class\n\nOnline: Webex (see link below)"
  },
  {
    "objectID": "weeks/week_11.html#webex-link-for-week-11",
    "href": "weeks/week_11.html#webex-link-for-week-11",
    "title": "Week 11",
    "section": "Webex link for Week 11",
    "text": "Webex link for Week 11\nWhen it’s time, join your Webex meeting here.\nJoin Meeting: https://ohsu.webex.com/ohsu/j.php?MTID=m38226d405dacf95ecc49b56b04f38e20\nMeeting password: WCkwHmUy564\nPhone (audio only):\n1-503-388-9555 Portland, OR\n1-206-207-1700 Seattle, WA\nMeeting number (access code): 2865 488 1852\nAdditional call-in numbers:\nhttps://ohsu.webex.com/ohsu/globalcallin.php\nMobile phone (one-touch):\n206-207-1700,,28654881852##"
  },
  {
    "objectID": "weeks/week_11.html#exam-3---coming-soon",
    "href": "weeks/week_11.html#exam-3---coming-soon",
    "title": "Week 11",
    "section": "Exam 3 - coming soon!",
    "text": "Exam 3 - coming soon!\nI will notify you on Slack and via email when Exam 3 information is posted."
  }
]